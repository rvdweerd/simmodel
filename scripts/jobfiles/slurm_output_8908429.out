batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: Dual
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: q
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
node_dim: 7
lstm_on: True
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy(
  (FE): FeatureExtractor(
    (gat): GATv2(7, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (lstm): LSTM(24, 24)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (lstm): LSTM(24, 24)
    (theta6_v): Linear(in_features=24, out_features=24, bias=True)
    (theta7_v): Linear(in_features=24, out_features=24, bias=True)
    (theta5_v): Linear(in_features=48, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with A+C LSTMs and GATv2 feature extraction
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.lstm.weight_ih_l0     [96, 24]     requires_grad=True
PI.lstm.weight_hh_l0     [96, 24]     requires_grad=True
PI.lstm.bias_ih_l0       [96]         requires_grad=True
PI.lstm.bias_hh_l0       [96]         requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.lstm.weight_ih_l0      [96, 24]     requires_grad=True
V.lstm.weight_hh_l0      [96, 24]     requires_grad=True
V.lstm.bias_ih_l0        [96]         requires_grad=True
V.lstm.bias_hh_l0        [96]         requires_grad=True
V.theta6_v.weight        [24, 24]     requires_grad=True
V.theta6_v.bias          [24]         requires_grad=True
V.theta7_v.weight        [24, 24]     requires_grad=True
V.theta7_v.bias          [24]         requires_grad=True
V.theta5_v.weight        [1, 48]      requires_grad=True
V.theta5_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 17522
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -1.941860465116279, Mean Entropy: 0.9963990449905396, complete_episode_count: 43.0, Gather time: 5.46s, Train time: 3.46s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -4.675, Mean Entropy: 0.99639892578125, complete_episode_count: 40.0, Gather time: 0.59s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -1.7553191489361701, Mean Entropy: 0.9963991045951843, complete_episode_count: 47.0, Gather time: 1.04s, Train time: 1.59s
Iteration: 3,  Mean reward: -4.864864864864865, Mean Entropy: 0.8953151702880859, complete_episode_count: 37.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 4,  Mean reward: -4.841463414634147, Mean Entropy: 0.9747382998466492, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 5,  Mean reward: -4.590909090909091, Mean Entropy: 0.931416392326355, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 6,  Mean reward: -6.453488372093023, Mean Entropy: 0.8953146934509277, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 7,  Mean reward: -1.5465116279069768, Mean Entropy: 0.938636302947998, complete_episode_count: 43.0, Gather time: 0.62s, Train time: 1.60s
Iteration: 8,  Mean reward: -5.226190476190476, Mean Entropy: 0.9747371673583984, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 9,  Mean reward: -2.880434782608696, Mean Entropy: 0.9747357368469238, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 10,  Mean reward: -3.0952380952380953, Mean Entropy: 0.9891754984855652, complete_episode_count: 42.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 11,  Mean reward: -4.5625, Mean Entropy: 0.9169692993164062, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 12,  Mean reward: -4.2073170731707314, Mean Entropy: 0.945836067199707, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 13,  Mean reward: -5.523809523809524, Mean Entropy: 0.9386036396026611, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 14,  Mean reward: -2.697674418604651, Mean Entropy: 0.902476966381073, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.68s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 15,  Mean reward: -1.5116279069767442, Mean Entropy: 0.9673843383789062, complete_episode_count: 43.0, Gather time: 0.60s, Train time: 1.63s
Iteration: 16,  Mean reward: -3.883720930232558, Mean Entropy: 1.02491295337677, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 17,  Mean reward: -4.678571428571429, Mean Entropy: 0.8875774145126343, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 18,  Mean reward: -1.7045454545454546, Mean Entropy: 0.9306957721710205, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 19,  Mean reward: -2.272727272727273, Mean Entropy: 0.9602926969528198, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 20,  Mean reward: -2.1707317073170733, Mean Entropy: 0.9673859477043152, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 21,  Mean reward: -2.5, Mean Entropy: 0.9360933303833008, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 22,  Mean reward: -4.121951219512195, Mean Entropy: 0.8915534019470215, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 23,  Mean reward: -2.1931818181818183, Mean Entropy: 0.9566532969474792, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.86s
Iteration: 24,  Mean reward: -5.522222222222222, Mean Entropy: 0.9486198425292969, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 25,  Mean reward: -6.232558139534884, Mean Entropy: 0.8932980895042419, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 26,  Mean reward: -2.546511627906977, Mean Entropy: 0.919575572013855, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 27,  Mean reward: -4.215909090909091, Mean Entropy: 0.9858149290084839, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 28,  Mean reward: -3.372093023255814, Mean Entropy: 0.9648302793502808, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 29,  Mean reward: -3.55, Mean Entropy: 0.9517638087272644, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 30,  Mean reward: -4.6, Mean Entropy: 0.858304500579834, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 31,  Mean reward: -3.5, Mean Entropy: 0.9419420957565308, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 32,  Mean reward: -3.7976190476190474, Mean Entropy: 0.910326361656189, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 33,  Mean reward: -1.8333333333333333, Mean Entropy: 0.8760113716125488, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.65s
Iteration: 34,  Mean reward: -4.574468085106383, Mean Entropy: 0.8916956782341003, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.64s
Iteration: 35,  Mean reward: -5.976190476190476, Mean Entropy: 0.8835250735282898, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.64s
Iteration: 36,  Mean reward: -2.0833333333333335, Mean Entropy: 0.7951306104660034, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 37,  Mean reward: -3.0816326530612246, Mean Entropy: 0.8762876391410828, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 38,  Mean reward: -3.25, Mean Entropy: 0.8707930445671082, complete_episode_count: 48.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 39,  Mean reward: -2.3333333333333335, Mean Entropy: 0.7119311690330505, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 40,  Mean reward: -3.62, Mean Entropy: 0.8157486915588379, complete_episode_count: 50.0, Gather time: 0.59s, Train time: 1.62s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 41,  Mean reward: -0.010638297872340425, Mean Entropy: 0.6854279041290283, complete_episode_count: 47.0, Gather time: 0.60s, Train time: 1.62s
Iteration: 42,  Mean reward: -1.7980769230769231, Mean Entropy: 0.7422467470169067, complete_episode_count: 52.0, Gather time: 0.60s, Train time: 1.62s
Iteration: 43,  Mean reward: -2.3181818181818183, Mean Entropy: 0.871334969997406, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.61s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 44,  Mean reward: 0.7358490566037735, Mean Entropy: 0.7904679179191589, complete_episode_count: 53.0, Gather time: 2.00s, Train time: 1.62s
Iteration: 45,  Mean reward: -0.8673469387755102, Mean Entropy: 0.7459184527397156, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 46,  Mean reward: -1.7735849056603774, Mean Entropy: 0.7761564254760742, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 47,  Mean reward: -3.36, Mean Entropy: 0.8212399482727051, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.61s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 48,  Mean reward: 1.4895833333333333, Mean Entropy: 0.7750428915023804, complete_episode_count: 48.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 49,  Mean reward: 0.6132075471698113, Mean Entropy: 0.8466038703918457, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 50,  Mean reward: 1.2346938775510203, Mean Entropy: 0.8508801460266113, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 51,  Mean reward: 1.27, Mean Entropy: 0.794024646282196, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 52,  Mean reward: 1.2058823529411764, Mean Entropy: 0.8103771209716797, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 53,  Mean reward: 1.3877551020408163, Mean Entropy: 0.8359813690185547, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.62s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 54,  Mean reward: 1.8796296296296295, Mean Entropy: 0.7857128381729126, complete_episode_count: 54.0, Gather time: 0.61s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 55,  Mean reward: 3.701923076923077, Mean Entropy: 0.9350967407226562, complete_episode_count: 52.0, Gather time: 0.60s, Train time: 1.83s
Iteration: 56,  Mean reward: -1.5888888888888888, Mean Entropy: 0.881767213344574, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 57,  Mean reward: -1.711111111111111, Mean Entropy: 0.7891520261764526, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 58,  Mean reward: 0.22115384615384615, Mean Entropy: 0.9273936152458191, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 59,  Mean reward: -0.1956521739130435, Mean Entropy: 0.8702511191368103, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.65s
Iteration: 60,  Mean reward: -0.6818181818181818, Mean Entropy: 0.9226629734039307, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.64s
Iteration: 61,  Mean reward: -2.2195121951219514, Mean Entropy: 0.8882893323898315, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.65s
Iteration: 62,  Mean reward: -0.6025641025641025, Mean Entropy: 0.8177345991134644, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.66s
Iteration: 63,  Mean reward: -0.7702702702702703, Mean Entropy: 0.7515295743942261, complete_episode_count: 37.0, Gather time: 0.57s, Train time: 1.64s
Iteration: 64,  Mean reward: 2.6818181818181817, Mean Entropy: 0.7335297465324402, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 65,  Mean reward: -0.40540540540540543, Mean Entropy: 0.7166206240653992, complete_episode_count: 37.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 66,  Mean reward: 1.3974358974358974, Mean Entropy: 0.7240405082702637, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 67,  Mean reward: 2.207317073170732, Mean Entropy: 0.7260656356811523, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 68,  Mean reward: 1.9487179487179487, Mean Entropy: 0.7214798927307129, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.64s
Iteration: 69,  Mean reward: 0.7894736842105263, Mean Entropy: 0.7215971350669861, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 70,  Mean reward: 2.3625, Mean Entropy: 0.7146603465080261, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 71,  Mean reward: 2.8289473684210527, Mean Entropy: 0.7073528170585632, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 72,  Mean reward: 1.4534883720930232, Mean Entropy: 0.7227658629417419, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 73,  Mean reward: 1.5595238095238095, Mean Entropy: 0.7004796862602234, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 74,  Mean reward: 2.1951219512195124, Mean Entropy: 0.7230844497680664, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 75,  Mean reward: 3.075, Mean Entropy: 0.7395707964897156, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 76,  Mean reward: 2.130952380952381, Mean Entropy: 0.7035414576530457, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 77,  Mean reward: 3.688888888888889, Mean Entropy: 0.694821834564209, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.64s
Iteration: 78,  Mean reward: 3.388888888888889, Mean Entropy: 0.6858736276626587, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 79,  Mean reward: 2.8292682926829267, Mean Entropy: 0.6811187267303467, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 80,  Mean reward: 2.35, Mean Entropy: 0.6770601272583008, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 81,  Mean reward: 1.5384615384615385, Mean Entropy: 0.6688276529312134, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 82,  Mean reward: 3.5714285714285716, Mean Entropy: 0.7009272575378418, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 83,  Mean reward: 3.511904761904762, Mean Entropy: 0.6838952898979187, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.61s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 84,  Mean reward: 4.423913043478261, Mean Entropy: 0.6814488172531128, complete_episode_count: 46.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 85,  Mean reward: 3.4069767441860463, Mean Entropy: 0.6407066583633423, complete_episode_count: 43.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 86,  Mean reward: 3.697674418604651, Mean Entropy: 0.6510879993438721, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 87,  Mean reward: 3.3375, Mean Entropy: 0.6280137896537781, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 88,  Mean reward: 4.190476190476191, Mean Entropy: 0.6600461006164551, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 89,  Mean reward: 3.5813953488372094, Mean Entropy: 0.6738899946212769, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.79s
Iteration: 90,  Mean reward: 1.548780487804878, Mean Entropy: 0.6467010974884033, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 91,  Mean reward: 2.710526315789474, Mean Entropy: 0.6655139327049255, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 92,  Mean reward: 3.9404761904761907, Mean Entropy: 0.6722718477249146, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 93,  Mean reward: 2.231707317073171, Mean Entropy: 0.6611630916595459, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 94,  Mean reward: 4.104651162790698, Mean Entropy: 0.6569849252700806, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.62s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 95,  Mean reward: 4.989130434782608, Mean Entropy: 0.6201337575912476, complete_episode_count: 46.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 96,  Mean reward: 2.125, Mean Entropy: 0.6303765177726746, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 97,  Mean reward: 4.0813953488372094, Mean Entropy: 0.608259379863739, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 98,  Mean reward: 4.585106382978723, Mean Entropy: 0.6067904829978943, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 99,  Mean reward: 2.5980392156862746, Mean Entropy: 0.6186137199401855, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 100,  Mean reward: 0.8173076923076923, Mean Entropy: 0.6302341222763062, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.57s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 3.245614035087719, Mean Entropy: 0.4355350732803345, complete_episode_count: 57.0, Gather time: 0.60s, Train time: 1.56s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 102,  Mean reward: 6.611111111111111, Mean Entropy: 0.3488669991493225, complete_episode_count: 63.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 103,  Mean reward: 3.7280701754385963, Mean Entropy: 0.4687190651893616, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 104,  Mean reward: 5.3090909090909095, Mean Entropy: 0.5697027444839478, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 105,  Mean reward: 2.2, Mean Entropy: 0.5757332444190979, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 106,  Mean reward: 4.754901960784314, Mean Entropy: 0.5629458427429199, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 107,  Mean reward: 4.434782608695652, Mean Entropy: 0.5695955753326416, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 108,  Mean reward: 4.226415094339623, Mean Entropy: 0.4301754832267761, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 109,  Mean reward: 3.6666666666666665, Mean Entropy: 0.5575944185256958, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 110,  Mean reward: 3.7857142857142856, Mean Entropy: 0.43460413813591003, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 111,  Mean reward: 6.293103448275862, Mean Entropy: 0.4482395350933075, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 112,  Mean reward: 4.588709677419355, Mean Entropy: 0.415670245885849, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 113,  Mean reward: 4.818181818181818, Mean Entropy: 0.3590162396430969, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.71s
Iteration: 114,  Mean reward: 5.991379310344827, Mean Entropy: 0.4348106384277344, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 115,  Mean reward: 4.798076923076923, Mean Entropy: 0.5152400732040405, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 116,  Mean reward: 5.959016393442623, Mean Entropy: 0.40878820419311523, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 117,  Mean reward: 6.5, Mean Entropy: 0.4051376283168793, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 118,  Mean reward: 6.640625, Mean Entropy: 0.3309042155742645, complete_episode_count: 64.0, Gather time: 0.62s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 119,  Mean reward: 6.758620689655173, Mean Entropy: 0.3554961085319519, complete_episode_count: 58.0, Gather time: 0.61s, Train time: 1.59s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 120,  Mean reward: 6.909836065573771, Mean Entropy: 0.2872121334075928, complete_episode_count: 61.0, Gather time: 0.61s, Train time: 1.61s
Iteration: 121,  Mean reward: 6.8515625, Mean Entropy: 0.3071304261684418, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 122,  Mean reward: 6.887931034482759, Mean Entropy: 0.2521390914916992, complete_episode_count: 58.0, Gather time: 0.60s, Train time: 1.82s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 123,  Mean reward: 7.340909090909091, Mean Entropy: 0.2663780152797699, complete_episode_count: 66.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 124,  Mean reward: 6.164383561643835, Mean Entropy: 0.2707503139972687, complete_episode_count: 73.0, Gather time: 0.62s, Train time: 0.86s
Iteration: 125,  Mean reward: -1.4596774193548387, Mean Entropy: 0.3099246025085449, complete_episode_count: 62.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 126,  Mean reward: -4.17741935483871, Mean Entropy: 0.1653774380683899, complete_episode_count: 62.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 127,  Mean reward: 7.053846153846154, Mean Entropy: 0.14577864110469818, complete_episode_count: 65.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 128,  Mean reward: 7.297297297297297, Mean Entropy: 0.16800616681575775, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 129,  Mean reward: 6.410256410256411, Mean Entropy: 0.18548128008842468, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 130,  Mean reward: 5.443037974683544, Mean Entropy: 0.2697148323059082, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 131,  Mean reward: 4.352564102564102, Mean Entropy: 0.17253538966178894, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 132,  Mean reward: 6.507246376811594, Mean Entropy: 0.3407125174999237, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 133,  Mean reward: 2.6714285714285713, Mean Entropy: 0.19522425532341003, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 134,  Mean reward: 6.673076923076923, Mean Entropy: 0.179625004529953, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 135,  Mean reward: 7.808219178082192, Mean Entropy: 0.15583288669586182, complete_episode_count: 73.0, Gather time: 0.63s, Train time: 0.82s
Iteration: 136,  Mean reward: 7.773972602739726, Mean Entropy: 0.09179724752902985, complete_episode_count: 73.0, Gather time: 0.62s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 137,  Mean reward: 7.962025316455696, Mean Entropy: 0.009030002169311047, complete_episode_count: 79.0, Gather time: 0.64s, Train time: 0.84s
Iteration: 138,  Mean reward: -2.1455696202531644, Mean Entropy: 0.02008822187781334, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 139,  Mean reward: -3.5, Mean Entropy: 0.11889146268367767, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 140,  Mean reward: -1.162162162162162, Mean Entropy: 0.35932132601737976, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 141,  Mean reward: -2.4347826086956523, Mean Entropy: 0.43668079376220703, complete_episode_count: 69.0, Gather time: 0.64s, Train time: 0.76s
Iteration: 142,  Mean reward: -4.126984126984127, Mean Entropy: 0.6352097392082214, complete_episode_count: 63.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 143,  Mean reward: -2.4444444444444446, Mean Entropy: 0.5835555791854858, complete_episode_count: 63.0, Gather time: 0.63s, Train time: 1.64s
Iteration: 144,  Mean reward: -2.888888888888889, Mean Entropy: 0.7603244781494141, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 145,  Mean reward: -1.9181818181818182, Mean Entropy: 0.7738027572631836, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 146,  Mean reward: -1.1037735849056605, Mean Entropy: 0.7838346362113953, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 147,  Mean reward: -2.56, Mean Entropy: 0.7423779368400574, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 148,  Mean reward: 0.41935483870967744, Mean Entropy: 0.4430786073207855, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 1.64s
Iteration: 149,  Mean reward: -1.4196428571428572, Mean Entropy: 0.37426432967185974, complete_episode_count: 56.0, Gather time: 0.61s, Train time: 1.60s
Iteration: 150,  Mean reward: -2.980392156862745, Mean Entropy: 0.3354906439781189, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 151,  Mean reward: -1.4433962264150944, Mean Entropy: 0.1641998589038849, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 152,  Mean reward: 0.10784313725490197, Mean Entropy: 0.15985146164894104, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 153,  Mean reward: -5.8088235294117645, Mean Entropy: 0.6593265533447266, complete_episode_count: 34.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 154,  Mean reward: -5.081818181818182, Mean Entropy: 0.7810854911804199, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 155,  Mean reward: -5.25, Mean Entropy: 0.6105018854141235, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 156,  Mean reward: -4.5, Mean Entropy: 0.6582492589950562, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 157,  Mean reward: 3.2314814814814814, Mean Entropy: 0.6082124710083008, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.84s
Iteration: 158,  Mean reward: 3.518181818181818, Mean Entropy: 0.6015411615371704, complete_episode_count: 55.0, Gather time: 0.60s, Train time: 1.58s
Iteration: 159,  Mean reward: -4.416666666666667, Mean Entropy: 0.6780942678451538, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 160,  Mean reward: -1.9148936170212767, Mean Entropy: 0.7798875570297241, complete_episode_count: 47.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 161,  Mean reward: -4.931818181818182, Mean Entropy: 0.9277447462081909, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 162,  Mean reward: -3.6875, Mean Entropy: 0.7560212016105652, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 163,  Mean reward: -0.058823529411764705, Mean Entropy: 0.6990866661071777, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 164,  Mean reward: 0.89, Mean Entropy: 0.6434893608093262, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.64s
Iteration: 165,  Mean reward: 0.25, Mean Entropy: 0.8301784992218018, complete_episode_count: 50.0, Gather time: 0.59s, Train time: 1.66s
Iteration: 166,  Mean reward: 0.20588235294117646, Mean Entropy: 0.8327012062072754, complete_episode_count: 51.0, Gather time: 0.60s, Train time: 1.66s
Iteration: 167,  Mean reward: -0.9883720930232558, Mean Entropy: 0.7866334319114685, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 168,  Mean reward: 0.32558139534883723, Mean Entropy: 0.789331316947937, complete_episode_count: 43.0, Gather time: 0.61s, Train time: 1.62s
Iteration: 169,  Mean reward: 0.46511627906976744, Mean Entropy: 0.7583945989608765, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 170,  Mean reward: 1.52, Mean Entropy: 0.7897800207138062, complete_episode_count: 50.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 171,  Mean reward: 2.372549019607843, Mean Entropy: 0.6411736011505127, complete_episode_count: 51.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 172,  Mean reward: -0.9385964912280702, Mean Entropy: 0.6578385829925537, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 173,  Mean reward: -1.4035087719298245, Mean Entropy: 0.6277145743370056, complete_episode_count: 57.0, Gather time: 0.62s, Train time: 1.62s
Iteration: 174,  Mean reward: 2.125, Mean Entropy: 0.4618977904319763, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 0.86s
Iteration: 175,  Mean reward: -2.3656716417910446, Mean Entropy: 0.3403240442276001, complete_episode_count: 67.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 176,  Mean reward: -4.338461538461538, Mean Entropy: 0.5401433110237122, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 177,  Mean reward: 2.846774193548387, Mean Entropy: 0.5835734605789185, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 178,  Mean reward: 2.180327868852459, Mean Entropy: 0.6052005290985107, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 179,  Mean reward: 2.7966101694915255, Mean Entropy: 0.37468308210372925, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.64s
Iteration: 180,  Mean reward: 6.059322033898305, Mean Entropy: 0.49267229437828064, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.65s
Iteration: 181,  Mean reward: 6.107142857142857, Mean Entropy: 0.37706682085990906, complete_episode_count: 56.0, Gather time: 0.61s, Train time: 1.64s
Iteration: 182,  Mean reward: 7.586956521739131, Mean Entropy: 0.20985734462738037, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 183,  Mean reward: 7.82, Mean Entropy: 0.1503303349018097, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 184,  Mean reward: -2.625, Mean Entropy: 0.13216669857501984, complete_episode_count: 76.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 185,  Mean reward: -0.16883116883116883, Mean Entropy: 0.03490132838487625, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 186,  Mean reward: -2.75, Mean Entropy: 0.024191608652472496, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 187,  Mean reward: -0.6265822784810127, Mean Entropy: 0.011485649272799492, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 188,  Mean reward: 0.0, Mean Entropy: 0.1662599742412567, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 189,  Mean reward: -1.3309859154929577, Mean Entropy: 0.26599636673927307, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 190,  Mean reward: -0.7205882352941176, Mean Entropy: 0.20133505761623383, complete_episode_count: 68.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 191,  Mean reward: -0.4859154929577465, Mean Entropy: 0.20040035247802734, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 192,  Mean reward: 1.9705882352941178, Mean Entropy: 0.29051902890205383, complete_episode_count: 68.0, Gather time: 0.80s, Train time: 0.81s
Iteration: 193,  Mean reward: 7.6231884057971016, Mean Entropy: 0.4131544530391693, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 194,  Mean reward: 4.968253968253968, Mean Entropy: 0.3183457851409912, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 195,  Mean reward: 7.536764705882353, Mean Entropy: 0.2702348828315735, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 196,  Mean reward: 7.586956521739131, Mean Entropy: 0.17796242237091064, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 197,  Mean reward: 7.5928571428571425, Mean Entropy: 0.1885834038257599, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 198,  Mean reward: 7.095238095238095, Mean Entropy: 0.1780090630054474, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 199,  Mean reward: 1.8115942028985508, Mean Entropy: 0.19584375619888306, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 200,  Mean reward: -2.5, Mean Entropy: 0.36012959480285645, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.77s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.4191176470588234, Mean Entropy: 0.5046594142913818, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 202,  Mean reward: -4.620689655172414, Mean Entropy: 0.8924119472503662, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 203,  Mean reward: -8.25, Mean Entropy: 0.9975239634513855, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 204,  Mean reward: -6.551282051282051, Mean Entropy: 0.9302105903625488, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 205,  Mean reward: -2.941860465116279, Mean Entropy: 0.9646214246749878, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 206,  Mean reward: -3.6744186046511627, Mean Entropy: 0.9477803707122803, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 207,  Mean reward: -1.3421052631578947, Mean Entropy: 0.8951756358146667, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 208,  Mean reward: -3.9125, Mean Entropy: 0.9487713575363159, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 209,  Mean reward: -0.8170731707317073, Mean Entropy: 0.9632471799850464, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 210,  Mean reward: -1.7142857142857142, Mean Entropy: 0.7673557996749878, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 211,  Mean reward: 2.5625, Mean Entropy: 0.8157607316970825, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 212,  Mean reward: 3.311320754716981, Mean Entropy: 0.6965241432189941, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 213,  Mean reward: 0.8255813953488372, Mean Entropy: 0.8190554976463318, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 214,  Mean reward: 2.663265306122449, Mean Entropy: 0.6619477272033691, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 215,  Mean reward: 4.05, Mean Entropy: 0.7152574062347412, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 216,  Mean reward: 1.1929824561403508, Mean Entropy: 0.7659987211227417, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 217,  Mean reward: -0.009433962264150943, Mean Entropy: 0.7566165924072266, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 218,  Mean reward: -0.35106382978723405, Mean Entropy: 0.7231046557426453, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 219,  Mean reward: 2.21, Mean Entropy: 0.6254302859306335, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 220,  Mean reward: -2.881818181818182, Mean Entropy: 0.621767520904541, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 221,  Mean reward: 6.607142857142857, Mean Entropy: 0.48242613673210144, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 222,  Mean reward: 5.180327868852459, Mean Entropy: 0.14770011603832245, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 223,  Mean reward: 7.6231884057971016, Mean Entropy: 0.17343679070472717, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 224,  Mean reward: 4.2368421052631575, Mean Entropy: 0.24374648928642273, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 225,  Mean reward: 1.7164179104477613, Mean Entropy: 0.507968544960022, complete_episode_count: 67.0, Gather time: 0.62s, Train time: 0.77s
Iteration: 226,  Mean reward: 2.207692307692308, Mean Entropy: 0.391371488571167, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 227,  Mean reward: 5.565573770491803, Mean Entropy: 0.2710784077644348, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 228,  Mean reward: 7.595588235294118, Mean Entropy: 0.4024046063423157, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 229,  Mean reward: 4.015151515151516, Mean Entropy: 0.34157150983810425, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 230,  Mean reward: 6.889705882352941, Mean Entropy: 0.29809367656707764, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 231,  Mean reward: 7.183333333333334, Mean Entropy: 0.1929234117269516, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 232,  Mean reward: 5.226666666666667, Mean Entropy: 0.15426892042160034, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 233,  Mean reward: 5.668918918918919, Mean Entropy: 0.26049306988716125, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 234,  Mean reward: 4.823943661971831, Mean Entropy: 0.40486741065979004, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 235,  Mean reward: 7.153225806451613, Mean Entropy: 0.4271382689476013, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 236,  Mean reward: 7.415384615384616, Mean Entropy: 0.5009447932243347, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 237,  Mean reward: 2.2580645161290325, Mean Entropy: 0.3655167520046234, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 238,  Mean reward: 7.193548387096774, Mean Entropy: 0.3849008083343506, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 239,  Mean reward: 6.597014925373134, Mean Entropy: 0.27117788791656494, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 240,  Mean reward: 7.257352941176471, Mean Entropy: 0.21304915845394135, complete_episode_count: 68.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 241,  Mean reward: 7.75, Mean Entropy: 0.17376956343650818, complete_episode_count: 72.0, Gather time: 0.62s, Train time: 0.84s
Iteration: 242,  Mean reward: 7.527027027027027, Mean Entropy: 0.14714950323104858, complete_episode_count: 74.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 243,  Mean reward: 7.733333333333333, Mean Entropy: 0.09741121530532837, complete_episode_count: 75.0, Gather time: 0.62s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 244,  Mean reward: 8.0, Mean Entropy: 0.03774828463792801, complete_episode_count: 80.0, Gather time: 0.76s, Train time: 0.82s
Iteration: 245,  Mean reward: 8.0, Mean Entropy: 0.025241784751415253, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 246,  Mean reward: 7.974683544303797, Mean Entropy: 0.026645129546523094, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 247,  Mean reward: 8.0, Mean Entropy: 0.009985806420445442, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 248,  Mean reward: 8.0, Mean Entropy: 0.004806871991604567, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.85s
Iteration: 249,  Mean reward: 8.0, Mean Entropy: 0.003296859096735716, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 250,  Mean reward: 8.0, Mean Entropy: 0.004548647440969944, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 251,  Mean reward: 8.0, Mean Entropy: 0.002209298312664032, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 252,  Mean reward: 8.0, Mean Entropy: 0.0023210779763758183, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 253,  Mean reward: 8.0, Mean Entropy: 0.002293150406330824, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 254,  Mean reward: 8.0, Mean Entropy: 0.0018795221112668514, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 255,  Mean reward: 8.0, Mean Entropy: 0.0024648760445415974, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 256,  Mean reward: 8.0, Mean Entropy: 0.0019003055058419704, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 257,  Mean reward: 8.0, Mean Entropy: 0.0014712139964103699, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 258,  Mean reward: 8.0, Mean Entropy: 0.0012072359677404165, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 259,  Mean reward: 8.0, Mean Entropy: 0.0012401926796883345, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 260,  Mean reward: 8.0, Mean Entropy: 0.001142059569247067, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 261,  Mean reward: 8.0, Mean Entropy: 0.0009343420970253646, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 262,  Mean reward: 8.0, Mean Entropy: 0.0008927038870751858, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 263,  Mean reward: 8.0, Mean Entropy: 0.0008002858376130462, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 264,  Mean reward: 8.0, Mean Entropy: 0.0008873782935552299, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 265,  Mean reward: 8.0, Mean Entropy: 0.0010265043238177896, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 266,  Mean reward: 8.0, Mean Entropy: 0.000935039424803108, complete_episode_count: 80.0, Gather time: 0.80s, Train time: 0.81s
Iteration: 267,  Mean reward: 8.0, Mean Entropy: 0.0007843518978916109, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 268,  Mean reward: 8.0, Mean Entropy: 0.0009340833639726043, complete_episode_count: 80.0, Gather time: 0.78s, Train time: 0.85s
Iteration: 269,  Mean reward: 8.0, Mean Entropy: 0.0008939909748733044, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 270,  Mean reward: 8.0, Mean Entropy: 0.000881021551322192, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 271,  Mean reward: 8.0, Mean Entropy: 0.0007804028573445976, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 272,  Mean reward: 8.0, Mean Entropy: 0.0006792710046283901, complete_episode_count: 80.0, Gather time: 0.64s, Train time: 0.80s
Iteration: 273,  Mean reward: 8.0, Mean Entropy: 0.0008414908079430461, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 274,  Mean reward: 8.0, Mean Entropy: 0.000613060430623591, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.79s
Iteration: 275,  Mean reward: 8.0, Mean Entropy: 0.0005837222561240196, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.79s
Iteration: 276,  Mean reward: 8.0, Mean Entropy: 0.0007000508485361934, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 277,  Mean reward: 8.0, Mean Entropy: 0.0008517317473888397, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 278,  Mean reward: 8.0, Mean Entropy: 0.00084219581913203, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 279,  Mean reward: 8.0, Mean Entropy: 0.0008103279396891594, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 280,  Mean reward: 8.0, Mean Entropy: 0.0006224934477359056, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 281,  Mean reward: 8.0, Mean Entropy: 0.000875516445375979, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 282,  Mean reward: 8.0, Mean Entropy: 0.000714879366569221, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 283,  Mean reward: 8.0, Mean Entropy: 0.0007775949779897928, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 284,  Mean reward: 8.0, Mean Entropy: 0.000978849595412612, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 285,  Mean reward: 8.0, Mean Entropy: 0.000698006886523217, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 286,  Mean reward: 8.0, Mean Entropy: 0.0006956850993447006, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 287,  Mean reward: 8.0, Mean Entropy: 0.0008768356638029218, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 288,  Mean reward: 8.0, Mean Entropy: 0.0007260936545208097, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 289,  Mean reward: 8.0, Mean Entropy: 0.0007290142821148038, complete_episode_count: 80.0, Gather time: 0.76s, Train time: 0.78s
Iteration: 290,  Mean reward: 8.0, Mean Entropy: 0.0005485871224664152, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 291,  Mean reward: 8.0, Mean Entropy: 0.0007683636504225433, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 292,  Mean reward: 8.0, Mean Entropy: 0.000875636818818748, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 293,  Mean reward: 8.0, Mean Entropy: 0.0006899612490087748, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 294,  Mean reward: 8.0, Mean Entropy: 0.0007120232912711799, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 295,  Mean reward: 8.0, Mean Entropy: 0.0006215124158188701, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 296,  Mean reward: 8.0, Mean Entropy: 0.0007672488573007286, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 297,  Mean reward: 8.0, Mean Entropy: 0.0005602813325822353, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 298,  Mean reward: 8.0, Mean Entropy: 0.0007370418170467019, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 299,  Mean reward: 8.0, Mean Entropy: 0.0006183490622788668, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 300,  Mean reward: 8.0, Mean Entropy: 0.0007107811397872865, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 8.0, Mean Entropy: 0.0008523432770743966, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.79s
Iteration: 302,  Mean reward: 8.0, Mean Entropy: 0.0007685298332944512, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 303,  Mean reward: 8.0, Mean Entropy: 0.0007038493640720844, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 304,  Mean reward: 8.0, Mean Entropy: 0.0006489287479780614, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 305,  Mean reward: 8.0, Mean Entropy: 0.0007319875876419246, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 306,  Mean reward: 8.0, Mean Entropy: 0.0007064050296321511, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 307,  Mean reward: 8.0, Mean Entropy: 0.0006732227629981935, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 308,  Mean reward: 8.0, Mean Entropy: 0.0007041129283607006, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 309,  Mean reward: 8.0, Mean Entropy: 0.0008734883740544319, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 310,  Mean reward: 8.0, Mean Entropy: 0.0007008174434304237, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 311,  Mean reward: 8.0, Mean Entropy: 0.0007570150191895664, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 312,  Mean reward: 7.981012658227848, Mean Entropy: 0.01370982639491558, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 313,  Mean reward: 7.5, Mean Entropy: 0.009979696944355965, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 314,  Mean reward: 8.0, Mean Entropy: 0.0011090408079326153, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 315,  Mean reward: 8.0, Mean Entropy: 0.0006329228635877371, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 316,  Mean reward: -1.0, Mean Entropy: 0.08733341097831726, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 317,  Mean reward: 5.75, Mean Entropy: 0.004725285340100527, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 318,  Mean reward: 8.0, Mean Entropy: 0.06118779629468918, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 319,  Mean reward: 7.328947368421052, Mean Entropy: 0.05219705030322075, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 320,  Mean reward: 8.0, Mean Entropy: 0.04876617714762688, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 321,  Mean reward: 7.961538461538462, Mean Entropy: 0.008352920413017273, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 322,  Mean reward: 8.0, Mean Entropy: 0.0002406310522928834, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 323,  Mean reward: 8.0, Mean Entropy: 0.00034372112713754177, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 324,  Mean reward: 8.0, Mean Entropy: 0.00022076789173297584, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 325,  Mean reward: 8.0, Mean Entropy: 0.00020216821576468647, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 326,  Mean reward: 8.0, Mean Entropy: 0.0002063177525997162, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 327,  Mean reward: 8.0, Mean Entropy: 0.00017736066365614533, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 328,  Mean reward: 8.0, Mean Entropy: 0.0001572450710227713, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 329,  Mean reward: 8.0, Mean Entropy: 0.0001791185641195625, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 330,  Mean reward: 8.0, Mean Entropy: 0.0001665873423917219, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 331,  Mean reward: 8.0, Mean Entropy: 0.00017879632650874555, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 332,  Mean reward: 8.0, Mean Entropy: 0.00018430348427500576, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 333,  Mean reward: 8.0, Mean Entropy: 0.00020661871531046927, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 334,  Mean reward: 8.0, Mean Entropy: 0.00016843690536916256, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 335,  Mean reward: 8.0, Mean Entropy: 0.000204500334803015, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 336,  Mean reward: 8.0, Mean Entropy: 0.0002075981319649145, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 337,  Mean reward: 8.0, Mean Entropy: 0.00016705342568457127, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 338,  Mean reward: 8.0, Mean Entropy: 0.000207182252779603, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 339,  Mean reward: 8.0, Mean Entropy: 0.00019547021656762809, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 340,  Mean reward: 8.0, Mean Entropy: 0.0002303429355379194, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 341,  Mean reward: 8.0, Mean Entropy: 0.00017091137124225497, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 342,  Mean reward: 8.0, Mean Entropy: 0.0002470403560437262, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 343,  Mean reward: 8.0, Mean Entropy: 0.0002114254457410425, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 344,  Mean reward: 8.0, Mean Entropy: 0.00024054624373093247, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 345,  Mean reward: 8.0, Mean Entropy: 0.00023770410916768014, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 346,  Mean reward: 8.0, Mean Entropy: 0.00021624487999361008, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 347,  Mean reward: 8.0, Mean Entropy: 0.0001670161436777562, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 348,  Mean reward: 8.0, Mean Entropy: 0.00016958445485215634, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 349,  Mean reward: 8.0, Mean Entropy: 0.00019405255443416536, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 350,  Mean reward: 8.0, Mean Entropy: 0.00024510265211574733, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 351,  Mean reward: 8.0, Mean Entropy: 0.00016051475540734828, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 352,  Mean reward: 8.0, Mean Entropy: 0.0002612647949717939, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 353,  Mean reward: 8.0, Mean Entropy: 0.00022564898245036602, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 354,  Mean reward: 8.0, Mean Entropy: 0.0002060174010694027, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 355,  Mean reward: 8.0, Mean Entropy: 0.0002635525306686759, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 356,  Mean reward: 8.0, Mean Entropy: 0.00022297310351859778, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 357,  Mean reward: 8.0, Mean Entropy: 0.00026095291832461953, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 358,  Mean reward: 8.0, Mean Entropy: 0.0002967074979096651, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 359,  Mean reward: 8.0, Mean Entropy: 0.0003041022864636034, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 360,  Mean reward: 8.0, Mean Entropy: 0.0002727784449234605, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.83s
Iteration: 361,  Mean reward: 8.0, Mean Entropy: 0.0002790478465612978, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 362,  Mean reward: 8.0, Mean Entropy: 0.0002934336371254176, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 363,  Mean reward: 8.0, Mean Entropy: 0.00027526472695171833, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 364,  Mean reward: 8.0, Mean Entropy: 0.0003261864185333252, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 365,  Mean reward: 8.0, Mean Entropy: 0.0002948981709778309, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 366,  Mean reward: 8.0, Mean Entropy: 0.00029662862652912736, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 367,  Mean reward: 8.0, Mean Entropy: 0.00035340385511517525, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 368,  Mean reward: 8.0, Mean Entropy: 0.00027799297822639346, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 369,  Mean reward: 8.0, Mean Entropy: 0.00038663839222863317, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 370,  Mean reward: 8.0, Mean Entropy: 0.00032298173755407333, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 371,  Mean reward: 8.0, Mean Entropy: 0.00024925824254751205, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 372,  Mean reward: 8.0, Mean Entropy: 0.0003340001858305186, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 373,  Mean reward: 8.0, Mean Entropy: 0.0003814683295786381, complete_episode_count: 80.0, Gather time: 0.76s, Train time: 0.79s
Iteration: 374,  Mean reward: 8.0, Mean Entropy: 0.00029896548949182034, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 375,  Mean reward: 8.0, Mean Entropy: 0.0003856073599308729, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 376,  Mean reward: 8.0, Mean Entropy: 0.0004288652562536299, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 377,  Mean reward: 8.0, Mean Entropy: 0.00035055610351264477, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 378,  Mean reward: 8.0, Mean Entropy: 0.0003521471517160535, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 379,  Mean reward: 8.0, Mean Entropy: 0.0003858142299577594, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 1.03s
Iteration: 380,  Mean reward: 8.0, Mean Entropy: 0.0004324197652749717, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 381,  Mean reward: 8.0, Mean Entropy: 0.00036572309909388423, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 382,  Mean reward: 8.0, Mean Entropy: 0.0003223984967917204, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 383,  Mean reward: 8.0, Mean Entropy: 0.00038683004095219076, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 384,  Mean reward: 8.0, Mean Entropy: 0.00039154334808699787, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 385,  Mean reward: 8.0, Mean Entropy: 0.000368741515558213, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 386,  Mean reward: 8.0, Mean Entropy: 0.0004063724773004651, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 0.00036513275699689984, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 0.0005178043502382934, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 389,  Mean reward: 8.0, Mean Entropy: 0.000359908037353307, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 390,  Mean reward: 8.0, Mean Entropy: 0.0004362913314253092, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 0.0003776765370275825, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 392,  Mean reward: 8.0, Mean Entropy: 0.0005351155996322632, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 0.0004891898715868592, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 0.0004988349392078817, complete_episode_count: 80.0, Gather time: 0.76s, Train time: 0.82s
Iteration: 395,  Mean reward: 8.0, Mean Entropy: 0.0004335278645157814, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 396,  Mean reward: 8.0, Mean Entropy: 0.00038210555794648826, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 0.00036787494900636375, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 0.0004417860182002187, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 0.0005302135832607746, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 0.0005672021070495248, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 8.0, Mean Entropy: 0.0004283266607671976, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 402,  Mean reward: 8.0, Mean Entropy: 0.0004860712797380984, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 403,  Mean reward: 8.0, Mean Entropy: 0.0005667904042638838, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.84s
Iteration: 404,  Mean reward: 8.0, Mean Entropy: 0.0005388938589021564, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 405,  Mean reward: 8.0, Mean Entropy: 0.000620649429038167, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 0.0006700727972202003, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.84s
Iteration: 407,  Mean reward: 8.0, Mean Entropy: 0.0005532882642000914, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 0.0005484386347234249, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 0.0007624090649187565, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 410,  Mean reward: 8.0, Mean Entropy: 0.0005798169877380133, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 411,  Mean reward: 8.0, Mean Entropy: 0.0006392493378371, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 412,  Mean reward: 8.0, Mean Entropy: 0.0005864845588803291, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 413,  Mean reward: 8.0, Mean Entropy: 0.0006028037751093507, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 0.0006064555491320789, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 415,  Mean reward: 8.0, Mean Entropy: 0.0008556465618312359, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 416,  Mean reward: 8.0, Mean Entropy: 0.0007752412930130959, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 417,  Mean reward: 8.0, Mean Entropy: 0.0006157623138278723, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 1.08s
Iteration: 418,  Mean reward: 8.0, Mean Entropy: 0.0008052570628933609, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 419,  Mean reward: 8.0, Mean Entropy: 0.0005388425197452307, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 0.0006578523898497224, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 421,  Mean reward: 8.0, Mean Entropy: 0.000578203413169831, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.000777445500716567, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 423,  Mean reward: 8.0, Mean Entropy: 0.0008924794383347034, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 424,  Mean reward: 8.0, Mean Entropy: 0.0006895849946886301, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 0.0004884792724624276, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 0.0006449699285440147, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 427,  Mean reward: 8.0, Mean Entropy: 0.0005230661481618881, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 428,  Mean reward: 8.0, Mean Entropy: 0.0005383017123676836, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 429,  Mean reward: 8.0, Mean Entropy: 0.0005215559503994882, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 430,  Mean reward: 8.0, Mean Entropy: 0.0003780711558647454, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 0.0005634591216221452, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 432,  Mean reward: 8.0, Mean Entropy: 0.0005281656049191952, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 433,  Mean reward: 8.0, Mean Entropy: 0.0006109070382080972, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 434,  Mean reward: 8.0, Mean Entropy: 0.0006086442153900862, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 435,  Mean reward: 8.0, Mean Entropy: 0.0003982979687862098, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 436,  Mean reward: 8.0, Mean Entropy: 0.0003868333005812019, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.85s
Iteration: 437,  Mean reward: 8.0, Mean Entropy: 0.00047742854803800583, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 438,  Mean reward: 8.0, Mean Entropy: 0.0004741579759865999, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.84s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 0.0004410864785313606, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.79s
Iteration: 440,  Mean reward: 8.0, Mean Entropy: 0.0003760866238735616, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 441,  Mean reward: 8.0, Mean Entropy: 0.00038060740916989744, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 0.0004564658156596124, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 443,  Mean reward: 8.0, Mean Entropy: 0.00032882310915738344, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 444,  Mean reward: 8.0, Mean Entropy: 0.00028963584918528795, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.75s
Iteration: 445,  Mean reward: 8.0, Mean Entropy: 0.00034948502434417605, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 446,  Mean reward: 8.0, Mean Entropy: 0.0002836714847944677, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 447,  Mean reward: 8.0, Mean Entropy: 0.0003404771559871733, complete_episode_count: 80.0, Gather time: 0.64s, Train time: 0.82s
Iteration: 448,  Mean reward: 8.0, Mean Entropy: 0.0002943916479125619, complete_episode_count: 80.0, Gather time: 0.64s, Train time: 0.81s
Iteration: 449,  Mean reward: 8.0, Mean Entropy: 0.00028047821251675487, complete_episode_count: 80.0, Gather time: 0.64s, Train time: 0.81s
Iteration: 450,  Mean reward: 8.0, Mean Entropy: 0.00036150356754660606, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.85s
Iteration: 451,  Mean reward: 8.0, Mean Entropy: 0.0003219854552298784, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 452,  Mean reward: 8.0, Mean Entropy: 0.00030129615333862603, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 453,  Mean reward: 8.0, Mean Entropy: 0.0002509435289539397, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 454,  Mean reward: 8.0, Mean Entropy: 0.0003138024767395109, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 455,  Mean reward: 8.0, Mean Entropy: 0.0003123007481917739, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 456,  Mean reward: 8.0, Mean Entropy: 0.0003387644828762859, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 457,  Mean reward: 8.0, Mean Entropy: 0.00023708640947006643, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 458,  Mean reward: 8.0, Mean Entropy: 0.00031696868245489895, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 459,  Mean reward: 8.0, Mean Entropy: 0.00021869689226150513, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 460,  Mean reward: 8.0, Mean Entropy: 0.00026746821822598577, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 461,  Mean reward: 8.0, Mean Entropy: 0.00018443676526658237, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 462,  Mean reward: 8.0, Mean Entropy: 0.00021756612113676965, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 463,  Mean reward: 8.0, Mean Entropy: 0.00025556556647643447, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 464,  Mean reward: 8.0, Mean Entropy: 0.0002813804894685745, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 465,  Mean reward: 8.0, Mean Entropy: 0.00027727015549317, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 466,  Mean reward: 8.0, Mean Entropy: 0.0003307550214231014, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 467,  Mean reward: 8.0, Mean Entropy: 0.0003763477725442499, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 468,  Mean reward: 8.0, Mean Entropy: 0.0005068480386398733, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 469,  Mean reward: 8.0, Mean Entropy: 0.0005432409234344959, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 470,  Mean reward: 8.0, Mean Entropy: 0.00047508598072454333, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.83s
Iteration: 471,  Mean reward: 8.0, Mean Entropy: 0.0005447232397273183, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 472,  Mean reward: 8.0, Mean Entropy: 0.0008638444705866277, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 473,  Mean reward: 8.0, Mean Entropy: 0.0009217289043590426, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 474,  Mean reward: 8.0, Mean Entropy: 0.0017343409126624465, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 475,  Mean reward: 8.0, Mean Entropy: 0.0022346600890159607, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 476,  Mean reward: 8.0, Mean Entropy: 0.0024944543838500977, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 477,  Mean reward: 8.0, Mean Entropy: 0.005814665462821722, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 478,  Mean reward: 8.0, Mean Entropy: 0.024215247482061386, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 479,  Mean reward: 7.5576923076923075, Mean Entropy: 0.07356130331754684, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 480,  Mean reward: 8.0, Mean Entropy: 0.3090022802352905, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 481,  Mean reward: 7.614285714285714, Mean Entropy: 0.07887786626815796, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 482,  Mean reward: 7.961538461538462, Mean Entropy: 0.14830587804317474, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 483,  Mean reward: 7.25, Mean Entropy: 0.0004447047831490636, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 484,  Mean reward: -2.0, Mean Entropy: 0.009855554439127445, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 485,  Mean reward: -3.1582278481012658, Mean Entropy: 0.14971713721752167, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.77s
Iteration: 486,  Mean reward: -1.5256410256410255, Mean Entropy: 0.6468889117240906, complete_episode_count: 78.0, Gather time: 0.63s, Train time: 0.83s
Iteration: 487,  Mean reward: -2.86734693877551, Mean Entropy: 0.8651680946350098, complete_episode_count: 49.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 488,  Mean reward: 2.8867924528301887, Mean Entropy: 0.6076576113700867, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 489,  Mean reward: -0.875, Mean Entropy: 0.7091912031173706, complete_episode_count: 64.0, Gather time: 0.63s, Train time: 0.79s
Iteration: 490,  Mean reward: 0.08928571428571429, Mean Entropy: 0.6636701822280884, complete_episode_count: 56.0, Gather time: 0.61s, Train time: 1.59s
Iteration: 491,  Mean reward: 2.341666666666667, Mean Entropy: 0.6384525299072266, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 492,  Mean reward: 4.190476190476191, Mean Entropy: 0.5419119596481323, complete_episode_count: 63.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 493,  Mean reward: 4.795081967213115, Mean Entropy: 0.33693015575408936, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 1.80s
Iteration: 494,  Mean reward: 7.1118421052631575, Mean Entropy: 0.24596147239208221, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 495,  Mean reward: 1.9866666666666666, Mean Entropy: 0.22801916301250458, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 496,  Mean reward: 5.597402597402597, Mean Entropy: 0.14080257713794708, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 497,  Mean reward: 7.727848101265823, Mean Entropy: 0.09764401614665985, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.84s
Iteration: 498,  Mean reward: 7.727848101265823, Mean Entropy: 0.06685366481542587, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 499,  Mean reward: 7.1558441558441555, Mean Entropy: 0.02842370979487896, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 500,  Mean reward: 7.698717948717949, Mean Entropy: 0.028067229315638542, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 7.5, Mean Entropy: 0.011753242462873459, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 502,  Mean reward: 8.0, Mean Entropy: 0.018113484606146812, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 503,  Mean reward: 8.0, Mean Entropy: 0.0025618658401072025, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 504,  Mean reward: -3.5, Mean Entropy: 0.09156899154186249, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 505,  Mean reward: -0.6455696202531646, Mean Entropy: 0.14334744215011597, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 506,  Mean reward: -1.0320512820512822, Mean Entropy: 0.28591907024383545, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 507,  Mean reward: -3.661764705882353, Mean Entropy: 0.4474146068096161, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 508,  Mean reward: -3.4015151515151514, Mean Entropy: 0.49037784337997437, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 509,  Mean reward: -3.634920634920635, Mean Entropy: 0.5923491716384888, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 510,  Mean reward: -3.4558823529411766, Mean Entropy: 0.48243069648742676, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 511,  Mean reward: -4.217391304347826, Mean Entropy: 0.30857548117637634, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 512,  Mean reward: -0.9931506849315068, Mean Entropy: 0.22794881463050842, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 513,  Mean reward: -3.0933333333333333, Mean Entropy: 0.283017098903656, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 514,  Mean reward: -2.76056338028169, Mean Entropy: 0.26815661787986755, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 515,  Mean reward: -4.595890410958904, Mean Entropy: 0.2537170648574829, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 516,  Mean reward: -1.5724637681159421, Mean Entropy: 0.28212323784828186, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 517,  Mean reward: -3.361111111111111, Mean Entropy: 0.3236837089061737, complete_episode_count: 72.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 518,  Mean reward: 0.09523809523809523, Mean Entropy: 0.46516770124435425, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 519,  Mean reward: -0.47794117647058826, Mean Entropy: 0.44193360209465027, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 520,  Mean reward: -0.4112903225806452, Mean Entropy: 0.5316982865333557, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 521,  Mean reward: 1.0, Mean Entropy: 0.5003304481506348, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 522,  Mean reward: -1.2894736842105263, Mean Entropy: 0.6482120156288147, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 523,  Mean reward: 0.4672131147540984, Mean Entropy: 0.6189637184143066, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 524,  Mean reward: 3.396551724137931, Mean Entropy: 0.5826884508132935, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 525,  Mean reward: 4.6875, Mean Entropy: 0.5168541669845581, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 526,  Mean reward: 5.236363636363636, Mean Entropy: 0.5337722301483154, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 527,  Mean reward: 5.409836065573771, Mean Entropy: 0.5807821154594421, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 1.63s
Iteration: 528,  Mean reward: 5.508928571428571, Mean Entropy: 0.3917759656906128, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 529,  Mean reward: 7.051470588235294, Mean Entropy: 0.3442765176296234, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 530,  Mean reward: 0.16666666666666666, Mean Entropy: 0.4382570683956146, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 531,  Mean reward: 1.3015873015873016, Mean Entropy: 0.6712121963500977, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 532,  Mean reward: 0.5423728813559322, Mean Entropy: 0.6192919015884399, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 533,  Mean reward: -2.0172413793103448, Mean Entropy: 0.9436818957328796, complete_episode_count: 58.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 534,  Mean reward: -7.7368421052631575, Mean Entropy: 0.9645290374755859, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 535,  Mean reward: -5.431818181818182, Mean Entropy: 0.9598776698112488, complete_episode_count: 44.0, Gather time: 0.61s, Train time: 1.63s
Iteration: 536,  Mean reward: -3.011627906976744, Mean Entropy: 0.9580153822898865, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 537,  Mean reward: -3.880434782608696, Mean Entropy: 0.9529412984848022, complete_episode_count: 46.0, Gather time: 0.60s, Train time: 1.63s
Iteration: 538,  Mean reward: -4.375, Mean Entropy: 0.8950687646865845, complete_episode_count: 40.0, Gather time: 0.60s, Train time: 1.64s
Iteration: 539,  Mean reward: -4.829268292682927, Mean Entropy: 0.959444522857666, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 540,  Mean reward: -2.7209302325581395, Mean Entropy: 0.9124801158905029, complete_episode_count: 43.0, Gather time: 0.61s, Train time: 1.62s
Iteration: 541,  Mean reward: -3.3255813953488373, Mean Entropy: 0.9256452322006226, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 542,  Mean reward: -3.795918367346939, Mean Entropy: 0.9602011442184448, complete_episode_count: 49.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 543,  Mean reward: -5.435897435897436, Mean Entropy: 0.9313271045684814, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 544,  Mean reward: -4.025, Mean Entropy: 0.9251646995544434, complete_episode_count: 40.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 545,  Mean reward: -6.0476190476190474, Mean Entropy: 0.8933042287826538, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 546,  Mean reward: -3.7906976744186047, Mean Entropy: 0.8729249835014343, complete_episode_count: 43.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 547,  Mean reward: -3.9285714285714284, Mean Entropy: 0.9413579702377319, complete_episode_count: 42.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 548,  Mean reward: -3.0, Mean Entropy: 0.9269064664840698, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 549,  Mean reward: -4.923913043478261, Mean Entropy: 0.9367690086364746, complete_episode_count: 46.0, Gather time: 0.60s, Train time: 1.63s
Iteration: 550,  Mean reward: -4.46875, Mean Entropy: 0.9042288064956665, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.64s
Iteration: 551,  Mean reward: -4.959183673469388, Mean Entropy: 0.8829978704452515, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 552,  Mean reward: -4.627659574468085, Mean Entropy: 0.9357646703720093, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 553,  Mean reward: -2.96875, Mean Entropy: 0.8993965983390808, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 554,  Mean reward: -3.5408163265306123, Mean Entropy: 0.9557929039001465, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 555,  Mean reward: -3.1122448979591835, Mean Entropy: 0.8776211738586426, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 556,  Mean reward: -4.088235294117647, Mean Entropy: 0.8727278113365173, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 557,  Mean reward: -4.648936170212766, Mean Entropy: 0.9029104709625244, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 558,  Mean reward: -1.6020408163265305, Mean Entropy: 0.8333980441093445, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 559,  Mean reward: 0.4166666666666667, Mean Entropy: 0.6836187243461609, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 560,  Mean reward: -2.1724137931034484, Mean Entropy: 0.709874153137207, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 561,  Mean reward: -0.16037735849056603, Mean Entropy: 0.8331989049911499, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 562,  Mean reward: 3.5849056603773586, Mean Entropy: 0.5189446806907654, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 563,  Mean reward: -3.016949152542373, Mean Entropy: 0.6458646655082703, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.81s
Iteration: 564,  Mean reward: 1.5192307692307692, Mean Entropy: 0.7590498924255371, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 565,  Mean reward: 1.7222222222222223, Mean Entropy: 0.6307908892631531, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 566,  Mean reward: 3.787037037037037, Mean Entropy: 0.674036979675293, complete_episode_count: 54.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 567,  Mean reward: 3.0545454545454547, Mean Entropy: 0.6851457953453064, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 568,  Mean reward: 4.925925925925926, Mean Entropy: 0.6293269991874695, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 569,  Mean reward: 4.556451612903226, Mean Entropy: 0.48103660345077515, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 570,  Mean reward: 6.5859375, Mean Entropy: 0.3898906111717224, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 571,  Mean reward: 6.1015625, Mean Entropy: 0.40880998969078064, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 572,  Mean reward: 5.368421052631579, Mean Entropy: 0.34467244148254395, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 573,  Mean reward: 6.333333333333333, Mean Entropy: 0.33624228835105896, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 574,  Mean reward: 7.069230769230769, Mean Entropy: 0.28139305114746094, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 575,  Mean reward: 6.430769230769231, Mean Entropy: 0.3534773290157318, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 576,  Mean reward: 7.586956521739131, Mean Entropy: 0.2830004096031189, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 577,  Mean reward: 2.8815789473684212, Mean Entropy: 0.15761877596378326, complete_episode_count: 76.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 578,  Mean reward: 7.86, Mean Entropy: 0.1832805573940277, complete_episode_count: 75.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 579,  Mean reward: 0.782051282051282, Mean Entropy: 0.06576070934534073, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 580,  Mean reward: 7.881578947368421, Mean Entropy: 0.17764812707901, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 581,  Mean reward: -0.7397260273972602, Mean Entropy: 0.20414166152477264, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 582,  Mean reward: 3.2467532467532467, Mean Entropy: 0.4192872941493988, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 583,  Mean reward: 5.064285714285714, Mean Entropy: 0.21922890841960907, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 584,  Mean reward: 6.449275362318841, Mean Entropy: 0.34488576650619507, complete_episode_count: 69.0, Gather time: 0.64s, Train time: 0.81s
Iteration: 585,  Mean reward: 6.947761194029851, Mean Entropy: 0.16940462589263916, complete_episode_count: 67.0, Gather time: 0.63s, Train time: 0.83s
Iteration: 586,  Mean reward: 7.619718309859155, Mean Entropy: 0.44133830070495605, complete_episode_count: 71.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 587,  Mean reward: 5.031746031746032, Mean Entropy: 0.31561505794525146, complete_episode_count: 63.0, Gather time: 0.63s, Train time: 0.79s
Iteration: 588,  Mean reward: 7.239726027397261, Mean Entropy: 0.15664850175380707, complete_episode_count: 73.0, Gather time: 0.64s, Train time: 0.77s
Iteration: 589,  Mean reward: 7.902597402597403, Mean Entropy: 0.14923730492591858, complete_episode_count: 77.0, Gather time: 0.64s, Train time: 0.83s
Iteration: 590,  Mean reward: 8.0, Mean Entropy: 0.3863772749900818, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 591,  Mean reward: -9.51639344262295, Mean Entropy: 0.5196606516838074, complete_episode_count: 61.0, Gather time: 0.62s, Train time: 1.60s
Iteration: 592,  Mean reward: -8.715686274509803, Mean Entropy: 0.6344091296195984, complete_episode_count: 51.0, Gather time: 0.60s, Train time: 1.62s
Iteration: 593,  Mean reward: -7.131147540983607, Mean Entropy: 0.44856223464012146, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 594,  Mean reward: -3.2152777777777777, Mean Entropy: 0.12374725192785263, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 595,  Mean reward: -0.9935897435897436, Mean Entropy: 0.017530202865600586, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 596,  Mean reward: -2.75, Mean Entropy: 0.02033098042011261, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 597,  Mean reward: -2.3987341772151898, Mean Entropy: 0.022620033472776413, complete_episode_count: 79.0, Gather time: 0.76s, Train time: 0.76s
Iteration: 598,  Mean reward: -1.0, Mean Entropy: 0.02317383512854576, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 599,  Mean reward: -1.639240506329114, Mean Entropy: 0.016634967178106308, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 600,  Mean reward: -2.8076923076923075, Mean Entropy: 0.01285199262201786, complete_episode_count: 78.0, Gather time: 0.81s, Train time: 0.82s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.632911392405063, Mean Entropy: 0.013522960245609283, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 602,  Mean reward: -3.411392405063291, Mean Entropy: 0.013890210539102554, complete_episode_count: 79.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 603,  Mean reward: -2.75, Mean Entropy: 0.0431884303689003, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 604,  Mean reward: 0.0, Mean Entropy: 0.1633439064025879, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 605,  Mean reward: -2.727272727272727, Mean Entropy: 0.4139098525047302, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 606,  Mean reward: -0.42857142857142855, Mean Entropy: 0.41384944319725037, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 607,  Mean reward: -0.4375, Mean Entropy: 0.446821928024292, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 608,  Mean reward: -0.9454545454545454, Mean Entropy: 0.4462258219718933, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 609,  Mean reward: 1.0703125, Mean Entropy: 0.4433692395687103, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 610,  Mean reward: 4.4818181818181815, Mean Entropy: 0.4310916066169739, complete_episode_count: 55.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 611,  Mean reward: 6.333333333333333, Mean Entropy: 0.47369906306266785, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 612,  Mean reward: 5.950704225352113, Mean Entropy: 0.37914666533470154, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 613,  Mean reward: 6.707692307692308, Mean Entropy: 0.42994317412376404, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 614,  Mean reward: -0.8260869565217391, Mean Entropy: 0.33981308341026306, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 615,  Mean reward: 6.0661764705882355, Mean Entropy: 0.3109874725341797, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 616,  Mean reward: 5.054794520547945, Mean Entropy: 0.32238835096359253, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 617,  Mean reward: 5.465277777777778, Mean Entropy: 0.34875768423080444, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 618,  Mean reward: 6.514285714285714, Mean Entropy: 0.2150193452835083, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 619,  Mean reward: 6.9520547945205475, Mean Entropy: 0.25209301710128784, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 620,  Mean reward: 6.8133333333333335, Mean Entropy: 0.20232133567333221, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 621,  Mean reward: 7.226027397260274, Mean Entropy: 0.06416981667280197, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 622,  Mean reward: 7.383116883116883, Mean Entropy: 0.05576404929161072, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 623,  Mean reward: 8.0, Mean Entropy: 0.09552264213562012, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 624,  Mean reward: -8.981012658227849, Mean Entropy: 0.17253975570201874, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 625,  Mean reward: -0.3287671232876712, Mean Entropy: 0.13850811123847961, complete_episode_count: 73.0, Gather time: 0.64s, Train time: 0.82s
Iteration: 626,  Mean reward: -2.0, Mean Entropy: 0.07190012186765671, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 627,  Mean reward: -2.4177215189873418, Mean Entropy: 0.06126381456851959, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 628,  Mean reward: -2.6052631578947367, Mean Entropy: 0.08883869647979736, complete_episode_count: 76.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 629,  Mean reward: -2.9050632911392404, Mean Entropy: 0.28187084197998047, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.83s
Iteration: 630,  Mean reward: -1.3860759493670887, Mean Entropy: 0.3199560046195984, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 631,  Mean reward: 1.5460526315789473, Mean Entropy: 0.2655232846736908, complete_episode_count: 76.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 632,  Mean reward: 5.5131578947368425, Mean Entropy: 0.11047445237636566, complete_episode_count: 76.0, Gather time: 0.62s, Train time: 0.87s
Iteration: 633,  Mean reward: 6.968354430379747, Mean Entropy: 0.09533411264419556, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 634,  Mean reward: 7.42948717948718, Mean Entropy: 0.14244547486305237, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 635,  Mean reward: 6.208860759493671, Mean Entropy: 0.0850997194647789, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 636,  Mean reward: 7.474683544303797, Mean Entropy: 0.10123376548290253, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 637,  Mean reward: 6.5, Mean Entropy: 0.057418446987867355, complete_episode_count: 80.0, Gather time: 0.75s, Train time: 0.82s
Iteration: 638,  Mean reward: 7.8076923076923075, Mean Entropy: 0.03643481805920601, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 1.01s
Iteration: 639,  Mean reward: 8.0, Mean Entropy: 0.02137349173426628, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.86s
Iteration: 640,  Mean reward: 8.0, Mean Entropy: 0.18566809594631195, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.85s
Iteration: 641,  Mean reward: 7.794520547945205, Mean Entropy: 0.06563863158226013, complete_episode_count: 73.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 642,  Mean reward: 7.705128205128205, Mean Entropy: 0.03371850401163101, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 643,  Mean reward: 7.981012658227848, Mean Entropy: 0.02605995163321495, complete_episode_count: 79.0, Gather time: 0.63s, Train time: 0.78s
Iteration: 644,  Mean reward: 7.0, Mean Entropy: 0.050861623138189316, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 645,  Mean reward: 7.75, Mean Entropy: 0.15915833413600922, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 646,  Mean reward: 7.777027027027027, Mean Entropy: 0.13345104455947876, complete_episode_count: 74.0, Gather time: 0.63s, Train time: 0.78s
Iteration: 647,  Mean reward: 7.794520547945205, Mean Entropy: 0.10034313052892685, complete_episode_count: 73.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 648,  Mean reward: 7.941558441558442, Mean Entropy: 0.031072210520505905, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 649,  Mean reward: 7.75, Mean Entropy: 0.13387635350227356, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 650,  Mean reward: 7.75, Mean Entropy: 0.09188471734523773, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 651,  Mean reward: 7.705128205128205, Mean Entropy: 0.03270968049764633, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 652,  Mean reward: 8.0, Mean Entropy: 0.1253170669078827, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 653,  Mean reward: 7.708333333333333, Mean Entropy: 0.15642878413200378, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 654,  Mean reward: 7.961538461538462, Mean Entropy: 0.10916070640087128, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 655,  Mean reward: 7.801282051282051, Mean Entropy: 0.0636858195066452, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 656,  Mean reward: 7.2025316455696204, Mean Entropy: 0.006767227780073881, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 657,  Mean reward: 7.75, Mean Entropy: 0.04282369464635849, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 658,  Mean reward: 7.5, Mean Entropy: 0.0072625307366251945, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 659,  Mean reward: 8.0, Mean Entropy: 0.008880142122507095, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 660,  Mean reward: 8.0, Mean Entropy: 0.3240378201007843, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 661,  Mean reward: 0.8103448275862069, Mean Entropy: 0.08167318254709244, complete_episode_count: 58.0, Gather time: 0.60s, Train time: 1.63s
Iteration: 662,  Mean reward: 7.981012658227848, Mean Entropy: 0.0653616264462471, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.84s
Iteration: 663,  Mean reward: 7.981012658227848, Mean Entropy: 0.04066462069749832, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 664,  Mean reward: 8.0, Mean Entropy: 0.05242186784744263, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 665,  Mean reward: 7.779220779220779, Mean Entropy: 0.03388572111725807, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 666,  Mean reward: 8.0, Mean Entropy: 0.14360465109348297, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 667,  Mean reward: 7.2153846153846155, Mean Entropy: 0.03623709827661514, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 668,  Mean reward: 7.2025316455696204, Mean Entropy: 0.039324089884757996, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 669,  Mean reward: 8.0, Mean Entropy: 0.0201539508998394, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 670,  Mean reward: 8.0, Mean Entropy: 0.01957601308822632, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 671,  Mean reward: 7.981012658227848, Mean Entropy: 0.005946478806436062, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 672,  Mean reward: 8.0, Mean Entropy: 0.1380612552165985, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 673,  Mean reward: 2.0, Mean Entropy: 0.026486996561288834, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 674,  Mean reward: 8.0, Mean Entropy: 0.0038475184701383114, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 675,  Mean reward: 8.0, Mean Entropy: 0.02632256969809532, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 676,  Mean reward: 7.922077922077922, Mean Entropy: 0.0022647459991276264, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 1.01s
Iteration: 677,  Mean reward: 8.0, Mean Entropy: 0.0032348870299756527, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 678,  Mean reward: 8.0, Mean Entropy: 0.002805194817483425, complete_episode_count: 80.0, Gather time: 0.75s, Train time: 0.78s
Iteration: 679,  Mean reward: 8.0, Mean Entropy: 0.0014243023470044136, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 680,  Mean reward: 8.0, Mean Entropy: 0.00496729277074337, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 681,  Mean reward: 8.0, Mean Entropy: 0.26630961894989014, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 682,  Mean reward: 7.527027027027027, Mean Entropy: 0.11043310165405273, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 683,  Mean reward: 7.961538461538462, Mean Entropy: 0.034106239676475525, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 684,  Mean reward: 8.0, Mean Entropy: 0.17841951549053192, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 685,  Mean reward: 6.107843137254902, Mean Entropy: 0.029074108228087425, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 686,  Mean reward: 7.981012658227848, Mean Entropy: 0.02137286588549614, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 687,  Mean reward: 8.0, Mean Entropy: 0.1538493037223816, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 688,  Mean reward: 7.084745762711864, Mean Entropy: 0.0637737438082695, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 689,  Mean reward: 7.980769230769231, Mean Entropy: 0.02409128099679947, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 690,  Mean reward: 8.0, Mean Entropy: 0.018607351928949356, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 691,  Mean reward: 0.25, Mean Entropy: 0.0016648678574711084, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 692,  Mean reward: -1.75, Mean Entropy: 0.1040974110364914, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 693,  Mean reward: -3.013157894736842, Mean Entropy: 0.2200121134519577, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 694,  Mean reward: 0.3561643835616438, Mean Entropy: 0.26969069242477417, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 695,  Mean reward: -1.0757575757575757, Mean Entropy: 0.32664012908935547, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 696,  Mean reward: 0.75, Mean Entropy: 0.42490434646606445, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 697,  Mean reward: 5.268115942028985, Mean Entropy: 0.4511389136314392, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 698,  Mean reward: 1.4918032786885247, Mean Entropy: 0.4761608839035034, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.92s
Iteration: 699,  Mean reward: 5.830508474576271, Mean Entropy: 0.37080690264701843, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 700,  Mean reward: -1.1805555555555556, Mean Entropy: 0.41357922554016113, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -4.732394366197183, Mean Entropy: 0.3387483060359955, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 702,  Mean reward: -3.2388059701492535, Mean Entropy: 0.42339372634887695, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 703,  Mean reward: -1.8070175438596492, Mean Entropy: 0.6311442852020264, complete_episode_count: 57.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 704,  Mean reward: 4.877049180327869, Mean Entropy: 0.5895142555236816, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 705,  Mean reward: 3.9017857142857144, Mean Entropy: 0.44924432039260864, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 706,  Mean reward: 5.765151515151516, Mean Entropy: 0.4617587924003601, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 707,  Mean reward: 5.666666666666667, Mean Entropy: 0.42872679233551025, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 708,  Mean reward: 7.171052631578948, Mean Entropy: 0.19100229442119598, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 709,  Mean reward: 7.033333333333333, Mean Entropy: 0.23277871310710907, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 710,  Mean reward: 7.323943661971831, Mean Entropy: 0.18586163222789764, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 711,  Mean reward: 7.955128205128205, Mean Entropy: 0.06593319028615952, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 712,  Mean reward: 8.0, Mean Entropy: 0.20614539086818695, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 713,  Mean reward: 5.5928571428571425, Mean Entropy: 0.23971542716026306, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 714,  Mean reward: 4.5, Mean Entropy: 0.26997268199920654, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 715,  Mean reward: 7.159722222222222, Mean Entropy: 0.1971878707408905, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 1.00s
Iteration: 716,  Mean reward: 7.82, Mean Entropy: 0.2486976683139801, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 717,  Mean reward: 1.726027397260274, Mean Entropy: 0.12605296075344086, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 718,  Mean reward: 7.902597402597403, Mean Entropy: 0.0961199402809143, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 719,  Mean reward: 7.28, Mean Entropy: 0.1376463919878006, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 720,  Mean reward: 2.0, Mean Entropy: 0.06768921762704849, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 721,  Mean reward: 7.162337662337662, Mean Entropy: 0.0642576664686203, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 722,  Mean reward: 8.0, Mean Entropy: 0.00043316534720361233, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 723,  Mean reward: -1.0, Mean Entropy: 0.0028698998503386974, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 724,  Mean reward: -1.75, Mean Entropy: 0.2545730173587799, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 725,  Mean reward: 1.6180555555555556, Mean Entropy: 0.31302517652511597, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 726,  Mean reward: -3.377049180327869, Mean Entropy: 0.4931136965751648, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 727,  Mean reward: -3.8947368421052633, Mean Entropy: 0.7787324786186218, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 728,  Mean reward: -3.617021276595745, Mean Entropy: 0.8451049327850342, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 729,  Mean reward: -5.951219512195122, Mean Entropy: 0.8931252956390381, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 730,  Mean reward: -4.965909090909091, Mean Entropy: 0.9530448913574219, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 731,  Mean reward: -7.069767441860465, Mean Entropy: 0.8878964185714722, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 732,  Mean reward: -3.4625, Mean Entropy: 0.8810697793960571, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 733,  Mean reward: -1.0, Mean Entropy: 0.5837440490722656, complete_episode_count: 45.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 734,  Mean reward: -5.285714285714286, Mean Entropy: 0.5243996977806091, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 735,  Mean reward: -5.358695652173913, Mean Entropy: 0.8335773348808289, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 736,  Mean reward: -2.630952380952381, Mean Entropy: 0.9421108961105347, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 737,  Mean reward: -2.8536585365853657, Mean Entropy: 0.8679245710372925, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 738,  Mean reward: -4.1, Mean Entropy: 0.9323757886886597, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 739,  Mean reward: -2.75, Mean Entropy: 0.8316898345947266, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 740,  Mean reward: -3.0918367346938775, Mean Entropy: 0.8067697286605835, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 741,  Mean reward: -3.96875, Mean Entropy: 0.7566534876823425, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 742,  Mean reward: -3.8076923076923075, Mean Entropy: 0.833106517791748, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 743,  Mean reward: -1.5196078431372548, Mean Entropy: 0.4818509519100189, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 744,  Mean reward: -3.941666666666667, Mean Entropy: 0.5482385754585266, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.62s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -1.951219512195122, Mean Entropy: 0.880874514579773, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.58s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -3.630952380952381, Mean Entropy: 0.99639892578125, complete_episode_count: 42.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 2,  Mean reward: -4.3, Mean Entropy: 0.945857048034668, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.70s
Iteration: 3,  Mean reward: -4.8977272727272725, Mean Entropy: 0.9602975845336914, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 4,  Mean reward: -3.3048780487804876, Mean Entropy: 0.9458565711975098, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.78s
Iteration: 5,  Mean reward: -5.85, Mean Entropy: 0.9314157366752625, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 6,  Mean reward: -3.4468085106382977, Mean Entropy: 0.9675137400627136, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 7,  Mean reward: -2.9642857142857144, Mean Entropy: 0.9675110578536987, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 8,  Mean reward: -5.593023255813954, Mean Entropy: 0.9241851568222046, complete_episode_count: 43.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 9,  Mean reward: -4.325581395348837, Mean Entropy: 0.9675018191337585, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 10,  Mean reward: -6.583333333333333, Mean Entropy: 1.0180296897888184, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 11,  Mean reward: -2.6511627906976742, Mean Entropy: 0.9819061160087585, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 12,  Mean reward: -2.5476190476190474, Mean Entropy: 0.9818627238273621, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.56s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 13,  Mean reward: -1.7093023255813953, Mean Entropy: 1.0106430053710938, complete_episode_count: 43.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 14,  Mean reward: -5.877777777777778, Mean Entropy: 0.9312409162521362, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 15,  Mean reward: -5.430232558139535, Mean Entropy: 0.9241681098937988, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 16,  Mean reward: -3.268292682926829, Mean Entropy: 0.945828378200531, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.71s
Iteration: 17,  Mean reward: -4.273809523809524, Mean Entropy: 0.9024847745895386, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 18,  Mean reward: -2.1136363636363638, Mean Entropy: 0.9236885905265808, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 19,  Mean reward: -5.280487804878049, Mean Entropy: 0.9662266969680786, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 20,  Mean reward: -3.9390243902439024, Mean Entropy: 0.9416837692260742, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 21,  Mean reward: -5.878048780487805, Mean Entropy: 0.9704756736755371, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.55s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 22,  Mean reward: -0.84375, Mean Entropy: 0.970474123954773, complete_episode_count: 48.0, Gather time: 0.60s, Train time: 1.57s
Iteration: 23,  Mean reward: -3.3222222222222224, Mean Entropy: 0.9445827007293701, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 24,  Mean reward: -2.75, Mean Entropy: 0.9421857595443726, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 25,  Mean reward: -4.704545454545454, Mean Entropy: 0.9912154078483582, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 26,  Mean reward: -3.7875, Mean Entropy: 0.8947493433952332, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 27,  Mean reward: -2.6511627906976742, Mean Entropy: 1.002760887145996, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 28,  Mean reward: -6.088888888888889, Mean Entropy: 0.9229875802993774, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 29,  Mean reward: -4.476744186046512, Mean Entropy: 0.907354474067688, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 30,  Mean reward: -4.844444444444444, Mean Entropy: 0.9114105701446533, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.71s
Iteration: 31,  Mean reward: -6.947368421052632, Mean Entropy: 0.962155818939209, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 32,  Mean reward: -4.2375, Mean Entropy: 0.8039938807487488, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 33,  Mean reward: -3.048780487804878, Mean Entropy: 0.9541663527488708, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 34,  Mean reward: -2.8555555555555556, Mean Entropy: 0.9049538969993591, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 35,  Mean reward: -6.0, Mean Entropy: 0.8633236289024353, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 36,  Mean reward: -2.875, Mean Entropy: 0.8427886962890625, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 37,  Mean reward: -1.946808510638298, Mean Entropy: 0.7634283900260925, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 38,  Mean reward: -4.5212765957446805, Mean Entropy: 0.8713304400444031, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.79s
Iteration: 39,  Mean reward: -0.89, Mean Entropy: 0.8341683149337769, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 40,  Mean reward: -3.2596153846153846, Mean Entropy: 0.7761112451553345, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.64s
Iteration: 41,  Mean reward: -1.4313725490196079, Mean Entropy: 0.7049020528793335, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 42,  Mean reward: -1.396551724137931, Mean Entropy: 0.6239508986473083, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 43,  Mean reward: -1.2314814814814814, Mean Entropy: 0.6825820207595825, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 44,  Mean reward: -2.0588235294117645, Mean Entropy: 0.8546785116195679, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 45,  Mean reward: -1.4038461538461537, Mean Entropy: 0.8346731066703796, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.57s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 46,  Mean reward: 0.48936170212765956, Mean Entropy: 0.8273671865463257, complete_episode_count: 47.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 47,  Mean reward: -1.6555555555555554, Mean Entropy: 0.9039468169212341, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 48,  Mean reward: -3.731707317073171, Mean Entropy: 0.9339293837547302, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 49,  Mean reward: -1.434782608695652, Mean Entropy: 1.0299665927886963, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 50,  Mean reward: -6.511627906976744, Mean Entropy: 0.8816694021224976, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 51,  Mean reward: -1.2875, Mean Entropy: 0.8333964347839355, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 52,  Mean reward: -3.182926829268293, Mean Entropy: 0.8654836416244507, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 53,  Mean reward: -3.6739130434782608, Mean Entropy: 0.8916934728622437, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 54,  Mean reward: -2.2111111111111112, Mean Entropy: 0.8357076048851013, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 55,  Mean reward: 2.4716981132075473, Mean Entropy: 0.8802971839904785, complete_episode_count: 53.0, Gather time: 0.61s, Train time: 1.56s
Iteration: 56,  Mean reward: -2.0, Mean Entropy: 0.9053177833557129, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 57,  Mean reward: -2.3214285714285716, Mean Entropy: 0.8527180552482605, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 58,  Mean reward: 2.51, Mean Entropy: 0.7007896900177002, complete_episode_count: 50.0, Gather time: 0.60s, Train time: 1.61s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 59,  Mean reward: 4.215686274509804, Mean Entropy: 0.592492938041687, complete_episode_count: 51.0, Gather time: 0.60s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 60,  Mean reward: 5.313559322033898, Mean Entropy: 0.5552918314933777, complete_episode_count: 59.0, Gather time: 0.63s, Train time: 1.64s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 61,  Mean reward: 5.398305084745763, Mean Entropy: 0.6188690662384033, complete_episode_count: 59.0, Gather time: 0.61s, Train time: 1.59s
Iteration: 62,  Mean reward: 4.678571428571429, Mean Entropy: 0.522903561592102, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 63,  Mean reward: 6.729508196721311, Mean Entropy: 0.6617464423179626, complete_episode_count: 61.0, Gather time: 0.61s, Train time: 1.60s
Iteration: 64,  Mean reward: 5.141666666666667, Mean Entropy: 0.6360465884208679, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 65,  Mean reward: 5.052238805970149, Mean Entropy: 0.48697733879089355, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 66,  Mean reward: 7.2063492063492065, Mean Entropy: 0.37900614738464355, complete_episode_count: 63.0, Gather time: 0.62s, Train time: 1.57s
Iteration: 67,  Mean reward: 7.03125, Mean Entropy: 0.39819806814193726, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 68,  Mean reward: 6.951612903225806, Mean Entropy: 0.32152026891708374, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 69,  Mean reward: 7.683098591549296, Mean Entropy: 0.41292721033096313, complete_episode_count: 71.0, Gather time: 0.62s, Train time: 0.77s
Iteration: 70,  Mean reward: 4.774193548387097, Mean Entropy: 0.5934128761291504, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 71,  Mean reward: 7.307142857142857, Mean Entropy: 0.28103551268577576, complete_episode_count: 70.0, Gather time: 0.62s, Train time: 1.03s
Iteration: 72,  Mean reward: 7.6571428571428575, Mean Entropy: 0.16473045945167542, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 73,  Mean reward: 7.773972602739726, Mean Entropy: 0.16126781702041626, complete_episode_count: 73.0, Gather time: 0.63s, Train time: 0.82s
Iteration: 74,  Mean reward: 7.142857142857143, Mean Entropy: 0.13635700941085815, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 75,  Mean reward: 7.383116883116883, Mean Entropy: 0.11214753240346909, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 76,  Mean reward: 7.941558441558442, Mean Entropy: 0.09413331747055054, complete_episode_count: 77.0, Gather time: 0.64s, Train time: 0.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 77,  Mean reward: 7.981012658227848, Mean Entropy: 0.06869804859161377, complete_episode_count: 79.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 78,  Mean reward: 7.961538461538462, Mean Entropy: 0.4478161036968231, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 79,  Mean reward: 3.875, Mean Entropy: 0.793921172618866, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 80,  Mean reward: -4.526785714285714, Mean Entropy: 0.9650839567184448, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 81,  Mean reward: -3.1904761904761907, Mean Entropy: 0.9743832349777222, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 82,  Mean reward: -5.022222222222222, Mean Entropy: 0.9238622188568115, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 83,  Mean reward: -2.2023809523809526, Mean Entropy: 0.9169203639030457, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 84,  Mean reward: -4.666666666666667, Mean Entropy: 1.010591983795166, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 85,  Mean reward: -6.166666666666667, Mean Entropy: 0.9413391947746277, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 86,  Mean reward: -4.0, Mean Entropy: 0.8756362795829773, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 87,  Mean reward: -3.2261904761904763, Mean Entropy: 0.9176187515258789, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 88,  Mean reward: -5.031914893617022, Mean Entropy: 0.9414633512496948, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 89,  Mean reward: -2.6630434782608696, Mean Entropy: 0.9465274214744568, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 90,  Mean reward: -4.113636363636363, Mean Entropy: 0.8781266212463379, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 91,  Mean reward: -4.0638297872340425, Mean Entropy: 0.7647919654846191, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 92,  Mean reward: -1.2264150943396226, Mean Entropy: 0.8538939356803894, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 93,  Mean reward: -2.9591836734693877, Mean Entropy: 0.8834923505783081, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 94,  Mean reward: -1.6666666666666667, Mean Entropy: 0.7841023802757263, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 95,  Mean reward: -0.875, Mean Entropy: 0.6831929087638855, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 96,  Mean reward: -1.5803571428571428, Mean Entropy: 0.6779476404190063, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 97,  Mean reward: -0.04310344827586207, Mean Entropy: 0.6758222579956055, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 98,  Mean reward: -2.4017857142857144, Mean Entropy: 0.6525706648826599, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 99,  Mean reward: 1.2966101694915255, Mean Entropy: 0.6116703748703003, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.64s
Iteration: 100,  Mean reward: 3.9166666666666665, Mean Entropy: 0.612270712852478, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.58s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 5.466101694915254, Mean Entropy: 0.5729155540466309, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 102,  Mean reward: 4.398305084745763, Mean Entropy: 0.5604934692382812, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 103,  Mean reward: 3.669491525423729, Mean Entropy: 0.5825881361961365, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.55s
Iteration: 104,  Mean reward: 4.428571428571429, Mean Entropy: 0.5529056787490845, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 105,  Mean reward: 3.9461538461538463, Mean Entropy: 0.5077617168426514, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 106,  Mean reward: 5.267241379310345, Mean Entropy: 0.4425829350948334, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 107,  Mean reward: 6.0227272727272725, Mean Entropy: 0.46667248010635376, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 108,  Mean reward: 6.1461538461538465, Mean Entropy: 0.43689918518066406, complete_episode_count: 65.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 109,  Mean reward: 6.825757575757576, Mean Entropy: 0.40032219886779785, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 110,  Mean reward: 6.654411764705882, Mean Entropy: 0.41035959124565125, complete_episode_count: 68.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 111,  Mean reward: 5.7164179104477615, Mean Entropy: 0.33288729190826416, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 112,  Mean reward: 7.422535211267606, Mean Entropy: 0.3483719825744629, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 113,  Mean reward: 5.962121212121212, Mean Entropy: 0.26454371213912964, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 114,  Mean reward: 6.736486486486487, Mean Entropy: 0.27721381187438965, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 115,  Mean reward: 7.5588235294117645, Mean Entropy: 0.18511518836021423, complete_episode_count: 68.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 116,  Mean reward: 7.5, Mean Entropy: 0.14535613358020782, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 117,  Mean reward: 7.6381578947368425, Mean Entropy: 0.18812677264213562, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 118,  Mean reward: 2.8141025641025643, Mean Entropy: 0.11448895186185837, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 119,  Mean reward: 7.727848101265823, Mean Entropy: 0.15889209508895874, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 120,  Mean reward: 4.25, Mean Entropy: 0.09178324043750763, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 121,  Mean reward: 7.474683544303797, Mean Entropy: 0.08090289682149887, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 122,  Mean reward: 7.961538461538462, Mean Entropy: 0.04612552374601364, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 123,  Mean reward: 7.75, Mean Entropy: 0.06936950236558914, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 124,  Mean reward: 7.0, Mean Entropy: 0.02742168866097927, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.92s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 125,  Mean reward: 8.0, Mean Entropy: 0.027216818183660507, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 126,  Mean reward: 8.0, Mean Entropy: 0.5373152494430542, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 127,  Mean reward: 0.37012987012987014, Mean Entropy: 0.2927302122116089, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 128,  Mean reward: -1.639240506329114, Mean Entropy: 0.2541061043739319, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 129,  Mean reward: 2.25, Mean Entropy: 0.24296395480632782, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 130,  Mean reward: 1.5, Mean Entropy: 0.21831786632537842, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 131,  Mean reward: -0.25, Mean Entropy: 0.2866981625556946, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 132,  Mean reward: 0.25, Mean Entropy: 0.26786181330680847, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 133,  Mean reward: 3.25, Mean Entropy: 0.17646390199661255, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 134,  Mean reward: 6.189873417721519, Mean Entropy: 0.10921192169189453, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 135,  Mean reward: 7.883116883116883, Mean Entropy: 0.10255472362041473, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 136,  Mean reward: 8.0, Mean Entropy: 0.07979826629161835, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 137,  Mean reward: 7.173076923076923, Mean Entropy: 0.06218677759170532, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 138,  Mean reward: 8.0, Mean Entropy: 0.05766028165817261, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 139,  Mean reward: 7.9423076923076925, Mean Entropy: 0.04073790833353996, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.76s
Iteration: 140,  Mean reward: 7.75, Mean Entropy: 0.05910869687795639, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 141,  Mean reward: 7.981012658227848, Mean Entropy: 0.1267952024936676, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 142,  Mean reward: 4.25, Mean Entropy: 0.1635621190071106, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 143,  Mean reward: 4.0, Mean Entropy: 0.024569258093833923, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 1.02s
Iteration: 144,  Mean reward: 8.0, Mean Entropy: 0.038887202739715576, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 145,  Mean reward: 8.0, Mean Entropy: 0.1073124036192894, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.93s
Iteration: 146,  Mean reward: -0.5, Mean Entropy: 0.05351956933736801, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 147,  Mean reward: -1.8924050632911393, Mean Entropy: 0.07571103423833847, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 148,  Mean reward: -0.25, Mean Entropy: 0.12538793683052063, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 149,  Mean reward: 0.8924050632911392, Mean Entropy: 0.21238842606544495, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 150,  Mean reward: 4.045454545454546, Mean Entropy: 0.08715462684631348, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 151,  Mean reward: 7.0, Mean Entropy: 0.010146142914891243, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 152,  Mean reward: 8.0, Mean Entropy: 0.0057316916063427925, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 153,  Mean reward: 8.0, Mean Entropy: 0.007133241277188063, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 154,  Mean reward: 8.0, Mean Entropy: 0.01270584762096405, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 155,  Mean reward: 8.0, Mean Entropy: 0.1649860143661499, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 156,  Mean reward: 2.1582278481012658, Mean Entropy: 0.39513087272644043, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 157,  Mean reward: -1.1883116883116882, Mean Entropy: 0.5514299869537354, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 158,  Mean reward: -0.8716216216216216, Mean Entropy: 0.5403772592544556, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 159,  Mean reward: -0.5592105263157895, Mean Entropy: 0.461036741733551, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 160,  Mean reward: 1.019736842105263, Mean Entropy: 0.3774039149284363, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 161,  Mean reward: 2.411392405063291, Mean Entropy: 0.09188386052846909, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 162,  Mean reward: 7.75, Mean Entropy: 0.008507108315825462, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 163,  Mean reward: 8.0, Mean Entropy: 0.017052901908755302, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 164,  Mean reward: 7.961538461538462, Mean Entropy: 0.006787043064832687, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 165,  Mean reward: 8.0, Mean Entropy: 0.01244333665817976, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 166,  Mean reward: 8.0, Mean Entropy: 0.014622852206230164, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.94s
Iteration: 167,  Mean reward: 8.0, Mean Entropy: 0.09945935010910034, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 168,  Mean reward: 5.25, Mean Entropy: 0.04335598275065422, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 169,  Mean reward: 8.0, Mean Entropy: 0.17090992629528046, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 170,  Mean reward: 4.1835443037974684, Mean Entropy: 0.15585242211818695, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 171,  Mean reward: 5.19620253164557, Mean Entropy: 0.03962013125419617, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 172,  Mean reward: 8.0, Mean Entropy: 0.034924671053886414, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 173,  Mean reward: 8.0, Mean Entropy: 0.16009832918643951, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 174,  Mean reward: 2.25, Mean Entropy: 0.12392427027225494, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 175,  Mean reward: 7.2215189873417724, Mean Entropy: 0.019775083288550377, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 176,  Mean reward: 7.75, Mean Entropy: 0.007954781875014305, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 177,  Mean reward: 8.0, Mean Entropy: 0.009355604648590088, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 178,  Mean reward: 8.0, Mean Entropy: 0.018656430765986443, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 179,  Mean reward: 8.0, Mean Entropy: 0.07018536329269409, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 180,  Mean reward: -3.25, Mean Entropy: 0.02984478324651718, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 181,  Mean reward: -2.0, Mean Entropy: 0.013990610837936401, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 1.01s
Iteration: 182,  Mean reward: -2.0, Mean Entropy: 0.009131496772170067, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 183,  Mean reward: -3.25, Mean Entropy: 0.006319262087345123, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 184,  Mean reward: -2.75, Mean Entropy: 0.007896512746810913, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 185,  Mean reward: -2.5, Mean Entropy: 0.012901469133794308, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 186,  Mean reward: -3.0, Mean Entropy: 0.1283491849899292, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 187,  Mean reward: 0.0, Mean Entropy: 0.4167773425579071, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.89s
Iteration: 188,  Mean reward: -3.323943661971831, Mean Entropy: 0.7864575386047363, complete_episode_count: 71.0, Gather time: 0.65s, Train time: 0.81s
Iteration: 189,  Mean reward: -2.598360655737705, Mean Entropy: 0.7629562616348267, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 190,  Mean reward: -1.043859649122807, Mean Entropy: 0.6566729545593262, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 191,  Mean reward: 1.828358208955224, Mean Entropy: 0.5206371545791626, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 192,  Mean reward: 4.7272727272727275, Mean Entropy: 0.3792365491390228, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 193,  Mean reward: 7.15625, Mean Entropy: 0.21491703391075134, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 194,  Mean reward: 7.151515151515151, Mean Entropy: 0.277137815952301, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 195,  Mean reward: 6.313432835820896, Mean Entropy: 0.2286733239889145, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 196,  Mean reward: 7.614285714285714, Mean Entropy: 0.2391965389251709, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 197,  Mean reward: 7.75, Mean Entropy: 0.16969548165798187, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 198,  Mean reward: 7.683098591549296, Mean Entropy: 0.23670315742492676, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 199,  Mean reward: 6.020833333333333, Mean Entropy: 0.11529677361249924, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 200,  Mean reward: 7.842105263157895, Mean Entropy: 0.07935754954814911, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.78s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 7.881578947368421, Mean Entropy: 0.042646780610084534, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 202,  Mean reward: 7.981012658227848, Mean Entropy: 0.15570068359375, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 203,  Mean reward: -0.7564102564102564, Mean Entropy: 0.04032198339700699, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 204,  Mean reward: -4.170886075949367, Mean Entropy: 0.011358419433236122, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 205,  Mean reward: -0.75, Mean Entropy: 0.021043939515948296, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 206,  Mean reward: -3.411392405063291, Mean Entropy: 0.5717684030532837, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 207,  Mean reward: -1.4166666666666667, Mean Entropy: 0.5925354957580566, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 208,  Mean reward: 0.47794117647058826, Mean Entropy: 0.6960824728012085, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 209,  Mean reward: -0.16666666666666666, Mean Entropy: 0.6972987651824951, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 210,  Mean reward: 0.7164179104477612, Mean Entropy: 0.4749913215637207, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 211,  Mean reward: 5.669014084507042, Mean Entropy: 0.14540934562683105, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 212,  Mean reward: 7.9423076923076925, Mean Entropy: 0.10652729123830795, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 213,  Mean reward: 7.205479452054795, Mean Entropy: 0.31933459639549255, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 214,  Mean reward: 5.3768115942028984, Mean Entropy: 0.33149251341819763, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 215,  Mean reward: 5.864285714285714, Mean Entropy: 0.32946985960006714, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 216,  Mean reward: 6.514285714285714, Mean Entropy: 0.21974459290504456, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 217,  Mean reward: 7.797297297297297, Mean Entropy: 0.26358360052108765, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 218,  Mean reward: 7.219178082191781, Mean Entropy: 0.23144221305847168, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 219,  Mean reward: 7.84, Mean Entropy: 0.15714578330516815, complete_episode_count: 75.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 220,  Mean reward: 7.84, Mean Entropy: 0.1168963611125946, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 1.02s
Iteration: 221,  Mean reward: 7.383116883116883, Mean Entropy: 0.08335511386394501, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 222,  Mean reward: 7.962025316455696, Mean Entropy: 0.05372332036495209, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 223,  Mean reward: 7.961538461538462, Mean Entropy: 0.05993540212512016, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 224,  Mean reward: 7.25, Mean Entropy: 0.0681382343173027, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 225,  Mean reward: 7.5, Mean Entropy: 0.030254539102315903, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 226,  Mean reward: 7.75, Mean Entropy: 0.015634017065167427, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 227,  Mean reward: 8.0, Mean Entropy: 0.020528826862573624, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 228,  Mean reward: 7.981012658227848, Mean Entropy: 0.042270831763744354, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 229,  Mean reward: 6.0, Mean Entropy: 0.021622832864522934, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 230,  Mean reward: 7.5, Mean Entropy: 0.007783064618706703, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 231,  Mean reward: 8.0, Mean Entropy: 0.018147513270378113, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 232,  Mean reward: 8.0, Mean Entropy: 0.16734562814235687, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 1.04s
Iteration: 233,  Mean reward: 6.22, Mean Entropy: 0.42767757177352905, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 234,  Mean reward: 6.3428571428571425, Mean Entropy: 0.28459811210632324, complete_episode_count: 70.0, Gather time: 0.66s, Train time: 0.79s
Iteration: 235,  Mean reward: 7.536764705882353, Mean Entropy: 0.07547709345817566, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 236,  Mean reward: 7.922077922077922, Mean Entropy: 0.028482889756560326, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 237,  Mean reward: 8.0, Mean Entropy: 0.12381881475448608, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 238,  Mean reward: 7.215277777777778, Mean Entropy: 0.3876146674156189, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 239,  Mean reward: 6.090909090909091, Mean Entropy: 0.34700387716293335, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 240,  Mean reward: 7.451388888888889, Mean Entropy: 0.17274872958660126, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 241,  Mean reward: 7.84, Mean Entropy: 0.10238109529018402, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 242,  Mean reward: 7.881578947368421, Mean Entropy: 0.033471010625362396, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 243,  Mean reward: 7.981012658227848, Mean Entropy: 0.20624896883964539, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 244,  Mean reward: -0.25, Mean Entropy: 0.4172890782356262, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 245,  Mean reward: 1.7884615384615385, Mean Entropy: 0.35631924867630005, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 246,  Mean reward: 4.782894736842105, Mean Entropy: 0.19902269542217255, complete_episode_count: 76.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 247,  Mean reward: 6.166666666666667, Mean Entropy: 0.11604940891265869, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 248,  Mean reward: 6.734177215189874, Mean Entropy: 0.021262651309370995, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 249,  Mean reward: 7.981012658227848, Mean Entropy: 0.025082360953092575, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 250,  Mean reward: 7.961538461538462, Mean Entropy: 0.018100615590810776, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.85s
Iteration: 251,  Mean reward: 8.0, Mean Entropy: 0.03495310992002487, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 252,  Mean reward: 7.981012658227848, Mean Entropy: 0.023701710626482964, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 253,  Mean reward: 8.0, Mean Entropy: 0.007471737451851368, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 254,  Mean reward: 7.981012658227848, Mean Entropy: 0.02353658899664879, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 255,  Mean reward: 7.5, Mean Entropy: 0.009241512045264244, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 256,  Mean reward: 7.75, Mean Entropy: 0.007887514308094978, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 257,  Mean reward: 8.0, Mean Entropy: 0.004517574794590473, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 258,  Mean reward: 8.0, Mean Entropy: 0.0975562259554863, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.03s
Iteration: 259,  Mean reward: 6.195945945945946, Mean Entropy: 0.4955320358276367, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 260,  Mean reward: 5.064285714285714, Mean Entropy: 0.3794911801815033, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 261,  Mean reward: 7.006756756756757, Mean Entropy: 0.17175054550170898, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 262,  Mean reward: 7.902597402597403, Mean Entropy: 0.08991651982069016, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 263,  Mean reward: 7.86, Mean Entropy: 0.05958832427859306, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 264,  Mean reward: 8.0, Mean Entropy: 0.04245053976774216, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 265,  Mean reward: 8.0, Mean Entropy: 0.009588804095983505, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 266,  Mean reward: 8.0, Mean Entropy: 0.2597707509994507, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 267,  Mean reward: 0.38961038961038963, Mean Entropy: 0.2822555899620056, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 268,  Mean reward: -0.5, Mean Entropy: 0.32954955101013184, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 269,  Mean reward: -1.0657894736842106, Mean Entropy: 0.3807395398616791, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 270,  Mean reward: -1.3092105263157894, Mean Entropy: 0.5034859776496887, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 271,  Mean reward: -0.581081081081081, Mean Entropy: 0.5348207354545593, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 272,  Mean reward: 0.12666666666666668, Mean Entropy: 0.5161644220352173, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 273,  Mean reward: -0.11038961038961038, Mean Entropy: 0.42926549911499023, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 274,  Mean reward: -0.6265822784810127, Mean Entropy: 0.418144553899765, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 275,  Mean reward: 1.1733333333333333, Mean Entropy: 0.2645125389099121, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 276,  Mean reward: 5.378205128205129, Mean Entropy: 0.13748130202293396, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 277,  Mean reward: 6.25, Mean Entropy: 0.005943679716438055, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.83s
Iteration: 278,  Mean reward: 8.0, Mean Entropy: 0.11230089515447617, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 279,  Mean reward: 6.5, Mean Entropy: 0.16857528686523438, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 280,  Mean reward: 5.19620253164557, Mean Entropy: 0.04540659487247467, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.85s
Iteration: 281,  Mean reward: 7.75, Mean Entropy: 0.014415240846574306, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 282,  Mean reward: 8.0, Mean Entropy: 0.011398553848266602, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 283,  Mean reward: 8.0, Mean Entropy: 0.014085639268159866, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 284,  Mean reward: 7.5, Mean Entropy: 0.18810418248176575, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 285,  Mean reward: 5.0, Mean Entropy: 0.019086085259914398, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 286,  Mean reward: 8.0, Mean Entropy: 0.08061941713094711, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 287,  Mean reward: -1.0, Mean Entropy: 0.041317544877529144, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 288,  Mean reward: -1.25, Mean Entropy: 0.15640103816986084, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 289,  Mean reward: 4.25, Mean Entropy: 0.033243753015995026, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 290,  Mean reward: 7.75, Mean Entropy: 0.04906918853521347, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 291,  Mean reward: 7.75, Mean Entropy: 0.0073688095435500145, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 292,  Mean reward: 8.0, Mean Entropy: 0.005127071402966976, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 293,  Mean reward: 8.0, Mean Entropy: 0.03467715531587601, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 294,  Mean reward: 7.75, Mean Entropy: 0.002293948084115982, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 295,  Mean reward: 8.0, Mean Entropy: 0.03285884112119675, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 296,  Mean reward: 7.25, Mean Entropy: 0.020710546523332596, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.05s
Iteration: 297,  Mean reward: 7.5, Mean Entropy: 0.004301968030631542, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 298,  Mean reward: 8.0, Mean Entropy: 0.05122332274913788, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 299,  Mean reward: 7.0, Mean Entropy: 0.005041840486228466, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 300,  Mean reward: 8.0, Mean Entropy: 0.0028594720643013716, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 8.0, Mean Entropy: 0.004814440384507179, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 302,  Mean reward: 7.75, Mean Entropy: 0.002251191297546029, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 303,  Mean reward: 8.0, Mean Entropy: 0.0013630869798362255, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 304,  Mean reward: 8.0, Mean Entropy: 0.0008578839479014277, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 305,  Mean reward: 8.0, Mean Entropy: 0.0008410752634517848, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 306,  Mean reward: 8.0, Mean Entropy: 0.0014804606325924397, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 307,  Mean reward: 8.0, Mean Entropy: 0.005586724728345871, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 308,  Mean reward: 8.0, Mean Entropy: 0.053921036422252655, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 309,  Mean reward: 5.25, Mean Entropy: 0.03150847554206848, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 310,  Mean reward: 7.75, Mean Entropy: 0.017069730907678604, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 311,  Mean reward: 8.0, Mean Entropy: 0.024516349658370018, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 312,  Mean reward: 7.5, Mean Entropy: 0.001549302483908832, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 313,  Mean reward: 8.0, Mean Entropy: 0.0005120843998156488, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 314,  Mean reward: 8.0, Mean Entropy: 0.00037954788422212005, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 315,  Mean reward: 8.0, Mean Entropy: 0.00024065081379376352, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 316,  Mean reward: 8.0, Mean Entropy: 0.00023827183758839965, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 317,  Mean reward: 8.0, Mean Entropy: 0.00010811141692101955, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 318,  Mean reward: 8.0, Mean Entropy: 0.0002848082804121077, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 319,  Mean reward: 8.0, Mean Entropy: 0.00023853886523284018, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 320,  Mean reward: 8.0, Mean Entropy: 0.00022468523820862174, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 321,  Mean reward: 8.0, Mean Entropy: 0.0002668381785042584, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 322,  Mean reward: 8.0, Mean Entropy: 0.00021647621178999543, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 323,  Mean reward: 8.0, Mean Entropy: 0.0003388065379112959, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 324,  Mean reward: 8.0, Mean Entropy: 0.0001989824668271467, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 325,  Mean reward: 8.0, Mean Entropy: 0.00038911422598175704, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 326,  Mean reward: 8.0, Mean Entropy: 0.00047913932939991355, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 327,  Mean reward: 8.0, Mean Entropy: 0.0007448887336067855, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 328,  Mean reward: 8.0, Mean Entropy: 0.0010232708882540464, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 329,  Mean reward: 8.0, Mean Entropy: 0.0015581727493554354, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 330,  Mean reward: 8.0, Mean Entropy: 0.0334235280752182, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 331,  Mean reward: 7.0, Mean Entropy: 0.0025614574551582336, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 332,  Mean reward: 8.0, Mean Entropy: 0.0001648507604841143, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 333,  Mean reward: 8.0, Mean Entropy: 0.00011192201054655015, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 334,  Mean reward: 8.0, Mean Entropy: 9.375605441164225e-05, complete_episode_count: 80.0, Gather time: 0.80s, Train time: 0.82s
Iteration: 335,  Mean reward: 8.0, Mean Entropy: 0.00011003173858625814, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 336,  Mean reward: 8.0, Mean Entropy: 0.00010410246613901109, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 337,  Mean reward: 8.0, Mean Entropy: 8.508993778377771e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 338,  Mean reward: 8.0, Mean Entropy: 8.170231012627482e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 339,  Mean reward: 8.0, Mean Entropy: 6.342161213979125e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 340,  Mean reward: 8.0, Mean Entropy: 0.00013664786820299923, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 341,  Mean reward: 8.0, Mean Entropy: 0.0001532350725028664, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 342,  Mean reward: 8.0, Mean Entropy: 0.0001484559033997357, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 343,  Mean reward: 8.0, Mean Entropy: 0.00012707908172160387, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 344,  Mean reward: 8.0, Mean Entropy: 0.00019750365754589438, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 345,  Mean reward: 8.0, Mean Entropy: 0.00021131258108653128, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 346,  Mean reward: 8.0, Mean Entropy: 0.00026670872466638684, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 347,  Mean reward: 8.0, Mean Entropy: 0.0003437662380747497, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 348,  Mean reward: 8.0, Mean Entropy: 0.0005201557069085538, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 349,  Mean reward: 8.0, Mean Entropy: 0.0011032536858692765, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 350,  Mean reward: 8.0, Mean Entropy: 0.0011484726564958692, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 351,  Mean reward: 8.0, Mean Entropy: 0.024277061223983765, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 352,  Mean reward: 7.5, Mean Entropy: 0.001221891026943922, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 353,  Mean reward: 8.0, Mean Entropy: 0.0074050938710570335, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 354,  Mean reward: 7.75, Mean Entropy: 0.0006580741610378027, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 355,  Mean reward: 8.0, Mean Entropy: 0.00027390324976295233, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 356,  Mean reward: 8.0, Mean Entropy: 0.0001678109692875296, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 357,  Mean reward: 8.0, Mean Entropy: 9.340579708805308e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 358,  Mean reward: 8.0, Mean Entropy: 6.293093611020595e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 359,  Mean reward: 8.0, Mean Entropy: 6.155925075290725e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 360,  Mean reward: 8.0, Mean Entropy: 3.8585909351240844e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 361,  Mean reward: 8.0, Mean Entropy: 3.412902515265159e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 362,  Mean reward: 8.0, Mean Entropy: 2.8548518457682803e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 363,  Mean reward: 8.0, Mean Entropy: 2.9350716431508772e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 364,  Mean reward: 8.0, Mean Entropy: 2.073411815217696e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 365,  Mean reward: 8.0, Mean Entropy: 2.3291800971492194e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 366,  Mean reward: 8.0, Mean Entropy: 2.6378915208624676e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 367,  Mean reward: 8.0, Mean Entropy: 1.804958446882665e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 368,  Mean reward: 8.0, Mean Entropy: 1.8732318494585343e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 369,  Mean reward: 8.0, Mean Entropy: 2.0914168999297544e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 370,  Mean reward: 8.0, Mean Entropy: 1.3284225133247674e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 371,  Mean reward: 8.0, Mean Entropy: 1.8940860172733665e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 372,  Mean reward: 8.0, Mean Entropy: 1.4229788575903513e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 373,  Mean reward: 8.0, Mean Entropy: 1.493839044997003e-05, complete_episode_count: 80.0, Gather time: 0.80s, Train time: 0.81s
Iteration: 374,  Mean reward: 8.0, Mean Entropy: 1.5135983630898409e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 375,  Mean reward: 8.0, Mean Entropy: 1.1853340765810572e-05, complete_episode_count: 80.0, Gather time: 0.64s, Train time: 0.77s
Iteration: 376,  Mean reward: 8.0, Mean Entropy: 1.335122487944318e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 377,  Mean reward: 8.0, Mean Entropy: 1.4210818335413933e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 378,  Mean reward: 8.0, Mean Entropy: 1.1737656677723862e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 379,  Mean reward: 8.0, Mean Entropy: 1.2120081009925343e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 380,  Mean reward: 8.0, Mean Entropy: 1.3042537830187939e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 381,  Mean reward: 8.0, Mean Entropy: 1.3937407857156359e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 382,  Mean reward: 8.0, Mean Entropy: 1.532678106741514e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 383,  Mean reward: 8.0, Mean Entropy: 1.3093951565679163e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 384,  Mean reward: 8.0, Mean Entropy: 1.1462560905783903e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 385,  Mean reward: 8.0, Mean Entropy: 1.1392779924790375e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 386,  Mean reward: 8.0, Mean Entropy: 1.1822015039797407e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 1.3279413906275295e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 1.1240314051974565e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 389,  Mean reward: 8.0, Mean Entropy: 1.5731984603917226e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 390,  Mean reward: 8.0, Mean Entropy: 1.4143177395453677e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 1.4600834219891112e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.87s
Iteration: 392,  Mean reward: 8.0, Mean Entropy: 1.2611468264367431e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 1.1629413165792357e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 1.3123655662639067e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 395,  Mean reward: 8.0, Mean Entropy: 1.3633216440211982e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 396,  Mean reward: 8.0, Mean Entropy: 1.1645034646790009e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 1.0152873073820956e-05, complete_episode_count: 80.0, Gather time: 0.75s, Train time: 0.82s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 1.4193936294759624e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 1.020783201965969e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 1.4285891666077077e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 8.0, Mean Entropy: 1.0787454812088981e-05, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.85s
Iteration: 402,  Mean reward: 8.0, Mean Entropy: 1.489089845563285e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 403,  Mean reward: 8.0, Mean Entropy: 1.1880900274263695e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 404,  Mean reward: 8.0, Mean Entropy: 1.702107510936912e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 405,  Mean reward: 8.0, Mean Entropy: 1.3461219168675598e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 1.0960602594423108e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 407,  Mean reward: 8.0, Mean Entropy: 1.3119766663294286e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 8.50831565912813e-06, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 1.3851611583959311e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 410,  Mean reward: 8.0, Mean Entropy: 1.1279039426881354e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.00s
Iteration: 411,  Mean reward: 8.0, Mean Entropy: 1.2426069588400424e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 412,  Mean reward: 8.0, Mean Entropy: 1.3581501661974471e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 413,  Mean reward: 8.0, Mean Entropy: 1.3139624570612796e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 7.777505743433721e-06, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 415,  Mean reward: 8.0, Mean Entropy: 1.2775602954206988e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 416,  Mean reward: 8.0, Mean Entropy: 1.4476543583441526e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 417,  Mean reward: 8.0, Mean Entropy: 1.5131363397813402e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 418,  Mean reward: 8.0, Mean Entropy: 1.3583302461483981e-05, complete_episode_count: 80.0, Gather time: 0.75s, Train time: 0.83s
Iteration: 419,  Mean reward: 8.0, Mean Entropy: 1.0877185559365898e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 1.4392264347407036e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 421,  Mean reward: 8.0, Mean Entropy: 1.3915999261371326e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 1.4618398381571751e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 423,  Mean reward: 8.0, Mean Entropy: 1.0056965948024299e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 424,  Mean reward: 8.0, Mean Entropy: 1.3713291991734877e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 1.2061987945344299e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 1.8182241547037847e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 427,  Mean reward: 8.0, Mean Entropy: 1.707095725578256e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 428,  Mean reward: 8.0, Mean Entropy: 1.6616466382401995e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 429,  Mean reward: 8.0, Mean Entropy: 1.6821943063405342e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 430,  Mean reward: 8.0, Mean Entropy: 2.0175833924440667e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 1.0266967365168966e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 432,  Mean reward: 8.0, Mean Entropy: 1.5586945664836094e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 433,  Mean reward: 8.0, Mean Entropy: 1.9019056708202697e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 434,  Mean reward: 8.0, Mean Entropy: 1.57593131007161e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 435,  Mean reward: 8.0, Mean Entropy: 1.4727502275491133e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 436,  Mean reward: 8.0, Mean Entropy: 1.6499874618602917e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 437,  Mean reward: 8.0, Mean Entropy: 1.5409645129693672e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 438,  Mean reward: 8.0, Mean Entropy: 2.0225030311848968e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 1.8268556232214905e-05, complete_episode_count: 80.0, Gather time: 0.74s, Train time: 0.80s
Iteration: 440,  Mean reward: 8.0, Mean Entropy: 2.0810897694900632e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 441,  Mean reward: 8.0, Mean Entropy: 2.031831354543101e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 2.3842538212193176e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 443,  Mean reward: 8.0, Mean Entropy: 2.0246126950951293e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 444,  Mean reward: 8.0, Mean Entropy: 1.9819683075184003e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 445,  Mean reward: 8.0, Mean Entropy: 2.272915116918739e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 446,  Mean reward: 8.0, Mean Entropy: 1.8915399778052233e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 447,  Mean reward: 8.0, Mean Entropy: 1.8484541215002537e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 448,  Mean reward: 8.0, Mean Entropy: 2.334707460249774e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.00s
Iteration: 449,  Mean reward: 8.0, Mean Entropy: 1.7850126823759638e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 450,  Mean reward: 8.0, Mean Entropy: 2.8199337975820526e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 451,  Mean reward: 8.0, Mean Entropy: 2.332917028979864e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 452,  Mean reward: 8.0, Mean Entropy: 2.6386105673736893e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 453,  Mean reward: 8.0, Mean Entropy: 3.2445721444673836e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 454,  Mean reward: 8.0, Mean Entropy: 2.6105753931915388e-05, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 455,  Mean reward: 8.0, Mean Entropy: 2.0699973902083002e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 456,  Mean reward: 8.0, Mean Entropy: 2.5453700800426304e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 457,  Mean reward: 8.0, Mean Entropy: 2.8075359296053648e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 458,  Mean reward: 8.0, Mean Entropy: 2.8795446269214153e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.87s
Iteration: 459,  Mean reward: 8.0, Mean Entropy: 2.5641173124313354e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 460,  Mean reward: 8.0, Mean Entropy: 2.739538103924133e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 461,  Mean reward: 8.0, Mean Entropy: 3.331961124786176e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 462,  Mean reward: 8.0, Mean Entropy: 3.0123563192319125e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 463,  Mean reward: 8.0, Mean Entropy: 2.7888952899957076e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 464,  Mean reward: 8.0, Mean Entropy: 2.912853960879147e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 465,  Mean reward: 8.0, Mean Entropy: 3.6908848414896056e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 466,  Mean reward: 8.0, Mean Entropy: 3.9012415072647855e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 467,  Mean reward: 8.0, Mean Entropy: 4.4974782213103026e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 468,  Mean reward: 8.0, Mean Entropy: 4.3524265493033454e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 469,  Mean reward: 8.0, Mean Entropy: 4.4687476474791765e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 470,  Mean reward: 8.0, Mean Entropy: 3.8198089896468446e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 471,  Mean reward: 8.0, Mean Entropy: 3.0289411370176822e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 472,  Mean reward: 8.0, Mean Entropy: 3.58973557013087e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 473,  Mean reward: 8.0, Mean Entropy: 4.15024405810982e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 474,  Mean reward: 8.0, Mean Entropy: 6.006167677696794e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 475,  Mean reward: 8.0, Mean Entropy: 5.3220457630231977e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 476,  Mean reward: 8.0, Mean Entropy: 6.114938150858507e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 477,  Mean reward: 8.0, Mean Entropy: 4.832550621358678e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 478,  Mean reward: 8.0, Mean Entropy: 4.624481880455278e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 479,  Mean reward: 8.0, Mean Entropy: 3.9521193684777245e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 480,  Mean reward: 8.0, Mean Entropy: 4.3650390580296516e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 481,  Mean reward: 8.0, Mean Entropy: 5.82099673920311e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 482,  Mean reward: 8.0, Mean Entropy: 6.0126720200059935e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 483,  Mean reward: 8.0, Mean Entropy: 8.152936061378568e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 484,  Mean reward: 8.0, Mean Entropy: 7.25878489902243e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 485,  Mean reward: 8.0, Mean Entropy: 5.2862065786030143e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 486,  Mean reward: 8.0, Mean Entropy: 3.9730482967570424e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.01s
Iteration: 487,  Mean reward: 8.0, Mean Entropy: 5.81682070333045e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 488,  Mean reward: 8.0, Mean Entropy: 7.449869008269161e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 489,  Mean reward: 8.0, Mean Entropy: 8.659008017275482e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 490,  Mean reward: 8.0, Mean Entropy: 8.902314584702253e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 491,  Mean reward: 8.0, Mean Entropy: 8.231826359406114e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 492,  Mean reward: 8.0, Mean Entropy: 6.962878978811204e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 493,  Mean reward: 8.0, Mean Entropy: 5.83955661568325e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 494,  Mean reward: 8.0, Mean Entropy: 6.315294012892991e-05, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 495,  Mean reward: 8.0, Mean Entropy: 8.129504567477852e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 496,  Mean reward: 8.0, Mean Entropy: 0.00010647560702636838, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 497,  Mean reward: 8.0, Mean Entropy: 0.00013650296023115516, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 498,  Mean reward: 8.0, Mean Entropy: 7.419209578074515e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 499,  Mean reward: 8.0, Mean Entropy: 6.3979925471358e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 500,  Mean reward: 8.0, Mean Entropy: 7.569276203867048e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 8.0, Mean Entropy: 7.824109343346208e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 502,  Mean reward: 8.0, Mean Entropy: 0.00010712510993471369, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 503,  Mean reward: 8.0, Mean Entropy: 0.0001323955220868811, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 504,  Mean reward: 8.0, Mean Entropy: 0.00015462288865819573, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 505,  Mean reward: 8.0, Mean Entropy: 8.92849566298537e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 506,  Mean reward: 8.0, Mean Entropy: 7.612667832290754e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 507,  Mean reward: 8.0, Mean Entropy: 6.407468754332513e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 508,  Mean reward: 8.0, Mean Entropy: 0.00010243091674055904, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 509,  Mean reward: 8.0, Mean Entropy: 0.00013741705333814025, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 510,  Mean reward: 8.0, Mean Entropy: 0.00017366019892506301, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 511,  Mean reward: 8.0, Mean Entropy: 0.00012397876707836986, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 512,  Mean reward: 8.0, Mean Entropy: 0.0001028968472382985, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 513,  Mean reward: 8.0, Mean Entropy: 6.995291914790869e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 514,  Mean reward: 8.0, Mean Entropy: 6.754015339538455e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 515,  Mean reward: 8.0, Mean Entropy: 8.937505481299013e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 516,  Mean reward: 8.0, Mean Entropy: 0.00016921250789891928, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 517,  Mean reward: 8.0, Mean Entropy: 0.00023092555056791753, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 518,  Mean reward: 8.0, Mean Entropy: 0.0001727636845316738, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 519,  Mean reward: 8.0, Mean Entropy: 6.439600110752508e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 520,  Mean reward: 8.0, Mean Entropy: 5.905425496166572e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 521,  Mean reward: 8.0, Mean Entropy: 7.870303670642897e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 522,  Mean reward: 8.0, Mean Entropy: 0.00013181444955989718, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 523,  Mean reward: 8.0, Mean Entropy: 0.00019677150703500956, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 524,  Mean reward: 8.0, Mean Entropy: 0.0002182702301070094, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.01s
Iteration: 525,  Mean reward: 8.0, Mean Entropy: 9.469224460190162e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 526,  Mean reward: 8.0, Mean Entropy: 8.919685933506116e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 527,  Mean reward: 8.0, Mean Entropy: 7.925032696221024e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 528,  Mean reward: 8.0, Mean Entropy: 7.365095370914787e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 529,  Mean reward: 8.0, Mean Entropy: 0.00011833191820187494, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 530,  Mean reward: 8.0, Mean Entropy: 0.00020745419897139072, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 531,  Mean reward: 8.0, Mean Entropy: 0.0002516253152862191, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 532,  Mean reward: 8.0, Mean Entropy: 0.0001146038921433501, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 533,  Mean reward: 8.0, Mean Entropy: 8.489882748108357e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 534,  Mean reward: 8.0, Mean Entropy: 7.616076618432999e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 535,  Mean reward: 8.0, Mean Entropy: 0.0001710003416519612, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 536,  Mean reward: 8.0, Mean Entropy: 0.00029704489861615, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 537,  Mean reward: 8.0, Mean Entropy: 0.00026491069002076983, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 538,  Mean reward: 8.0, Mean Entropy: 0.00013573328033089638, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 539,  Mean reward: 8.0, Mean Entropy: 8.465340215479955e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 540,  Mean reward: 8.0, Mean Entropy: 6.924829358467832e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 541,  Mean reward: 8.0, Mean Entropy: 7.375507266260684e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 542,  Mean reward: 8.0, Mean Entropy: 0.0001385941286571324, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 543,  Mean reward: 8.0, Mean Entropy: 0.0002825392293743789, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 544,  Mean reward: 8.0, Mean Entropy: 0.0001721386652207002, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 545,  Mean reward: 8.0, Mean Entropy: 0.00013138311624061316, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 546,  Mean reward: 8.0, Mean Entropy: 4.963394530932419e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 547,  Mean reward: 8.0, Mean Entropy: 7.9915756941773e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 548,  Mean reward: 8.0, Mean Entropy: 7.564878615085036e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 549,  Mean reward: 8.0, Mean Entropy: 0.00017418926290702075, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 550,  Mean reward: 8.0, Mean Entropy: 0.0002241865440737456, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 551,  Mean reward: 8.0, Mean Entropy: 0.00016134128964040428, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 552,  Mean reward: 8.0, Mean Entropy: 9.756924555404112e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 553,  Mean reward: 8.0, Mean Entropy: 9.289417357649654e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 554,  Mean reward: 8.0, Mean Entropy: 7.856454612920061e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 555,  Mean reward: 8.0, Mean Entropy: 0.00013295623648446053, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 556,  Mean reward: 8.0, Mean Entropy: 0.00024959593429230154, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.86s
Iteration: 557,  Mean reward: 8.0, Mean Entropy: 0.00019722161232493818, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.00011734940198948607, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 7.239951082738116e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 560,  Mean reward: 8.0, Mean Entropy: 8.696356962900609e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 561,  Mean reward: 8.0, Mean Entropy: 0.00012229790445417166, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 562,  Mean reward: 8.0, Mean Entropy: 0.00018558965530246496, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 563,  Mean reward: 8.0, Mean Entropy: 0.0002241353940917179, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 564,  Mean reward: 8.0, Mean Entropy: 0.0001408192765666172, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 565,  Mean reward: 8.0, Mean Entropy: 5.781136860605329e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 566,  Mean reward: 8.0, Mean Entropy: 5.9843303461093456e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 567,  Mean reward: 8.0, Mean Entropy: 0.0001093708851840347, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 568,  Mean reward: 8.0, Mean Entropy: 0.00020471878815442324, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 569,  Mean reward: 8.0, Mean Entropy: 0.00028239263338036835, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 570,  Mean reward: 8.0, Mean Entropy: 0.0002071816415991634, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 571,  Mean reward: 8.0, Mean Entropy: 7.431278936564922e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 572,  Mean reward: 8.0, Mean Entropy: 7.494794408557937e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 573,  Mean reward: 8.0, Mean Entropy: 6.754394416930154e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 574,  Mean reward: 8.0, Mean Entropy: 0.00015182739298325032, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 575,  Mean reward: 8.0, Mean Entropy: 0.00018157940940000117, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 576,  Mean reward: 8.0, Mean Entropy: 0.00020063930423930287, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 577,  Mean reward: 8.0, Mean Entropy: 0.00010474887676537037, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 578,  Mean reward: 8.0, Mean Entropy: 8.014677587198094e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 579,  Mean reward: 8.0, Mean Entropy: 5.466377115226351e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 580,  Mean reward: 8.0, Mean Entropy: 0.00012076368875568733, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 581,  Mean reward: 8.0, Mean Entropy: 0.00016455503646284342, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 582,  Mean reward: 8.0, Mean Entropy: 0.00023174157831817865, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 583,  Mean reward: 8.0, Mean Entropy: 7.978922803886235e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 584,  Mean reward: 8.0, Mean Entropy: 5.821756712975912e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 585,  Mean reward: 8.0, Mean Entropy: 5.8664794778451324e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 586,  Mean reward: 8.0, Mean Entropy: 0.0001353306433884427, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 587,  Mean reward: 8.0, Mean Entropy: 0.00018197426106780767, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 588,  Mean reward: 8.0, Mean Entropy: 0.00021731809829361737, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 589,  Mean reward: 8.0, Mean Entropy: 0.00012457351840566844, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 590,  Mean reward: 8.0, Mean Entropy: 7.546252891188487e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 591,  Mean reward: 8.0, Mean Entropy: 6.917593418620527e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 592,  Mean reward: 8.0, Mean Entropy: 8.234436972998083e-05, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.82s
Iteration: 593,  Mean reward: 8.0, Mean Entropy: 0.00015458623238373548, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 594,  Mean reward: 8.0, Mean Entropy: 0.00023164832964539528, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 595,  Mean reward: 8.0, Mean Entropy: 0.00017157783440779895, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 596,  Mean reward: 8.0, Mean Entropy: 4.290528886485845e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 597,  Mean reward: 8.0, Mean Entropy: 7.400439790217206e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 598,  Mean reward: 8.0, Mean Entropy: 8.250035170931369e-05, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.83s
Iteration: 599,  Mean reward: 8.0, Mean Entropy: 0.0001494965108577162, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 600,  Mean reward: 8.0, Mean Entropy: 0.00031858094735071063, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 1.00s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 8.0, Mean Entropy: 0.0001574446796439588, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 602,  Mean reward: 8.0, Mean Entropy: 7.523642852902412e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 603,  Mean reward: 8.0, Mean Entropy: 4.7767371142981574e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 604,  Mean reward: 8.0, Mean Entropy: 8.002841786947101e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 605,  Mean reward: 8.0, Mean Entropy: 9.881285950541496e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.00018241586803924292, complete_episode_count: 80.0, Gather time: 0.67s, Train time: 0.84s
Iteration: 607,  Mean reward: 8.0, Mean Entropy: 0.00018261431250721216, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 608,  Mean reward: 8.0, Mean Entropy: 0.00010862357157748193, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 609,  Mean reward: 8.0, Mean Entropy: 5.611423330265097e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 610,  Mean reward: 8.0, Mean Entropy: 6.420627323677763e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 611,  Mean reward: 8.0, Mean Entropy: 8.760076889302582e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 612,  Mean reward: 8.0, Mean Entropy: 9.232506272383034e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 613,  Mean reward: 8.0, Mean Entropy: 0.00018447164620738477, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 614,  Mean reward: 8.0, Mean Entropy: 0.000135112670250237, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 615,  Mean reward: 8.0, Mean Entropy: 6.394973024725914e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 616,  Mean reward: 8.0, Mean Entropy: 5.0374372221995145e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 617,  Mean reward: 8.0, Mean Entropy: 5.93625009059906e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 618,  Mean reward: 8.0, Mean Entropy: 6.835730164311826e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 619,  Mean reward: 8.0, Mean Entropy: 0.00010473452130099759, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 620,  Mean reward: 8.0, Mean Entropy: 0.0001410029362887144, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 621,  Mean reward: 8.0, Mean Entropy: 9.015810064738616e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 622,  Mean reward: 8.0, Mean Entropy: 5.4483040003106e-05, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 623,  Mean reward: 8.0, Mean Entropy: 7.316652772715315e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 624,  Mean reward: 8.0, Mean Entropy: 8.503325807396322e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 625,  Mean reward: 8.0, Mean Entropy: 0.00013109741848893464, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -2.9069767441860463, Mean Entropy: 0.9747373461723328, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.59s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -4.5, Mean Entropy: 0.9747347831726074, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.64s
Iteration: 2,  Mean reward: -3.3658536585365852, Mean Entropy: 0.9025260806083679, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 3,  Mean reward: -3.9680851063829787, Mean Entropy: 0.8519906997680664, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 4,  Mean reward: -4.025, Mean Entropy: 0.9314101934432983, complete_episode_count: 40.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 5,  Mean reward: -6.9875, Mean Entropy: 0.9530696272850037, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 6,  Mean reward: -3.313953488372093, Mean Entropy: 0.9675037264823914, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 7,  Mean reward: -3.768292682926829, Mean Entropy: 0.9385799169540405, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 8,  Mean reward: -3.141025641025641, Mean Entropy: 0.9745255708694458, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.57s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 9,  Mean reward: -2.1710526315789473, Mean Entropy: 0.9090098738670349, complete_episode_count: 38.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 10,  Mean reward: -5.4, Mean Entropy: 0.9151085615158081, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.80s
Iteration: 11,  Mean reward: -3.697674418604651, Mean Entropy: 0.9732016921043396, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 12,  Mean reward: -4.0, Mean Entropy: 0.8934909701347351, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 13,  Mean reward: -4.659090909090909, Mean Entropy: 0.95831698179245, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.63s
Iteration: 14,  Mean reward: -6.380952380952381, Mean Entropy: 0.9529790282249451, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 15,  Mean reward: -8.232558139534884, Mean Entropy: 0.974704384803772, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 16,  Mean reward: -3.852272727272727, Mean Entropy: 0.8736316561698914, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 17,  Mean reward: -4.618421052631579, Mean Entropy: 0.9168125987052917, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 18,  Mean reward: -4.653846153846154, Mean Entropy: 0.9154521822929382, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.59s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 19,  Mean reward: -1.25, Mean Entropy: 0.9168996810913086, complete_episode_count: 40.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 20,  Mean reward: -4.141025641025641, Mean Entropy: 0.9239243268966675, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 21,  Mean reward: -5.089743589743589, Mean Entropy: 0.8943151235580444, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 22,  Mean reward: -5.905405405405405, Mean Entropy: 0.8734825849533081, complete_episode_count: 37.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 23,  Mean reward: -5.607142857142857, Mean Entropy: 0.9675115346908569, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 24,  Mean reward: -4.2926829268292686, Mean Entropy: 0.9747253656387329, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 25,  Mean reward: -4.766666666666667, Mean Entropy: 0.9602717161178589, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 26,  Mean reward: -5.178571428571429, Mean Entropy: 0.9386219382286072, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 27,  Mean reward: -3.546511627906977, Mean Entropy: 0.9386043548583984, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 28,  Mean reward: -6.309523809523809, Mean Entropy: 0.8736319541931152, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 29,  Mean reward: -1.8888888888888888, Mean Entropy: 0.9675029516220093, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.64s
Iteration: 30,  Mean reward: -2.7222222222222223, Mean Entropy: 0.924102783203125, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 31,  Mean reward: -7.744186046511628, Mean Entropy: 0.9457356929779053, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 32,  Mean reward: -5.0, Mean Entropy: 0.9530550837516785, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 33,  Mean reward: -2.988095238095238, Mean Entropy: 0.9674601554870605, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 34,  Mean reward: -5.0, Mean Entropy: 0.9169301390647888, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 35,  Mean reward: -5.095238095238095, Mean Entropy: 0.8807903528213501, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 36,  Mean reward: -2.7906976744186047, Mean Entropy: 0.9169008731842041, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 37,  Mean reward: -6.476190476190476, Mean Entropy: 0.8736252188682556, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 38,  Mean reward: -5.353658536585366, Mean Entropy: 0.8952783942222595, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 39,  Mean reward: -4.9125, Mean Entropy: 0.9458087682723999, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 40,  Mean reward: -4.3875, Mean Entropy: 0.8808531761169434, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 41,  Mean reward: -4.337209302325581, Mean Entropy: 0.8808524012565613, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 42,  Mean reward: -4.261904761904762, Mean Entropy: 0.9602174162864685, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 43,  Mean reward: -2.988095238095238, Mean Entropy: 0.9601420760154724, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.62s
Iteration: 44,  Mean reward: -5.5, Mean Entropy: 0.931308925151825, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.80s
Iteration: 45,  Mean reward: -3.4875, Mean Entropy: 0.9385435581207275, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 46,  Mean reward: -3.55, Mean Entropy: 1.017735481262207, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 47,  Mean reward: -4.605263157894737, Mean Entropy: 0.8947620391845703, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 48,  Mean reward: -5.383720930232558, Mean Entropy: 0.8661057949066162, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 49,  Mean reward: -3.3947368421052633, Mean Entropy: 0.9306971430778503, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 50,  Mean reward: -4.255813953488372, Mean Entropy: 0.9244839549064636, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 51,  Mean reward: -6.325581395348837, Mean Entropy: 0.9608280062675476, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 52,  Mean reward: -8.035714285714286, Mean Entropy: 0.8806662559509277, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 53,  Mean reward: -4.2926829268292686, Mean Entropy: 0.9385115504264832, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 54,  Mean reward: -6.571428571428571, Mean Entropy: 0.9240476489067078, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 55,  Mean reward: -3.5121951219512195, Mean Entropy: 0.9167283773422241, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 56,  Mean reward: -3.4523809523809526, Mean Entropy: 0.9141982793807983, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 57,  Mean reward: -3.4767441860465116, Mean Entropy: 0.9807507991790771, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 58,  Mean reward: -4.9868421052631575, Mean Entropy: 0.9009098410606384, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 59,  Mean reward: -6.4625, Mean Entropy: 0.9332020282745361, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 60,  Mean reward: -2.1777777777777776, Mean Entropy: 1.024087905883789, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 61,  Mean reward: -3.125, Mean Entropy: 0.9240118861198425, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 62,  Mean reward: -2.9456521739130435, Mean Entropy: 0.9235267043113708, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 63,  Mean reward: -2.8255813953488373, Mean Entropy: 1.0215699672698975, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 64,  Mean reward: -5.813953488372093, Mean Entropy: 0.9502825140953064, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 65,  Mean reward: -4.627906976744186, Mean Entropy: 1.0338678359985352, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 66,  Mean reward: -5.186046511627907, Mean Entropy: 0.9700910449028015, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 67,  Mean reward: -3.409090909090909, Mean Entropy: 0.9663097858428955, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 68,  Mean reward: -4.933333333333334, Mean Entropy: 0.9314358234405518, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 69,  Mean reward: -4.7682926829268295, Mean Entropy: 0.9786192774772644, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 70,  Mean reward: -4.7, Mean Entropy: 0.9218804240226746, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 71,  Mean reward: -3.7738095238095237, Mean Entropy: 0.8791863918304443, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 72,  Mean reward: -5.212765957446808, Mean Entropy: 0.9231653213500977, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 73,  Mean reward: -4.348837209302325, Mean Entropy: 0.9194375872612, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 74,  Mean reward: -4.476744186046512, Mean Entropy: 0.9489786028862, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 75,  Mean reward: -1.1195652173913044, Mean Entropy: 0.9117507934570312, complete_episode_count: 46.0, Gather time: 0.60s, Train time: 1.57s
Iteration: 76,  Mean reward: -4.722222222222222, Mean Entropy: 0.9393894672393799, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 77,  Mean reward: -5.1, Mean Entropy: 0.8942813873291016, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 78,  Mean reward: -5.155555555555556, Mean Entropy: 0.8301563262939453, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 79,  Mean reward: -0.9772727272727273, Mean Entropy: 0.9024786353111267, complete_episode_count: 44.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 80,  Mean reward: -3.8636363636363638, Mean Entropy: 0.8882216811180115, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 81,  Mean reward: -4.430232558139535, Mean Entropy: 0.8828080892562866, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 82,  Mean reward: -2.7333333333333334, Mean Entropy: 0.8072995543479919, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 83,  Mean reward: -4.40625, Mean Entropy: 0.8279904127120972, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 84,  Mean reward: -2.4166666666666665, Mean Entropy: 0.7297877073287964, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 85,  Mean reward: -1.4166666666666667, Mean Entropy: 0.682506263256073, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 86,  Mean reward: -1.0267857142857142, Mean Entropy: 0.8514558672904968, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.57s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 87,  Mean reward: -0.3673469387755102, Mean Entropy: 0.7276284098625183, complete_episode_count: 49.0, Gather time: 0.60s, Train time: 1.57s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 88,  Mean reward: 2.5350877192982457, Mean Entropy: 0.5662352442741394, complete_episode_count: 57.0, Gather time: 0.65s, Train time: 1.58s
Iteration: 89,  Mean reward: 0.8225806451612904, Mean Entropy: 0.445499062538147, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 90,  Mean reward: -2.1, Mean Entropy: 0.6774829626083374, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 91,  Mean reward: -2.1228070175438596, Mean Entropy: 0.7938792109489441, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 92,  Mean reward: -2.453488372093023, Mean Entropy: 0.6797739267349243, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 93,  Mean reward: 1.8333333333333333, Mean Entropy: 0.8798362612724304, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 94,  Mean reward: -0.05555555555555555, Mean Entropy: 0.7580642700195312, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 95,  Mean reward: 2.0, Mean Entropy: 0.7815014123916626, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 96,  Mean reward: 2.4711538461538463, Mean Entropy: 0.7707774639129639, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.55s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 97,  Mean reward: 2.7, Mean Entropy: 0.7698086500167847, complete_episode_count: 50.0, Gather time: 0.60s, Train time: 1.56s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 98,  Mean reward: 4.2924528301886795, Mean Entropy: 0.5627478957176208, complete_episode_count: 53.0, Gather time: 0.61s, Train time: 1.57s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 99,  Mean reward: 5.3, Mean Entropy: 0.6602061986923218, complete_episode_count: 55.0, Gather time: 0.61s, Train time: 1.69s
Iteration: 100,  Mean reward: 3.685185185185185, Mean Entropy: 0.837228000164032, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.62s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 1.7058823529411764, Mean Entropy: 0.8177103400230408, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 102,  Mean reward: 3.8333333333333335, Mean Entropy: 0.8384603261947632, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 103,  Mean reward: -0.5816326530612245, Mean Entropy: 0.6257916688919067, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 104,  Mean reward: -0.7916666666666666, Mean Entropy: 0.7682723999023438, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 105,  Mean reward: 1.3673469387755102, Mean Entropy: 0.9372204542160034, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 106,  Mean reward: -4.6375, Mean Entropy: 0.5804086923599243, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 107,  Mean reward: 2.2232142857142856, Mean Entropy: 0.9617570638656616, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 108,  Mean reward: -0.046511627906976744, Mean Entropy: 0.9031731486320496, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.64s
Iteration: 109,  Mean reward: 2.0125, Mean Entropy: 0.864241361618042, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 110,  Mean reward: -1.430232558139535, Mean Entropy: 0.8232178092002869, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.82s
Iteration: 111,  Mean reward: -0.8214285714285714, Mean Entropy: 0.8130800127983093, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 112,  Mean reward: 0.9565217391304348, Mean Entropy: 0.7439872622489929, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 113,  Mean reward: 1.653061224489796, Mean Entropy: 0.5747640132904053, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 114,  Mean reward: 3.564814814814815, Mean Entropy: 0.8279027342796326, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 115,  Mean reward: 2.95, Mean Entropy: 0.5821776390075684, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 116,  Mean reward: 2.44, Mean Entropy: 0.8162213563919067, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 117,  Mean reward: 4.6826923076923075, Mean Entropy: 0.49366459250450134, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.58s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 118,  Mean reward: 5.850877192982456, Mean Entropy: 0.4668399691581726, complete_episode_count: 57.0, Gather time: 0.62s, Train time: 1.58s
Iteration: 119,  Mean reward: 3.6296296296296298, Mean Entropy: 0.8049898147583008, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 120,  Mean reward: 1.5888888888888888, Mean Entropy: 0.7220486402511597, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 121,  Mean reward: 3.9479166666666665, Mean Entropy: 0.5428679585456848, complete_episode_count: 48.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 122,  Mean reward: 4.982758620689655, Mean Entropy: 0.8403381109237671, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 123,  Mean reward: 2.43, Mean Entropy: 0.8027950525283813, complete_episode_count: 50.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 124,  Mean reward: 0.18478260869565216, Mean Entropy: 0.8414437174797058, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 125,  Mean reward: -3.5652173913043477, Mean Entropy: 0.8359888792037964, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 126,  Mean reward: 2.685185185185185, Mean Entropy: 0.859857439994812, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 127,  Mean reward: 1.16, Mean Entropy: 0.7137326002120972, complete_episode_count: 50.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 128,  Mean reward: 1.5, Mean Entropy: 0.7223895788192749, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 129,  Mean reward: 2.383720930232558, Mean Entropy: 0.5327756404876709, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 130,  Mean reward: 2.642857142857143, Mean Entropy: 0.7163748145103455, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 131,  Mean reward: 4.709090909090909, Mean Entropy: 0.6323477029800415, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 132,  Mean reward: 4.072727272727272, Mean Entropy: 0.5467676520347595, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 133,  Mean reward: 5.231481481481482, Mean Entropy: 0.6726549863815308, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 134,  Mean reward: 5.311320754716981, Mean Entropy: 0.49366748332977295, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.55s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 135,  Mean reward: 6.420634920634921, Mean Entropy: 0.45328813791275024, complete_episode_count: 63.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 136,  Mean reward: 4.528301886792453, Mean Entropy: 0.513797402381897, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 137,  Mean reward: 4.964285714285714, Mean Entropy: 0.46038490533828735, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 138,  Mean reward: 6.221311475409836, Mean Entropy: 0.46016550064086914, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 139,  Mean reward: 6.491803278688525, Mean Entropy: 0.4546046257019043, complete_episode_count: 61.0, Gather time: 0.61s, Train time: 1.54s
Iteration: 140,  Mean reward: 5.737704918032787, Mean Entropy: 0.47777652740478516, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 141,  Mean reward: 6.56140350877193, Mean Entropy: 0.42703601717948914, complete_episode_count: 57.0, Gather time: 0.61s, Train time: 1.55s
Iteration: 142,  Mean reward: 6.42741935483871, Mean Entropy: 0.33695679903030396, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 1.02s
Iteration: 143,  Mean reward: 5.783333333333333, Mean Entropy: 0.34083813428878784, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 144,  Mean reward: 6.122950819672131, Mean Entropy: 0.381742000579834, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 145,  Mean reward: 6.78030303030303, Mean Entropy: 0.2742983400821686, complete_episode_count: 66.0, Gather time: 0.63s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 146,  Mean reward: 7.3106060606060606, Mean Entropy: 0.31761038303375244, complete_episode_count: 66.0, Gather time: 0.63s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 147,  Mean reward: 7.419117647058823, Mean Entropy: 0.20325860381126404, complete_episode_count: 68.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 148,  Mean reward: 7.318840579710145, Mean Entropy: 0.24516648054122925, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 149,  Mean reward: 7.430769230769231, Mean Entropy: 0.2490636110305786, complete_episode_count: 65.0, Gather time: 0.62s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 150,  Mean reward: 7.65, Mean Entropy: 0.26541489362716675, complete_episode_count: 70.0, Gather time: 0.63s, Train time: 0.80s
Iteration: 151,  Mean reward: 5.12280701754386, Mean Entropy: 0.48882612586021423, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 152,  Mean reward: 4.217741935483871, Mean Entropy: 0.5687950849533081, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 153,  Mean reward: 4.824074074074074, Mean Entropy: 0.39775973558425903, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 154,  Mean reward: 6.630769230769231, Mean Entropy: 0.32392358779907227, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 155,  Mean reward: 7.6231884057971016, Mean Entropy: 0.2754451036453247, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 156,  Mean reward: 6.0546875, Mean Entropy: 0.34696531295776367, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 157,  Mean reward: 6.139705882352941, Mean Entropy: 0.2539941966533661, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 158,  Mean reward: 6.6893939393939394, Mean Entropy: 0.2310071587562561, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 159,  Mean reward: 7.6875, Mean Entropy: 0.33329325914382935, complete_episode_count: 72.0, Gather time: 0.63s, Train time: 0.78s
Iteration: 160,  Mean reward: 6.095238095238095, Mean Entropy: 0.3207409083843231, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 161,  Mean reward: 7.279411764705882, Mean Entropy: 0.24589568376541138, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 162,  Mean reward: 7.397058823529412, Mean Entropy: 0.2125629484653473, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 163,  Mean reward: 7.704225352112676, Mean Entropy: 0.24487346410751343, complete_episode_count: 71.0, Gather time: 0.63s, Train time: 0.79s
Iteration: 164,  Mean reward: 4.579710144927536, Mean Entropy: 0.5829477310180664, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 165,  Mean reward: -0.7894736842105263, Mean Entropy: 0.5172492265701294, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 166,  Mean reward: 1.6111111111111112, Mean Entropy: 0.31043124198913574, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 167,  Mean reward: 1.528169014084507, Mean Entropy: 0.3051193952560425, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 168,  Mean reward: 7.131944444444445, Mean Entropy: 0.524253249168396, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 169,  Mean reward: -3.9903846153846154, Mean Entropy: 0.272035151720047, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 170,  Mean reward: 4.395833333333333, Mean Entropy: 0.33863139152526855, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 171,  Mean reward: 5.315068493150685, Mean Entropy: 0.765099048614502, complete_episode_count: 73.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 172,  Mean reward: -1.2, Mean Entropy: 0.5803321599960327, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 173,  Mean reward: 5.094594594594595, Mean Entropy: 0.45040857791900635, complete_episode_count: 74.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 174,  Mean reward: 5.088235294117647, Mean Entropy: 0.5903100967407227, complete_episode_count: 68.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 175,  Mean reward: 3.153225806451613, Mean Entropy: 0.5981388092041016, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 176,  Mean reward: -2.1982758620689653, Mean Entropy: 0.2797614336013794, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 177,  Mean reward: -1.792857142857143, Mean Entropy: 0.29993709921836853, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 178,  Mean reward: 2.0, Mean Entropy: 0.6077801585197449, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 1.00s
Iteration: 179,  Mean reward: 0.10344827586206896, Mean Entropy: 0.8778533339500427, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 180,  Mean reward: -2.1122448979591835, Mean Entropy: 0.8063473701477051, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 181,  Mean reward: -6.064102564102564, Mean Entropy: 0.9206008911132812, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 182,  Mean reward: -7.402439024390244, Mean Entropy: 0.9210896492004395, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 183,  Mean reward: -6.5, Mean Entropy: 0.8924877643585205, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 184,  Mean reward: -4.923076923076923, Mean Entropy: 0.9036690592765808, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 185,  Mean reward: -7.392857142857143, Mean Entropy: 0.9114910364151001, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 186,  Mean reward: -4.433333333333334, Mean Entropy: 0.9446526765823364, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 187,  Mean reward: -3.8214285714285716, Mean Entropy: 0.8747859001159668, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 188,  Mean reward: -5.459183673469388, Mean Entropy: 0.9838451743125916, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 189,  Mean reward: -5.261363636363637, Mean Entropy: 0.9433152079582214, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 190,  Mean reward: -1.1222222222222222, Mean Entropy: 0.9109870791435242, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 191,  Mean reward: -3.0833333333333335, Mean Entropy: 0.8710876703262329, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 192,  Mean reward: -2.893617021276596, Mean Entropy: 0.6561959981918335, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 193,  Mean reward: 4.433962264150943, Mean Entropy: 0.48777318000793457, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 194,  Mean reward: 6.569230769230769, Mean Entropy: 0.4767491817474365, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 195,  Mean reward: 5.024193548387097, Mean Entropy: 0.46555933356285095, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 196,  Mean reward: 6.176923076923077, Mean Entropy: 0.4473419189453125, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 197,  Mean reward: 6.269230769230769, Mean Entropy: 0.5650599598884583, complete_episode_count: 65.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 198,  Mean reward: 0.3467741935483871, Mean Entropy: 0.6169106960296631, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 1.55s
Iteration: 199,  Mean reward: -0.6538461538461539, Mean Entropy: 0.6432770490646362, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 200,  Mean reward: -2.0681818181818183, Mean Entropy: 0.7526690363883972, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.58s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 0.5978260869565217, Mean Entropy: 0.6591110229492188, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 202,  Mean reward: 0.11224489795918367, Mean Entropy: 0.7739202380180359, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 203,  Mean reward: -2.6704545454545454, Mean Entropy: 0.9101523160934448, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 204,  Mean reward: -2.0357142857142856, Mean Entropy: 0.917810320854187, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 205,  Mean reward: 1.2553191489361701, Mean Entropy: 0.7497532367706299, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 206,  Mean reward: -1.88135593220339, Mean Entropy: 0.5945467948913574, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.70s
Iteration: 207,  Mean reward: 1.8137254901960784, Mean Entropy: 0.6976243853569031, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 208,  Mean reward: 5.0964912280701755, Mean Entropy: 0.5662108659744263, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 209,  Mean reward: 5.303571428571429, Mean Entropy: 0.5476568937301636, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 210,  Mean reward: 6.0092592592592595, Mean Entropy: 0.4233693778514862, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 211,  Mean reward: 7.130769230769231, Mean Entropy: 0.3536456525325775, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 1.02s
Iteration: 212,  Mean reward: 7.253731343283582, Mean Entropy: 0.3388512134552002, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 213,  Mean reward: 7.298507462686567, Mean Entropy: 0.24816715717315674, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 214,  Mean reward: 7.784722222222222, Mean Entropy: 0.1488497108221054, complete_episode_count: 72.0, Gather time: 0.63s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 215,  Mean reward: 7.915584415584416, Mean Entropy: 0.07935048639774323, complete_episode_count: 77.0, Gather time: 0.63s, Train time: 0.77s
Iteration: 216,  Mean reward: 7.75, Mean Entropy: 0.19862651824951172, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 217,  Mean reward: 3.6944444444444446, Mean Entropy: 0.08389844745397568, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 218,  Mean reward: 5.110169491525424, Mean Entropy: 0.0782599076628685, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 219,  Mean reward: -0.7982456140350878, Mean Entropy: 0.14735671877861023, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 220,  Mean reward: 3.7844827586206895, Mean Entropy: 0.33733272552490234, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 221,  Mean reward: 6.712121212121212, Mean Entropy: 0.293242871761322, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 222,  Mean reward: 7.5928571428571425, Mean Entropy: 0.303891122341156, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 223,  Mean reward: -0.1693548387096774, Mean Entropy: 0.4694315791130066, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 224,  Mean reward: 0.9538461538461539, Mean Entropy: 0.6333642601966858, complete_episode_count: 65.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 225,  Mean reward: -0.6517857142857143, Mean Entropy: 0.7701753377914429, complete_episode_count: 56.0, Gather time: 0.60s, Train time: 1.57s
Iteration: 226,  Mean reward: -0.6444444444444445, Mean Entropy: 0.7024275660514832, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 227,  Mean reward: 0.9583333333333334, Mean Entropy: 0.6422915458679199, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 228,  Mean reward: 2.7641509433962264, Mean Entropy: 0.5158641934394836, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 229,  Mean reward: 6.112903225806452, Mean Entropy: 0.5415579080581665, complete_episode_count: 62.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 230,  Mean reward: 4.483050847457627, Mean Entropy: 0.5130288600921631, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 231,  Mean reward: 6.038461538461538, Mean Entropy: 0.41469526290893555, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 232,  Mean reward: 7.098484848484849, Mean Entropy: 0.4569271504878998, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 233,  Mean reward: -3.181159420289855, Mean Entropy: 0.31117868423461914, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 234,  Mean reward: -2.0223880597014925, Mean Entropy: 0.47693976759910583, complete_episode_count: 67.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 235,  Mean reward: -1.5, Mean Entropy: 0.41355523467063904, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 236,  Mean reward: 4.838709677419355, Mean Entropy: 0.469505250453949, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 1.58s
Iteration: 237,  Mean reward: 6.39344262295082, Mean Entropy: 0.3497273921966553, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 238,  Mean reward: 7.384057971014493, Mean Entropy: 0.3680915832519531, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 239,  Mean reward: 6.606060606060606, Mean Entropy: 0.4374367594718933, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 240,  Mean reward: 6.384057971014493, Mean Entropy: 0.3240469694137573, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 241,  Mean reward: 7.767123287671233, Mean Entropy: 0.23471324145793915, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 242,  Mean reward: 7.66, Mean Entropy: 0.16231125593185425, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 243,  Mean reward: 7.423611111111111, Mean Entropy: 0.2531124949455261, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 244,  Mean reward: 6.865671641791045, Mean Entropy: 0.2164655178785324, complete_episode_count: 67.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 245,  Mean reward: 7.661971830985915, Mean Entropy: 0.11881983280181885, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 246,  Mean reward: 7.86, Mean Entropy: 0.13606461882591248, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 247,  Mean reward: 6.972602739726027, Mean Entropy: 0.14107000827789307, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 248,  Mean reward: 7.981012658227848, Mean Entropy: 0.05614195391535759, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 1.04s
Iteration: 249,  Mean reward: 7.981012658227848, Mean Entropy: 0.19662980735301971, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 250,  Mean reward: 5.673913043478261, Mean Entropy: 0.24470244348049164, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 251,  Mean reward: -2.6338028169014085, Mean Entropy: 0.36321529746055603, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 252,  Mean reward: -2.0, Mean Entropy: 0.5746540427207947, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 253,  Mean reward: -3.4361702127659575, Mean Entropy: 0.5180010199546814, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 254,  Mean reward: 5.145161290322581, Mean Entropy: 0.45949786901474, complete_episode_count: 62.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 255,  Mean reward: 5.943548387096774, Mean Entropy: 0.36739975214004517, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 256,  Mean reward: 6.384615384615385, Mean Entropy: 0.3602308928966522, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 257,  Mean reward: 6.5390625, Mean Entropy: 0.42094820737838745, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 258,  Mean reward: 7.577464788732394, Mean Entropy: 0.26545950770378113, complete_episode_count: 71.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 259,  Mean reward: 7.616438356164384, Mean Entropy: 0.16950182616710663, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 260,  Mean reward: 7.72, Mean Entropy: 0.1725776195526123, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 261,  Mean reward: 7.902597402597403, Mean Entropy: 0.10216587781906128, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 262,  Mean reward: 7.84, Mean Entropy: 0.02624073252081871, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 263,  Mean reward: 8.0, Mean Entropy: 0.039222657680511475, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 264,  Mean reward: 7.961538461538462, Mean Entropy: 0.034562960267066956, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 265,  Mean reward: 7.955128205128205, Mean Entropy: 0.01266543846577406, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 266,  Mean reward: 8.0, Mean Entropy: 0.002462984062731266, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 267,  Mean reward: 8.0, Mean Entropy: 0.003003903431817889, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 268,  Mean reward: 8.0, Mean Entropy: 0.05361248180270195, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 269,  Mean reward: 7.259740259740259, Mean Entropy: 0.17576973140239716, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 270,  Mean reward: 7.826923076923077, Mean Entropy: 0.07482334971427917, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 271,  Mean reward: 8.0, Mean Entropy: 0.08801811933517456, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 272,  Mean reward: 7.981012658227848, Mean Entropy: 0.02912301942706108, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 273,  Mean reward: 8.0, Mean Entropy: 0.05465632677078247, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 274,  Mean reward: 7.961538461538462, Mean Entropy: 0.00777105987071991, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 275,  Mean reward: 8.0, Mean Entropy: 0.011342313140630722, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 276,  Mean reward: 7.974683544303797, Mean Entropy: 0.035081520676612854, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 277,  Mean reward: 8.0, Mean Entropy: 0.057334624230861664, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 278,  Mean reward: 5.201754385964913, Mean Entropy: 0.2905834913253784, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 279,  Mean reward: 5.592592592592593, Mean Entropy: 0.14985385537147522, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 280,  Mean reward: 8.0, Mean Entropy: 0.025837615132331848, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 281,  Mean reward: 7.981012658227848, Mean Entropy: 0.07626593858003616, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 282,  Mean reward: 7.981012658227848, Mean Entropy: 0.0783553496003151, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 283,  Mean reward: 7.941558441558442, Mean Entropy: 0.0408192053437233, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 284,  Mean reward: 8.0, Mean Entropy: 0.06525351107120514, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 285,  Mean reward: 7.9423076923076925, Mean Entropy: 0.012292436324059963, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 1.03s
Iteration: 286,  Mean reward: 7.974683544303797, Mean Entropy: 0.01737835817039013, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 287,  Mean reward: 8.0, Mean Entropy: 0.0037303126882761717, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 288,  Mean reward: 8.0, Mean Entropy: 0.0027080029249191284, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.79s
Iteration: 289,  Mean reward: 8.0, Mean Entropy: 0.0020491417963057756, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 290,  Mean reward: 8.0, Mean Entropy: 0.002207646844908595, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 291,  Mean reward: 8.0, Mean Entropy: 0.002181979827582836, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 292,  Mean reward: 8.0, Mean Entropy: 0.13982248306274414, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 293,  Mean reward: 7.23015873015873, Mean Entropy: 0.027026072144508362, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 294,  Mean reward: 7.285714285714286, Mean Entropy: 0.028607001528143883, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 295,  Mean reward: 8.0, Mean Entropy: 0.02387005090713501, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 296,  Mean reward: 8.0, Mean Entropy: 0.00742098456248641, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 297,  Mean reward: 8.0, Mean Entropy: 0.04431317374110222, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 298,  Mean reward: 7.941558441558442, Mean Entropy: 0.001930357189849019, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 299,  Mean reward: 8.0, Mean Entropy: 0.0018803996499627829, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 300,  Mean reward: 8.0, Mean Entropy: 0.0015672328881919384, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 8.0, Mean Entropy: 0.005006830208003521, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 302,  Mean reward: 7.974683544303797, Mean Entropy: 0.00878087431192398, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 303,  Mean reward: 8.0, Mean Entropy: 0.0021190703846514225, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 304,  Mean reward: 8.0, Mean Entropy: 0.0017323242500424385, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 305,  Mean reward: 8.0, Mean Entropy: 0.0024327305145561695, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 306,  Mean reward: 8.0, Mean Entropy: 0.002971605397760868, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 307,  Mean reward: 8.0, Mean Entropy: 0.049619775265455246, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 308,  Mean reward: 6.684210526315789, Mean Entropy: 0.33091235160827637, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 309,  Mean reward: 4.1, Mean Entropy: 0.2703428864479065, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 310,  Mean reward: 7.23943661971831, Mean Entropy: 0.2992607057094574, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 311,  Mean reward: 7.635714285714286, Mean Entropy: 0.05050249025225639, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 312,  Mean reward: 8.0, Mean Entropy: 0.06819073855876923, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 313,  Mean reward: 8.0, Mean Entropy: 0.09925653785467148, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 314,  Mean reward: 7.191780821917808, Mean Entropy: 0.021920209750533104, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 315,  Mean reward: 8.0, Mean Entropy: 0.005451064556837082, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 316,  Mean reward: 8.0, Mean Entropy: 0.0019670939072966576, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 317,  Mean reward: 8.0, Mean Entropy: 0.003301796270534396, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 318,  Mean reward: 8.0, Mean Entropy: 0.002904131542891264, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 319,  Mean reward: 8.0, Mean Entropy: 0.0029007606208324432, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 320,  Mean reward: 8.0, Mean Entropy: 0.003666724544018507, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 321,  Mean reward: 8.0, Mean Entropy: 0.004183769691735506, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 322,  Mean reward: 8.0, Mean Entropy: 0.021879902109503746, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 323,  Mean reward: 7.814102564102564, Mean Entropy: 0.0714057981967926, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 1.00s
Iteration: 324,  Mean reward: 7.782894736842105, Mean Entropy: 0.028965508565306664, complete_episode_count: 76.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 325,  Mean reward: 8.0, Mean Entropy: 0.002123146317899227, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 326,  Mean reward: 8.0, Mean Entropy: 0.0018934117397293448, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 327,  Mean reward: 8.0, Mean Entropy: 0.001398361986503005, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 328,  Mean reward: 8.0, Mean Entropy: 0.0015312188770622015, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 329,  Mean reward: 8.0, Mean Entropy: 0.12576143443584442, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 330,  Mean reward: 7.565217391304348, Mean Entropy: 0.046318404376506805, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 331,  Mean reward: 7.981012658227848, Mean Entropy: 0.008826833218336105, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 332,  Mean reward: 8.0, Mean Entropy: 0.008694622665643692, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 333,  Mean reward: 8.0, Mean Entropy: 0.026404965668916702, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 334,  Mean reward: 8.0, Mean Entropy: 0.012040006928145885, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 335,  Mean reward: 7.974683544303797, Mean Entropy: 0.006799705326557159, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 336,  Mean reward: 8.0, Mean Entropy: 0.05954546853899956, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 337,  Mean reward: 7.729166666666667, Mean Entropy: 0.005838282406330109, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 338,  Mean reward: 8.0, Mean Entropy: 0.0005742406938225031, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 339,  Mean reward: 8.0, Mean Entropy: 0.0006196452886797488, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 340,  Mean reward: 8.0, Mean Entropy: 0.0005096274544484913, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 341,  Mean reward: 8.0, Mean Entropy: 0.000557374965865165, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 342,  Mean reward: 8.0, Mean Entropy: 0.0005487891612574458, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 343,  Mean reward: 8.0, Mean Entropy: 0.0004985929117538035, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 344,  Mean reward: 8.0, Mean Entropy: 0.0004794717242475599, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 345,  Mean reward: 8.0, Mean Entropy: 0.0005187849747017026, complete_episode_count: 80.0, Gather time: 0.74s, Train time: 0.81s
Iteration: 346,  Mean reward: 8.0, Mean Entropy: 0.0005916727241128683, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 347,  Mean reward: 8.0, Mean Entropy: 0.0007229666225612164, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 348,  Mean reward: 8.0, Mean Entropy: 0.000950125977396965, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 349,  Mean reward: 8.0, Mean Entropy: 0.00147229281719774, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 350,  Mean reward: 8.0, Mean Entropy: 0.0021614660508930683, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 351,  Mean reward: 8.0, Mean Entropy: 0.009270813316106796, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 352,  Mean reward: 8.0, Mean Entropy: 0.016157325357198715, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 353,  Mean reward: 7.75, Mean Entropy: 0.0005126702017150819, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 354,  Mean reward: 8.0, Mean Entropy: 0.0003504366031847894, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 355,  Mean reward: 8.0, Mean Entropy: 0.00035192782524973154, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 356,  Mean reward: 8.0, Mean Entropy: 0.00031436249264515936, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 357,  Mean reward: 8.0, Mean Entropy: 0.00031165394466370344, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 358,  Mean reward: 8.0, Mean Entropy: 0.0003149466938339174, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 359,  Mean reward: 8.0, Mean Entropy: 0.0004106367123313248, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 360,  Mean reward: 8.0, Mean Entropy: 0.00030841241823509336, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 361,  Mean reward: 8.0, Mean Entropy: 0.000385104154702276, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 1.01s
Iteration: 362,  Mean reward: 8.0, Mean Entropy: 0.00039666236261837184, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 363,  Mean reward: 8.0, Mean Entropy: 0.00045168152428232133, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 364,  Mean reward: 8.0, Mean Entropy: 0.0004946194239892066, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 365,  Mean reward: 8.0, Mean Entropy: 0.000518303771968931, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 366,  Mean reward: 8.0, Mean Entropy: 0.0005090680788271129, complete_episode_count: 80.0, Gather time: 0.74s, Train time: 0.79s
Iteration: 367,  Mean reward: 8.0, Mean Entropy: 0.0006937018479220569, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 368,  Mean reward: 8.0, Mean Entropy: 0.001227569649927318, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 369,  Mean reward: 8.0, Mean Entropy: 0.0017258629668504, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 370,  Mean reward: 8.0, Mean Entropy: 0.0038642098661512136, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 371,  Mean reward: 8.0, Mean Entropy: 0.014080330729484558, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 372,  Mean reward: 8.0, Mean Entropy: 0.23553282022476196, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 373,  Mean reward: 5.231343283582089, Mean Entropy: 0.2956615090370178, complete_episode_count: 67.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 374,  Mean reward: 6.714285714285714, Mean Entropy: 0.296499103307724, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 375,  Mean reward: 7.4391891891891895, Mean Entropy: 0.08571558445692062, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 376,  Mean reward: 7.974683544303797, Mean Entropy: 0.045936133712530136, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 377,  Mean reward: 8.0, Mean Entropy: 0.013860195875167847, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 378,  Mean reward: 7.948717948717949, Mean Entropy: 0.008856321685016155, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 379,  Mean reward: 8.0, Mean Entropy: 0.0048476774245500565, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 380,  Mean reward: 8.0, Mean Entropy: 0.0022452145349234343, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 381,  Mean reward: 8.0, Mean Entropy: 0.00210538599640131, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 382,  Mean reward: 8.0, Mean Entropy: 0.0017524457070976496, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 383,  Mean reward: 8.0, Mean Entropy: 0.0020742269698530436, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 384,  Mean reward: 8.0, Mean Entropy: 0.004995652474462986, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 385,  Mean reward: 8.0, Mean Entropy: 0.2374235987663269, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 386,  Mean reward: 0.6805555555555556, Mean Entropy: 0.3050469160079956, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 387,  Mean reward: -1.3492063492063493, Mean Entropy: 0.3465763032436371, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 388,  Mean reward: -2.4274193548387095, Mean Entropy: 0.3842671811580658, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 389,  Mean reward: 6.947368421052632, Mean Entropy: 0.28830820322036743, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 390,  Mean reward: 7.82, Mean Entropy: 0.14020881056785583, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 391,  Mean reward: 6.384615384615385, Mean Entropy: 0.03795480355620384, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.76s
Iteration: 392,  Mean reward: 7.981012658227848, Mean Entropy: 0.20017705857753754, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 393,  Mean reward: 7.794520547945205, Mean Entropy: 0.17656123638153076, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 394,  Mean reward: 7.725352112676056, Mean Entropy: 0.13789072632789612, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 395,  Mean reward: 7.335714285714285, Mean Entropy: 0.1391022950410843, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 396,  Mean reward: 4.866197183098592, Mean Entropy: 0.20150354504585266, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 397,  Mean reward: 7.613333333333333, Mean Entropy: 0.06467700004577637, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 398,  Mean reward: 7.801282051282051, Mean Entropy: 0.05699067562818527, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 399,  Mean reward: 7.974683544303797, Mean Entropy: 0.013453103601932526, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 0.005790629889816046, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.02s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 8.0, Mean Entropy: 0.007503176108002663, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 402,  Mean reward: 8.0, Mean Entropy: 0.0033638272434473038, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 403,  Mean reward: 8.0, Mean Entropy: 0.004813550040125847, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 404,  Mean reward: 8.0, Mean Entropy: 0.004348290152847767, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 405,  Mean reward: 8.0, Mean Entropy: 0.005135431420058012, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 0.004747806116938591, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 407,  Mean reward: 7.974683544303797, Mean Entropy: 0.006661762949079275, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 0.0035844873636960983, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 0.0032556895166635513, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 410,  Mean reward: 8.0, Mean Entropy: 0.0023229531943798065, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 411,  Mean reward: 8.0, Mean Entropy: 0.002236525993794203, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 412,  Mean reward: 8.0, Mean Entropy: 0.0029943399131298065, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 413,  Mean reward: 8.0, Mean Entropy: 0.005484565626829863, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 0.12094974517822266, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 415,  Mean reward: 7.3283582089552235, Mean Entropy: 0.2905118465423584, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 416,  Mean reward: 7.729166666666667, Mean Entropy: 0.15067818760871887, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 417,  Mean reward: 7.861111111111111, Mean Entropy: 0.12250149250030518, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 418,  Mean reward: 7.9144736842105265, Mean Entropy: 0.11817893385887146, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 419,  Mean reward: 7.54, Mean Entropy: 0.042612891644239426, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 420,  Mean reward: 7.9480519480519485, Mean Entropy: 0.012560682371258736, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 421,  Mean reward: 8.0, Mean Entropy: 0.0027142956387251616, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.0018943871837109327, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 423,  Mean reward: 8.0, Mean Entropy: 0.0016027309466153383, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 424,  Mean reward: 8.0, Mean Entropy: 0.0016412176191806793, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 0.0036730202846229076, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 0.17266646027565002, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 427,  Mean reward: 7.630434782608695, Mean Entropy: 0.16482457518577576, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 428,  Mean reward: 3.4577464788732395, Mean Entropy: 0.11819928884506226, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 429,  Mean reward: 7.7534246575342465, Mean Entropy: 0.06059819459915161, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 430,  Mean reward: 7.981012658227848, Mean Entropy: 0.01333601027727127, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 0.02492785081267357, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 432,  Mean reward: 8.0, Mean Entropy: 0.10434716939926147, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 433,  Mean reward: 7.922077922077922, Mean Entropy: 0.05365501344203949, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 434,  Mean reward: 7.981012658227848, Mean Entropy: 0.010113688185811043, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 435,  Mean reward: 8.0, Mean Entropy: 0.0051082344725728035, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 436,  Mean reward: 8.0, Mean Entropy: 0.011793936602771282, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 437,  Mean reward: 7.974683544303797, Mean Entropy: 0.0026875443290919065, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 438,  Mean reward: 8.0, Mean Entropy: 0.0017808740958571434, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.01s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 0.001764261513017118, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 440,  Mean reward: 8.0, Mean Entropy: 0.0024504149332642555, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 441,  Mean reward: 8.0, Mean Entropy: 0.00908008124679327, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 0.29651832580566406, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 443,  Mean reward: 0.4420289855072464, Mean Entropy: 0.18754635751247406, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 444,  Mean reward: 0.12837837837837837, Mean Entropy: 0.15665528178215027, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 445,  Mean reward: -0.910958904109589, Mean Entropy: 0.18161019682884216, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 446,  Mean reward: -2.2733333333333334, Mean Entropy: 0.39803001284599304, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 447,  Mean reward: -6.126760563380282, Mean Entropy: 0.5545092821121216, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 448,  Mean reward: -9.134328358208956, Mean Entropy: 0.7263976335525513, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 449,  Mean reward: -7.089285714285714, Mean Entropy: 0.896333634853363, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 450,  Mean reward: -3.4166666666666665, Mean Entropy: 0.9089953303337097, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 451,  Mean reward: -5.023809523809524, Mean Entropy: 0.881335437297821, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 452,  Mean reward: -3.5113636363636362, Mean Entropy: 0.870201587677002, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 453,  Mean reward: -4.382978723404255, Mean Entropy: 0.9007984399795532, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 454,  Mean reward: -4.7926829268292686, Mean Entropy: 0.9248323440551758, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 455,  Mean reward: -3.635135135135135, Mean Entropy: 0.9549312591552734, complete_episode_count: 37.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 456,  Mean reward: -4.048780487804878, Mean Entropy: 0.8806051015853882, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 457,  Mean reward: -3.4761904761904763, Mean Entropy: 0.8761317133903503, complete_episode_count: 42.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 458,  Mean reward: -4.5256410256410255, Mean Entropy: 0.9262139201164246, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 459,  Mean reward: -3.238095238095238, Mean Entropy: 0.9686505198478699, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 460,  Mean reward: -5.313953488372093, Mean Entropy: 0.9065364599227905, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 461,  Mean reward: -2.2386363636363638, Mean Entropy: 0.9443835616111755, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 462,  Mean reward: -0.5465116279069767, Mean Entropy: 0.8469681739807129, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 463,  Mean reward: -3.1931818181818183, Mean Entropy: 0.8925250768661499, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 464,  Mean reward: -2.186046511627907, Mean Entropy: 0.7921454906463623, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 465,  Mean reward: -2.5348837209302326, Mean Entropy: 0.8209024667739868, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 466,  Mean reward: 0.4375, Mean Entropy: 0.8411190509796143, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 467,  Mean reward: -0.03125, Mean Entropy: 0.8816345930099487, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 468,  Mean reward: 2.86734693877551, Mean Entropy: 0.7711825370788574, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 469,  Mean reward: -0.24, Mean Entropy: 0.8437680602073669, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 470,  Mean reward: 3.3627450980392157, Mean Entropy: 0.8351893424987793, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 471,  Mean reward: -0.3333333333333333, Mean Entropy: 0.7981343269348145, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 472,  Mean reward: 0.7980769230769231, Mean Entropy: 0.7120693922042847, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 473,  Mean reward: 3.2264150943396226, Mean Entropy: 0.6952289342880249, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.78s
Iteration: 474,  Mean reward: 1.5416666666666667, Mean Entropy: 0.6508223414421082, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 475,  Mean reward: 3.7916666666666665, Mean Entropy: 0.9083433151245117, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 476,  Mean reward: 3.5760869565217392, Mean Entropy: 0.6616801023483276, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 477,  Mean reward: 2.282608695652174, Mean Entropy: 0.7259771823883057, complete_episode_count: 46.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 478,  Mean reward: 3.3877551020408165, Mean Entropy: 0.7015816569328308, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 479,  Mean reward: 3.13, Mean Entropy: 0.7074100375175476, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 480,  Mean reward: 5.716981132075472, Mean Entropy: 0.620716392993927, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 481,  Mean reward: 5.2727272727272725, Mean Entropy: 0.631280779838562, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 482,  Mean reward: 4.509615384615385, Mean Entropy: 0.5930986404418945, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 483,  Mean reward: 5.138888888888889, Mean Entropy: 0.5927221775054932, complete_episode_count: 54.0, Gather time: 0.60s, Train time: 1.54s
Iteration: 484,  Mean reward: 6.336206896551724, Mean Entropy: 0.47046956419944763, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 485,  Mean reward: 5.62280701754386, Mean Entropy: 0.483785480260849, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 486,  Mean reward: 6.080357142857143, Mean Entropy: 0.5100034475326538, complete_episode_count: 56.0, Gather time: 0.60s, Train time: 1.56s
Iteration: 487,  Mean reward: 4.0, Mean Entropy: 0.5999720096588135, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 488,  Mean reward: 4.5423728813559325, Mean Entropy: 0.5638123750686646, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 489,  Mean reward: 5.572727272727272, Mean Entropy: 0.47200965881347656, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 490,  Mean reward: 5.9035087719298245, Mean Entropy: 0.5089177489280701, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 491,  Mean reward: 5.6875, Mean Entropy: 0.5180754661560059, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 492,  Mean reward: 6.127118644067797, Mean Entropy: 0.40134701132774353, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 493,  Mean reward: 5.149122807017544, Mean Entropy: 0.5068107843399048, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 494,  Mean reward: 6.166666666666667, Mean Entropy: 0.4685906767845154, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 495,  Mean reward: 6.357142857142857, Mean Entropy: 0.4132891595363617, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 496,  Mean reward: 6.3, Mean Entropy: 0.39376187324523926, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 497,  Mean reward: 5.321428571428571, Mean Entropy: 0.4975787103176117, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 498,  Mean reward: 5.071428571428571, Mean Entropy: 0.4254017770290375, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 499,  Mean reward: 3.9019607843137254, Mean Entropy: 0.4598044157028198, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 500,  Mean reward: 5.836206896551724, Mean Entropy: 0.46832507848739624, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.62s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 6.211864406779661, Mean Entropy: 0.36740899085998535, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 502,  Mean reward: 5.2105263157894735, Mean Entropy: 0.4236135184764862, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 503,  Mean reward: 6.067796610169491, Mean Entropy: 0.3529592454433441, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 504,  Mean reward: 5.009090909090909, Mean Entropy: 0.4524226188659668, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.84s
Iteration: 505,  Mean reward: 6.658333333333333, Mean Entropy: 0.4304100275039673, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 0.93s
Iteration: 506,  Mean reward: 5.912280701754386, Mean Entropy: 0.44524073600769043, complete_episode_count: 57.0, Gather time: 0.60s, Train time: 1.58s
Iteration: 507,  Mean reward: 5.483050847457627, Mean Entropy: 0.42762207984924316, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 508,  Mean reward: 4.952830188679245, Mean Entropy: 0.4287448823451996, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 509,  Mean reward: 3.6847826086956523, Mean Entropy: 0.5151684284210205, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 510,  Mean reward: 6.067796610169491, Mean Entropy: 0.4210833013057709, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 511,  Mean reward: 5.216981132075472, Mean Entropy: 0.37024468183517456, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 512,  Mean reward: 5.785714285714286, Mean Entropy: 0.4595050811767578, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 513,  Mean reward: 5.385964912280702, Mean Entropy: 0.5132383704185486, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 514,  Mean reward: 6.163793103448276, Mean Entropy: 0.36783143877983093, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 515,  Mean reward: 5.418181818181818, Mean Entropy: 0.5089452862739563, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 516,  Mean reward: 5.684210526315789, Mean Entropy: 0.4684790074825287, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 517,  Mean reward: 6.526315789473684, Mean Entropy: 0.5233391523361206, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 518,  Mean reward: 5.678571428571429, Mean Entropy: 0.43765079975128174, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 519,  Mean reward: 5.758620689655173, Mean Entropy: 0.43271490931510925, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.71s
Iteration: 520,  Mean reward: 6.067796610169491, Mean Entropy: 0.3590216338634491, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 521,  Mean reward: 6.432203389830509, Mean Entropy: 0.46923476457595825, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 522,  Mean reward: 5.133928571428571, Mean Entropy: 0.49739760160446167, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 523,  Mean reward: 5.774193548387097, Mean Entropy: 0.3595961034297943, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 524,  Mean reward: 5.6454545454545455, Mean Entropy: 0.5187230110168457, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 525,  Mean reward: 5.947368421052632, Mean Entropy: 0.4601113498210907, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 526,  Mean reward: 5.398305084745763, Mean Entropy: 0.49393391609191895, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 527,  Mean reward: 5.107142857142857, Mean Entropy: 0.5099168419837952, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 528,  Mean reward: 4.657407407407407, Mean Entropy: 0.41046127676963806, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 529,  Mean reward: 5.385964912280702, Mean Entropy: 0.3770376145839691, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 530,  Mean reward: 6.6328125, Mean Entropy: 0.3541851043701172, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 531,  Mean reward: 6.008771929824562, Mean Entropy: 0.4217418134212494, complete_episode_count: 57.0, Gather time: 0.60s, Train time: 1.56s
Iteration: 532,  Mean reward: 4.584745762711864, Mean Entropy: 0.4476759135723114, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 533,  Mean reward: 4.944444444444445, Mean Entropy: 0.43007510900497437, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.70s
Iteration: 534,  Mean reward: 4.174418604651163, Mean Entropy: 0.4611996114253998, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 535,  Mean reward: 3.5652173913043477, Mean Entropy: 0.37412509322166443, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.79s
Iteration: 536,  Mean reward: 3.622448979591837, Mean Entropy: 0.41751980781555176, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 537,  Mean reward: 4.54, Mean Entropy: 0.3618345260620117, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 538,  Mean reward: 4.84, Mean Entropy: 0.3497611880302429, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 539,  Mean reward: 5.054545454545455, Mean Entropy: 0.31314629316329956, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 540,  Mean reward: 4.489583333333333, Mean Entropy: 0.3708498477935791, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 541,  Mean reward: 4.83, Mean Entropy: 0.3580015301704407, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 542,  Mean reward: 5.221153846153846, Mean Entropy: 0.3202434480190277, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.55s
Iteration: 543,  Mean reward: 5.886792452830188, Mean Entropy: 0.2614881694316864, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 544,  Mean reward: 6.1909090909090905, Mean Entropy: 0.21043430268764496, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 545,  Mean reward: 5.5, Mean Entropy: 0.23892971873283386, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 546,  Mean reward: 5.481132075471698, Mean Entropy: 0.2064129114151001, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 547,  Mean reward: 4.32, Mean Entropy: 0.2902509868144989, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 548,  Mean reward: 5.26, Mean Entropy: 0.25149622559547424, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 549,  Mean reward: 5.009433962264151, Mean Entropy: 0.25647157430648804, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.64s
Iteration: 550,  Mean reward: 6.212962962962963, Mean Entropy: 0.24543260037899017, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 551,  Mean reward: 4.395833333333333, Mean Entropy: 0.27101045846939087, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 552,  Mean reward: 4.86734693877551, Mean Entropy: 0.27126362919807434, complete_episode_count: 49.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 553,  Mean reward: 5.6923076923076925, Mean Entropy: 0.22165414690971375, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 554,  Mean reward: 6.211538461538462, Mean Entropy: 0.24807578325271606, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 555,  Mean reward: 5.1938775510204085, Mean Entropy: 0.2372833788394928, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 556,  Mean reward: 6.689655172413793, Mean Entropy: 0.2899628281593323, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 557,  Mean reward: 5.867924528301887, Mean Entropy: 0.26120856404304504, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 558,  Mean reward: 5.588235294117647, Mean Entropy: 0.22271740436553955, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 559,  Mean reward: 7.016949152542373, Mean Entropy: 0.16062507033348083, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.62s
Iteration: 560,  Mean reward: 6.25, Mean Entropy: 0.16527605056762695, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 561,  Mean reward: 6.25, Mean Entropy: 0.2141420841217041, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 562,  Mean reward: 6.816666666666666, Mean Entropy: 0.08918362855911255, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 563,  Mean reward: 6.912280701754386, Mean Entropy: 0.0719211995601654, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 564,  Mean reward: 7.217741935483871, Mean Entropy: 0.04233422502875328, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 565,  Mean reward: 4.412698412698413, Mean Entropy: 0.3296077251434326, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 566,  Mean reward: 1.303921568627451, Mean Entropy: 0.2771835923194885, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 567,  Mean reward: 0.4891304347826087, Mean Entropy: 0.5838070511817932, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.82s
Iteration: 568,  Mean reward: -2.8207547169811322, Mean Entropy: 0.44700726866722107, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 569,  Mean reward: 6.053571428571429, Mean Entropy: 0.493736207485199, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 570,  Mean reward: -6.66, Mean Entropy: 0.7812651991844177, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 571,  Mean reward: 1.3867924528301887, Mean Entropy: 0.564895510673523, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 572,  Mean reward: 6.62280701754386, Mean Entropy: 0.5888816714286804, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 573,  Mean reward: -6.981132075471698, Mean Entropy: 0.7571840286254883, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 574,  Mean reward: -2.0185185185185186, Mean Entropy: 0.6390848159790039, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 575,  Mean reward: 1.8189655172413792, Mean Entropy: 0.572311282157898, complete_episode_count: 58.0, Gather time: 0.60s, Train time: 1.70s
Iteration: 576,  Mean reward: 4.658333333333333, Mean Entropy: 0.3466758131980896, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.63s
Iteration: 577,  Mean reward: 7.394736842105263, Mean Entropy: 0.2698401212692261, complete_episode_count: 76.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 578,  Mean reward: 7.421428571428572, Mean Entropy: 0.20637735724449158, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 579,  Mean reward: 7.141891891891892, Mean Entropy: 0.1693718433380127, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 580,  Mean reward: 7.662337662337662, Mean Entropy: 0.1689687818288803, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 581,  Mean reward: 7.02027027027027, Mean Entropy: 0.1597813069820404, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 582,  Mean reward: 7.7534246575342465, Mean Entropy: 0.08653714507818222, complete_episode_count: 73.0, Gather time: 0.62s, Train time: 0.83s
Iteration: 583,  Mean reward: 7.8618421052631575, Mean Entropy: 0.07285991311073303, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 584,  Mean reward: 7.935064935064935, Mean Entropy: 0.03643741458654404, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 585,  Mean reward: 7.961538461538462, Mean Entropy: 0.019051551818847656, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 586,  Mean reward: 7.75, Mean Entropy: 0.02093350514769554, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 587,  Mean reward: 8.0, Mean Entropy: 0.022323554381728172, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 588,  Mean reward: 7.981012658227848, Mean Entropy: 0.03896531090140343, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 589,  Mean reward: 7.922077922077922, Mean Entropy: 0.013526402413845062, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 590,  Mean reward: 8.0, Mean Entropy: 0.008972285315394402, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 591,  Mean reward: 7.961538461538462, Mean Entropy: 0.0014819324715062976, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 592,  Mean reward: 8.0, Mean Entropy: 0.002353920601308346, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 593,  Mean reward: 8.0, Mean Entropy: 0.010730540379881859, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 594,  Mean reward: 8.0, Mean Entropy: 0.12817490100860596, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 595,  Mean reward: 7.86, Mean Entropy: 0.009558801539242268, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 596,  Mean reward: 7.981012658227848, Mean Entropy: 0.01850545033812523, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 597,  Mean reward: 7.448717948717949, Mean Entropy: 0.007397368550300598, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 598,  Mean reward: 8.0, Mean Entropy: 0.0008887717849574983, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 599,  Mean reward: 8.0, Mean Entropy: 0.0009687504498288035, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 600,  Mean reward: 8.0, Mean Entropy: 0.0006322752451524138, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 8.0, Mean Entropy: 0.0004583647823892534, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 602,  Mean reward: 8.0, Mean Entropy: 0.0006097110453993082, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.01s
Iteration: 603,  Mean reward: 8.0, Mean Entropy: 0.0006871150690130889, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 604,  Mean reward: 8.0, Mean Entropy: 0.0008696226868778467, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.84s
Iteration: 605,  Mean reward: 8.0, Mean Entropy: 0.005615811794996262, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.49180173873901367, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 607,  Mean reward: -8.852272727272727, Mean Entropy: 0.4721068739891052, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 608,  Mean reward: -3.246031746031746, Mean Entropy: 0.3797276020050049, complete_episode_count: 63.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 609,  Mean reward: -4.1484375, Mean Entropy: 0.3460891842842102, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 610,  Mean reward: -1.4705882352941178, Mean Entropy: 0.2815696597099304, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 611,  Mean reward: -2.9402985074626864, Mean Entropy: 0.3508867025375366, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 612,  Mean reward: -6.515151515151516, Mean Entropy: 0.40433669090270996, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 613,  Mean reward: -0.21739130434782608, Mean Entropy: 0.3413289487361908, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 614,  Mean reward: -3.4857142857142858, Mean Entropy: 0.47350892424583435, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 615,  Mean reward: -2.8828125, Mean Entropy: 0.47505801916122437, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 616,  Mean reward: -2.55, Mean Entropy: 0.49198994040489197, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 617,  Mean reward: -0.9166666666666666, Mean Entropy: 0.7192296385765076, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.59s
Iteration: 618,  Mean reward: -1.9795918367346939, Mean Entropy: 0.6702955961227417, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 619,  Mean reward: -3.8157894736842106, Mean Entropy: 0.6981143951416016, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 620,  Mean reward: -3.2788461538461537, Mean Entropy: 0.6664246916770935, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 621,  Mean reward: -3.979591836734694, Mean Entropy: 0.6571895480155945, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 622,  Mean reward: -0.46551724137931033, Mean Entropy: 0.5972754955291748, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.65s
Iteration: 623,  Mean reward: -1.2857142857142858, Mean Entropy: 0.5231473445892334, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 624,  Mean reward: -0.5729166666666666, Mean Entropy: 0.38149935007095337, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 625,  Mean reward: -2.460526315789474, Mean Entropy: 0.2815379500389099, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 626,  Mean reward: -1.3888888888888888, Mean Entropy: 0.23190534114837646, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 627,  Mean reward: 0.031914893617021274, Mean Entropy: 0.20495860278606415, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 628,  Mean reward: -0.3111111111111111, Mean Entropy: 0.563002347946167, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.58s
Iteration: 629,  Mean reward: -4.055555555555555, Mean Entropy: 0.610663890838623, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 630,  Mean reward: -0.52, Mean Entropy: 0.4480665624141693, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 631,  Mean reward: 0.6176470588235294, Mean Entropy: 0.5409724116325378, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 632,  Mean reward: 2.324561403508772, Mean Entropy: 0.4033048152923584, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.65s
Iteration: 633,  Mean reward: 2.53, Mean Entropy: 0.6131829023361206, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 634,  Mean reward: 2.12, Mean Entropy: 0.5639090538024902, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 635,  Mean reward: 4.607142857142857, Mean Entropy: 0.5757592916488647, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 636,  Mean reward: 5.728813559322034, Mean Entropy: 0.3356829583644867, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 637,  Mean reward: -3.824324324324324, Mean Entropy: 0.39981764554977417, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 1.01s
Iteration: 638,  Mean reward: -13.016949152542374, Mean Entropy: 0.29391419887542725, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 639,  Mean reward: -5.7875, Mean Entropy: 0.4731127619743347, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 640,  Mean reward: -4.122448979591836, Mean Entropy: 0.8307889699935913, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 641,  Mean reward: -3.490740740740741, Mean Entropy: 0.8553621172904968, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.65s
Iteration: 642,  Mean reward: -2.6333333333333333, Mean Entropy: 0.7116526365280151, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 643,  Mean reward: -4.01, Mean Entropy: 0.7206989526748657, complete_episode_count: 50.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 644,  Mean reward: -4.468085106382978, Mean Entropy: 0.8045129776000977, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 645,  Mean reward: -4.829545454545454, Mean Entropy: 0.7731636762619019, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 646,  Mean reward: -3.1444444444444444, Mean Entropy: 0.589737057685852, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 647,  Mean reward: 2.106382978723404, Mean Entropy: 0.592303991317749, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 648,  Mean reward: 2.1818181818181817, Mean Entropy: 0.6668550968170166, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 649,  Mean reward: -1.830188679245283, Mean Entropy: 0.3389548659324646, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 650,  Mean reward: 6.823076923076923, Mean Entropy: 0.3916356563568115, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 651,  Mean reward: -2.977272727272727, Mean Entropy: 0.46795082092285156, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 652,  Mean reward: -2.8488372093023258, Mean Entropy: 0.5755490064620972, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 653,  Mean reward: -0.2857142857142857, Mean Entropy: 0.4981642961502075, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.64s
Iteration: 654,  Mean reward: 5.763636363636364, Mean Entropy: 0.3511134386062622, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 655,  Mean reward: 2.196078431372549, Mean Entropy: 0.4831635653972626, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 656,  Mean reward: 1.7843137254901962, Mean Entropy: 0.3641192317008972, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 657,  Mean reward: 3.6018518518518516, Mean Entropy: 0.4090415835380554, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 658,  Mean reward: 2.69811320754717, Mean Entropy: 0.4508689343929291, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 659,  Mean reward: 3.9074074074074074, Mean Entropy: 0.4339737892150879, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 660,  Mean reward: 6.4296875, Mean Entropy: 0.4887081980705261, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 661,  Mean reward: 2.764705882352941, Mean Entropy: 0.20640692114830017, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 662,  Mean reward: 5.508771929824562, Mean Entropy: 0.4873640537261963, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.61s
Iteration: 663,  Mean reward: 3.650943396226415, Mean Entropy: 0.282116174697876, complete_episode_count: 53.0, Gather time: 0.60s, Train time: 1.58s
Iteration: 664,  Mean reward: 6.080357142857143, Mean Entropy: 0.35908716917037964, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 665,  Mean reward: 6.169642857142857, Mean Entropy: 0.311981201171875, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.64s
Iteration: 666,  Mean reward: 6.55, Mean Entropy: 0.22760948538780212, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.63s
Iteration: 667,  Mean reward: 7.376923076923077, Mean Entropy: 0.206434965133667, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 668,  Mean reward: 7.73943661971831, Mean Entropy: 0.3484290540218353, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 669,  Mean reward: 7.125, Mean Entropy: 0.2083953320980072, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 670,  Mean reward: 7.614285714285714, Mean Entropy: 0.15984220802783966, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 671,  Mean reward: 7.732876712328767, Mean Entropy: 0.1580001413822174, complete_episode_count: 73.0, Gather time: 0.81s, Train time: 0.78s
Iteration: 672,  Mean reward: 7.676056338028169, Mean Entropy: 0.1576140969991684, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.85s
Iteration: 673,  Mean reward: 7.9423076923076925, Mean Entropy: 0.02525309845805168, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 674,  Mean reward: 8.0, Mean Entropy: 0.040625277906656265, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 675,  Mean reward: 7.961538461538462, Mean Entropy: 0.023999765515327454, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 676,  Mean reward: 7.981012658227848, Mean Entropy: 0.00906713493168354, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 677,  Mean reward: 8.0, Mean Entropy: 0.0052123600617051125, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 678,  Mean reward: 8.0, Mean Entropy: 0.004194230772554874, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 679,  Mean reward: 8.0, Mean Entropy: 0.004951836541295052, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 680,  Mean reward: 8.0, Mean Entropy: 0.004706121981143951, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 681,  Mean reward: 8.0, Mean Entropy: 0.003344241064041853, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 682,  Mean reward: 8.0, Mean Entropy: 0.003355435561388731, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.86s
Iteration: 683,  Mean reward: 8.0, Mean Entropy: 0.003343349788337946, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 684,  Mean reward: 8.0, Mean Entropy: 0.0036006621085107327, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 685,  Mean reward: 8.0, Mean Entropy: 0.0047399019822478294, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 686,  Mean reward: 8.0, Mean Entropy: 0.008880064822733402, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 687,  Mean reward: -2.0, Mean Entropy: 0.029948068782687187, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 688,  Mean reward: -2.3987341772151898, Mean Entropy: 0.631058931350708, complete_episode_count: 79.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 689,  Mean reward: -0.26515151515151514, Mean Entropy: 0.1975172758102417, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 690,  Mean reward: -1.0, Mean Entropy: 0.20966720581054688, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 691,  Mean reward: -2.411764705882353, Mean Entropy: 0.2748701274394989, complete_episode_count: 68.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 692,  Mean reward: -1.3181818181818181, Mean Entropy: 0.35792621970176697, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 693,  Mean reward: -3.5762711864406778, Mean Entropy: 0.4874797463417053, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.62s
Iteration: 694,  Mean reward: -0.890625, Mean Entropy: 0.6848132014274597, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 695,  Mean reward: -0.2711864406779661, Mean Entropy: 0.5810959339141846, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 696,  Mean reward: -0.24561403508771928, Mean Entropy: 0.5616282224655151, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.59s
Iteration: 697,  Mean reward: -0.5925925925925926, Mean Entropy: 0.563917875289917, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.64s
Iteration: 698,  Mean reward: 1.0737704918032787, Mean Entropy: 0.8392667770385742, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 699,  Mean reward: -1.2234042553191489, Mean Entropy: 0.8474173545837402, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 700,  Mean reward: -1.875, Mean Entropy: 0.7590731382369995, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.61s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -2.4711538461538463, Mean Entropy: 0.644012451171875, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 702,  Mean reward: -1.5263157894736843, Mean Entropy: 0.4968421459197998, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 703,  Mean reward: 0.5084745762711864, Mean Entropy: 0.5846198797225952, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 704,  Mean reward: 0.4519230769230769, Mean Entropy: 0.6226202845573425, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 705,  Mean reward: 2.0655737704918034, Mean Entropy: 0.5690755844116211, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 706,  Mean reward: 5.051724137931035, Mean Entropy: 0.8926753997802734, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.57s
Iteration: 707,  Mean reward: -0.6595744680851063, Mean Entropy: 0.945553183555603, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.81s
Iteration: 708,  Mean reward: -4.878048780487805, Mean Entropy: 0.845967173576355, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 709,  Mean reward: -4.865853658536586, Mean Entropy: 0.668055534362793, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 710,  Mean reward: -7.175, Mean Entropy: 0.5590875744819641, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 711,  Mean reward: -5.085714285714285, Mean Entropy: 0.4684182107448578, complete_episode_count: 35.0, Gather time: 0.56s, Train time: 1.64s
Iteration: 712,  Mean reward: -5.583333333333333, Mean Entropy: 0.6375705003738403, complete_episode_count: 36.0, Gather time: 0.56s, Train time: 1.62s
Iteration: 713,  Mean reward: -4.571428571428571, Mean Entropy: 0.7345069646835327, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 714,  Mean reward: -2.9634146341463414, Mean Entropy: 0.6784775257110596, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 715,  Mean reward: -4.130434782608695, Mean Entropy: 0.6837730407714844, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 716,  Mean reward: -5.112244897959184, Mean Entropy: 0.7223024368286133, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 717,  Mean reward: -4.130434782608695, Mean Entropy: 0.7038442492485046, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.60s
Iteration: 718,  Mean reward: -4.06, Mean Entropy: 0.6998353004455566, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 719,  Mean reward: -2.141509433962264, Mean Entropy: 0.6289172768592834, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 720,  Mean reward: -5.0, Mean Entropy: 0.704624354839325, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 721,  Mean reward: -3.4722222222222223, Mean Entropy: 0.7291616201400757, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 722,  Mean reward: -5.0, Mean Entropy: 0.7507240772247314, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 723,  Mean reward: -5.15, Mean Entropy: 0.7833056449890137, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 724,  Mean reward: -4.184782608695652, Mean Entropy: 0.7772530317306519, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.65s
Iteration: 725,  Mean reward: -3.5, Mean Entropy: 0.8287376165390015, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.65s
Iteration: 726,  Mean reward: -5.695121951219512, Mean Entropy: 0.8056544065475464, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.63s
Iteration: 727,  Mean reward: -3.852272727272727, Mean Entropy: 0.8427528142929077, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.60s
Iteration: 728,  Mean reward: -2.4468085106382977, Mean Entropy: 0.8653324842453003, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 729,  Mean reward: -1.8333333333333333, Mean Entropy: 0.8593422770500183, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 730,  Mean reward: -4.829787234042553, Mean Entropy: 0.8466174006462097, complete_episode_count: 47.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 731,  Mean reward: -4.040816326530612, Mean Entropy: 0.9201450347900391, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.58s
Iteration: 732,  Mean reward: -4.552083333333333, Mean Entropy: 0.8387476205825806, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 733,  Mean reward: -2.509433962264151, Mean Entropy: 0.780272364616394, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 734,  Mean reward: -4.5, Mean Entropy: 0.8087705373764038, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 735,  Mean reward: -1.4090909090909092, Mean Entropy: 0.8063721656799316, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.62s
Iteration: 736,  Mean reward: -4.086538461538462, Mean Entropy: 0.7705137133598328, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 737,  Mean reward: -2.2549019607843137, Mean Entropy: 0.7210758924484253, complete_episode_count: 51.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 738,  Mean reward: -4.777777777777778, Mean Entropy: 0.7878556251525879, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 739,  Mean reward: -3.83, Mean Entropy: 0.8278790712356567, complete_episode_count: 50.0, Gather time: 0.59s, Train time: 1.81s
Iteration: 740,  Mean reward: -0.8725490196078431, Mean Entropy: 0.8780479431152344, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 741,  Mean reward: -2.7058823529411766, Mean Entropy: 0.8280550837516785, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.59s
Iteration: 742,  Mean reward: 0.09433962264150944, Mean Entropy: 0.7690285444259644, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.58s
Iteration: 743,  Mean reward: -0.4745762711864407, Mean Entropy: 0.763051450252533, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 744,  Mean reward: 0.6864406779661016, Mean Entropy: 0.7030166387557983, complete_episode_count: 59.0, Gather time: 0.60s, Train time: 1.62s
Iteration: 745,  Mean reward: 2.2857142857142856, Mean Entropy: 0.6866631507873535, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.84s
Iteration: 746,  Mean reward: 3.4152542372881354, Mean Entropy: 0.5423228144645691, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.60s
Iteration: 747,  Mean reward: 3.446969696969697, Mean Entropy: 0.5857269763946533, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 748,  Mean reward: 4.25, Mean Entropy: 0.4288247227668762, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 749,  Mean reward: 3.818840579710145, Mean Entropy: 0.5102246999740601, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 750,  Mean reward: 6.875, Mean Entropy: 0.434116929769516, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 751,  Mean reward: 6.088709677419355, Mean Entropy: 0.35853704810142517, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 752,  Mean reward: 6.809859154929577, Mean Entropy: 0.23161821067333221, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 753,  Mean reward: 7.413043478260869, Mean Entropy: 0.24825388193130493, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 754,  Mean reward: 6.736486486486487, Mean Entropy: 0.3893529772758484, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.83s
Iteration: 755,  Mean reward: 3.6538461538461537, Mean Entropy: 0.44421499967575073, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 756,  Mean reward: 1.7049180327868851, Mean Entropy: 0.332143634557724, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 757,  Mean reward: 5.914285714285715, Mean Entropy: 0.41466355323791504, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.85s
Iteration: 758,  Mean reward: 4.3538461538461535, Mean Entropy: 0.2449263036251068, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 759,  Mean reward: 7.028985507246377, Mean Entropy: 0.35765522718429565, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 760,  Mean reward: 6.5227272727272725, Mean Entropy: 0.2857767939567566, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.83s
Iteration: 761,  Mean reward: 7.367647058823529, Mean Entropy: 0.1746312826871872, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 762,  Mean reward: 7.6875, Mean Entropy: 0.1462918221950531, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 763,  Mean reward: 7.573333333333333, Mean Entropy: 0.211911141872406, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.83s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.43it/s]100%|| 1/1 [00:00<00:00,  1.43it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.78it/s]100%|| 1/1 [00:00<00:00,  1.78it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.40it/s]100%|| 1/1 [00:00<00:00,  1.40it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: 8.0
  std over seeds: 0.0
  per seed: [8.000 8.000 8.000]

success_rate.......
  avg over seeds: 1.0
  std over seeds: 0.0
  per seed: [1.000 1.000 1.000]

batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: None
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: q
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
node_dim: 7
lstm_on: False
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy(
  (FE): FeatureExtractor(
    (gat): GATv2(7, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (theta6_v): Linear(in_features=24, out_features=24, bias=True)
    (theta7_v): Linear(in_features=24, out_features=24, bias=True)
    (theta5_v): Linear(in_features=48, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with LSTM switched off and GATv2 feature extraction
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.theta6_v.weight        [24, 24]     requires_grad=True
V.theta6_v.bias          [24]         requires_grad=True
V.theta7_v.weight        [24, 24]     requires_grad=True
V.theta7_v.bias          [24]         requires_grad=True
V.theta5_v.weight        [1, 48]      requires_grad=True
V.theta5_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 7922
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -6.034883720930233, Mean Entropy: 0.9602975845336914, complete_episode_count: 43.0, Gather time: 5.33s, Train time: 3.49s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -2.2857142857142856, Mean Entropy: 0.9458556175231934, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 2,  Mean reward: -3.892857142857143, Mean Entropy: 0.9458569884300232, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 3,  Mean reward: -4.871794871794871, Mean Entropy: 0.9169754385948181, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 4,  Mean reward: -5.605263157894737, Mean Entropy: 0.938635528087616, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.54s
Iteration: 5,  Mean reward: -4.7368421052631575, Mean Entropy: 0.9097554683685303, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 6,  Mean reward: -1.6363636363636365, Mean Entropy: 0.9386293888092041, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 7,  Mean reward: -5.3023255813953485, Mean Entropy: 0.9025101065635681, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 8,  Mean reward: -4.476190476190476, Mean Entropy: 0.924125075340271, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 9,  Mean reward: -5.975, Mean Entropy: 0.9673546552658081, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 10,  Mean reward: -2.588888888888889, Mean Entropy: 0.9168679118156433, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 11,  Mean reward: -3.5357142857142856, Mean Entropy: 1.010695457458496, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 12,  Mean reward: -3.5543478260869565, Mean Entropy: 0.938382625579834, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 13,  Mean reward: -4.25, Mean Entropy: 0.9094209671020508, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.52s
Iteration: 14,  Mean reward: -5.138297872340425, Mean Entropy: 0.9743382334709167, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 15,  Mean reward: -1.7222222222222223, Mean Entropy: 0.9383792281150818, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 16,  Mean reward: -6.5875, Mean Entropy: 0.9671947360038757, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.49s
Iteration: 17,  Mean reward: -2.1627906976744184, Mean Entropy: 0.9599367380142212, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 18,  Mean reward: -3.2222222222222223, Mean Entropy: 0.8949186205863953, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 19,  Mean reward: -1.3068181818181819, Mean Entropy: 0.945487916469574, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 20,  Mean reward: -5.690476190476191, Mean Entropy: 0.9309698939323425, complete_episode_count: 42.0, Gather time: 0.63s, Train time: 1.47s
Iteration: 21,  Mean reward: -5.523809523809524, Mean Entropy: 0.9738011360168457, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 22,  Mean reward: -5.315789473684211, Mean Entropy: 0.9083697199821472, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 23,  Mean reward: -4.690476190476191, Mean Entropy: 0.9516795873641968, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.71s
Iteration: 24,  Mean reward: -3.7023809523809526, Mean Entropy: 1.0024935007095337, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 25,  Mean reward: -3.0697674418604652, Mean Entropy: 0.9233788251876831, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.56s
Iteration: 26,  Mean reward: -2.125, Mean Entropy: 0.9663716554641724, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 27,  Mean reward: -4.1022727272727275, Mean Entropy: 0.9730607867240906, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 28,  Mean reward: -2.840909090909091, Mean Entropy: 1.0165519714355469, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 29,  Mean reward: -5.538461538461538, Mean Entropy: 1.0012532472610474, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 30,  Mean reward: -2.2023809523809526, Mean Entropy: 0.9651360511779785, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 31,  Mean reward: -6.0875, Mean Entropy: 0.8928060531616211, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.53s
Iteration: 32,  Mean reward: -2.8085106382978724, Mean Entropy: 0.9438161253929138, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 33,  Mean reward: -6.7894736842105265, Mean Entropy: 0.9070922136306763, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 34,  Mean reward: -5.402439024390244, Mean Entropy: 1.0057904720306396, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.65s
Iteration: 35,  Mean reward: -3.302325581395349, Mean Entropy: 0.9490097761154175, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.54s
Iteration: 36,  Mean reward: -4.464285714285714, Mean Entropy: 0.9483922719955444, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.53s
Iteration: 37,  Mean reward: -4.065217391304348, Mean Entropy: 0.9201361536979675, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.57s
Iteration: 38,  Mean reward: -5.181818181818182, Mean Entropy: 0.9851959943771362, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.57s
Iteration: 39,  Mean reward: -3.8777777777777778, Mean Entropy: 0.9207191467285156, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.54s
Iteration: 40,  Mean reward: -6.384615384615385, Mean Entropy: 0.996346652507782, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 41,  Mean reward: -5.035714285714286, Mean Entropy: 0.9327117800712585, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 42,  Mean reward: -4.395348837209302, Mean Entropy: 0.9903435707092285, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.53s
Iteration: 43,  Mean reward: -2.768292682926829, Mean Entropy: 1.0502893924713135, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.52s
Iteration: 44,  Mean reward: -2.533333333333333, Mean Entropy: 0.9729675054550171, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 45,  Mean reward: -3.9886363636363638, Mean Entropy: 0.9369043707847595, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 46,  Mean reward: -4.290697674418604, Mean Entropy: 0.9644478559494019, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.49s
Iteration: 47,  Mean reward: -4.630952380952381, Mean Entropy: 0.9633512496948242, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 48,  Mean reward: -2.813953488372093, Mean Entropy: 0.9214835166931152, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 49,  Mean reward: -2.869047619047619, Mean Entropy: 1.030800700187683, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.64s
Iteration: 50,  Mean reward: -4.2682926829268295, Mean Entropy: 1.001318335533142, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 51,  Mean reward: -3.5555555555555554, Mean Entropy: 0.9638681411743164, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 52,  Mean reward: -6.512195121951219, Mean Entropy: 0.9106402397155762, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 53,  Mean reward: -5.7727272727272725, Mean Entropy: 0.9543272852897644, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 54,  Mean reward: -2.9, Mean Entropy: 0.9790105819702148, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 55,  Mean reward: -4.75, Mean Entropy: 0.9139622449874878, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.54s
Iteration: 56,  Mean reward: -5.093023255813954, Mean Entropy: 0.9782487154006958, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 57,  Mean reward: -3.75, Mean Entropy: 0.9558445811271667, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.72s
Iteration: 58,  Mean reward: -5.321428571428571, Mean Entropy: 0.9181826114654541, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.52s
Iteration: 59,  Mean reward: -4.7, Mean Entropy: 0.9684737920761108, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 60,  Mean reward: -5.463414634146342, Mean Entropy: 0.9104030132293701, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 61,  Mean reward: -4.357142857142857, Mean Entropy: 0.9251426458358765, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 62,  Mean reward: -3.425, Mean Entropy: 0.955569863319397, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 63,  Mean reward: -3.926829268292683, Mean Entropy: 0.9382867813110352, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 64,  Mean reward: -4.146341463414634, Mean Entropy: 0.9242490530014038, complete_episode_count: 41.0, Gather time: 0.68s, Train time: 1.49s
Iteration: 65,  Mean reward: -4.5227272727272725, Mean Entropy: 0.916151225566864, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 66,  Mean reward: -3.6630434782608696, Mean Entropy: 0.920036792755127, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 67,  Mean reward: -5.364583333333333, Mean Entropy: 0.9960347414016724, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.56s
Iteration: 68,  Mean reward: -3.5232558139534884, Mean Entropy: 0.9623111486434937, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.57s
Iteration: 69,  Mean reward: -3.27, Mean Entropy: 0.8223681449890137, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 70,  Mean reward: -3.5, Mean Entropy: 0.8248352408409119, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 71,  Mean reward: -1.1636363636363636, Mean Entropy: 0.8359972834587097, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 72,  Mean reward: -2.656862745098039, Mean Entropy: 0.8346506953239441, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.54s
Iteration: 73,  Mean reward: -2.790909090909091, Mean Entropy: 0.8048598766326904, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 74,  Mean reward: -4.381818181818182, Mean Entropy: 0.8060773611068726, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 75,  Mean reward: -1.6851851851851851, Mean Entropy: 0.9279448390007019, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 76,  Mean reward: -2.7127659574468086, Mean Entropy: 0.8914159536361694, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 77,  Mean reward: -2.923076923076923, Mean Entropy: 0.43058499693870544, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 78,  Mean reward: -2.341666666666667, Mean Entropy: 0.3967331051826477, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.64s
Iteration: 79,  Mean reward: -1.3412698412698412, Mean Entropy: 0.27462780475616455, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 80,  Mean reward: -3.6838235294117645, Mean Entropy: 0.1813839077949524, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 81,  Mean reward: -1.0869565217391304, Mean Entropy: 0.25185948610305786, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 82,  Mean reward: -1.9782608695652173, Mean Entropy: 0.19777731597423553, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 83,  Mean reward: -2.6285714285714286, Mean Entropy: 0.23427997529506683, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 84,  Mean reward: -2.6865671641791047, Mean Entropy: 0.19207724928855896, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 85,  Mean reward: -2.9571428571428573, Mean Entropy: 0.23510432243347168, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 86,  Mean reward: -2.6194029850746268, Mean Entropy: 0.28937411308288574, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 87,  Mean reward: -2.3671875, Mean Entropy: 0.25470221042633057, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 88,  Mean reward: -2.213235294117647, Mean Entropy: 0.1508205235004425, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 89,  Mean reward: -1.3985507246376812, Mean Entropy: 0.21788808703422546, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 90,  Mean reward: -3.9296875, Mean Entropy: 0.1964135617017746, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 91,  Mean reward: -3.427536231884058, Mean Entropy: 0.19122986495494843, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 92,  Mean reward: -1.2, Mean Entropy: 0.3075537085533142, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.98s
Iteration: 93,  Mean reward: -2.6031746031746033, Mean Entropy: 0.2776939272880554, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 94,  Mean reward: -2.08955223880597, Mean Entropy: 0.19135811924934387, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 95,  Mean reward: 0.3561643835616438, Mean Entropy: 0.21178005635738373, complete_episode_count: 73.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 96,  Mean reward: -1.8357142857142856, Mean Entropy: 0.22710195183753967, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 97,  Mean reward: -2.3656716417910446, Mean Entropy: 0.23189297318458557, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 98,  Mean reward: -3.9338235294117645, Mean Entropy: 0.15297099947929382, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 99,  Mean reward: -4.992424242424242, Mean Entropy: 0.20894154906272888, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 100,  Mean reward: -2.7573529411764706, Mean Entropy: 0.24506764113903046, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.92s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -1.2013888888888888, Mean Entropy: 0.30773696303367615, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 102,  Mean reward: -3.2388059701492535, Mean Entropy: 0.2938718795776367, complete_episode_count: 67.0, Gather time: 0.61s, Train time: 0.74s
Iteration: 103,  Mean reward: -1.5808823529411764, Mean Entropy: 0.1913013458251953, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 104,  Mean reward: -1.4583333333333333, Mean Entropy: 0.1905217170715332, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 105,  Mean reward: -4.666666666666667, Mean Entropy: 0.20935939252376556, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 106,  Mean reward: -2.3857142857142857, Mean Entropy: 0.19731663167476654, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 107,  Mean reward: -3.6176470588235294, Mean Entropy: 0.30110615491867065, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 108,  Mean reward: -0.40714285714285714, Mean Entropy: 0.2484988272190094, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 109,  Mean reward: -1.0492957746478873, Mean Entropy: 0.21627682447433472, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 110,  Mean reward: -1.492537313432836, Mean Entropy: 0.1992589235305786, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 111,  Mean reward: -1.4477611940298507, Mean Entropy: 0.23113825917243958, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.83s
Iteration: 112,  Mean reward: -2.917910447761194, Mean Entropy: 0.22924791276454926, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 113,  Mean reward: -3.13768115942029, Mean Entropy: 0.29271841049194336, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 114,  Mean reward: -3.647887323943662, Mean Entropy: 0.3592184782028198, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 115,  Mean reward: -2.287878787878788, Mean Entropy: 0.3221256732940674, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 116,  Mean reward: -1.4477611940298507, Mean Entropy: 0.25880342721939087, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 117,  Mean reward: -3.449275362318841, Mean Entropy: 0.1789441704750061, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 118,  Mean reward: -1.0492957746478873, Mean Entropy: 0.22509513795375824, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.81s
Iteration: 119,  Mean reward: -1.1597222222222223, Mean Entropy: 0.2554757595062256, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 120,  Mean reward: -1.091549295774648, Mean Entropy: 0.21722429990768433, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 121,  Mean reward: -0.13013698630136986, Mean Entropy: 0.23727604746818542, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 122,  Mean reward: -3.8257575757575757, Mean Entropy: 0.2693580389022827, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 123,  Mean reward: -3.4545454545454546, Mean Entropy: 0.23161308467388153, complete_episode_count: 66.0, Gather time: 0.70s, Train time: 0.78s
Iteration: 124,  Mean reward: -1.3732394366197183, Mean Entropy: 0.2562786042690277, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 125,  Mean reward: -1.0492957746478873, Mean Entropy: 0.27812904119491577, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 126,  Mean reward: -4.142857142857143, Mean Entropy: 0.22684922814369202, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 127,  Mean reward: -0.8819444444444444, Mean Entropy: 0.2068336308002472, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 128,  Mean reward: -2.3642857142857143, Mean Entropy: 0.2240646779537201, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 129,  Mean reward: -4.037878787878788, Mean Entropy: 0.27593696117401123, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 130,  Mean reward: -3.073529411764706, Mean Entropy: 0.2895190715789795, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 131,  Mean reward: -3.3059701492537314, Mean Entropy: 0.26993831992149353, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 132,  Mean reward: -1.6666666666666667, Mean Entropy: 0.25114676356315613, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 133,  Mean reward: -2.847826086956522, Mean Entropy: 0.2780371606349945, complete_episode_count: 69.0, Gather time: 0.75s, Train time: 0.76s
Iteration: 134,  Mean reward: -1.591549295774648, Mean Entropy: 0.2732369899749756, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 135,  Mean reward: -2.3863636363636362, Mean Entropy: 0.22994643449783325, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 136,  Mean reward: -2.6714285714285713, Mean Entropy: 0.21310409903526306, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 137,  Mean reward: -2.6641791044776117, Mean Entropy: 0.264723002910614, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 138,  Mean reward: -0.6285714285714286, Mean Entropy: 0.30676281452178955, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 139,  Mean reward: -3.953125, Mean Entropy: 0.3702127933502197, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 140,  Mean reward: -4.015873015873016, Mean Entropy: 0.2894097566604614, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 141,  Mean reward: -2.7573529411764706, Mean Entropy: 0.26009637117385864, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 142,  Mean reward: -3.8257575757575757, Mean Entropy: 0.23601941764354706, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.82s
Iteration: 143,  Mean reward: -2.9402985074626864, Mean Entropy: 0.20692205429077148, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 144,  Mean reward: -1.75, Mean Entropy: 0.1896713227033615, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 145,  Mean reward: -1.8571428571428572, Mean Entropy: 0.24375973641872406, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 146,  Mean reward: -0.8928571428571429, Mean Entropy: 0.3142696022987366, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 147,  Mean reward: -1.1923076923076923, Mean Entropy: 0.354625403881073, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 148,  Mean reward: -4.84375, Mean Entropy: 0.30918535590171814, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.83s
Iteration: 149,  Mean reward: -3.13768115942029, Mean Entropy: 0.21569621562957764, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 150,  Mean reward: -2.5694444444444446, Mean Entropy: 0.2113383710384369, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 151,  Mean reward: -3.2388059701492535, Mean Entropy: 0.2433643341064453, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 152,  Mean reward: -5.095238095238095, Mean Entropy: 0.338267982006073, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 153,  Mean reward: -3.13768115942029, Mean Entropy: 0.37436389923095703, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 154,  Mean reward: -3.377049180327869, Mean Entropy: 0.19177672266960144, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 155,  Mean reward: -3.7246376811594204, Mean Entropy: 0.20388536155223846, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 156,  Mean reward: -2.76056338028169, Mean Entropy: 0.1936626136302948, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 157,  Mean reward: -1.2428571428571429, Mean Entropy: 0.3134879767894745, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 158,  Mean reward: -1.5461538461538462, Mean Entropy: 0.38348159193992615, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 159,  Mean reward: -2.9206349206349205, Mean Entropy: 0.35885700583457947, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 160,  Mean reward: -1.625, Mean Entropy: 0.25757333636283875, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 161,  Mean reward: -0.09027777777777778, Mean Entropy: 0.2394474744796753, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 162,  Mean reward: -3.5285714285714285, Mean Entropy: 0.2842441201210022, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 163,  Mean reward: -2.9402985074626864, Mean Entropy: 0.2301074117422104, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 164,  Mean reward: -4.25, Mean Entropy: 0.24659490585327148, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 165,  Mean reward: -1.3732394366197183, Mean Entropy: 0.19832411408424377, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 166,  Mean reward: -3.2388059701492535, Mean Entropy: 0.24927937984466553, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 167,  Mean reward: -2.3857142857142857, Mean Entropy: 0.282897412776947, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 168,  Mean reward: -0.4855072463768116, Mean Entropy: 0.21382057666778564, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 169,  Mean reward: -2.6714285714285713, Mean Entropy: 0.18903867900371552, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 170,  Mean reward: -0.7887323943661971, Mean Entropy: 0.20767951011657715, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 171,  Mean reward: -0.3472222222222222, Mean Entropy: 0.2379494607448578, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 172,  Mean reward: -2.235294117647059, Mean Entropy: 0.2776411771774292, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 173,  Mean reward: -0.7887323943661971, Mean Entropy: 0.2683146893978119, complete_episode_count: 71.0, Gather time: 0.77s, Train time: 0.76s
Iteration: 174,  Mean reward: -2.463235294117647, Mean Entropy: 0.2682965397834778, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 175,  Mean reward: -3.0294117647058822, Mean Entropy: 0.22833451628684998, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 176,  Mean reward: -2.1911764705882355, Mean Entropy: 0.20060960948467255, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 177,  Mean reward: -2.5694444444444446, Mean Entropy: 0.1844201385974884, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 178,  Mean reward: -2.289855072463768, Mean Entropy: 0.21163710951805115, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 179,  Mean reward: -1.591549295774648, Mean Entropy: 0.24948793649673462, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 180,  Mean reward: -1.9366197183098592, Mean Entropy: 0.21333251893520355, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 181,  Mean reward: -3.0955882352941178, Mean Entropy: 0.2711535096168518, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 182,  Mean reward: -2.6417910447761193, Mean Entropy: 0.24972185492515564, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 183,  Mean reward: -2.08955223880597, Mean Entropy: 0.24215559661388397, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 184,  Mean reward: -1.9366197183098592, Mean Entropy: 0.23809604346752167, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 185,  Mean reward: -3.13768115942029, Mean Entropy: 0.20031039416790009, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 186,  Mean reward: -3.514925373134328, Mean Entropy: 0.237782284617424, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 187,  Mean reward: -4.142857142857143, Mean Entropy: 0.2504315972328186, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 188,  Mean reward: -3.0942028985507246, Mean Entropy: 0.24589157104492188, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 189,  Mean reward: -3.582089552238806, Mean Entropy: 0.2271866500377655, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 190,  Mean reward: -2.343283582089552, Mean Entropy: 0.24338309466838837, complete_episode_count: 67.0, Gather time: 0.71s, Train time: 0.75s
Iteration: 191,  Mean reward: -0.9236111111111112, Mean Entropy: 0.25574642419815063, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 192,  Mean reward: -2.76056338028169, Mean Entropy: 0.2566303312778473, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 193,  Mean reward: -2.692857142857143, Mean Entropy: 0.25896137952804565, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 194,  Mean reward: -2.893939393939394, Mean Entropy: 0.2637591063976288, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 195,  Mean reward: -3.8805970149253732, Mean Entropy: 0.2677727937698364, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 196,  Mean reward: -1.1044776119402986, Mean Entropy: 0.2784767746925354, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 197,  Mean reward: -2.7573529411764706, Mean Entropy: 0.26390212774276733, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 198,  Mean reward: -2.6417910447761193, Mean Entropy: 0.22971054911613464, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 199,  Mean reward: -2.1, Mean Entropy: 0.22369733452796936, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 200,  Mean reward: -2.4191176470588234, Mean Entropy: 0.2389088124036789, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -3.6267605633802815, Mean Entropy: 0.2359618842601776, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 202,  Mean reward: -1.4, Mean Entropy: 0.23604416847229004, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 203,  Mean reward: -2.1691176470588234, Mean Entropy: 0.24766677618026733, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 204,  Mean reward: -3.887323943661972, Mean Entropy: 0.24343693256378174, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 205,  Mean reward: -2.735294117647059, Mean Entropy: 0.1906614750623703, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 206,  Mean reward: -2.2196969696969697, Mean Entropy: 0.1953321397304535, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 207,  Mean reward: -2.5579710144927534, Mean Entropy: 0.1848948448896408, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 208,  Mean reward: -2.1214285714285714, Mean Entropy: 0.15033432841300964, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 209,  Mean reward: -0.8819444444444444, Mean Entropy: 0.2601434886455536, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 210,  Mean reward: -4.96969696969697, Mean Entropy: 0.2937213182449341, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 211,  Mean reward: -3.9338235294117645, Mean Entropy: 0.2580173909664154, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 212,  Mean reward: -0.9444444444444444, Mean Entropy: 0.17744319140911102, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 1.11s
Iteration: 213,  Mean reward: -1.0869565217391304, Mean Entropy: 0.20707325637340546, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 214,  Mean reward: -3.9779411764705883, Mean Entropy: 0.23288416862487793, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 215,  Mean reward: -3.739130434782609, Mean Entropy: 0.21540510654449463, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.82s
Iteration: 216,  Mean reward: -2.692857142857143, Mean Entropy: 0.22005045413970947, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 217,  Mean reward: -1.3309859154929577, Mean Entropy: 0.25371935963630676, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 218,  Mean reward: -2.4788732394366195, Mean Entropy: 0.24038581550121307, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 219,  Mean reward: -3.6769230769230767, Mean Entropy: 0.19139894843101501, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 220,  Mean reward: -0.8188405797101449, Mean Entropy: 0.15156397223472595, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 221,  Mean reward: -4.565217391304348, Mean Entropy: 0.17847172915935516, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 222,  Mean reward: -1.4202898550724639, Mean Entropy: 0.17185822129249573, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 223,  Mean reward: -3.859375, Mean Entropy: 0.2293650358915329, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 224,  Mean reward: -3.760869565217391, Mean Entropy: 0.23981642723083496, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 225,  Mean reward: -4.522058823529412, Mean Entropy: 0.3119526505470276, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 226,  Mean reward: -2.6641791044776117, Mean Entropy: 0.333934485912323, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 227,  Mean reward: -4.663934426229508, Mean Entropy: 0.24443738162517548, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 228,  Mean reward: -2.5579710144927534, Mean Entropy: 0.19785666465759277, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 229,  Mean reward: -2.5277777777777777, Mean Entropy: 0.2282029390335083, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 230,  Mean reward: -2.342857142857143, Mean Entropy: 0.24268150329589844, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 231,  Mean reward: -1.2, Mean Entropy: 0.2608550786972046, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 232,  Mean reward: -1.036764705882353, Mean Entropy: 0.1598932296037674, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 233,  Mean reward: -3.5597014925373136, Mean Entropy: 0.15792407095432281, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 234,  Mean reward: -1.4642857142857142, Mean Entropy: 0.12727633118629456, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.88s
Iteration: 235,  Mean reward: -4.5, Mean Entropy: 0.16824601590633392, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 236,  Mean reward: -3.5285714285714285, Mean Entropy: 0.21053731441497803, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 237,  Mean reward: -2.536231884057971, Mean Entropy: 0.23009786009788513, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.82s
Iteration: 238,  Mean reward: -4.8161764705882355, Mean Entropy: 0.29018568992614746, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 239,  Mean reward: -2.9402985074626864, Mean Entropy: 0.25733014941215515, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 240,  Mean reward: -1.792857142857143, Mean Entropy: 0.25836804509162903, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.81s
Iteration: 241,  Mean reward: -1.5071428571428571, Mean Entropy: 0.21259966492652893, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 242,  Mean reward: -3.911764705882353, Mean Entropy: 0.2072385549545288, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 243,  Mean reward: -2.601449275362319, Mean Entropy: 0.1444569230079651, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 244,  Mean reward: -2.044776119402985, Mean Entropy: 0.17661750316619873, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 245,  Mean reward: -2.713235294117647, Mean Entropy: 0.22597865760326385, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 246,  Mean reward: -3.1458333333333335, Mean Entropy: 0.2621175944805145, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 247,  Mean reward: -1.8732394366197183, Mean Entropy: 0.2960122525691986, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 248,  Mean reward: -1.2426470588235294, Mean Entropy: 0.31078100204467773, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 249,  Mean reward: -2.7794117647058822, Mean Entropy: 0.26043835282325745, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 250,  Mean reward: -2.047945205479452, Mean Entropy: 0.1976771205663681, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 251,  Mean reward: -3.911764705882353, Mean Entropy: 0.20232892036437988, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 252,  Mean reward: -3.9779411764705883, Mean Entropy: 0.22048042714595795, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.98s
Iteration: 253,  Mean reward: -2.3857142857142857, Mean Entropy: 0.23393851518630981, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 254,  Mean reward: -2.289855072463768, Mean Entropy: 0.22380158305168152, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 255,  Mean reward: -1.3333333333333333, Mean Entropy: 0.20620378851890564, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 256,  Mean reward: -0.9142857142857143, Mean Entropy: 0.2287723422050476, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 257,  Mean reward: -2.2681159420289854, Mean Entropy: 0.24711012840270996, complete_episode_count: 69.0, Gather time: 0.72s, Train time: 0.73s
Iteration: 258,  Mean reward: -0.9357142857142857, Mean Entropy: 0.21953925490379333, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 259,  Mean reward: -1.6666666666666667, Mean Entropy: 0.20668867230415344, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 260,  Mean reward: -1.8943661971830985, Mean Entropy: 0.20930102467536926, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 261,  Mean reward: -2.692857142857143, Mean Entropy: 0.24832108616828918, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.71s
Iteration: 262,  Mean reward: -2.4191176470588234, Mean Entropy: 0.25834038853645325, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 263,  Mean reward: -3.1041666666666665, Mean Entropy: 0.2658332586288452, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 264,  Mean reward: -3.0955882352941178, Mean Entropy: 0.3082526922225952, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 265,  Mean reward: -1.8307692307692307, Mean Entropy: 0.24710536003112793, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 266,  Mean reward: -1.3309859154929577, Mean Entropy: 0.1997726559638977, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 267,  Mean reward: -4.838235294117647, Mean Entropy: 0.18029145896434784, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 268,  Mean reward: -1.028169014084507, Mean Entropy: 0.1764001101255417, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 269,  Mean reward: -1.8529411764705883, Mean Entropy: 0.1947290301322937, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 270,  Mean reward: -3.196969696969697, Mean Entropy: 0.21654385328292847, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 271,  Mean reward: -2.869565217391304, Mean Entropy: 0.181332528591156, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.82s
Iteration: 272,  Mean reward: -2.013888888888889, Mean Entropy: 0.1953149139881134, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 273,  Mean reward: -2.2681159420289854, Mean Entropy: 0.18863384425640106, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 274,  Mean reward: -0.6544117647058824, Mean Entropy: 0.17941030859947205, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 275,  Mean reward: -1.0492957746478873, Mean Entropy: 0.2587370276451111, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 276,  Mean reward: -3.0074626865671643, Mean Entropy: 0.3189309537410736, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 277,  Mean reward: -1.1470588235294117, Mean Entropy: 0.24380362033843994, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 278,  Mean reward: -1.6338028169014085, Mean Entropy: 0.22219198942184448, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 279,  Mean reward: -2.3333333333333335, Mean Entropy: 0.19376316666603088, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.88s
Iteration: 280,  Mean reward: -3.063380281690141, Mean Entropy: 0.2759845554828644, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 281,  Mean reward: -2.601449275362319, Mean Entropy: 0.29753464460372925, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 282,  Mean reward: 0.028985507246376812, Mean Entropy: 0.22916769981384277, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 283,  Mean reward: -1.3732394366197183, Mean Entropy: 0.19514551758766174, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 284,  Mean reward: -0.36428571428571427, Mean Entropy: 0.17942146956920624, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 285,  Mean reward: -1.2214285714285715, Mean Entropy: 0.19520720839500427, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 286,  Mean reward: -2.485294117647059, Mean Entropy: 0.20960280299186707, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 287,  Mean reward: -4.161764705882353, Mean Entropy: 0.1811242550611496, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 288,  Mean reward: -0.7191780821917808, Mean Entropy: 0.14960840344429016, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 289,  Mean reward: -3.584507042253521, Mean Entropy: 0.21144312620162964, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 290,  Mean reward: -2.2916666666666665, Mean Entropy: 0.22120462357997894, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 291,  Mean reward: -0.528169014084507, Mean Entropy: 0.19035296142101288, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 292,  Mean reward: -2.6641791044776117, Mean Entropy: 0.20581702888011932, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.99s
Iteration: 293,  Mean reward: -2.8043478260869565, Mean Entropy: 0.23192380368709564, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 294,  Mean reward: -1.4583333333333333, Mean Entropy: 0.24328526854515076, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 295,  Mean reward: -3.9558823529411766, Mean Entropy: 0.2694528102874756, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 296,  Mean reward: -3.28125, Mean Entropy: 0.28206393122673035, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 297,  Mean reward: -1.8529411764705883, Mean Entropy: 0.20994874835014343, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 298,  Mean reward: -3.0422535211267605, Mean Entropy: 0.20725451409816742, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 299,  Mean reward: -2.579710144927536, Mean Entropy: 0.2349310964345932, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 300,  Mean reward: -1.6338028169014085, Mean Entropy: 0.2433379888534546, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.76s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -1.1304347826086956, Mean Entropy: 0.21801172196865082, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 302,  Mean reward: -0.04861111111111111, Mean Entropy: 0.1875716894865036, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 303,  Mean reward: -2.2916666666666665, Mean Entropy: 0.17568495869636536, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 304,  Mean reward: -4.386363636363637, Mean Entropy: 0.18466489017009735, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 305,  Mean reward: -2.536231884057971, Mean Entropy: 0.18276187777519226, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 306,  Mean reward: -3.471014492753623, Mean Entropy: 0.21668261289596558, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 307,  Mean reward: -1.9930555555555556, Mean Entropy: 0.22241032123565674, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 308,  Mean reward: -2.3115942028985508, Mean Entropy: 0.23194853961467743, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 309,  Mean reward: -2.4788732394366195, Mean Entropy: 0.23320463299751282, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 310,  Mean reward: -1.355072463768116, Mean Entropy: 0.16410349309444427, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 311,  Mean reward: -4.007246376811594, Mean Entropy: 0.18399927020072937, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 312,  Mean reward: -1.4375, Mean Entropy: 0.1760871857404709, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 313,  Mean reward: -5.3283582089552235, Mean Entropy: 0.24400341510772705, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 314,  Mean reward: -3.5285714285714285, Mean Entropy: 0.252987802028656, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 315,  Mean reward: -4.6461538461538465, Mean Entropy: 0.23525884747505188, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 316,  Mean reward: -3.8805970149253732, Mean Entropy: 0.22684797644615173, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 317,  Mean reward: -1.0869565217391304, Mean Entropy: 0.21169815957546234, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 318,  Mean reward: -2.9785714285714286, Mean Entropy: 0.2145797312259674, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 319,  Mean reward: -3.5597014925373136, Mean Entropy: 0.26038262248039246, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 320,  Mean reward: -2.289855072463768, Mean Entropy: 0.22412392497062683, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 321,  Mean reward: -2.9357142857142855, Mean Entropy: 0.14971593022346497, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 322,  Mean reward: -4.297101449275362, Mean Entropy: 0.21891003847122192, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 323,  Mean reward: -4.794117647058823, Mean Entropy: 0.2730674147605896, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 324,  Mean reward: -1.108695652173913, Mean Entropy: 0.236256405711174, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 325,  Mean reward: -3.2388059701492535, Mean Entropy: 0.23185734450817108, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 326,  Mean reward: -2.3214285714285716, Mean Entropy: 0.1665956676006317, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 327,  Mean reward: -1.9565217391304348, Mean Entropy: 0.18597011268138885, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 328,  Mean reward: -1.7152777777777777, Mean Entropy: 0.23822341859340668, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 329,  Mean reward: -2.65, Mean Entropy: 0.24081651866436005, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 330,  Mean reward: -1.6666666666666667, Mean Entropy: 0.19542454183101654, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 331,  Mean reward: -1.7777777777777777, Mean Entropy: 0.15452048182487488, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 332,  Mean reward: -2.3642857142857143, Mean Entropy: 0.1800481379032135, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 333,  Mean reward: -4.318840579710145, Mean Entropy: 0.1846780776977539, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 334,  Mean reward: -2.407142857142857, Mean Entropy: 0.209309384226799, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 335,  Mean reward: -2.7573529411764706, Mean Entropy: 0.21884486079216003, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 336,  Mean reward: -3.283582089552239, Mean Entropy: 0.20763590931892395, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 337,  Mean reward: -2.917910447761194, Mean Entropy: 0.23109376430511475, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 338,  Mean reward: -0.9571428571428572, Mean Entropy: 0.2031044363975525, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 339,  Mean reward: 0.4861111111111111, Mean Entropy: 0.1807042956352234, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 340,  Mean reward: -1.8943661971830985, Mean Entropy: 0.21738243103027344, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 341,  Mean reward: -1.8732394366197183, Mean Entropy: 0.18386432528495789, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 342,  Mean reward: -4.083333333333333, Mean Entropy: 0.17153874039649963, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 343,  Mean reward: -1.0492957746478873, Mean Entropy: 0.16527581214904785, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 344,  Mean reward: -1.5285714285714285, Mean Entropy: 0.20061922073364258, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 345,  Mean reward: -1.5285714285714285, Mean Entropy: 0.20989727973937988, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 346,  Mean reward: -0.7253521126760564, Mean Entropy: 0.24369268119335175, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 347,  Mean reward: -1.5714285714285714, Mean Entropy: 0.2611269950866699, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 348,  Mean reward: -2.1971830985915495, Mean Entropy: 0.22280459105968475, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 349,  Mean reward: -1.355072463768116, Mean Entropy: 0.18371152877807617, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 350,  Mean reward: -0.7191780821917808, Mean Entropy: 0.15043067932128906, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 351,  Mean reward: -2.782608695652174, Mean Entropy: 0.15972918272018433, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 352,  Mean reward: -1.3098591549295775, Mean Entropy: 0.17564313113689423, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 353,  Mean reward: -2.3642857142857143, Mean Entropy: 0.20583109557628632, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 354,  Mean reward: -3.2214285714285715, Mean Entropy: 0.2565819025039673, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 355,  Mean reward: -3.7714285714285714, Mean Entropy: 0.23972246050834656, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 356,  Mean reward: -0.9571428571428572, Mean Entropy: 0.24974344670772552, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 357,  Mean reward: -3.537313432835821, Mean Entropy: 0.22819189727306366, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 358,  Mean reward: -1.9930555555555556, Mean Entropy: 0.18353596329689026, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 359,  Mean reward: -4.666666666666667, Mean Entropy: 0.19482704997062683, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 360,  Mean reward: -2.1095890410958904, Mean Entropy: 0.19579064846038818, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 361,  Mean reward: -2.548611111111111, Mean Entropy: 0.17224577069282532, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.81s
Iteration: 362,  Mean reward: -2.8043478260869565, Mean Entropy: 0.1949804723262787, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 363,  Mean reward: -1.3768115942028984, Mean Entropy: 0.17664960026741028, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 364,  Mean reward: -1.4857142857142858, Mean Entropy: 0.17222879827022552, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 365,  Mean reward: -1.4791666666666667, Mean Entropy: 0.16993194818496704, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 366,  Mean reward: -0.4452054794520548, Mean Entropy: 0.22109286487102509, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 367,  Mean reward: -2.536231884057971, Mean Entropy: 0.2339971512556076, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 368,  Mean reward: -4.6692307692307695, Mean Entropy: 0.22095879912376404, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 369,  Mean reward: -1.8943661971830985, Mean Entropy: 0.20051322877407074, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 370,  Mean reward: -1.2, Mean Entropy: 0.18602517247200012, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 371,  Mean reward: -5.295454545454546, Mean Entropy: 0.2175760269165039, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 372,  Mean reward: -4.028985507246377, Mean Entropy: 0.21126016974449158, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 1.02s
Iteration: 373,  Mean reward: -2.65, Mean Entropy: 0.20451974868774414, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 374,  Mean reward: -2.4577464788732395, Mean Entropy: 0.20924031734466553, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 375,  Mean reward: -2.8028169014084505, Mean Entropy: 0.2501179873943329, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 376,  Mean reward: -3.0, Mean Entropy: 0.23556563258171082, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 377,  Mean reward: -2.0142857142857142, Mean Entropy: 0.20715314149856567, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 378,  Mean reward: -2.176056338028169, Mean Entropy: 0.17053352296352386, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 379,  Mean reward: -1.4583333333333333, Mean Entropy: 0.16421331465244293, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 380,  Mean reward: -1.8142857142857143, Mean Entropy: 0.18553149700164795, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 381,  Mean reward: -1.7569444444444444, Mean Entropy: 0.19435910880565643, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 382,  Mean reward: -1.0704225352112675, Mean Entropy: 0.21357902884483337, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 383,  Mean reward: -1.5285714285714285, Mean Entropy: 0.20184117555618286, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 384,  Mean reward: -1.9930555555555556, Mean Entropy: 0.1786768138408661, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 385,  Mean reward: -0.4657534246575342, Mean Entropy: 0.1828537881374359, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 386,  Mean reward: -1.0492957746478873, Mean Entropy: 0.1978369951248169, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 387,  Mean reward: -4.297101449275362, Mean Entropy: 0.206269308924675, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 388,  Mean reward: -3.3455882352941178, Mean Entropy: 0.19410046935081482, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 389,  Mean reward: -0.9931506849315068, Mean Entropy: 0.21267950534820557, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 390,  Mean reward: -3.125, Mean Entropy: 0.20593464374542236, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 391,  Mean reward: -2.9315068493150687, Mean Entropy: 0.21650275588035583, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 392,  Mean reward: -3.639705882352941, Mean Entropy: 0.18268726766109467, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 393,  Mean reward: -2.25, Mean Entropy: 0.19230030477046967, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 394,  Mean reward: 0.22916666666666666, Mean Entropy: 0.18412970006465912, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 395,  Mean reward: -0.9357142857142857, Mean Entropy: 0.22445856034755707, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 396,  Mean reward: -0.9142857142857143, Mean Entropy: 0.22649243474006653, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 397,  Mean reward: -3.323943661971831, Mean Entropy: 0.22891859710216522, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 398,  Mean reward: -0.9142857142857143, Mean Entropy: 0.18300187587738037, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 399,  Mean reward: -4.65, Mean Entropy: 0.2119937539100647, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 400,  Mean reward: -1.0492957746478873, Mean Entropy: 0.18672290444374084, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -1.8943661971830985, Mean Entropy: 0.21026575565338135, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 402,  Mean reward: -1.8943661971830985, Mean Entropy: 0.1994837522506714, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 403,  Mean reward: -2.1971830985915495, Mean Entropy: 0.1818448007106781, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 404,  Mean reward: -1.6338028169014085, Mean Entropy: 0.21951709687709808, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 405,  Mean reward: -2.7183098591549295, Mean Entropy: 0.20216624438762665, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 406,  Mean reward: -3.2214285714285715, Mean Entropy: 0.19533643126487732, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 407,  Mean reward: -1.7714285714285714, Mean Entropy: 0.17209014296531677, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 408,  Mean reward: -1.9577464788732395, Mean Entropy: 0.17397943139076233, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 409,  Mean reward: -3.2214285714285715, Mean Entropy: 0.21387755870819092, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 410,  Mean reward: -2.3214285714285716, Mean Entropy: 0.20732882618904114, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 411,  Mean reward: -1.4791666666666667, Mean Entropy: 0.19236242771148682, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 412,  Mean reward: -0.20422535211267606, Mean Entropy: 0.1669398546218872, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.94s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 413,  Mean reward: 1.2253521126760563, Mean Entropy: 0.13053396344184875, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 414,  Mean reward: -2.23943661971831, Mean Entropy: 0.15126940608024597, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 415,  Mean reward: -2.536231884057971, Mean Entropy: 0.13536107540130615, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 416,  Mean reward: -4.321917808219178, Mean Entropy: 0.16663075983524323, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 417,  Mean reward: -3.242857142857143, Mean Entropy: 0.15314613282680511, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 418,  Mean reward: -2.3333333333333335, Mean Entropy: 0.17752675712108612, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 419,  Mean reward: -3.449275362318841, Mean Entropy: 0.20318704843521118, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 420,  Mean reward: -2.7816901408450705, Mean Entropy: 0.18129000067710876, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 421,  Mean reward: -2.176056338028169, Mean Entropy: 0.18568909168243408, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 422,  Mean reward: -3.1159420289855073, Mean Entropy: 0.1717551201581955, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 423,  Mean reward: -0.9931506849315068, Mean Entropy: 0.1616828590631485, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 424,  Mean reward: -2.089041095890411, Mean Entropy: 0.12422838062047958, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 425,  Mean reward: -6.957142857142857, Mean Entropy: 0.2050773799419403, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 426,  Mean reward: -0.8928571428571429, Mean Entropy: 0.12919826805591583, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 427,  Mean reward: -0.6933333333333334, Mean Entropy: 0.13374826312065125, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 428,  Mean reward: -1.6944444444444444, Mean Entropy: 0.16674654185771942, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 429,  Mean reward: -1.1216216216216217, Mean Entropy: 0.18914207816123962, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 430,  Mean reward: -0.1619718309859155, Mean Entropy: 0.2039393186569214, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 431,  Mean reward: -0.8402777777777778, Mean Entropy: 0.1744140386581421, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 432,  Mean reward: -1.1388888888888888, Mean Entropy: 0.1884569525718689, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 433,  Mean reward: -1.4375, Mean Entropy: 0.1658407747745514, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 434,  Mean reward: -2.73943661971831, Mean Entropy: 0.16600731015205383, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 435,  Mean reward: -4.565217391304348, Mean Entropy: 0.19597014784812927, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 436,  Mean reward: -4.027397260273973, Mean Entropy: 0.20962440967559814, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 437,  Mean reward: -3.1594202898550723, Mean Entropy: 0.1775091141462326, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 438,  Mean reward: -2.2027027027027026, Mean Entropy: 0.2023935317993164, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 439,  Mean reward: -1.4375, Mean Entropy: 0.16972556710243225, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 440,  Mean reward: -4.205882352941177, Mean Entropy: 0.19512078166007996, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 441,  Mean reward: -2.342857142857143, Mean Entropy: 0.156734436750412, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 442,  Mean reward: -3.2, Mean Entropy: 0.206140398979187, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 443,  Mean reward: -3.0422535211267605, Mean Entropy: 0.1880856156349182, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 444,  Mean reward: -2.73943661971831, Mean Entropy: 0.15976637601852417, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 445,  Mean reward: -3.5285714285714285, Mean Entropy: 0.19052694737911224, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 446,  Mean reward: -2.404109589041096, Mean Entropy: 0.14901992678642273, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 447,  Mean reward: -0.8402777777777778, Mean Entropy: 0.17648795247077942, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 448,  Mean reward: -3.3028169014084505, Mean Entropy: 0.15612410008907318, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 449,  Mean reward: -1.7152777777777777, Mean Entropy: 0.15395942330360413, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 450,  Mean reward: -0.7676056338028169, Mean Entropy: 0.15883654356002808, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 451,  Mean reward: -3.4027777777777777, Mean Entropy: 0.20364999771118164, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 452,  Mean reward: -2.914285714285714, Mean Entropy: 0.20079919695854187, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 453,  Mean reward: -1.6549295774647887, Mean Entropy: 0.2017255425453186, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 454,  Mean reward: -2.4577464788732395, Mean Entropy: 0.2141854465007782, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 455,  Mean reward: -2.847826086956522, Mean Entropy: 0.18680474162101746, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 456,  Mean reward: -3.9558823529411766, Mean Entropy: 0.1917589157819748, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 457,  Mean reward: -1.028169014084507, Mean Entropy: 0.13763953745365143, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 458,  Mean reward: -1.9930555555555556, Mean Entropy: 0.1713205873966217, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 459,  Mean reward: -4.1838235294117645, Mean Entropy: 0.19542968273162842, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 460,  Mean reward: -2.76056338028169, Mean Entropy: 0.1984723061323166, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 461,  Mean reward: -3.8582089552238807, Mean Entropy: 0.1778961718082428, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 462,  Mean reward: -3.0211267605633805, Mean Entropy: 0.18048898875713348, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 463,  Mean reward: -3.507142857142857, Mean Entropy: 0.17905251681804657, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 464,  Mean reward: -3.458904109589041, Mean Entropy: 0.21228228509426117, complete_episode_count: 73.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 465,  Mean reward: -0.4657534246575342, Mean Entropy: 0.1571727693080902, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 466,  Mean reward: -2.3214285714285716, Mean Entropy: 0.17283694446086884, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 467,  Mean reward: -0.9236111111111112, Mean Entropy: 0.12584581971168518, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 468,  Mean reward: -1.2876712328767124, Mean Entropy: 0.16503304243087769, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 469,  Mean reward: -2.404109589041096, Mean Entropy: 0.15629272162914276, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 470,  Mean reward: -0.9236111111111112, Mean Entropy: 0.18807348608970642, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 471,  Mean reward: -3.3450704225352115, Mean Entropy: 0.19962143898010254, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 472,  Mean reward: 0.1875, Mean Entropy: 0.15597477555274963, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 473,  Mean reward: -2.891304347826087, Mean Entropy: 0.186542809009552, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 474,  Mean reward: -2.6575342465753424, Mean Entropy: 0.20147931575775146, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 475,  Mean reward: -3.563380281690141, Mean Entropy: 0.1900680512189865, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 476,  Mean reward: -4.643939393939394, Mean Entropy: 0.17014005780220032, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 477,  Mean reward: -2.1549295774647885, Mean Entropy: 0.18617674708366394, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 478,  Mean reward: -2.5, Mean Entropy: 0.13752420246601105, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 479,  Mean reward: -5.654411764705882, Mean Entropy: 0.1968453824520111, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 480,  Mean reward: -1.5205479452054795, Mean Entropy: 0.1420881301164627, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 481,  Mean reward: -4.105633802816901, Mean Entropy: 0.19163429737091064, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 482,  Mean reward: -4.992957746478873, Mean Entropy: 0.20604801177978516, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 483,  Mean reward: -4.450704225352113, Mean Entropy: 0.1816064417362213, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 484,  Mean reward: -2.415492957746479, Mean Entropy: 0.1946049928665161, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 485,  Mean reward: -2.363013698630137, Mean Entropy: 0.19197921454906464, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 486,  Mean reward: -4.078571428571428, Mean Entropy: 0.179216206073761, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 487,  Mean reward: -2.6780821917808217, Mean Entropy: 0.15888437628746033, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 488,  Mean reward: -3.26056338028169, Mean Entropy: 0.16259443759918213, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 489,  Mean reward: -1.1597222222222223, Mean Entropy: 0.11651437729597092, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 490,  Mean reward: -3.6805555555555554, Mean Entropy: 0.1801510453224182, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.99s
Iteration: 491,  Mean reward: -4.914285714285715, Mean Entropy: 0.1878361701965332, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 492,  Mean reward: -2.8680555555555554, Mean Entropy: 0.17848503589630127, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 493,  Mean reward: -1.7361111111111112, Mean Entropy: 0.1528300642967224, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 494,  Mean reward: -3.2260273972602738, Mean Entropy: 0.12617164850234985, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 495,  Mean reward: -1.2465753424657535, Mean Entropy: 0.15229767560958862, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 496,  Mean reward: -2.5, Mean Entropy: 0.17342659831047058, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 497,  Mean reward: -2.56, Mean Entropy: 0.16664348542690277, complete_episode_count: 75.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 498,  Mean reward: -3.5211267605633805, Mean Entropy: 0.17513684928417206, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 499,  Mean reward: -1.1418918918918919, Mean Entropy: 0.15375308692455292, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 500,  Mean reward: -0.3108108108108108, Mean Entropy: 0.1395205557346344, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.77s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -1.9782608695652173, Mean Entropy: 0.18779394030570984, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 502,  Mean reward: 0.25, Mean Entropy: 0.1308276355266571, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 503,  Mean reward: -0.5625, Mean Entropy: 0.14674517512321472, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 504,  Mean reward: 0.10273972602739725, Mean Entropy: 0.13132613897323608, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 505,  Mean reward: -2.089041095890411, Mean Entropy: 0.16648991405963898, complete_episode_count: 73.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 506,  Mean reward: -4.035714285714286, Mean Entropy: 0.15600232779979706, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 507,  Mean reward: -1.6126760563380282, Mean Entropy: 0.1530773937702179, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 508,  Mean reward: -3.717391304347826, Mean Entropy: 0.1592002809047699, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 509,  Mean reward: -2.1971830985915495, Mean Entropy: 0.1790827363729477, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 510,  Mean reward: -4.340579710144928, Mean Entropy: 0.21546295285224915, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 511,  Mean reward: -0.8611111111111112, Mean Entropy: 0.17465084791183472, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 512,  Mean reward: -4.732394366197183, Mean Entropy: 0.19906219840049744, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 513,  Mean reward: -3.125, Mean Entropy: 0.17549586296081543, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 514,  Mean reward: -0.9931506849315068, Mean Entropy: 0.15905416011810303, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 515,  Mean reward: -2.3333333333333335, Mean Entropy: 0.17536561191082, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 516,  Mean reward: -2.2916666666666665, Mean Entropy: 0.16643938422203064, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 517,  Mean reward: -3.8142857142857145, Mean Entropy: 0.20703348517417908, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 518,  Mean reward: -1.662162162162162, Mean Entropy: 0.13213539123535156, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 519,  Mean reward: -2.6164383561643834, Mean Entropy: 0.16009104251861572, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 520,  Mean reward: -3.323943661971831, Mean Entropy: 0.13569855690002441, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 521,  Mean reward: -3.55, Mean Entropy: 0.1701277792453766, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 522,  Mean reward: -1.4583333333333333, Mean Entropy: 0.13947996497154236, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 523,  Mean reward: -4.364864864864865, Mean Entropy: 0.1907845437526703, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 524,  Mean reward: -4.0479452054794525, Mean Entropy: 0.17227135598659515, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 525,  Mean reward: -1.1418918918918919, Mean Entropy: 0.12475614249706268, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 526,  Mean reward: -2.5277777777777777, Mean Entropy: 0.1618817299604416, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 527,  Mean reward: -0.8108108108108109, Mean Entropy: 0.14019456505775452, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 528,  Mean reward: -1.3716216216216217, Mean Entropy: 0.1360391229391098, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 529,  Mean reward: -0.9931506849315068, Mean Entropy: 0.16464683413505554, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 530,  Mean reward: -2.7183098591549295, Mean Entropy: 0.1855972856283188, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 531,  Mean reward: -2.722972972972973, Mean Entropy: 0.15723848342895508, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 532,  Mean reward: -3.5285714285714285, Mean Entropy: 0.17109990119934082, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 533,  Mean reward: -0.060810810810810814, Mean Entropy: 0.12956398725509644, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 534,  Mean reward: -1.9527027027027026, Mean Entropy: 0.15390948951244354, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 535,  Mean reward: -1.8356164383561644, Mean Entropy: 0.19397518038749695, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 536,  Mean reward: -0.5833333333333334, Mean Entropy: 0.13682833313941956, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 537,  Mean reward: -2.4577464788732395, Mean Entropy: 0.15935242176055908, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 538,  Mean reward: -1.78, Mean Entropy: 0.1482761651277542, complete_episode_count: 75.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 539,  Mean reward: -2.2916666666666665, Mean Entropy: 0.15068382024765015, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 540,  Mean reward: -4.27536231884058, Mean Entropy: 0.19940337538719177, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 541,  Mean reward: -3.4027777777777777, Mean Entropy: 0.17019495368003845, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 542,  Mean reward: -2.1621621621621623, Mean Entropy: 0.15528899431228638, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 543,  Mean reward: -1.662162162162162, Mean Entropy: 0.1602783352136612, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 544,  Mean reward: -1.5333333333333334, Mean Entropy: 0.1538694202899933, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 545,  Mean reward: -1.894736842105263, Mean Entropy: 0.17757686972618103, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 546,  Mean reward: -1.3098591549295775, Mean Entropy: 0.15169823169708252, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 547,  Mean reward: -2.4788732394366195, Mean Entropy: 0.15359045565128326, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 548,  Mean reward: -0.94, Mean Entropy: 0.12548212707042694, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 549,  Mean reward: -3.6805555555555554, Mean Entropy: 0.1598568856716156, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 550,  Mean reward: -1.352112676056338, Mean Entropy: 0.15877115726470947, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 551,  Mean reward: -1.9366197183098592, Mean Entropy: 0.1188550516963005, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 552,  Mean reward: -2.7432432432432434, Mean Entropy: 0.15188905596733093, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 553,  Mean reward: -2.722972972972973, Mean Entropy: 0.14745472371578217, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 554,  Mean reward: -3.2837837837837838, Mean Entropy: 0.1842252016067505, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 555,  Mean reward: -2.401315789473684, Mean Entropy: 0.14761722087860107, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 556,  Mean reward: -3.0211267605633805, Mean Entropy: 0.15743756294250488, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 557,  Mean reward: -2.472972972972973, Mean Entropy: 0.12930871546268463, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 558,  Mean reward: -0.5608108108108109, Mean Entropy: 0.1257908046245575, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 559,  Mean reward: -3.9375, Mean Entropy: 0.1581117957830429, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 560,  Mean reward: -2.472972972972973, Mean Entropy: 0.13525985181331635, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 561,  Mean reward: -1.6824324324324325, Mean Entropy: 0.14757081866264343, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 562,  Mean reward: -2.8472222222222223, Mean Entropy: 0.15762704610824585, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 563,  Mean reward: -1.6418918918918919, Mean Entropy: 0.14693254232406616, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 564,  Mean reward: -2.5694444444444446, Mean Entropy: 0.15916308760643005, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 565,  Mean reward: -0.96, Mean Entropy: 0.11253486573696136, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 566,  Mean reward: -4.147887323943662, Mean Entropy: 0.1711394488811493, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 567,  Mean reward: -0.4066666666666667, Mean Entropy: 0.12176880240440369, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 568,  Mean reward: -2.2733333333333334, Mean Entropy: 0.1523977518081665, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 569,  Mean reward: -3.804054054054054, Mean Entropy: 0.13669244945049286, complete_episode_count: 74.0, Gather time: 0.77s, Train time: 0.74s
Iteration: 570,  Mean reward: -2.6575342465753424, Mean Entropy: 0.12390419095754623, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 571,  Mean reward: -1.2671232876712328, Mean Entropy: 0.14217936992645264, complete_episode_count: 73.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 572,  Mean reward: -2.3424657534246576, Mean Entropy: 0.1305999457836151, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 573,  Mean reward: -2.25, Mean Entropy: 0.14187607169151306, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 574,  Mean reward: -2.910958904109589, Mean Entropy: 0.1468566656112671, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 575,  Mean reward: -1.912162162162162, Mean Entropy: 0.1514032781124115, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 576,  Mean reward: -2.006666666666667, Mean Entropy: 0.16283005475997925, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 577,  Mean reward: -2.9078947368421053, Mean Entropy: 0.15043698251247406, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 578,  Mean reward: -2.363013698630137, Mean Entropy: 0.1306591033935547, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 579,  Mean reward: -1.3716216216216217, Mean Entropy: 0.10931079089641571, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 580,  Mean reward: -2.227272727272727, Mean Entropy: 0.14114706218242645, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 581,  Mean reward: -1.448051948051948, Mean Entropy: 0.15877944231033325, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 582,  Mean reward: -0.4657534246575342, Mean Entropy: 0.1254805326461792, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 583,  Mean reward: -3.0135135135135136, Mean Entropy: 0.15191105008125305, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 584,  Mean reward: -1.3918918918918919, Mean Entropy: 0.12987226247787476, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 585,  Mean reward: -0.23684210526315788, Mean Entropy: 0.1299630105495453, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 586,  Mean reward: -4.534246575342466, Mean Entropy: 0.18375363945960999, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 587,  Mean reward: -1.6418918918918919, Mean Entropy: 0.1422279328107834, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 588,  Mean reward: -1.2466666666666666, Mean Entropy: 0.10644000768661499, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 589,  Mean reward: -3.638888888888889, Mean Entropy: 0.1282857358455658, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 590,  Mean reward: -2.401315789473684, Mean Entropy: 0.13485251367092133, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 591,  Mean reward: -0.7828947368421053, Mean Entropy: 0.11076109111309052, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 592,  Mean reward: -1.662162162162162, Mean Entropy: 0.1378462314605713, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 593,  Mean reward: -3.2465753424657535, Mean Entropy: 0.14347611367702484, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 594,  Mean reward: -2.404109589041096, Mean Entropy: 0.139602929353714, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 595,  Mean reward: -3.8933333333333335, Mean Entropy: 0.1393498182296753, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.82s
Iteration: 596,  Mean reward: -3.36, Mean Entropy: 0.13493698835372925, complete_episode_count: 75.0, Gather time: 0.63s, Train time: 0.71s
Iteration: 597,  Mean reward: -1.2077922077922079, Mean Entropy: 0.12925782799720764, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 598,  Mean reward: -2.138157894736842, Mean Entropy: 0.11916377395391464, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 599,  Mean reward: -1.8157894736842106, Mean Entropy: 0.14862948656082153, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 600,  Mean reward: -0.6298701298701299, Mean Entropy: 0.11839760839939117, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.5277777777777777, Mean Entropy: 0.12755455076694489, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.82s
Iteration: 602,  Mean reward: -2.1301369863013697, Mean Entropy: 0.11319594085216522, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 603,  Mean reward: -3.3028169014084505, Mean Entropy: 0.1345653533935547, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 604,  Mean reward: -2.56, Mean Entropy: 0.13400551676750183, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 605,  Mean reward: -1.5410958904109588, Mean Entropy: 0.10954828560352325, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 606,  Mean reward: -3.574324324324324, Mean Entropy: 0.14085015654563904, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 607,  Mean reward: -0.9090909090909091, Mean Entropy: 0.12411749362945557, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 608,  Mean reward: -4.12, Mean Entropy: 0.1306103765964508, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 609,  Mean reward: -0.7133333333333334, Mean Entropy: 0.0956668108701706, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 610,  Mean reward: -4.236111111111111, Mean Entropy: 0.12248643487691879, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 611,  Mean reward: -2.1621621621621623, Mean Entropy: 0.12354292720556259, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 612,  Mean reward: -1.9324324324324325, Mean Entropy: 0.12417547404766083, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 613,  Mean reward: -4.006849315068493, Mean Entropy: 0.13647238910198212, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 614,  Mean reward: -3.5972222222222223, Mean Entropy: 0.17512944340705872, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 615,  Mean reward: -2.3333333333333335, Mean Entropy: 0.1278262734413147, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 616,  Mean reward: -1.2066666666666668, Mean Entropy: 0.11262127012014389, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 617,  Mean reward: -3.6776315789473686, Mean Entropy: 0.13605338335037231, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 618,  Mean reward: -1.7077922077922079, Mean Entropy: 0.13034652173519135, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 619,  Mean reward: -1.9285714285714286, Mean Entropy: 0.12677517533302307, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 620,  Mean reward: -2.227272727272727, Mean Entropy: 0.11534158885478973, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 621,  Mean reward: 0.38961038961038963, Mean Entropy: 0.11123301833868027, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 622,  Mean reward: -1.8355263157894737, Mean Entropy: 0.10762742161750793, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 623,  Mean reward: -1.3092105263157894, Mean Entropy: 0.10370998084545135, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 624,  Mean reward: -2.8680555555555554, Mean Entropy: 0.14568056166172028, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 625,  Mean reward: -1.74, Mean Entropy: 0.13231608271598816, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 626,  Mean reward: -1.2066666666666668, Mean Entropy: 0.10703328251838684, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.81s
Iteration: 627,  Mean reward: -3.6066666666666665, Mean Entropy: 0.14132151007652283, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 628,  Mean reward: -1.74, Mean Entropy: 0.08953443169593811, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 629,  Mean reward: -2.472972972972973, Mean Entropy: 0.10856986045837402, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 630,  Mean reward: -0.5405405405405406, Mean Entropy: 0.08539828658103943, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 631,  Mean reward: -1.5921052631578947, Mean Entropy: 0.11022940278053284, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 632,  Mean reward: -1.8012820512820513, Mean Entropy: 0.09814133495092392, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 633,  Mean reward: -1.7077922077922079, Mean Entropy: 0.1161789521574974, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 634,  Mean reward: -4.223684210526316, Mean Entropy: 0.14539051055908203, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 635,  Mean reward: -2.9078947368421053, Mean Entropy: 0.10517816990613937, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 636,  Mean reward: -1.1688311688311688, Mean Entropy: 0.11765436828136444, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 637,  Mean reward: -1.4090909090909092, Mean Entropy: 0.12190680205821991, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 638,  Mean reward: -3.732876712328767, Mean Entropy: 0.12191407382488251, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 639,  Mean reward: -0.8896103896103896, Mean Entropy: 0.11214756220579147, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 640,  Mean reward: -0.9285714285714286, Mean Entropy: 0.09093876928091049, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 641,  Mean reward: -1.5256410256410255, Mean Entropy: 0.13163606822490692, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 642,  Mean reward: -4.025974025974026, Mean Entropy: 0.12994186580181122, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 643,  Mean reward: -3.0933333333333333, Mean Entropy: 0.1008220836520195, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.81s
Iteration: 644,  Mean reward: 0.5131578947368421, Mean Entropy: 0.09212793409824371, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 645,  Mean reward: -2.1621621621621623, Mean Entropy: 0.10639221966266632, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 1.02s
Iteration: 646,  Mean reward: -3.6466666666666665, Mean Entropy: 0.10093586146831512, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 647,  Mean reward: -3.1133333333333333, Mean Entropy: 0.12246544659137726, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 648,  Mean reward: -0.7948717948717948, Mean Entropy: 0.08548080921173096, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 649,  Mean reward: -4.16, Mean Entropy: 0.12322752177715302, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 650,  Mean reward: -3.0733333333333333, Mean Entropy: 0.10548313707113266, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 651,  Mean reward: -2.8066666666666666, Mean Entropy: 0.08386987447738647, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 652,  Mean reward: -0.6933333333333334, Mean Entropy: 0.08449868857860565, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 653,  Mean reward: -3.34, Mean Entropy: 0.12732474505901337, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 654,  Mean reward: -0.8896103896103896, Mean Entropy: 0.10913949459791183, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 655,  Mean reward: -2.7027027027027026, Mean Entropy: 0.09137963503599167, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 656,  Mean reward: -2.8461538461538463, Mean Entropy: 0.09463151544332504, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 657,  Mean reward: -3.6973684210526314, Mean Entropy: 0.10731789469718933, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 658,  Mean reward: -3.38, Mean Entropy: 0.10610158741474152, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 659,  Mean reward: -2.361842105263158, Mean Entropy: 0.08931713551282883, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 660,  Mean reward: -4.305194805194805, Mean Entropy: 0.1059996485710144, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 661,  Mean reward: -3.0733333333333333, Mean Entropy: 0.10749389231204987, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 662,  Mean reward: -3.2467532467532467, Mean Entropy: 0.08456861972808838, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 663,  Mean reward: -2.727272727272727, Mean Entropy: 0.09217894077301025, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 664,  Mean reward: -2.6447368421052633, Mean Entropy: 0.09201934188604355, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 665,  Mean reward: -1.8924050632911393, Mean Entropy: 0.08042305707931519, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 666,  Mean reward: -2.5705128205128207, Mean Entropy: 0.08690684288740158, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 667,  Mean reward: -1.544871794871795, Mean Entropy: 0.06978167593479156, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 668,  Mean reward: -2.0576923076923075, Mean Entropy: 0.07463245838880539, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.83s
Iteration: 669,  Mean reward: -2.5705128205128207, Mean Entropy: 0.07485926151275635, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 670,  Mean reward: -1.0460526315789473, Mean Entropy: 0.05372919887304306, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 671,  Mean reward: -4.064935064935065, Mean Entropy: 0.07142890989780426, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 672,  Mean reward: -2.9050632911392404, Mean Entropy: 0.06824535131454468, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 673,  Mean reward: -0.28205128205128205, Mean Entropy: 0.056276023387908936, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 674,  Mean reward: -1.0657894736842106, Mean Entropy: 0.07123519480228424, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 675,  Mean reward: -2.532051282051282, Mean Entropy: 0.08533181995153427, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 676,  Mean reward: -3.3205128205128207, Mean Entropy: 0.09780018031597137, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.81s
Iteration: 677,  Mean reward: -1.0320512820512822, Mean Entropy: 0.0695308968424797, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 678,  Mean reward: -1.4733333333333334, Mean Entropy: 0.06952568888664246, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 679,  Mean reward: -0.7948717948717948, Mean Entropy: 0.07220350205898285, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 680,  Mean reward: -3.3205128205128207, Mean Entropy: 0.08359526842832565, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 681,  Mean reward: -1.8012820512820513, Mean Entropy: 0.07697318494319916, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 682,  Mean reward: -1.544871794871795, Mean Entropy: 0.059441160410642624, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 683,  Mean reward: -0.9090909090909091, Mean Entropy: 0.06391484290361404, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.97s
Iteration: 684,  Mean reward: -0.8896103896103896, Mean Entropy: 0.07334603369235992, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 685,  Mean reward: -3.487012987012987, Mean Entropy: 0.08075971901416779, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 686,  Mean reward: -1.8355263157894737, Mean Entropy: 0.07120278477668762, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 687,  Mean reward: -1.1688311688311688, Mean Entropy: 0.05439707636833191, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.82s
Iteration: 688,  Mean reward: -1.4285714285714286, Mean Entropy: 0.06966546922922134, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 689,  Mean reward: -3.6153846153846154, Mean Entropy: 0.07243765890598297, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 690,  Mean reward: -2.551282051282051, Mean Entropy: 0.0682542473077774, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 691,  Mean reward: -4.785714285714286, Mean Entropy: 0.09504111856222153, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 692,  Mean reward: -1.8012820512820513, Mean Entropy: 0.058477889746427536, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 693,  Mean reward: -2.727272727272727, Mean Entropy: 0.06869758665561676, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 694,  Mean reward: -4.108974358974359, Mean Entropy: 0.07236188650131226, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 695,  Mean reward: -1.8924050632911393, Mean Entropy: 0.054899346083402634, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 696,  Mean reward: -2.3987341772151898, Mean Entropy: 0.05592450499534607, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 697,  Mean reward: -2.8860759493670884, Mean Entropy: 0.06630231440067291, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 698,  Mean reward: -1.2692307692307692, Mean Entropy: 0.050735048949718475, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 699,  Mean reward: -3.2467532467532467, Mean Entropy: 0.06585624814033508, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 700,  Mean reward: -3.576923076923077, Mean Entropy: 0.0754682868719101, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -0.25, Mean Entropy: 0.042449839413166046, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 702,  Mean reward: 0.01282051282051282, Mean Entropy: 0.04199115186929703, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.81s
Iteration: 703,  Mean reward: -1.2692307692307692, Mean Entropy: 0.05676736682653427, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 704,  Mean reward: -2.2948717948717947, Mean Entropy: 0.07379119098186493, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 705,  Mean reward: -2.487012987012987, Mean Entropy: 0.060568928718566895, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 706,  Mean reward: -2.5705128205128207, Mean Entropy: 0.05468716472387314, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 707,  Mean reward: -1.5256410256410255, Mean Entropy: 0.05860384926199913, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 708,  Mean reward: -1.0, Mean Entropy: 0.05793633684515953, complete_episode_count: 80.0, Gather time: 0.66s, Train time: 0.74s
Iteration: 709,  Mean reward: -3.0641025641025643, Mean Entropy: 0.07350751757621765, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 710,  Mean reward: -0.6455696202531646, Mean Entropy: 0.0516090951859951, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 711,  Mean reward: -2.3141025641025643, Mean Entropy: 0.05694079026579857, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 712,  Mean reward: -1.3860759493670887, Mean Entropy: 0.0565999373793602, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 713,  Mean reward: -1.1329113924050633, Mean Entropy: 0.036610785871744156, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.82s
Iteration: 714,  Mean reward: -2.0384615384615383, Mean Entropy: 0.052296578884124756, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 715,  Mean reward: -2.2948717948717947, Mean Entropy: 0.05274825543165207, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 716,  Mean reward: -2.1455696202531644, Mean Entropy: 0.045076094567775726, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 717,  Mean reward: -2.8076923076923075, Mean Entropy: 0.059632740914821625, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 718,  Mean reward: -1.7820512820512822, Mean Entropy: 0.04616546630859375, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 719,  Mean reward: -3.0448717948717947, Mean Entropy: 0.04899665713310242, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 720,  Mean reward: -2.1455696202531644, Mean Entropy: 0.047918617725372314, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 721,  Mean reward: -1.1329113924050633, Mean Entropy: 0.04773521423339844, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 722,  Mean reward: -3.33974358974359, Mean Entropy: 0.04982501268386841, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 723,  Mean reward: -1.9113924050632911, Mean Entropy: 0.042456433176994324, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 724,  Mean reward: 0.3860759493670886, Mean Entropy: 0.03525588661432266, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 725,  Mean reward: -0.7756410256410257, Mean Entropy: 0.0472726970911026, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 726,  Mean reward: -3.9367088607594938, Mean Entropy: 0.05756299942731857, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 727,  Mean reward: -1.1493506493506493, Mean Entropy: 0.04038950055837631, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 728,  Mean reward: -2.651898734177215, Mean Entropy: 0.050769876688718796, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 729,  Mean reward: -1.9675324675324675, Mean Entropy: 0.04523051902651787, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 730,  Mean reward: -2.1455696202531644, Mean Entropy: 0.046915896236896515, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 731,  Mean reward: -1.1518987341772151, Mean Entropy: 0.036806270480155945, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 732,  Mean reward: -0.879746835443038, Mean Entropy: 0.045257460325956345, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 733,  Mean reward: -1.4050632911392404, Mean Entropy: 0.04389027878642082, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 734,  Mean reward: -3.301282051282051, Mean Entropy: 0.05661995708942413, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 735,  Mean reward: -2.0576923076923075, Mean Entropy: 0.039645008742809296, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 736,  Mean reward: -1.0128205128205128, Mean Entropy: 0.031439051032066345, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 737,  Mean reward: -1.75, Mean Entropy: 0.03984548896551132, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 738,  Mean reward: -2.0, Mean Entropy: 0.045645177364349365, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 739,  Mean reward: -0.7564102564102564, Mean Entropy: 0.031775567680597305, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 740,  Mean reward: -1.7820512820512822, Mean Entropy: 0.03745632618665695, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 741,  Mean reward: -0.9090909090909091, Mean Entropy: 0.040052562952041626, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 742,  Mean reward: -3.33974358974359, Mean Entropy: 0.04515534266829491, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 743,  Mean reward: -3.1582278481012658, Mean Entropy: 0.04475358873605728, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 744,  Mean reward: -2.651898734177215, Mean Entropy: 0.03694365546107292, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 745,  Mean reward: -1.2884615384615385, Mean Entropy: 0.031052744016051292, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 746,  Mean reward: -0.879746835443038, Mean Entropy: 0.03652329742908478, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 747,  Mean reward: -2.8076923076923075, Mean Entropy: 0.04895821213722229, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 748,  Mean reward: -2.551282051282051, Mean Entropy: 0.040077436715364456, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 749,  Mean reward: -0.5, Mean Entropy: 0.03772532194852829, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 750,  Mean reward: -4.189873417721519, Mean Entropy: 0.04765809699892998, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 751,  Mean reward: 0.6392405063291139, Mean Entropy: 0.0310861524194479, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 752,  Mean reward: -0.75, Mean Entropy: 0.03246647119522095, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 753,  Mean reward: -2.0384615384615383, Mean Entropy: 0.03790140897035599, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 754,  Mean reward: -2.1455696202531644, Mean Entropy: 0.03372468054294586, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 755,  Mean reward: -2.3987341772151898, Mean Entropy: 0.03614328056573868, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 756,  Mean reward: -3.0641025641025643, Mean Entropy: 0.040136031806468964, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 757,  Mean reward: -3.6835443037974684, Mean Entropy: 0.043322786688804626, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 758,  Mean reward: -3.0833333333333335, Mean Entropy: 0.036129795014858246, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 759,  Mean reward: -1.544871794871795, Mean Entropy: 0.031855322420597076, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.99s
Iteration: 760,  Mean reward: -1.639240506329114, Mean Entropy: 0.01839359849691391, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 761,  Mean reward: -0.25, Mean Entropy: 0.034592561423778534, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 762,  Mean reward: -1.5, Mean Entropy: 0.03101884201169014, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 763,  Mean reward: -1.639240506329114, Mean Entropy: 0.02919263206422329, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 764,  Mean reward: -1.75, Mean Entropy: 0.027047818526625633, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 765,  Mean reward: -3.1582278481012658, Mean Entropy: 0.027919700369238853, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 766,  Mean reward: -1.2692307692307692, Mean Entropy: 0.02457287721335888, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 767,  Mean reward: -2.9050632911392404, Mean Entropy: 0.03341178596019745, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 768,  Mean reward: -4.25, Mean Entropy: 0.035218559205532074, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 769,  Mean reward: -1.25, Mean Entropy: 0.026646923273801804, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 770,  Mean reward: -0.37341772151898733, Mean Entropy: 0.018993843346834183, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 771,  Mean reward: -0.5192307692307693, Mean Entropy: 0.025082966312766075, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 772,  Mean reward: -0.37341772151898733, Mean Entropy: 0.027119889855384827, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 773,  Mean reward: -3.9177215189873418, Mean Entropy: 0.037641964852809906, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 774,  Mean reward: -4.025974025974026, Mean Entropy: 0.038717128336429596, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 775,  Mean reward: -4.0064935064935066, Mean Entropy: 0.04197049140930176, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 776,  Mean reward: -0.12025316455696203, Mean Entropy: 0.02417634427547455, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 777,  Mean reward: -0.8896103896103896, Mean Entropy: 0.031112326309084892, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 778,  Mean reward: -1.75, Mean Entropy: 0.03157280012965202, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 779,  Mean reward: -2.651898734177215, Mean Entropy: 0.039224155247211456, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 780,  Mean reward: -0.7564102564102564, Mean Entropy: 0.03190051019191742, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 781,  Mean reward: -1.5, Mean Entropy: 0.040590666234493256, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 782,  Mean reward: -2.5, Mean Entropy: 0.040816083550453186, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 783,  Mean reward: -1.5256410256410255, Mean Entropy: 0.0308633204549551, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 784,  Mean reward: -3.0641025641025643, Mean Entropy: 0.040887169539928436, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 785,  Mean reward: -1.0320512820512822, Mean Entropy: 0.026405755430459976, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 786,  Mean reward: -0.8607594936708861, Mean Entropy: 0.030227381736040115, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 787,  Mean reward: -2.0384615384615383, Mean Entropy: 0.038481950759887695, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 788,  Mean reward: -1.2692307692307692, Mean Entropy: 0.024288319051265717, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 789,  Mean reward: -2.1455696202531644, Mean Entropy: 0.034002743661403656, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 790,  Mean reward: -2.25, Mean Entropy: 0.0353754460811615, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 791,  Mean reward: -0.8987341772151899, Mean Entropy: 0.028910724446177483, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 792,  Mean reward: -1.5256410256410255, Mean Entropy: 0.026585109531879425, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 793,  Mean reward: -2.3987341772151898, Mean Entropy: 0.03343667834997177, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 794,  Mean reward: -0.7564102564102564, Mean Entropy: 0.025677427649497986, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 795,  Mean reward: -2.8076923076923075, Mean Entropy: 0.03189018368721008, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 796,  Mean reward: -1.1329113924050633, Mean Entropy: 0.02409871481359005, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 797,  Mean reward: -0.5, Mean Entropy: 0.020774058997631073, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 798,  Mean reward: -1.5, Mean Entropy: 0.02506306767463684, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 799,  Mean reward: -1.2692307692307692, Mean Entropy: 0.026824533939361572, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 800,  Mean reward: -2.5, Mean Entropy: 0.02816733345389366, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -2.25, Mean Entropy: 0.028545256704092026, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 802,  Mean reward: -2.1455696202531644, Mean Entropy: 0.026988569647073746, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 803,  Mean reward: -0.5, Mean Entropy: 0.021536333486437798, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 804,  Mean reward: -4.170886075949367, Mean Entropy: 0.030170848593115807, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 805,  Mean reward: 0.25, Mean Entropy: 0.018104545772075653, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 806,  Mean reward: -4.189873417721519, Mean Entropy: 0.026094337925314903, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 807,  Mean reward: -2.551282051282051, Mean Entropy: 0.026443641632795334, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 808,  Mean reward: 0.3860759493670886, Mean Entropy: 0.016413986682891846, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 809,  Mean reward: -3.0, Mean Entropy: 0.02230110764503479, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 810,  Mean reward: -3.9177215189873418, Mean Entropy: 0.032801851630210876, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 811,  Mean reward: -3.411392405063291, Mean Entropy: 0.028946246951818466, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 812,  Mean reward: 1.294871794871795, Mean Entropy: 0.016879389062523842, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 813,  Mean reward: -0.25, Mean Entropy: 0.014903657138347626, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 814,  Mean reward: -4.089743589743589, Mean Entropy: 0.037684615701436996, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 815,  Mean reward: -2.25, Mean Entropy: 0.03231165185570717, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 816,  Mean reward: -1.0, Mean Entropy: 0.027477441355586052, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 817,  Mean reward: -1.0128205128205128, Mean Entropy: 0.026288311928510666, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 818,  Mean reward: -4.4868421052631575, Mean Entropy: 0.03541595861315727, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 819,  Mean reward: -2.3987341772151898, Mean Entropy: 0.032623808830976486, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 820,  Mean reward: -0.879746835443038, Mean Entropy: 0.028248097747564316, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 821,  Mean reward: -3.9367088607594938, Mean Entropy: 0.037591155618429184, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 822,  Mean reward: -3.411392405063291, Mean Entropy: 0.03458491712808609, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 823,  Mean reward: -1.639240506329114, Mean Entropy: 0.029843147844076157, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 824,  Mean reward: -0.12025316455696203, Mean Entropy: 0.026328500360250473, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 825,  Mean reward: -0.37012987012987014, Mean Entropy: 0.025505319237709045, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 826,  Mean reward: -2.9675324675324677, Mean Entropy: 0.03209444880485535, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 827,  Mean reward: -2.707792207792208, Mean Entropy: 0.032149430364370346, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 828,  Mean reward: -1.7820512820512822, Mean Entropy: 0.027432166039943695, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 829,  Mean reward: -1.75, Mean Entropy: 0.027215823531150818, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 830,  Mean reward: -3.3205128205128207, Mean Entropy: 0.033447328954935074, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 831,  Mean reward: -2.551282051282051, Mean Entropy: 0.030876372009515762, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 832,  Mean reward: -1.1329113924050633, Mean Entropy: 0.02287006936967373, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 833,  Mean reward: -0.6265822784810127, Mean Entropy: 0.02250640094280243, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 834,  Mean reward: -3.0, Mean Entropy: 0.03165080398321152, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 835,  Mean reward: -2.5, Mean Entropy: 0.02802146039903164, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 1.00s
Iteration: 836,  Mean reward: -0.5, Mean Entropy: 0.0232560932636261, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 837,  Mean reward: -1.75, Mean Entropy: 0.029268022626638412, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 838,  Mean reward: -5.1835443037974684, Mean Entropy: 0.03134232759475708, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 839,  Mean reward: -2.0, Mean Entropy: 0.025260239839553833, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 840,  Mean reward: -1.5, Mean Entropy: 0.027637261897325516, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 841,  Mean reward: -2.3987341772151898, Mean Entropy: 0.02339468151330948, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 842,  Mean reward: -0.5, Mean Entropy: 0.022472910583019257, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 843,  Mean reward: -1.639240506329114, Mean Entropy: 0.026296382769942284, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 844,  Mean reward: -5.324675324675325, Mean Entropy: 0.03590024635195732, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 845,  Mean reward: -1.3860759493670887, Mean Entropy: 0.023260507732629776, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 846,  Mean reward: -3.0641025641025643, Mean Entropy: 0.028611181303858757, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 847,  Mean reward: -0.6265822784810127, Mean Entropy: 0.01993875950574875, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 848,  Mean reward: -2.3987341772151898, Mean Entropy: 0.02582298032939434, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 849,  Mean reward: -2.651898734177215, Mean Entropy: 0.02141716703772545, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 850,  Mean reward: -1.1688311688311688, Mean Entropy: 0.02420196495950222, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 851,  Mean reward: -2.1455696202531644, Mean Entropy: 0.02204662561416626, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 852,  Mean reward: -2.9050632911392404, Mean Entropy: 0.022612666711211205, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 853,  Mean reward: -2.0, Mean Entropy: 0.022335520014166832, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 854,  Mean reward: -3.25, Mean Entropy: 0.02725960873067379, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 855,  Mean reward: -2.2948717948717947, Mean Entropy: 0.018675263971090317, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 856,  Mean reward: -3.0, Mean Entropy: 0.021382829174399376, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 857,  Mean reward: -4.089743589743589, Mean Entropy: 0.028489932417869568, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 858,  Mean reward: -3.2467532467532467, Mean Entropy: 0.02465500682592392, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 859,  Mean reward: -3.8141025641025643, Mean Entropy: 0.029973691329360008, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 860,  Mean reward: -4.75, Mean Entropy: 0.024413740262389183, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 861,  Mean reward: -2.0, Mean Entropy: 0.028392303735017776, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 862,  Mean reward: -1.7820512820512822, Mean Entropy: 0.02202361822128296, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 863,  Mean reward: -0.75, Mean Entropy: 0.014199793338775635, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 864,  Mean reward: -4.089743589743589, Mean Entropy: 0.02652464434504509, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 865,  Mean reward: -2.0, Mean Entropy: 0.021129991859197617, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 866,  Mean reward: 0.782051282051282, Mean Entropy: 0.011835195124149323, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 867,  Mean reward: -2.651898734177215, Mean Entropy: 0.024325568228960037, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 868,  Mean reward: -1.75, Mean Entropy: 0.021780511364340782, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 869,  Mean reward: -2.651898734177215, Mean Entropy: 0.019597046077251434, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 870,  Mean reward: -0.37341772151898733, Mean Entropy: 0.020856311544775963, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 871,  Mean reward: -0.879746835443038, Mean Entropy: 0.02128308080136776, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 872,  Mean reward: -2.3987341772151898, Mean Entropy: 0.024624502286314964, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 873,  Mean reward: -3.0, Mean Entropy: 0.02368120662868023, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 874,  Mean reward: -1.75, Mean Entropy: 0.021134940907359123, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 875,  Mean reward: -2.25, Mean Entropy: 0.022488076239824295, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 876,  Mean reward: -3.1582278481012658, Mean Entropy: 0.02303401380777359, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 877,  Mean reward: -2.632911392405063, Mean Entropy: 0.020390085875988007, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 878,  Mean reward: -2.75, Mean Entropy: 0.024105459451675415, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 879,  Mean reward: -2.2948717948717947, Mean Entropy: 0.019186552613973618, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 880,  Mean reward: -2.5, Mean Entropy: 0.01901635341346264, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 881,  Mean reward: -1.0, Mean Entropy: 0.01722818613052368, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 882,  Mean reward: -3.0, Mean Entropy: 0.020725473761558533, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 883,  Mean reward: -2.651898734177215, Mean Entropy: 0.023247525095939636, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 884,  Mean reward: -1.5, Mean Entropy: 0.019014891237020493, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 885,  Mean reward: -1.2692307692307692, Mean Entropy: 0.01779962331056595, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 886,  Mean reward: -2.8076923076923075, Mean Entropy: 0.019968001171946526, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 887,  Mean reward: -1.25, Mean Entropy: 0.014156237244606018, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 888,  Mean reward: -2.5, Mean Entropy: 0.019692765548825264, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 889,  Mean reward: -3.0, Mean Entropy: 0.018279246985912323, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 890,  Mean reward: -3.1582278481012658, Mean Entropy: 0.023230796679854393, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 891,  Mean reward: -0.879746835443038, Mean Entropy: 0.019078770652413368, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 892,  Mean reward: -2.1455696202531644, Mean Entropy: 0.017603736370801926, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.81s
Iteration: 893,  Mean reward: -1.25, Mean Entropy: 0.01511203870177269, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 894,  Mean reward: -1.8924050632911393, Mean Entropy: 0.01744895428419113, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 895,  Mean reward: -0.75, Mean Entropy: 0.016392281278967857, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 896,  Mean reward: -1.5, Mean Entropy: 0.017798811197280884, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 897,  Mean reward: -2.1455696202531644, Mean Entropy: 0.012969727627933025, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 898,  Mean reward: -2.25, Mean Entropy: 0.018515706062316895, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 899,  Mean reward: -1.5, Mean Entropy: 0.01778084598481655, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 900,  Mean reward: 0.0, Mean Entropy: 0.014752119779586792, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -2.0, Mean Entropy: 0.018879402428865433, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 902,  Mean reward: -4.170886075949367, Mean Entropy: 0.020187636837363243, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 903,  Mean reward: -1.5256410256410255, Mean Entropy: 0.01765909418463707, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 904,  Mean reward: -0.5, Mean Entropy: 0.01657763682305813, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 905,  Mean reward: -2.0, Mean Entropy: 0.014584977179765701, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 906,  Mean reward: -2.0, Mean Entropy: 0.016376707702875137, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 907,  Mean reward: -2.25, Mean Entropy: 0.01686570607125759, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 908,  Mean reward: -3.5, Mean Entropy: 0.021135104820132256, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 909,  Mean reward: -3.1582278481012658, Mean Entropy: 0.01863492839038372, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 910,  Mean reward: -2.25, Mean Entropy: 0.017247309908270836, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 911,  Mean reward: -3.5, Mean Entropy: 0.01998165249824524, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.98s
Iteration: 912,  Mean reward: -2.5, Mean Entropy: 0.01569247618317604, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 913,  Mean reward: -2.5, Mean Entropy: 0.021252740174531937, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 914,  Mean reward: -2.0, Mean Entropy: 0.01310613751411438, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 915,  Mean reward: -3.411392405063291, Mean Entropy: 0.017213944345712662, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 916,  Mean reward: -3.25, Mean Entropy: 0.017984114587306976, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 917,  Mean reward: -2.0, Mean Entropy: 0.016750184819102287, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 918,  Mean reward: -3.25, Mean Entropy: 0.0180096123367548, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 919,  Mean reward: -1.75, Mean Entropy: 0.014538604766130447, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 920,  Mean reward: -1.25, Mean Entropy: 0.014614466577768326, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 921,  Mean reward: -3.5, Mean Entropy: 0.01688932627439499, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 922,  Mean reward: -1.25, Mean Entropy: 0.014524541795253754, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 923,  Mean reward: -3.25, Mean Entropy: 0.01752944104373455, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 924,  Mean reward: -0.5, Mean Entropy: 0.010940955951809883, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.82s
Iteration: 925,  Mean reward: -2.532051282051282, Mean Entropy: 0.020116638392210007, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 926,  Mean reward: -1.8924050632911393, Mean Entropy: 0.015470338985323906, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 927,  Mean reward: -2.9050632911392404, Mean Entropy: 0.021195702254772186, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 928,  Mean reward: 0.75, Mean Entropy: 0.012941800057888031, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 929,  Mean reward: -2.3987341772151898, Mean Entropy: 0.019390277564525604, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 930,  Mean reward: -2.0, Mean Entropy: 0.019842971116304398, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 931,  Mean reward: -2.5, Mean Entropy: 0.021167591214179993, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 932,  Mean reward: -2.0384615384615383, Mean Entropy: 0.015481430105865002, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 933,  Mean reward: -2.707792207792208, Mean Entropy: 0.015602829866111279, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 934,  Mean reward: -3.5, Mean Entropy: 0.018304068595170975, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 935,  Mean reward: -3.5, Mean Entropy: 0.01721266657114029, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 936,  Mean reward: -2.5, Mean Entropy: 0.01982276700437069, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 937,  Mean reward: -3.9367088607594938, Mean Entropy: 0.018579717725515366, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 938,  Mean reward: -2.5, Mean Entropy: 0.016180098056793213, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 939,  Mean reward: -4.5, Mean Entropy: 0.018549352884292603, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 940,  Mean reward: -2.0, Mean Entropy: 0.01593533530831337, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 941,  Mean reward: -1.75, Mean Entropy: 0.01213720440864563, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 942,  Mean reward: -1.75, Mean Entropy: 0.014248736202716827, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 943,  Mean reward: -1.5, Mean Entropy: 0.014748651534318924, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 944,  Mean reward: -2.25, Mean Entropy: 0.013116639107465744, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 945,  Mean reward: -2.9050632911392404, Mean Entropy: 0.015465575270354748, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 946,  Mean reward: -1.5, Mean Entropy: 0.011187711730599403, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 947,  Mean reward: -2.25, Mean Entropy: 0.013395528309047222, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 948,  Mean reward: -1.75, Mean Entropy: 0.013132253661751747, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 949,  Mean reward: -2.75, Mean Entropy: 0.014617632143199444, complete_episode_count: 80.0, Gather time: 0.74s, Train time: 0.73s
Iteration: 950,  Mean reward: -1.5, Mean Entropy: 0.013448983430862427, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 951,  Mean reward: -3.6645569620253164, Mean Entropy: 0.013843025080859661, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 952,  Mean reward: -2.651898734177215, Mean Entropy: 0.015139142982661724, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 953,  Mean reward: -2.5, Mean Entropy: 0.013463390991091728, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 954,  Mean reward: -1.25, Mean Entropy: 0.011606788262724876, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 955,  Mean reward: -2.5, Mean Entropy: 0.01268715225160122, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 956,  Mean reward: -2.5, Mean Entropy: 0.013655081391334534, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 957,  Mean reward: -1.25, Mean Entropy: 0.010452507063746452, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 958,  Mean reward: -4.75, Mean Entropy: 0.017150072380900383, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 959,  Mean reward: -1.0, Mean Entropy: 0.010083838365972042, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 960,  Mean reward: -1.5256410256410255, Mean Entropy: 0.010903863236308098, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 961,  Mean reward: -2.5, Mean Entropy: 0.013634024187922478, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 962,  Mean reward: -2.5, Mean Entropy: 0.014447232708334923, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 963,  Mean reward: -2.9050632911392404, Mean Entropy: 0.01614130102097988, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 964,  Mean reward: -0.7564102564102564, Mean Entropy: 0.011347418650984764, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 965,  Mean reward: -2.9050632911392404, Mean Entropy: 0.013687198981642723, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 966,  Mean reward: -1.75, Mean Entropy: 0.0131471436470747, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 967,  Mean reward: -1.25, Mean Entropy: 0.011190245859324932, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 968,  Mean reward: -0.5, Mean Entropy: 0.012168509885668755, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 969,  Mean reward: 0.6392405063291139, Mean Entropy: 0.03493322804570198, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 970,  Mean reward: -2.3987341772151898, Mean Entropy: 0.012565433979034424, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 971,  Mean reward: -3.0, Mean Entropy: 0.013464033603668213, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 972,  Mean reward: -1.75, Mean Entropy: 0.010722274892032146, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 973,  Mean reward: -1.8924050632911393, Mean Entropy: 0.011769084259867668, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 974,  Mean reward: -2.25, Mean Entropy: 0.00911283865571022, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 975,  Mean reward: -4.0, Mean Entropy: 0.01426784973591566, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 976,  Mean reward: -0.6265822784810127, Mean Entropy: 0.00847675185650587, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 977,  Mean reward: -3.25, Mean Entropy: 0.010200520977377892, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 978,  Mean reward: -1.8924050632911393, Mean Entropy: 0.008026398718357086, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 979,  Mean reward: -1.75, Mean Entropy: 0.009640655480325222, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.89s
Iteration: 980,  Mean reward: -2.75, Mean Entropy: 0.012085095047950745, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 981,  Mean reward: -2.9050632911392404, Mean Entropy: 0.013493518345057964, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 982,  Mean reward: -2.75, Mean Entropy: 0.012422991916537285, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 983,  Mean reward: -3.9177215189873418, Mean Entropy: 0.015356389805674553, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 984,  Mean reward: -1.0128205128205128, Mean Entropy: 0.009986571967601776, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 985,  Mean reward: -2.5, Mean Entropy: 0.013390648178756237, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.81s
Iteration: 986,  Mean reward: -2.5, Mean Entropy: 0.014608475379645824, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 987,  Mean reward: -1.1329113924050633, Mean Entropy: 0.010486811399459839, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 988,  Mean reward: -1.8924050632911393, Mean Entropy: 0.012440207414329052, complete_episode_count: 79.0, Gather time: 0.78s, Train time: 0.77s
Iteration: 989,  Mean reward: -3.5, Mean Entropy: 0.01511838287115097, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 990,  Mean reward: -1.75, Mean Entropy: 0.010574936866760254, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 991,  Mean reward: -1.0, Mean Entropy: 0.01155366562306881, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 992,  Mean reward: -1.8924050632911393, Mean Entropy: 0.013495136052370071, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 993,  Mean reward: -2.3987341772151898, Mean Entropy: 0.01473313383758068, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 994,  Mean reward: -1.0, Mean Entropy: 0.012984855100512505, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 995,  Mean reward: -2.0, Mean Entropy: 0.013804099522531033, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 996,  Mean reward: -1.1329113924050633, Mean Entropy: 0.011985182762145996, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 997,  Mean reward: -1.0, Mean Entropy: 0.012301070615649223, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 998,  Mean reward: -2.25, Mean Entropy: 0.010675227269530296, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 999,  Mean reward: -2.75, Mean Entropy: 0.013302321545779705, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1000,  Mean reward: -3.0, Mean Entropy: 0.015123049728572369, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: -1.0, Mean Entropy: 0.009365547448396683, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1002,  Mean reward: -2.75, Mean Entropy: 0.012533905915915966, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1003,  Mean reward: 0.13291139240506328, Mean Entropy: 0.010133452713489532, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1004,  Mean reward: -2.75, Mean Entropy: 0.013227265328168869, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1005,  Mean reward: -3.9177215189873418, Mean Entropy: 0.015832826495170593, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 1006,  Mean reward: -3.5, Mean Entropy: 0.014460432343184948, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1007,  Mean reward: 0.0, Mean Entropy: 0.007835404947400093, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1008,  Mean reward: -1.5, Mean Entropy: 0.011471552774310112, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1009,  Mean reward: -3.5, Mean Entropy: 0.014260856434702873, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1010,  Mean reward: -0.5, Mean Entropy: 0.01249117311090231, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1011,  Mean reward: -2.9050632911392404, Mean Entropy: 0.012489190325140953, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1012,  Mean reward: -3.25, Mean Entropy: 0.013420439325273037, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1013,  Mean reward: -2.25, Mean Entropy: 0.012929504737257957, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1014,  Mean reward: -1.1329113924050633, Mean Entropy: 0.0094455536454916, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 1015,  Mean reward: -3.5, Mean Entropy: 0.015362645499408245, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1016,  Mean reward: -1.25, Mean Entropy: 0.011100631207227707, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1017,  Mean reward: -1.75, Mean Entropy: 0.011173777282238007, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1018,  Mean reward: -0.5, Mean Entropy: 0.008613603189587593, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 1019,  Mean reward: -2.3987341772151898, Mean Entropy: 0.012257492169737816, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 1020,  Mean reward: -2.0, Mean Entropy: 0.014093972742557526, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 1021,  Mean reward: -2.0, Mean Entropy: 0.012521132826805115, complete_episode_count: 80.0, Gather time: 0.70s, Train time: 0.76s
Iteration: 1022,  Mean reward: -1.0, Mean Entropy: 0.012176118791103363, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1023,  Mean reward: -1.3860759493670887, Mean Entropy: 0.009661165997385979, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 1024,  Mean reward: -0.25, Mean Entropy: 0.011310623958706856, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1025,  Mean reward: -1.639240506329114, Mean Entropy: 0.012679575011134148, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1026,  Mean reward: -1.0, Mean Entropy: 0.00994887575507164, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1027,  Mean reward: -1.0, Mean Entropy: 0.012237968854606152, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1028,  Mean reward: -3.25, Mean Entropy: 0.01842571422457695, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1029,  Mean reward: -2.651898734177215, Mean Entropy: 0.020703144371509552, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1030,  Mean reward: -2.651898734177215, Mean Entropy: 0.017958087846636772, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1031,  Mean reward: -1.8734177215189873, Mean Entropy: 0.01731446571648121, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1032,  Mean reward: -3.1582278481012658, Mean Entropy: 0.01696772500872612, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1033,  Mean reward: -0.5, Mean Entropy: 0.012608416378498077, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1034,  Mean reward: -3.8987341772151898, Mean Entropy: 0.017944514751434326, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1035,  Mean reward: -2.3987341772151898, Mean Entropy: 0.014256948605179787, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1036,  Mean reward: -2.1455696202531644, Mean Entropy: 0.017410801723599434, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 1037,  Mean reward: -1.5, Mean Entropy: 0.014623294584453106, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1038,  Mean reward: -2.0, Mean Entropy: 0.015023864805698395, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1039,  Mean reward: -3.0, Mean Entropy: 0.019017759710550308, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1040,  Mean reward: -0.75, Mean Entropy: 0.015410222113132477, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1041,  Mean reward: -3.75, Mean Entropy: 0.019343098625540733, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1042,  Mean reward: -1.639240506329114, Mean Entropy: 0.012669573538005352, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1043,  Mean reward: -2.5, Mean Entropy: 0.0156888198107481, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1044,  Mean reward: -2.75, Mean Entropy: 0.017545491456985474, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1045,  Mean reward: -3.5, Mean Entropy: 0.018022555857896805, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1046,  Mean reward: -1.0, Mean Entropy: 0.015519401989877224, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1047,  Mean reward: -1.8924050632911393, Mean Entropy: 0.01437266543507576, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1048,  Mean reward: -0.75, Mean Entropy: 0.014563946053385735, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1049,  Mean reward: -1.0, Mean Entropy: 0.011516723781824112, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1050,  Mean reward: -1.3860759493670887, Mean Entropy: 0.012697538360953331, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1051,  Mean reward: -2.25, Mean Entropy: 0.013364206068217754, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1052,  Mean reward: -1.25, Mean Entropy: 0.009430520236492157, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1053,  Mean reward: -3.1392405063291138, Mean Entropy: 0.01392603199928999, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1054,  Mean reward: -1.0, Mean Entropy: 0.01160323154181242, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1055,  Mean reward: -2.75, Mean Entropy: 0.015301302075386047, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 1056,  Mean reward: -2.0, Mean Entropy: 0.016758106648921967, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1057,  Mean reward: -3.5, Mean Entropy: 0.01834636926651001, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1058,  Mean reward: -1.2692307692307692, Mean Entropy: 0.013917272910475731, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 1059,  Mean reward: -0.6265822784810127, Mean Entropy: 0.011002260260283947, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 1060,  Mean reward: -3.1582278481012658, Mean Entropy: 0.012433294206857681, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1061,  Mean reward: -1.0, Mean Entropy: 0.013189239427447319, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 1062,  Mean reward: -1.0, Mean Entropy: 0.009183566085994244, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1063,  Mean reward: -2.25, Mean Entropy: 0.014733076095581055, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.99s
Iteration: 1064,  Mean reward: -2.25, Mean Entropy: 0.010236922651529312, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1065,  Mean reward: -2.25, Mean Entropy: 0.011992393992841244, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1066,  Mean reward: -1.75, Mean Entropy: 0.011587267741560936, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1067,  Mean reward: -1.5, Mean Entropy: 0.011470036581158638, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1068,  Mean reward: -2.0, Mean Entropy: 0.01008746400475502, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1069,  Mean reward: -2.0, Mean Entropy: 0.011861476115882397, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1070,  Mean reward: -3.1582278481012658, Mean Entropy: 0.011189430952072144, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 1071,  Mean reward: 0.25, Mean Entropy: 0.007361551281064749, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1072,  Mean reward: -1.3860759493670887, Mean Entropy: 0.009366744197905064, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1073,  Mean reward: -1.5, Mean Entropy: 0.009363438934087753, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 1074,  Mean reward: 0.75, Mean Entropy: 0.005057332571595907, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1075,  Mean reward: -2.3987341772151898, Mean Entropy: 0.011214178055524826, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 1076,  Mean reward: -2.3987341772151898, Mean Entropy: 0.014116927981376648, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1077,  Mean reward: -3.75, Mean Entropy: 0.01659270189702511, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1078,  Mean reward: -3.25, Mean Entropy: 0.020653365179896355, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1079,  Mean reward: -0.75, Mean Entropy: 0.015247880481183529, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1080,  Mean reward: -1.25, Mean Entropy: 0.012073828838765621, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1081,  Mean reward: -1.75, Mean Entropy: 0.012131514959037304, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1082,  Mean reward: -3.411392405063291, Mean Entropy: 0.014065530151128769, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 1083,  Mean reward: -1.5, Mean Entropy: 0.012819277122616768, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 1084,  Mean reward: -2.25, Mean Entropy: 0.01215177308768034, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1085,  Mean reward: -2.75, Mean Entropy: 0.013817094266414642, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 1086,  Mean reward: -1.8924050632911393, Mean Entropy: 0.013463444076478481, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.81s
Iteration: 1087,  Mean reward: -2.5, Mean Entropy: 0.013531985692679882, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1088,  Mean reward: -2.5, Mean Entropy: 0.014419496059417725, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1089,  Mean reward: -2.1455696202531644, Mean Entropy: 0.013350410386919975, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1090,  Mean reward: -5.0, Mean Entropy: 0.014529443345963955, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1091,  Mean reward: -3.25, Mean Entropy: 0.014886550605297089, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1092,  Mean reward: -3.25, Mean Entropy: 0.015587752684950829, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1093,  Mean reward: -2.0, Mean Entropy: 0.011822037398815155, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1094,  Mean reward: -4.25, Mean Entropy: 0.013186763040721416, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1095,  Mean reward: -2.25, Mean Entropy: 0.01339699700474739, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1096,  Mean reward: -1.639240506329114, Mean Entropy: 0.01128646545112133, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 1097,  Mean reward: -5.5, Mean Entropy: 0.014445158652961254, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1098,  Mean reward: -2.1455696202531644, Mean Entropy: 0.009763332083821297, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 1099,  Mean reward: -1.5, Mean Entropy: 0.008426958695054054, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1100,  Mean reward: -0.37341772151898733, Mean Entropy: 0.0068300627171993256, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.76s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: -1.5, Mean Entropy: 0.009457830339670181, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 1102,  Mean reward: -1.5, Mean Entropy: 0.008507156744599342, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 1103,  Mean reward: -3.25, Mean Entropy: 0.009471139870584011, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1104,  Mean reward: -3.0, Mean Entropy: 0.008877108804881573, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1105,  Mean reward: -2.5, Mean Entropy: 0.008251871913671494, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1106,  Mean reward: -1.0, Mean Entropy: 0.012157758697867393, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1107,  Mean reward: -3.0, Mean Entropy: 0.011013179086148739, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1108,  Mean reward: -0.25, Mean Entropy: 0.009053705260157585, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1109,  Mean reward: -2.1455696202531644, Mean Entropy: 0.011594757437705994, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1110,  Mean reward: -2.5, Mean Entropy: 0.012918205931782722, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1111,  Mean reward: -1.25, Mean Entropy: 0.010913060978055, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1112,  Mean reward: -0.25, Mean Entropy: 0.008722219616174698, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1113,  Mean reward: 0.0, Mean Entropy: 0.013135908171534538, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1114,  Mean reward: -2.25, Mean Entropy: 0.015238913707435131, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1115,  Mean reward: 0.75, Mean Entropy: 0.01034235768020153, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1116,  Mean reward: -0.37341772151898733, Mean Entropy: 0.011635927483439445, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1117,  Mean reward: -1.25, Mean Entropy: 0.017223810777068138, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1118,  Mean reward: -0.5, Mean Entropy: 0.015957681462168694, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1119,  Mean reward: -2.9050632911392404, Mean Entropy: 0.02265554666519165, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1120,  Mean reward: -3.411392405063291, Mean Entropy: 0.02193792723119259, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 1121,  Mean reward: -2.5, Mean Entropy: 0.024407438933849335, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1122,  Mean reward: -1.8924050632911393, Mean Entropy: 0.018139831721782684, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1123,  Mean reward: -1.0, Mean Entropy: 0.019174378365278244, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1124,  Mean reward: -3.0641025641025643, Mean Entropy: 0.017512399703264236, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1125,  Mean reward: -3.25, Mean Entropy: 0.019989553838968277, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1126,  Mean reward: -1.8924050632911393, Mean Entropy: 0.01906852051615715, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1127,  Mean reward: 0.13291139240506328, Mean Entropy: 0.013016074895858765, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1128,  Mean reward: -3.25, Mean Entropy: 0.01778579130768776, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1129,  Mean reward: -1.0, Mean Entropy: 0.01500721275806427, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1130,  Mean reward: -1.0, Mean Entropy: 0.010647120885550976, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1131,  Mean reward: 1.1265822784810127, Mean Entropy: 0.009021339938044548, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 1132,  Mean reward: -1.25, Mean Entropy: 0.011321671307086945, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1133,  Mean reward: -1.75, Mean Entropy: 0.012920282781124115, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1134,  Mean reward: -2.0, Mean Entropy: 0.01799193024635315, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1135,  Mean reward: -0.879746835443038, Mean Entropy: 0.013928843662142754, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 1136,  Mean reward: -3.411392405063291, Mean Entropy: 0.017341895028948784, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 1137,  Mean reward: -2.651898734177215, Mean Entropy: 0.015952829271554947, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1138,  Mean reward: -2.0, Mean Entropy: 0.013068288564682007, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 1139,  Mean reward: -2.651898734177215, Mean Entropy: 0.01571105420589447, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1140,  Mean reward: -2.8076923076923075, Mean Entropy: 0.015504725277423859, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1141,  Mean reward: -1.3860759493670887, Mean Entropy: 0.009682659059762955, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1142,  Mean reward: -1.8924050632911393, Mean Entropy: 0.01126297377049923, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1143,  Mean reward: -1.0, Mean Entropy: 0.00909392535686493, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1144,  Mean reward: -1.75, Mean Entropy: 0.00827962439507246, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1145,  Mean reward: -3.25, Mean Entropy: 0.012667479924857616, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1146,  Mean reward: -0.5, Mean Entropy: 0.007405036594718695, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1147,  Mean reward: -1.0, Mean Entropy: 0.007021717727184296, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1148,  Mean reward: -1.75, Mean Entropy: 0.007712017744779587, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1149,  Mean reward: -2.0, Mean Entropy: 0.009681709110736847, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1150,  Mean reward: -2.0, Mean Entropy: 0.008570238947868347, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1151,  Mean reward: -2.75, Mean Entropy: 0.01005797740072012, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1152,  Mean reward: -2.0, Mean Entropy: 0.010217341594398022, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1153,  Mean reward: -1.5, Mean Entropy: 0.007599690463393927, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1154,  Mean reward: -1.5, Mean Entropy: 0.00819222442805767, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1155,  Mean reward: -2.25, Mean Entropy: 0.009396563284099102, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1156,  Mean reward: -3.0, Mean Entropy: 0.011655258946120739, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1157,  Mean reward: -3.0, Mean Entropy: 0.011629618704319, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1158,  Mean reward: -2.5, Mean Entropy: 0.010741292499005795, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1159,  Mean reward: -3.5, Mean Entropy: 0.010551022365689278, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1160,  Mean reward: -2.5, Mean Entropy: 0.010883702896535397, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1161,  Mean reward: -1.25, Mean Entropy: 0.008723670616745949, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1162,  Mean reward: -2.0, Mean Entropy: 0.009380715899169445, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1163,  Mean reward: -2.1455696202531644, Mean Entropy: 0.010930020362138748, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1164,  Mean reward: -2.651898734177215, Mean Entropy: 0.010666201822459698, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1165,  Mean reward: -1.0, Mean Entropy: 0.006957290228456259, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1166,  Mean reward: -3.5, Mean Entropy: 0.010029671713709831, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1167,  Mean reward: -2.5, Mean Entropy: 0.007187198847532272, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 1168,  Mean reward: -3.25, Mean Entropy: 0.00867299921810627, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1169,  Mean reward: -1.0, Mean Entropy: 0.006777205970138311, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1170,  Mean reward: -3.0, Mean Entropy: 0.009894989430904388, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1171,  Mean reward: -1.8924050632911393, Mean Entropy: 0.007592346519231796, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 1172,  Mean reward: -2.0, Mean Entropy: 0.006971699185669422, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1173,  Mean reward: -2.75, Mean Entropy: 0.008505502715706825, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1174,  Mean reward: -4.0, Mean Entropy: 0.009942227974534035, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1175,  Mean reward: -2.0, Mean Entropy: 0.00817058514803648, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1176,  Mean reward: -2.1455696202531644, Mean Entropy: 0.0060279276221990585, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1177,  Mean reward: -2.25, Mean Entropy: 0.008271458558738232, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.98s
Iteration: 1178,  Mean reward: -0.75, Mean Entropy: 0.006581378169357777, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1179,  Mean reward: -0.75, Mean Entropy: 0.007308089639991522, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1180,  Mean reward: -2.0, Mean Entropy: 0.007088369689881802, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1181,  Mean reward: -2.25, Mean Entropy: 0.00698947673663497, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1182,  Mean reward: -1.25, Mean Entropy: 0.005726230330765247, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1183,  Mean reward: -2.75, Mean Entropy: 0.007480561267584562, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1184,  Mean reward: -2.25, Mean Entropy: 0.006666762288659811, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1185,  Mean reward: -3.25, Mean Entropy: 0.007934392429888248, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1186,  Mean reward: -2.25, Mean Entropy: 0.007645841222256422, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1187,  Mean reward: -3.411392405063291, Mean Entropy: 0.00838454905897379, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1188,  Mean reward: -0.879746835443038, Mean Entropy: 0.006025118753314018, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1189,  Mean reward: -1.25, Mean Entropy: 0.005951191298663616, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1190,  Mean reward: -4.25, Mean Entropy: 0.008331195451319218, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1191,  Mean reward: -1.25, Mean Entropy: 0.00628999387845397, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1192,  Mean reward: -1.3860759493670887, Mean Entropy: 0.006107974331825972, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1193,  Mean reward: -2.0, Mean Entropy: 0.005205473862588406, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1194,  Mean reward: -0.5, Mean Entropy: 0.005612201057374477, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1195,  Mean reward: -2.75, Mean Entropy: 0.006257790140807629, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1196,  Mean reward: 0.3860759493670886, Mean Entropy: 0.0045203110203146935, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1197,  Mean reward: -2.5, Mean Entropy: 0.005890027619898319, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1198,  Mean reward: -2.1455696202531644, Mean Entropy: 0.005142060574144125, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1199,  Mean reward: -1.75, Mean Entropy: 0.005264374427497387, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1200,  Mean reward: -1.75, Mean Entropy: 0.0045853229239583015, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: -2.5, Mean Entropy: 0.004152942448854446, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1202,  Mean reward: -3.1582278481012658, Mean Entropy: 0.005406791344285011, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 1203,  Mean reward: -0.5, Mean Entropy: 0.004796118941158056, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1204,  Mean reward: -1.5, Mean Entropy: 0.004598041530698538, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1205,  Mean reward: -1.25, Mean Entropy: 0.0046872240491211414, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 1206,  Mean reward: -1.5, Mean Entropy: 0.0055510676465928555, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 1207,  Mean reward: -2.75, Mean Entropy: 0.006459256634116173, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1208,  Mean reward: -1.5, Mean Entropy: 0.005385470576584339, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1209,  Mean reward: -1.0, Mean Entropy: 0.004514855798333883, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1210,  Mean reward: -2.9050632911392404, Mean Entropy: 0.00819434318691492, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1211,  Mean reward: -0.25, Mean Entropy: 0.005384382791817188, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1212,  Mean reward: -2.25, Mean Entropy: 0.008134151808917522, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1213,  Mean reward: -2.1455696202531644, Mean Entropy: 0.006050055846571922, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1214,  Mean reward: -3.0, Mean Entropy: 0.008377361111342907, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1215,  Mean reward: -1.75, Mean Entropy: 0.007190524600446224, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.96s
Iteration: 1216,  Mean reward: -2.9050632911392404, Mean Entropy: 0.007149076089262962, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1217,  Mean reward: 0.0, Mean Entropy: 0.005224389955401421, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1218,  Mean reward: -1.5, Mean Entropy: 0.006120610982179642, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1219,  Mean reward: -1.8924050632911393, Mean Entropy: 0.0066717807203531265, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 1220,  Mean reward: -2.551282051282051, Mean Entropy: 0.006274168845266104, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1221,  Mean reward: -1.75, Mean Entropy: 0.00584923243150115, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1222,  Mean reward: -2.3987341772151898, Mean Entropy: 0.005885708145797253, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1223,  Mean reward: 0.25, Mean Entropy: 0.004049751441925764, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1224,  Mean reward: -0.25, Mean Entropy: 0.0037467414513230324, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1225,  Mean reward: -1.620253164556962, Mean Entropy: 0.006598080974072218, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1226,  Mean reward: -4.0, Mean Entropy: 0.008788718841969967, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 1227,  Mean reward: -2.5, Mean Entropy: 0.007593940012156963, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1228,  Mean reward: -2.1265822784810124, Mean Entropy: 0.009534780867397785, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1229,  Mean reward: -2.0, Mean Entropy: 0.011811013333499432, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1230,  Mean reward: 0.0, Mean Entropy: 0.011952901259064674, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 1231,  Mean reward: -1.2692307692307692, Mean Entropy: 0.010669282637536526, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1232,  Mean reward: -0.25, Mean Entropy: 0.011223344132304192, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1233,  Mean reward: -1.0, Mean Entropy: 0.01015846710652113, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1234,  Mean reward: -1.5, Mean Entropy: 0.012639863416552544, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1235,  Mean reward: -2.75, Mean Entropy: 0.012075486592948437, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1236,  Mean reward: -3.75, Mean Entropy: 0.01469508744776249, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1237,  Mean reward: -2.75, Mean Entropy: 0.014336503110826015, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 1238,  Mean reward: -2.25, Mean Entropy: 0.01038411632180214, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1239,  Mean reward: -2.75, Mean Entropy: 0.010771235451102257, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1240,  Mean reward: -2.75, Mean Entropy: 0.013568371534347534, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1241,  Mean reward: -1.25, Mean Entropy: 0.009573832154273987, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1242,  Mean reward: -2.5, Mean Entropy: 0.00979067012667656, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1243,  Mean reward: -3.0641025641025643, Mean Entropy: 0.011766428127884865, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1244,  Mean reward: -2.0, Mean Entropy: 0.010817449539899826, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1245,  Mean reward: -2.0, Mean Entropy: 0.011453216895461082, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1246,  Mean reward: -2.5, Mean Entropy: 0.010078991763293743, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1247,  Mean reward: -0.5, Mean Entropy: 0.007440160494297743, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1248,  Mean reward: -1.1329113924050633, Mean Entropy: 0.010279359295964241, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1249,  Mean reward: -1.5256410256410255, Mean Entropy: 0.00939531996846199, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 1250,  Mean reward: -2.5, Mean Entropy: 0.009753087535500526, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 1251,  Mean reward: -2.25, Mean Entropy: 0.01179281435906887, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1252,  Mean reward: -1.0, Mean Entropy: 0.009603507816791534, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 1253,  Mean reward: -1.5, Mean Entropy: 0.009210745804011822, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 1254,  Mean reward: -2.0, Mean Entropy: 0.008764320984482765, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 1255,  Mean reward: -2.0384615384615383, Mean Entropy: 0.008732127957046032, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1256,  Mean reward: -0.24358974358974358, Mean Entropy: 0.00720295961946249, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1257,  Mean reward: -2.0, Mean Entropy: 0.008131565526127815, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1258,  Mean reward: 0.6392405063291139, Mean Entropy: 0.005560610443353653, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1259,  Mean reward: -1.75, Mean Entropy: 0.008496269583702087, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.27s
Iteration: 1260,  Mean reward: -1.5, Mean Entropy: 0.01016616728156805, complete_episode_count: 80.0, Gather time: 1.30s, Train time: 0.74s
Iteration: 1261,  Mean reward: -2.2948717948717947, Mean Entropy: 0.012043658643960953, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1262,  Mean reward: -2.75, Mean Entropy: 0.013309838250279427, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 1263,  Mean reward: 0.9113924050632911, Mean Entropy: 0.009720811620354652, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1264,  Mean reward: -2.3987341772151898, Mean Entropy: 0.013303279876708984, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1265,  Mean reward: -1.5, Mean Entropy: 0.011758234351873398, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1266,  Mean reward: -3.0, Mean Entropy: 0.011862458661198616, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1267,  Mean reward: -1.3860759493670887, Mean Entropy: 0.011078154668211937, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1268,  Mean reward: -0.75, Mean Entropy: 0.008571065030992031, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1269,  Mean reward: -1.0, Mean Entropy: 0.009112313389778137, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1270,  Mean reward: -1.25, Mean Entropy: 0.009489402174949646, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 1271,  Mean reward: -2.5, Mean Entropy: 0.01090171467512846, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1272,  Mean reward: -0.879746835443038, Mean Entropy: 0.008729550987482071, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1273,  Mean reward: -3.0, Mean Entropy: 0.009454065002501011, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1274,  Mean reward: -4.0, Mean Entropy: 0.01195613108575344, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1275,  Mean reward: -3.0, Mean Entropy: 0.01431858167052269, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1276,  Mean reward: -3.411392405063291, Mean Entropy: 0.011443153023719788, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1277,  Mean reward: -2.0, Mean Entropy: 0.009335774928331375, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1278,  Mean reward: -4.25, Mean Entropy: 0.010991960763931274, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1279,  Mean reward: -3.0, Mean Entropy: 0.009968104772269726, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1280,  Mean reward: -1.75, Mean Entropy: 0.009713254868984222, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1281,  Mean reward: -1.1329113924050633, Mean Entropy: 0.008845185860991478, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 1282,  Mean reward: -3.5, Mean Entropy: 0.009109409525990486, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1283,  Mean reward: -3.411392405063291, Mean Entropy: 0.00984238926321268, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 1284,  Mean reward: -1.0, Mean Entropy: 0.008448414504528046, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1285,  Mean reward: -2.632911392405063, Mean Entropy: 0.011288566514849663, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1286,  Mean reward: -2.0, Mean Entropy: 0.01264350675046444, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1287,  Mean reward: -1.25, Mean Entropy: 0.01068714540451765, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1288,  Mean reward: -0.75, Mean Entropy: 0.013261350803077221, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1289,  Mean reward: -3.0, Mean Entropy: 0.016707278788089752, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1290,  Mean reward: -1.25, Mean Entropy: 0.013420537114143372, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1291,  Mean reward: -2.1455696202531644, Mean Entropy: 0.011367574334144592, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 1292,  Mean reward: -1.639240506329114, Mean Entropy: 0.010694928467273712, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 1293,  Mean reward: -3.6835443037974684, Mean Entropy: 0.014360018074512482, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1294,  Mean reward: -0.75, Mean Entropy: 0.009961440227925777, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 1295,  Mean reward: -1.8012820512820513, Mean Entropy: 0.009875865653157234, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 1296,  Mean reward: -0.75, Mean Entropy: 0.0075292205438017845, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 1297,  Mean reward: -1.1329113924050633, Mean Entropy: 0.00932452455163002, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1298,  Mean reward: -0.5, Mean Entropy: 0.008918417617678642, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1299,  Mean reward: -1.25, Mean Entropy: 0.008807027712464333, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1300,  Mean reward: -2.651898734177215, Mean Entropy: 0.010149642825126648, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 1301,  Mean reward: -2.25, Mean Entropy: 0.008773138746619225, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1302,  Mean reward: -1.5, Mean Entropy: 0.00937969982624054, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1303,  Mean reward: -1.5, Mean Entropy: 0.010061195120215416, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 1304,  Mean reward: -0.5, Mean Entropy: 0.007028542459011078, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1305,  Mean reward: -2.0, Mean Entropy: 0.009039673022925854, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 1306,  Mean reward: -4.0, Mean Entropy: 0.00911056436598301, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1307,  Mean reward: -1.25, Mean Entropy: 0.0069954427890479565, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1308,  Mean reward: -2.25, Mean Entropy: 0.008856643922626972, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1309,  Mean reward: -2.75, Mean Entropy: 0.010194772854447365, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1310,  Mean reward: -2.0, Mean Entropy: 0.008880432695150375, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1311,  Mean reward: -2.25, Mean Entropy: 0.008906882256269455, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 1312,  Mean reward: 0.0, Mean Entropy: 0.008733975701034069, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -4.67948717948718, Mean Entropy: 0.9675179719924927, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.50s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.2317073170731705, Mean Entropy: 0.9386358261108398, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 2,  Mean reward: -7.036585365853658, Mean Entropy: 0.9386341571807861, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 3,  Mean reward: -4.686046511627907, Mean Entropy: 0.9386100769042969, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.44s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 4,  Mean reward: -2.875, Mean Entropy: 0.9746865034103394, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 5,  Mean reward: -6.556818181818182, Mean Entropy: 0.938557505607605, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 6,  Mean reward: -3.3214285714285716, Mean Entropy: 0.9889900088310242, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 7,  Mean reward: -4.6891891891891895, Mean Entropy: 0.9601237773895264, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 8,  Mean reward: -4.113636363636363, Mean Entropy: 0.9528708457946777, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 9,  Mean reward: -4.25, Mean Entropy: 0.9310309290885925, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 10,  Mean reward: -5.817073170731708, Mean Entropy: 0.9738611578941345, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 11,  Mean reward: -4.940476190476191, Mean Entropy: 0.8945274353027344, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 12,  Mean reward: -5.131578947368421, Mean Entropy: 0.9525434374809265, complete_episode_count: 38.0, Gather time: 0.50s, Train time: 1.51s
Iteration: 13,  Mean reward: -4.74468085106383, Mean Entropy: 0.9237517714500427, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 14,  Mean reward: -5.444444444444445, Mean Entropy: 0.9451934695243835, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.63s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 15,  Mean reward: -2.8444444444444446, Mean Entropy: 0.9454065561294556, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 16,  Mean reward: -6.732558139534884, Mean Entropy: 1.0029455423355103, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 17,  Mean reward: -3.6923076923076925, Mean Entropy: 0.8588072657585144, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 18,  Mean reward: -5.5875, Mean Entropy: 0.8370859026908875, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 19,  Mean reward: -3.4886363636363638, Mean Entropy: 0.9742894172668457, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 20,  Mean reward: -5.375, Mean Entropy: 0.9741415977478027, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.57s
Iteration: 21,  Mean reward: -3.5795454545454546, Mean Entropy: 0.9452912211418152, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 22,  Mean reward: -4.2926829268292686, Mean Entropy: 0.9739871025085449, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 23,  Mean reward: -4.329268292682927, Mean Entropy: 0.8870259523391724, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 24,  Mean reward: -3.4, Mean Entropy: 0.9222375750541687, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 25,  Mean reward: -5.5813953488372094, Mean Entropy: 0.9495053291320801, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 26,  Mean reward: -5.825581395348837, Mean Entropy: 0.9567580223083496, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 27,  Mean reward: -3.238095238095238, Mean Entropy: 0.8795565366744995, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 28,  Mean reward: -5.848837209302325, Mean Entropy: 0.901281476020813, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.52s
Iteration: 29,  Mean reward: -4.953488372093023, Mean Entropy: 0.9450916051864624, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 30,  Mean reward: -2.6463414634146343, Mean Entropy: 0.9852127432823181, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 31,  Mean reward: -5.182926829268292, Mean Entropy: 0.9780423641204834, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 32,  Mean reward: -6.592105263157895, Mean Entropy: 0.963469386100769, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 33,  Mean reward: -4.571428571428571, Mean Entropy: 0.8926391005516052, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 34,  Mean reward: -5.6, Mean Entropy: 0.979781985282898, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.73s
Iteration: 35,  Mean reward: -5.631578947368421, Mean Entropy: 0.8874028921127319, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 36,  Mean reward: -3.9555555555555557, Mean Entropy: 1.0379478931427002, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 37,  Mean reward: -5.345238095238095, Mean Entropy: 0.9224669337272644, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 38,  Mean reward: -5.573170731707317, Mean Entropy: 0.9791714549064636, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 39,  Mean reward: -4.075, Mean Entropy: 0.9494956731796265, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 40,  Mean reward: -5.048780487804878, Mean Entropy: 1.001030683517456, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 41,  Mean reward: -3.792682926829268, Mean Entropy: 0.9519436359405518, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 42,  Mean reward: -3.75, Mean Entropy: 0.9742822647094727, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 43,  Mean reward: -3.892857142857143, Mean Entropy: 1.0033812522888184, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 44,  Mean reward: -3.9, Mean Entropy: 0.9746400713920593, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 45,  Mean reward: -5.430232558139535, Mean Entropy: 1.0107426643371582, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 46,  Mean reward: -5.195121951219512, Mean Entropy: 0.9746044874191284, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.52s
Iteration: 47,  Mean reward: -5.353658536585366, Mean Entropy: 0.8807339668273926, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 48,  Mean reward: -2.395348837209302, Mean Entropy: 0.9385241866111755, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.61s
Iteration: 49,  Mean reward: -4.2023809523809526, Mean Entropy: 0.945706844329834, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 50,  Mean reward: -4.902439024390244, Mean Entropy: 0.9312137961387634, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 51,  Mean reward: -7.2375, Mean Entropy: 0.9889053702354431, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 52,  Mean reward: -7.556818181818182, Mean Entropy: 0.9815330505371094, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 53,  Mean reward: -6.011111111111111, Mean Entropy: 0.9743255376815796, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.49s
Iteration: 54,  Mean reward: -5.130952380952381, Mean Entropy: 0.9673212766647339, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 55,  Mean reward: -3.652173913043478, Mean Entropy: 0.9097002744674683, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 56,  Mean reward: -6.158536585365853, Mean Entropy: 0.902480959892273, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 57,  Mean reward: -4.511627906976744, Mean Entropy: 0.9530193209648132, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 58,  Mean reward: -6.011363636363637, Mean Entropy: 0.9457196593284607, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 59,  Mean reward: -6.0227272727272725, Mean Entropy: 0.9168125987052917, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 60,  Mean reward: -4.113636363636363, Mean Entropy: 0.9529614448547363, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 61,  Mean reward: -6.926829268292683, Mean Entropy: 0.9168077707290649, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 62,  Mean reward: -4.102564102564102, Mean Entropy: 0.8950656652450562, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 63,  Mean reward: -6.7073170731707314, Mean Entropy: 0.9526513814926147, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 64,  Mean reward: -3.658536585365854, Mean Entropy: 0.9815315008163452, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 65,  Mean reward: -6.646341463414634, Mean Entropy: 0.9598116874694824, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 66,  Mean reward: -2.7386363636363638, Mean Entropy: 0.9740607738494873, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 67,  Mean reward: -4.146341463414634, Mean Entropy: 0.9450929164886475, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 68,  Mean reward: -2.7875, Mean Entropy: 0.9876577258110046, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 69,  Mean reward: -5.535714285714286, Mean Entropy: 0.9785863757133484, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 70,  Mean reward: -3.6625, Mean Entropy: 0.8784890174865723, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 71,  Mean reward: -2.426829268292683, Mean Entropy: 0.8795284032821655, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 72,  Mean reward: -4.186046511627907, Mean Entropy: 0.9441629648208618, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 73,  Mean reward: -2.1341463414634148, Mean Entropy: 0.9433771967887878, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 74,  Mean reward: -3.95, Mean Entropy: 1.0054494142532349, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 75,  Mean reward: -1.872093023255814, Mean Entropy: 0.9605644941329956, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 76,  Mean reward: -4.524390243902439, Mean Entropy: 0.9169005155563354, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 77,  Mean reward: -3.7934782608695654, Mean Entropy: 0.9552590250968933, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 78,  Mean reward: -4.630952380952381, Mean Entropy: 0.9109455347061157, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 79,  Mean reward: -3.125, Mean Entropy: 0.9605164527893066, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 80,  Mean reward: -4.593023255813954, Mean Entropy: 0.9222909808158875, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 81,  Mean reward: -4.7, Mean Entropy: 0.8386000990867615, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 82,  Mean reward: -4.261363636363637, Mean Entropy: 0.7940065264701843, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 83,  Mean reward: -3.8979591836734695, Mean Entropy: 0.7571589350700378, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 84,  Mean reward: -3.2982456140350878, Mean Entropy: 0.899183452129364, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 85,  Mean reward: -4.352941176470588, Mean Entropy: 0.7272248268127441, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 86,  Mean reward: -2.4363636363636365, Mean Entropy: 0.7600828409194946, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 87,  Mean reward: -1.3076923076923077, Mean Entropy: 0.9038684368133545, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 88,  Mean reward: -4.965909090909091, Mean Entropy: 0.7508856654167175, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 89,  Mean reward: 0.17, Mean Entropy: 0.8368636965751648, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 90,  Mean reward: -3.1923076923076925, Mean Entropy: 0.9356132745742798, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 91,  Mean reward: -2.8020833333333335, Mean Entropy: 0.7715797424316406, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 92,  Mean reward: -2.0892857142857144, Mean Entropy: 0.8578637838363647, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 93,  Mean reward: -4.472222222222222, Mean Entropy: 0.7532106637954712, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 94,  Mean reward: -3.8055555555555554, Mean Entropy: 0.7162350416183472, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 95,  Mean reward: -3.609090909090909, Mean Entropy: 0.7912611961364746, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 96,  Mean reward: -2.0982142857142856, Mean Entropy: 0.9160542488098145, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 97,  Mean reward: -3.9711538461538463, Mean Entropy: 0.8014594912528992, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 98,  Mean reward: -0.6886792452830188, Mean Entropy: 0.8121749758720398, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 99,  Mean reward: -4.759615384615385, Mean Entropy: 0.8266094923019409, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 100,  Mean reward: -2.642857142857143, Mean Entropy: 0.7365689277648926, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.47s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -4.048076923076923, Mean Entropy: 0.6684601306915283, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 102,  Mean reward: -2.5689655172413794, Mean Entropy: 0.6999354362487793, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 103,  Mean reward: -3.5789473684210527, Mean Entropy: 0.6442078948020935, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 104,  Mean reward: -2.4482758620689653, Mean Entropy: 0.5245537757873535, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 105,  Mean reward: -1.1923076923076923, Mean Entropy: 0.5849514007568359, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 106,  Mean reward: -2.4322033898305087, Mean Entropy: 0.6885389685630798, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 107,  Mean reward: -1.9056603773584906, Mean Entropy: 0.6935097575187683, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 108,  Mean reward: -4.113207547169812, Mean Entropy: 0.8312792778015137, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 109,  Mean reward: -4.398148148148148, Mean Entropy: 0.7269322872161865, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 110,  Mean reward: -3.307017543859649, Mean Entropy: 0.7031068801879883, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.60s
Iteration: 111,  Mean reward: -2.324074074074074, Mean Entropy: 0.8201362490653992, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 112,  Mean reward: -1.6160714285714286, Mean Entropy: 0.6837239265441895, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 113,  Mean reward: -2.548076923076923, Mean Entropy: 0.802449643611908, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.64s
Iteration: 114,  Mean reward: -4.4363636363636365, Mean Entropy: 0.8306398391723633, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 115,  Mean reward: -5.330188679245283, Mean Entropy: 0.7794083952903748, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 116,  Mean reward: -2.2818181818181817, Mean Entropy: 0.8501930236816406, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 117,  Mean reward: -4.3125, Mean Entropy: 0.8766960501670837, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 118,  Mean reward: -3.372549019607843, Mean Entropy: 0.7695107460021973, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 119,  Mean reward: -3.314814814814815, Mean Entropy: 0.7647852897644043, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 120,  Mean reward: -5.150943396226415, Mean Entropy: 0.723232626914978, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 121,  Mean reward: -5.157894736842105, Mean Entropy: 0.7712607383728027, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 122,  Mean reward: 1.7456140350877194, Mean Entropy: 0.7828933596611023, complete_episode_count: 57.0, Gather time: 0.65s, Train time: 1.46s
Iteration: 123,  Mean reward: -3.5, Mean Entropy: 0.8380053043365479, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 124,  Mean reward: -4.169642857142857, Mean Entropy: 0.7974144816398621, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 125,  Mean reward: -2.280701754385965, Mean Entropy: 0.8528119921684265, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.58s
Iteration: 126,  Mean reward: -3.574074074074074, Mean Entropy: 0.8908514380455017, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 127,  Mean reward: -2.5576923076923075, Mean Entropy: 0.7986229658126831, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 128,  Mean reward: -3.7884615384615383, Mean Entropy: 0.8455270528793335, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 129,  Mean reward: -5.826923076923077, Mean Entropy: 0.8542816638946533, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 130,  Mean reward: -6.074074074074074, Mean Entropy: 0.826225996017456, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 131,  Mean reward: -5.81, Mean Entropy: 0.7857919931411743, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 132,  Mean reward: -5.294117647058823, Mean Entropy: 0.7524333000183105, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 133,  Mean reward: -1.8135593220338984, Mean Entropy: 0.7728390693664551, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 134,  Mean reward: -2.959016393442623, Mean Entropy: 0.7029972076416016, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 135,  Mean reward: -4.045454545454546, Mean Entropy: 0.7556859850883484, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 136,  Mean reward: -3.191666666666667, Mean Entropy: 0.7511934638023376, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 137,  Mean reward: -3.353448275862069, Mean Entropy: 0.7356864809989929, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 138,  Mean reward: -3.4901960784313726, Mean Entropy: 0.7724895477294922, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 139,  Mean reward: -4.183333333333334, Mean Entropy: 0.7669333219528198, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 140,  Mean reward: -4.37719298245614, Mean Entropy: 0.7243279218673706, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.57s
Iteration: 141,  Mean reward: -1.2982456140350878, Mean Entropy: 0.6610413789749146, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 142,  Mean reward: -4.091666666666667, Mean Entropy: 0.6809866428375244, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 143,  Mean reward: -2.875, Mean Entropy: 0.6067244410514832, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 144,  Mean reward: -2.2049180327868854, Mean Entropy: 0.6112890839576721, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 145,  Mean reward: -3.2033898305084745, Mean Entropy: 0.6652112007141113, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.64s
Iteration: 146,  Mean reward: -2.2758620689655173, Mean Entropy: 0.7395967245101929, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 147,  Mean reward: -3.2452830188679247, Mean Entropy: 0.7215315103530884, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 148,  Mean reward: -3.185185185185185, Mean Entropy: 0.5996208190917969, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 149,  Mean reward: -1.4596774193548387, Mean Entropy: 0.5696716904640198, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 150,  Mean reward: -4.693548387096774, Mean Entropy: 0.5421984195709229, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 151,  Mean reward: -1.5163934426229508, Mean Entropy: 0.4064684808254242, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 152,  Mean reward: -3.1307692307692307, Mean Entropy: 0.41562193632125854, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 153,  Mean reward: -4.805084745762712, Mean Entropy: 0.46748679876327515, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 154,  Mean reward: -5.1796875, Mean Entropy: 0.4393647611141205, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 155,  Mean reward: -2.3306451612903225, Mean Entropy: 0.36600595712661743, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 156,  Mean reward: -3.2611940298507465, Mean Entropy: 0.36499691009521484, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 157,  Mean reward: -5.0390625, Mean Entropy: 0.4018251299858093, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 158,  Mean reward: -1.791044776119403, Mean Entropy: 0.3881042003631592, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 159,  Mean reward: -1.6136363636363635, Mean Entropy: 0.45460790395736694, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 160,  Mean reward: -1.1209677419354838, Mean Entropy: 0.46663767099380493, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 161,  Mean reward: 0.21428571428571427, Mean Entropy: 0.35141634941101074, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 162,  Mean reward: -1.5153846153846153, Mean Entropy: 0.3634708523750305, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 163,  Mean reward: -3.3676470588235294, Mean Entropy: 0.36996757984161377, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 164,  Mean reward: -3.582089552238806, Mean Entropy: 0.39765453338623047, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 165,  Mean reward: -2.6774193548387095, Mean Entropy: 0.4010518491268158, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 166,  Mean reward: -2.4923076923076923, Mean Entropy: 0.3508499264717102, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 167,  Mean reward: -1.3828125, Mean Entropy: 0.32867756485939026, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 168,  Mean reward: -3.2388059701492535, Mean Entropy: 0.30861297249794006, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 169,  Mean reward: -3.13768115942029, Mean Entropy: 0.36798590421676636, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 170,  Mean reward: -3.9029850746268657, Mean Entropy: 0.38691776990890503, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 171,  Mean reward: -1.5, Mean Entropy: 0.449125736951828, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 172,  Mean reward: -4.725, Mean Entropy: 0.5073778629302979, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 173,  Mean reward: -2.7586206896551726, Mean Entropy: 0.3932499885559082, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 174,  Mean reward: -2.7063492063492065, Mean Entropy: 0.37471717596054077, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 175,  Mean reward: -2.1076923076923078, Mean Entropy: 0.3661952614784241, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 176,  Mean reward: -4.825396825396825, Mean Entropy: 0.386424720287323, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 177,  Mean reward: -3.3515625, Mean Entropy: 0.39312076568603516, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 178,  Mean reward: -3.158333333333333, Mean Entropy: 0.39987868070602417, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 179,  Mean reward: -1.3174603174603174, Mean Entropy: 0.32544004917144775, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 180,  Mean reward: -2.1846153846153844, Mean Entropy: 0.313262403011322, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.81s
Iteration: 181,  Mean reward: -2.5681818181818183, Mean Entropy: 0.3106878995895386, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 182,  Mean reward: -7.0476190476190474, Mean Entropy: 0.34443411231040955, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 183,  Mean reward: -3.8015873015873014, Mean Entropy: 0.31662535667419434, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 184,  Mean reward: -1.40625, Mean Entropy: 0.3159068822860718, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 185,  Mean reward: -1.876923076923077, Mean Entropy: 0.2968876361846924, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 186,  Mean reward: -2.9047619047619047, Mean Entropy: 0.29677286744117737, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 187,  Mean reward: -2.6865671641791047, Mean Entropy: 0.31199321150779724, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 188,  Mean reward: -1.9191176470588236, Mean Entropy: 0.32731306552886963, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 189,  Mean reward: -1.9848484848484849, Mean Entropy: 0.31477656960487366, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 190,  Mean reward: -0.10606060606060606, Mean Entropy: 0.2541730999946594, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 191,  Mean reward: -3.6769230769230767, Mean Entropy: 0.25414639711380005, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 192,  Mean reward: -2.1846153846153844, Mean Entropy: 0.2645445466041565, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 193,  Mean reward: -2.7794117647058822, Mean Entropy: 0.2571743130683899, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 194,  Mean reward: -4.338461538461538, Mean Entropy: 0.26551562547683716, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 195,  Mean reward: -2.753846153846154, Mean Entropy: 0.25940024852752686, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 196,  Mean reward: -1.036764705882353, Mean Entropy: 0.23643314838409424, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 197,  Mean reward: -2.111940298507463, Mean Entropy: 0.24356713891029358, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 198,  Mean reward: -3.283582089552239, Mean Entropy: 0.25778400897979736, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 199,  Mean reward: -4.223076923076923, Mean Entropy: 0.2292497158050537, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 200,  Mean reward: -1.6136363636363635, Mean Entropy: 0.231766015291214, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.9921875, Mean Entropy: 0.22855231165885925, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 202,  Mean reward: -0.20422535211267606, Mean Entropy: 0.2324889600276947, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 203,  Mean reward: -3.2196969696969697, Mean Entropy: 0.2358156144618988, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 204,  Mean reward: -2.590909090909091, Mean Entropy: 0.24412411451339722, complete_episode_count: 66.0, Gather time: 0.68s, Train time: 0.71s
Iteration: 205,  Mean reward: -3.1742424242424243, Mean Entropy: 0.23733024299144745, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 206,  Mean reward: -0.6985294117647058, Mean Entropy: 0.19407813251018524, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 207,  Mean reward: -0.6323529411764706, Mean Entropy: 0.1888345330953598, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 208,  Mean reward: -0.6714285714285714, Mean Entropy: 0.20216479897499084, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 209,  Mean reward: -5.074626865671642, Mean Entropy: 0.24373243749141693, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 210,  Mean reward: -1.9166666666666667, Mean Entropy: 0.25624895095825195, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 211,  Mean reward: -3.4318181818181817, Mean Entropy: 0.29035454988479614, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 212,  Mean reward: -0.38235294117647056, Mean Entropy: 0.2740563750267029, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 213,  Mean reward: -4.7164179104477615, Mean Entropy: 0.325298547744751, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 214,  Mean reward: -2.7573529411764706, Mean Entropy: 0.3035562038421631, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 215,  Mean reward: -2.893939393939394, Mean Entropy: 0.29131758213043213, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 216,  Mean reward: -3.84375, Mean Entropy: 0.3373192250728607, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 217,  Mean reward: -2.074626865671642, Mean Entropy: 0.31997156143188477, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 218,  Mean reward: -2.4921875, Mean Entropy: 0.33668917417526245, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 219,  Mean reward: -4.926229508196721, Mean Entropy: 0.3382493257522583, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 220,  Mean reward: -2.6641791044776117, Mean Entropy: 0.36117810010910034, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 221,  Mean reward: -1.6470588235294117, Mean Entropy: 0.3240581154823303, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 222,  Mean reward: -2.146153846153846, Mean Entropy: 0.30954253673553467, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 223,  Mean reward: -1.036764705882353, Mean Entropy: 0.31437215209007263, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 224,  Mean reward: -4.166666666666667, Mean Entropy: 0.32895219326019287, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 225,  Mean reward: -2.0390625, Mean Entropy: 0.3065209686756134, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 226,  Mean reward: -4.5390625, Mean Entropy: 0.31072288751602173, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 227,  Mean reward: -2.507936507936508, Mean Entropy: 0.28518688678741455, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 228,  Mean reward: -2.4318181818181817, Mean Entropy: 0.2898356020450592, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 229,  Mean reward: -2.7153846153846155, Mean Entropy: 0.30478760600090027, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 230,  Mean reward: -2.0217391304347827, Mean Entropy: 0.342531681060791, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 231,  Mean reward: -1.1567164179104477, Mean Entropy: 0.3655167818069458, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 232,  Mean reward: -1.2352941176470589, Mean Entropy: 0.3273242712020874, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 233,  Mean reward: -2.4076923076923076, Mean Entropy: 0.29757750034332275, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 234,  Mean reward: -3.621212121212121, Mean Entropy: 0.29087013006210327, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 235,  Mean reward: -2.028985507246377, Mean Entropy: 0.28271248936653137, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 236,  Mean reward: 0.7430555555555556, Mean Entropy: 0.2758517861366272, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 237,  Mean reward: -2.9477611940298507, Mean Entropy: 0.2856960892677307, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 238,  Mean reward: -1.8088235294117647, Mean Entropy: 0.23708787560462952, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 239,  Mean reward: -1.3943661971830985, Mean Entropy: 0.21593694388866425, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 240,  Mean reward: -0.2391304347826087, Mean Entropy: 0.20165449380874634, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 241,  Mean reward: -1.8970588235294117, Mean Entropy: 0.24818055331707, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 242,  Mean reward: -1.5285714285714285, Mean Entropy: 0.20713993906974792, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 243,  Mean reward: -1.7462686567164178, Mean Entropy: 0.2324567437171936, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 244,  Mean reward: -2.376923076923077, Mean Entropy: 0.230515718460083, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 245,  Mean reward: -2.0, Mean Entropy: 0.2288753241300583, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 246,  Mean reward: -4.953846153846154, Mean Entropy: 0.28413426876068115, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 247,  Mean reward: -1.4029850746268657, Mean Entropy: 0.284699022769928, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 248,  Mean reward: -4.6692307692307695, Mean Entropy: 0.2734125256538391, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 249,  Mean reward: -2.463235294117647, Mean Entropy: 0.24615630507469177, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 250,  Mean reward: -5.015151515151516, Mean Entropy: 0.2902856767177582, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 251,  Mean reward: 0.22857142857142856, Mean Entropy: 0.21474556624889374, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 252,  Mean reward: -2.111940298507463, Mean Entropy: 0.21292728185653687, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 253,  Mean reward: -2.146153846153846, Mean Entropy: 0.24481341242790222, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 254,  Mean reward: -1.5588235294117647, Mean Entropy: 0.25115150213241577, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 255,  Mean reward: -2.985074626865672, Mean Entropy: 0.23372501134872437, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 256,  Mean reward: -4.141791044776119, Mean Entropy: 0.2513236403465271, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 257,  Mean reward: -5.142857142857143, Mean Entropy: 0.28883546590805054, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 258,  Mean reward: 0.4632352941176471, Mean Entropy: 0.1849028319120407, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 259,  Mean reward: -2.343283582089552, Mean Entropy: 0.20134538412094116, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 260,  Mean reward: -2.985074626865672, Mean Entropy: 0.21187824010849, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 261,  Mean reward: 0.3380281690140845, Mean Entropy: 0.1959218829870224, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 262,  Mean reward: -2.007575757575758, Mean Entropy: 0.19563916325569153, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 263,  Mean reward: -4.17910447761194, Mean Entropy: 0.21164199709892273, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 264,  Mean reward: -4.432835820895522, Mean Entropy: 0.24016474187374115, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 265,  Mean reward: -2.463235294117647, Mean Entropy: 0.18475958704948425, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 266,  Mean reward: -2.3656716417910446, Mean Entropy: 0.20446845889091492, complete_episode_count: 67.0, Gather time: 0.71s, Train time: 0.74s
Iteration: 267,  Mean reward: -3.757575757575758, Mean Entropy: 0.22324639558792114, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 268,  Mean reward: -1.4583333333333333, Mean Entropy: 0.1654779464006424, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 269,  Mean reward: -2.1691176470588234, Mean Entropy: 0.1749231219291687, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 270,  Mean reward: -2.536231884057971, Mean Entropy: 0.19379016757011414, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 271,  Mean reward: -2.0671641791044775, Mean Entropy: 0.16683459281921387, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 272,  Mean reward: -1.108695652173913, Mean Entropy: 0.17236672341823578, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 273,  Mean reward: -2.5579710144927534, Mean Entropy: 0.17517173290252686, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 274,  Mean reward: -2.753846153846154, Mean Entropy: 0.15160799026489258, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 275,  Mean reward: -4.798507462686567, Mean Entropy: 0.1966935694217682, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 276,  Mean reward: -1.6666666666666667, Mean Entropy: 0.1791868805885315, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 277,  Mean reward: -4.455223880597015, Mean Entropy: 0.21493487060070038, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 278,  Mean reward: -2.579710144927536, Mean Entropy: 0.18091416358947754, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 279,  Mean reward: -4.0, Mean Entropy: 0.19581077992916107, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 280,  Mean reward: -1.5588235294117647, Mean Entropy: 0.21362122893333435, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 281,  Mean reward: -1.2647058823529411, Mean Entropy: 0.19248250126838684, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 282,  Mean reward: -1.6884057971014492, Mean Entropy: 0.17923998832702637, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 283,  Mean reward: -0.32142857142857145, Mean Entropy: 0.17341703176498413, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 284,  Mean reward: -0.8819444444444444, Mean Entropy: 0.180074542760849, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 285,  Mean reward: -2.343283582089552, Mean Entropy: 0.19588826596736908, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 286,  Mean reward: -1.3309859154929577, Mean Entropy: 0.23382523655891418, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 287,  Mean reward: -1.8142857142857143, Mean Entropy: 0.1921931505203247, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 288,  Mean reward: -1.6338028169014085, Mean Entropy: 0.19411200284957886, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 289,  Mean reward: -1.108695652173913, Mean Entropy: 0.19213199615478516, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 290,  Mean reward: 0.9577464788732394, Mean Entropy: 0.18091562390327454, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 291,  Mean reward: -3.063380281690141, Mean Entropy: 0.21725386381149292, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 292,  Mean reward: -2.176056338028169, Mean Entropy: 0.19347012042999268, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 293,  Mean reward: -4.962686567164179, Mean Entropy: 0.254616379737854, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 294,  Mean reward: -3.1940298507462686, Mean Entropy: 0.26198315620422363, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 295,  Mean reward: -2.242424242424242, Mean Entropy: 0.2367517352104187, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 296,  Mean reward: 0.07746478873239436, Mean Entropy: 0.20509976148605347, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 297,  Mean reward: -2.463235294117647, Mean Entropy: 0.2187015265226364, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 298,  Mean reward: -2.5579710144927534, Mean Entropy: 0.23061439394950867, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 299,  Mean reward: -2.289855072463768, Mean Entropy: 0.20796403288841248, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 300,  Mean reward: -0.8405797101449275, Mean Entropy: 0.16988533735275269, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -2.448529411764706, Mean Entropy: 0.19296006858348846, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 302,  Mean reward: -2.65, Mean Entropy: 0.19793960452079773, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 303,  Mean reward: -1.6884057971014492, Mean Entropy: 0.17730748653411865, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 304,  Mean reward: -1.3985507246376812, Mean Entropy: 0.17775362730026245, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 305,  Mean reward: -2.0, Mean Entropy: 0.1488913893699646, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 306,  Mean reward: -3.0422535211267605, Mean Entropy: 0.1880660504102707, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 307,  Mean reward: -4.156716417910448, Mean Entropy: 0.19972315430641174, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 308,  Mean reward: -2.914285714285714, Mean Entropy: 0.1958831250667572, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 309,  Mean reward: -2.962686567164179, Mean Entropy: 0.22584949433803558, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 310,  Mean reward: -0.22535211267605634, Mean Entropy: 0.19457559287548065, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 311,  Mean reward: -3.582089552238806, Mean Entropy: 0.21508827805519104, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 312,  Mean reward: -4.083333333333333, Mean Entropy: 0.20480191707611084, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 313,  Mean reward: -1.5285714285714285, Mean Entropy: 0.2182459533214569, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 314,  Mean reward: -3.0955882352941178, Mean Entropy: 0.1776564121246338, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 315,  Mean reward: -2.242424242424242, Mean Entropy: 0.16040340065956116, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 316,  Mean reward: -3.471014492753623, Mean Entropy: 0.20076286792755127, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 317,  Mean reward: -2.1911764705882355, Mean Entropy: 0.19526229798793793, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 318,  Mean reward: -2.713235294117647, Mean Entropy: 0.20785963535308838, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 319,  Mean reward: -3.301470588235294, Mean Entropy: 0.18374310433864594, complete_episode_count: 68.0, Gather time: 0.68s, Train time: 0.78s
Iteration: 320,  Mean reward: -2.8260869565217392, Mean Entropy: 0.18586182594299316, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 321,  Mean reward: -4.364285714285714, Mean Entropy: 0.23094725608825684, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 322,  Mean reward: -0.9485294117647058, Mean Entropy: 0.1619952768087387, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 323,  Mean reward: -2.0, Mean Entropy: 0.20298731327056885, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 324,  Mean reward: 0.4714285714285714, Mean Entropy: 0.16507776081562042, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 325,  Mean reward: -3.063380281690141, Mean Entropy: 0.19361647963523865, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 326,  Mean reward: -5.434782608695652, Mean Entropy: 0.23644396662712097, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 327,  Mean reward: -0.9926470588235294, Mean Entropy: 0.19320160150527954, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 328,  Mean reward: -3.477272727272727, Mean Entropy: 0.1781398057937622, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 329,  Mean reward: -1.4202898550724639, Mean Entropy: 0.17183856666088104, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 330,  Mean reward: -4.169014084507042, Mean Entropy: 0.22951683402061462, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 331,  Mean reward: -4.385714285714286, Mean Entropy: 0.22369639575481415, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 332,  Mean reward: -2.962686567164179, Mean Entropy: 0.2011656016111374, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 333,  Mean reward: -0.21739130434782608, Mean Entropy: 0.13296662271022797, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 334,  Mean reward: -0.6666666666666666, Mean Entropy: 0.1443227082490921, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 335,  Mean reward: -3.7803030303030303, Mean Entropy: 0.16614869236946106, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 336,  Mean reward: -0.65, Mean Entropy: 0.15664884448051453, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 337,  Mean reward: -3.1458333333333335, Mean Entropy: 0.19473472237586975, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 338,  Mean reward: -1.6338028169014085, Mean Entropy: 0.14733397960662842, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 339,  Mean reward: -1.3309859154929577, Mean Entropy: 0.16983121633529663, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 340,  Mean reward: -4.253623188405797, Mean Entropy: 0.18093952536582947, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 341,  Mean reward: -4.5, Mean Entropy: 0.22232608497142792, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 342,  Mean reward: -0.9485294117647058, Mean Entropy: 0.16180866956710815, complete_episode_count: 68.0, Gather time: 0.62s, Train time: 0.77s
Iteration: 343,  Mean reward: -2.8260869565217392, Mean Entropy: 0.1750727891921997, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 344,  Mean reward: -0.65, Mean Entropy: 0.16367825865745544, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 345,  Mean reward: -1.792857142857143, Mean Entropy: 0.1751805543899536, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 346,  Mean reward: -0.8188405797101449, Mean Entropy: 0.12981678545475006, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 347,  Mean reward: -2.0, Mean Entropy: 0.14080092310905457, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 348,  Mean reward: -0.65, Mean Entropy: 0.17331402003765106, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 349,  Mean reward: -4.028985507246377, Mean Entropy: 0.17938904464244843, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 350,  Mean reward: -2.7573529411764706, Mean Entropy: 0.17473602294921875, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 351,  Mean reward: -4.297101449275362, Mean Entropy: 0.20662996172904968, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 352,  Mean reward: -1.0704225352112675, Mean Entropy: 0.1545901894569397, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 353,  Mean reward: -0.05714285714285714, Mean Entropy: 0.1016196459531784, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 354,  Mean reward: -3.6739130434782608, Mean Entropy: 0.18189465999603271, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 355,  Mean reward: -3.1159420289855073, Mean Entropy: 0.2058989405632019, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 356,  Mean reward: -2.869565217391304, Mean Entropy: 0.14295867085456848, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 357,  Mean reward: -2.246376811594203, Mean Entropy: 0.1581663340330124, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 358,  Mean reward: -1.108695652173913, Mean Entropy: 0.16056697070598602, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 359,  Mean reward: -2.801470588235294, Mean Entropy: 0.11102627962827682, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 360,  Mean reward: -3.4318181818181817, Mean Entropy: 0.17710348963737488, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 361,  Mean reward: -1.4791666666666667, Mean Entropy: 0.13510912656784058, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 362,  Mean reward: -1.8357142857142856, Mean Entropy: 0.1642172634601593, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 363,  Mean reward: -1.1388888888888888, Mean Entropy: 0.15507517755031586, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 364,  Mean reward: -1.8150684931506849, Mean Entropy: 0.16142316162586212, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 365,  Mean reward: -2.5069444444444446, Mean Entropy: 0.1813828945159912, complete_episode_count: 72.0, Gather time: 0.64s, Train time: 0.78s
Iteration: 366,  Mean reward: -1.1126760563380282, Mean Entropy: 0.14722000062465668, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 367,  Mean reward: -1.3732394366197183, Mean Entropy: 0.18208497762680054, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 368,  Mean reward: -0.05714285714285714, Mean Entropy: 0.14604949951171875, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 369,  Mean reward: -1.8767123287671232, Mean Entropy: 0.15135115385055542, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 370,  Mean reward: 0.22916666666666666, Mean Entropy: 0.12353970855474472, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 371,  Mean reward: -0.7464788732394366, Mean Entropy: 0.12222372740507126, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 372,  Mean reward: -2.65, Mean Entropy: 0.17094098031520844, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 373,  Mean reward: -3.639705882352941, Mean Entropy: 0.18187235295772552, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 374,  Mean reward: -1.9782608695652173, Mean Entropy: 0.16493044793605804, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 375,  Mean reward: -3.0073529411764706, Mean Entropy: 0.1499728560447693, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 376,  Mean reward: -2.5073529411764706, Mean Entropy: 0.137259840965271, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 377,  Mean reward: -3.7928571428571427, Mean Entropy: 0.16846631467342377, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 378,  Mean reward: -2.76056338028169, Mean Entropy: 0.19197624921798706, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 379,  Mean reward: -1.2205882352941178, Mean Entropy: 0.1580786406993866, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 380,  Mean reward: -2.891304347826087, Mean Entropy: 0.17202851176261902, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 381,  Mean reward: -4.385714285714286, Mean Entropy: 0.2081535905599594, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.80s
Iteration: 382,  Mean reward: -2.1, Mean Entropy: 0.1599285900592804, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 383,  Mean reward: -3.507142857142857, Mean Entropy: 0.1787990778684616, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 384,  Mean reward: -4.190140845070423, Mean Entropy: 0.2061598300933838, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 385,  Mean reward: -1.3309859154929577, Mean Entropy: 0.134298175573349, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 386,  Mean reward: -1.1418918918918919, Mean Entropy: 0.17207607626914978, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 387,  Mean reward: -0.7887323943661971, Mean Entropy: 0.1216590628027916, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 388,  Mean reward: -1.6014492753623188, Mean Entropy: 0.14129464328289032, complete_episode_count: 69.0, Gather time: 0.68s, Train time: 0.74s
Iteration: 389,  Mean reward: -2.6285714285714286, Mean Entropy: 0.18161320686340332, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 390,  Mean reward: -3.9558823529411766, Mean Entropy: 0.16185683012008667, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 391,  Mean reward: -3.5, Mean Entropy: 0.19492030143737793, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 392,  Mean reward: -0.20422535211267606, Mean Entropy: 0.16147983074188232, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 393,  Mean reward: -5.029850746268656, Mean Entropy: 0.20423220098018646, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 394,  Mean reward: -1.2876712328767124, Mean Entropy: 0.14131826162338257, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 395,  Mean reward: -1.4857142857142858, Mean Entropy: 0.13299156725406647, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 396,  Mean reward: -4.5661764705882355, Mean Entropy: 0.18256697058677673, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 397,  Mean reward: -2.5579710144927534, Mean Entropy: 0.18015122413635254, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 398,  Mean reward: -2.407142857142857, Mean Entropy: 0.16655981540679932, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 399,  Mean reward: -1.8142857142857143, Mean Entropy: 0.15473198890686035, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 400,  Mean reward: 0.25, Mean Entropy: 0.1454373300075531, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -3.2214285714285715, Mean Entropy: 0.17458489537239075, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 402,  Mean reward: -3.063380281690141, Mean Entropy: 0.20424310863018036, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 403,  Mean reward: -1.1597222222222223, Mean Entropy: 0.13397470116615295, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 404,  Mean reward: -2.6641791044776117, Mean Entropy: 0.15458464622497559, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 405,  Mean reward: -1.4791666666666667, Mean Entropy: 0.1250842809677124, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 406,  Mean reward: -2.9571428571428573, Mean Entropy: 0.2086828351020813, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 407,  Mean reward: -1.0, Mean Entropy: 0.11173275113105774, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 408,  Mean reward: -2.176056338028169, Mean Entropy: 0.17300957441329956, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 409,  Mean reward: -3.0294117647058822, Mean Entropy: 0.15957540273666382, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 410,  Mean reward: -1.6944444444444444, Mean Entropy: 0.15922173857688904, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 411,  Mean reward: -0.65, Mean Entropy: 0.12446632981300354, complete_episode_count: 70.0, Gather time: 0.68s, Train time: 0.72s
Iteration: 412,  Mean reward: -0.9236111111111112, Mean Entropy: 0.14365679025650024, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 413,  Mean reward: -0.9931506849315068, Mean Entropy: 0.16163666546344757, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 414,  Mean reward: -2.3857142857142857, Mean Entropy: 0.14792977273464203, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 415,  Mean reward: -1.5205479452054795, Mean Entropy: 0.15499809384346008, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 416,  Mean reward: -1.4583333333333333, Mean Entropy: 0.12726733088493347, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 417,  Mean reward: -2.246376811594203, Mean Entropy: 0.14231795072555542, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 418,  Mean reward: -4.227941176470588, Mean Entropy: 0.16425272822380066, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 419,  Mean reward: -1.2876712328767124, Mean Entropy: 0.12746433913707733, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.79s
Iteration: 420,  Mean reward: -3.323529411764706, Mean Entropy: 0.15010957419872284, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 421,  Mean reward: -3.1594202898550723, Mean Entropy: 0.15652897953987122, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 422,  Mean reward: -0.18, Mean Entropy: 0.1347983181476593, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 423,  Mean reward: -1.6666666666666667, Mean Entropy: 0.1634645313024521, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 424,  Mean reward: -2.3857142857142857, Mean Entropy: 0.15150848031044006, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 425,  Mean reward: -2.6285714285714286, Mean Entropy: 0.13134759664535522, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 426,  Mean reward: -1.9366197183098592, Mean Entropy: 0.16454273462295532, complete_episode_count: 71.0, Gather time: 0.71s, Train time: 0.73s
Iteration: 427,  Mean reward: -2.407142857142857, Mean Entropy: 0.16491085290908813, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 428,  Mean reward: -1.2642857142857142, Mean Entropy: 0.1275794357061386, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 429,  Mean reward: -1.7714285714285714, Mean Entropy: 0.10534341633319855, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 430,  Mean reward: -3.5285714285714285, Mean Entropy: 0.14992010593414307, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 431,  Mean reward: -1.352112676056338, Mean Entropy: 0.15699033439159393, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 432,  Mean reward: -2.0785714285714287, Mean Entropy: 0.148329958319664, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 433,  Mean reward: -1.6549295774647887, Mean Entropy: 0.12739846110343933, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 434,  Mean reward: -1.207142857142857, Mean Entropy: 0.1439666599035263, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 435,  Mean reward: -4.028985507246377, Mean Entropy: 0.1490178406238556, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 436,  Mean reward: -4.450704225352113, Mean Entropy: 0.19070777297019958, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 437,  Mean reward: -1.8732394366197183, Mean Entropy: 0.13307829201221466, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 438,  Mean reward: -2.142857142857143, Mean Entropy: 0.12726590037345886, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 439,  Mean reward: -3.125, Mean Entropy: 0.14952822029590607, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 440,  Mean reward: -1.591549295774648, Mean Entropy: 0.1421370804309845, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 441,  Mean reward: -4.105633802816901, Mean Entropy: 0.1853821575641632, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 442,  Mean reward: -0.6986301369863014, Mean Entropy: 0.11594676971435547, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 443,  Mean reward: -2.1971830985915495, Mean Entropy: 0.17061655223369598, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 444,  Mean reward: -2.9130434782608696, Mean Entropy: 0.13619041442871094, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 445,  Mean reward: -3.887323943661972, Mean Entropy: 0.15153323113918304, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 446,  Mean reward: -4.671428571428572, Mean Entropy: 0.19744062423706055, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 447,  Mean reward: -1.8142857142857143, Mean Entropy: 0.1264616847038269, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 448,  Mean reward: -2.6986301369863015, Mean Entropy: 0.16828833520412445, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 449,  Mean reward: -3.2642857142857142, Mean Entropy: 0.13384133577346802, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 450,  Mean reward: -1.7945205479452055, Mean Entropy: 0.15429484844207764, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 451,  Mean reward: -2.5579710144927534, Mean Entropy: 0.16740266978740692, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 452,  Mean reward: -3.5285714285714285, Mean Entropy: 0.18385742604732513, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 453,  Mean reward: -1.2671232876712328, Mean Entropy: 0.1367429941892624, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 454,  Mean reward: -3.2794117647058822, Mean Entropy: 0.157962828874588, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 455,  Mean reward: -1.7152777777777777, Mean Entropy: 0.13052134215831757, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 456,  Mean reward: -2.2916666666666665, Mean Entropy: 0.16378967463970184, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 457,  Mean reward: -1.0342465753424657, Mean Entropy: 0.12686675786972046, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 458,  Mean reward: -2.3835616438356166, Mean Entropy: 0.13487103581428528, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 459,  Mean reward: 0.11971830985915492, Mean Entropy: 0.12077280879020691, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 460,  Mean reward: -5.274647887323944, Mean Entropy: 0.21220138669013977, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 461,  Mean reward: -2.013888888888889, Mean Entropy: 0.1512432098388672, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 462,  Mean reward: -2.9315068493150687, Mean Entropy: 0.1677521914243698, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 463,  Mean reward: -3.3455882352941178, Mean Entropy: 0.15022675693035126, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 464,  Mean reward: -2.6714285714285713, Mean Entropy: 0.1605146825313568, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 465,  Mean reward: -0.3680555555555556, Mean Entropy: 0.15895919501781464, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 466,  Mean reward: -2.2708333333333335, Mean Entropy: 0.17020061612129211, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 467,  Mean reward: -2.5694444444444446, Mean Entropy: 0.17741064727306366, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 468,  Mean reward: -1.5616438356164384, Mean Entropy: 0.16406545042991638, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 469,  Mean reward: -1.8356164383561644, Mean Entropy: 0.14968863129615784, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 470,  Mean reward: -0.8819444444444444, Mean Entropy: 0.16614793241024017, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 471,  Mean reward: -1.9930555555555556, Mean Entropy: 0.1409396231174469, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 472,  Mean reward: -0.6714285714285714, Mean Entropy: 0.13960778713226318, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 473,  Mean reward: -2.1, Mean Entropy: 0.15673470497131348, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 474,  Mean reward: -3.889705882352941, Mean Entropy: 0.17198339104652405, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 475,  Mean reward: -3.216417910447761, Mean Entropy: 0.16376090049743652, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 476,  Mean reward: -2.1971830985915495, Mean Entropy: 0.14662975072860718, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 477,  Mean reward: 0.050724637681159424, Mean Entropy: 0.15529370307922363, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 478,  Mean reward: -2.536231884057971, Mean Entropy: 0.16743570566177368, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 479,  Mean reward: -1.6338028169014085, Mean Entropy: 0.16140490770339966, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 480,  Mean reward: -2.536231884057971, Mean Entropy: 0.16697357594966888, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 481,  Mean reward: -0.6666666666666666, Mean Entropy: 0.12928207218647003, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 482,  Mean reward: -2.1971830985915495, Mean Entropy: 0.17797458171844482, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 483,  Mean reward: -0.6041666666666666, Mean Entropy: 0.1802489459514618, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 484,  Mean reward: -1.7152777777777777, Mean Entropy: 0.20594948530197144, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 485,  Mean reward: -3.449275362318841, Mean Entropy: 0.171145960688591, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 486,  Mean reward: -4.385714285714286, Mean Entropy: 0.1800861805677414, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 487,  Mean reward: -2.7816901408450705, Mean Entropy: 0.15102747082710266, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 488,  Mean reward: -4.27536231884058, Mean Entropy: 0.18906156718730927, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 489,  Mean reward: -3.2054794520547945, Mean Entropy: 0.20145241916179657, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 490,  Mean reward: -5.188405797101449, Mean Entropy: 0.1556454449892044, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 491,  Mean reward: 0.4594594594594595, Mean Entropy: 0.1063435822725296, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 492,  Mean reward: -2.7816901408450705, Mean Entropy: 0.1622186005115509, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 493,  Mean reward: -4.227941176470588, Mean Entropy: 0.17437027394771576, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 494,  Mean reward: -1.2671232876712328, Mean Entropy: 0.14128585159778595, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 495,  Mean reward: -2.579710144927536, Mean Entropy: 0.11665185540914536, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 496,  Mean reward: -2.6986301369863015, Mean Entropy: 0.17544904351234436, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 497,  Mean reward: -1.7152777777777777, Mean Entropy: 0.16050049662590027, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 498,  Mean reward: -1.0342465753424657, Mean Entropy: 0.12653517723083496, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 499,  Mean reward: -2.3835616438356166, Mean Entropy: 0.16930155456066132, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 500,  Mean reward: -2.1095890410958904, Mean Entropy: 0.2018974870443344, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -3.507142857142857, Mean Entropy: 0.19065001606941223, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 502,  Mean reward: -2.73943661971831, Mean Entropy: 0.17707619071006775, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 503,  Mean reward: -1.8732394366197183, Mean Entropy: 0.1874934881925583, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 504,  Mean reward: -2.826388888888889, Mean Entropy: 0.17465895414352417, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 505,  Mean reward: -1.5410958904109588, Mean Entropy: 0.1925562024116516, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.87s
Iteration: 506,  Mean reward: -3.908450704225352, Mean Entropy: 0.20761315524578094, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 507,  Mean reward: -2.047945205479452, Mean Entropy: 0.16971978545188904, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 508,  Mean reward: -0.7602739726027398, Mean Entropy: 0.15836910903453827, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 509,  Mean reward: -1.591549295774648, Mean Entropy: 0.17431753873825073, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 510,  Mean reward: -3.6956521739130435, Mean Entropy: 0.21700017154216766, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 511,  Mean reward: -1.9527027027027026, Mean Entropy: 0.19493262469768524, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 512,  Mean reward: -3.9583333333333335, Mean Entropy: 0.21435610949993134, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 513,  Mean reward: -3.125, Mean Entropy: 0.18472424149513245, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 514,  Mean reward: -2.952054794520548, Mean Entropy: 0.1659069061279297, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 515,  Mean reward: -3.661764705882353, Mean Entropy: 0.1849239319562912, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 516,  Mean reward: -2.952054794520548, Mean Entropy: 0.15517914295196533, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 517,  Mean reward: -3.6838235294117645, Mean Entropy: 0.16510944068431854, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 518,  Mean reward: -1.1597222222222223, Mean Entropy: 0.13926249742507935, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 519,  Mean reward: -2.1470588235294117, Mean Entropy: 0.1281719207763672, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 520,  Mean reward: -1.0136986301369864, Mean Entropy: 0.14300480484962463, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 521,  Mean reward: -2.8714285714285714, Mean Entropy: 0.1817459762096405, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 522,  Mean reward: -0.7464788732394366, Mean Entropy: 0.16848048567771912, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 523,  Mean reward: -0.7397260273972602, Mean Entropy: 0.17131057381629944, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 524,  Mean reward: -1.6884057971014492, Mean Entropy: 0.1989859938621521, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 525,  Mean reward: -2.536231884057971, Mean Entropy: 0.17454561591148376, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 526,  Mean reward: -2.210144927536232, Mean Entropy: 0.1859918087720871, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 527,  Mean reward: -1.4583333333333333, Mean Entropy: 0.2431105077266693, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 528,  Mean reward: -2.3125, Mean Entropy: 0.23781085014343262, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 529,  Mean reward: -3.4130434782608696, Mean Entropy: 0.22430050373077393, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 530,  Mean reward: -2.296875, Mean Entropy: 0.1488618552684784, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 531,  Mean reward: -2.786764705882353, Mean Entropy: 0.15947386622428894, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 532,  Mean reward: -2.9571428571428573, Mean Entropy: 0.20568645000457764, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 533,  Mean reward: -3.7714285714285714, Mean Entropy: 0.2011396586894989, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 534,  Mean reward: -4.920289855072464, Mean Entropy: 0.1871609091758728, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 535,  Mean reward: -2.4577464788732395, Mean Entropy: 0.21651098132133484, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 536,  Mean reward: -1.8356164383561644, Mean Entropy: 0.22445279359817505, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 537,  Mean reward: -0.1506849315068493, Mean Entropy: 0.21317057311534882, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 538,  Mean reward: -2.2916666666666665, Mean Entropy: 0.20826727151870728, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 539,  Mean reward: -3.0422535211267605, Mean Entropy: 0.1892026662826538, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 540,  Mean reward: -2.7847222222222223, Mean Entropy: 0.20559895038604736, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 541,  Mean reward: -2.595890410958904, Mean Entropy: 0.2098228931427002, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 542,  Mean reward: -2.436619718309859, Mean Entropy: 0.21878425776958466, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 543,  Mean reward: -0.9027777777777778, Mean Entropy: 0.21110045909881592, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 544,  Mean reward: -2.6231884057971016, Mean Entropy: 0.20755910873413086, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 545,  Mean reward: -0.7676056338028169, Mean Entropy: 0.23120179772377014, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 546,  Mean reward: 1.4142857142857144, Mean Entropy: 0.1965460479259491, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 547,  Mean reward: -2.698529411764706, Mean Entropy: 0.1756807565689087, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 548,  Mean reward: -2.183098591549296, Mean Entropy: 0.18593236804008484, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 549,  Mean reward: -2.1549295774647885, Mean Entropy: 0.2051571011543274, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 550,  Mean reward: -4.450704225352113, Mean Entropy: 0.21856479346752167, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 551,  Mean reward: -4.544117647058823, Mean Entropy: 0.1843254417181015, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 552,  Mean reward: -0.5833333333333334, Mean Entropy: 0.17477107048034668, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 553,  Mean reward: -1.4375, Mean Entropy: 0.15419664978981018, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 554,  Mean reward: -2.8472222222222223, Mean Entropy: 0.1888425052165985, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 555,  Mean reward: -2.222972972972973, Mean Entropy: 0.2049252688884735, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 556,  Mean reward: -1.2876712328767124, Mean Entropy: 0.18119731545448303, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 557,  Mean reward: -1.7361111111111112, Mean Entropy: 0.18223288655281067, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 558,  Mean reward: -4.432835820895522, Mean Entropy: 0.19287189841270447, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 559,  Mean reward: -2.7816901408450705, Mean Entropy: 0.18498116731643677, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 560,  Mean reward: -2.0347222222222223, Mean Entropy: 0.19690822064876556, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 561,  Mean reward: -3.9166666666666665, Mean Entropy: 0.21612457931041718, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 562,  Mean reward: -3.2816901408450705, Mean Entropy: 0.19837622344493866, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 563,  Mean reward: -2.0785714285714287, Mean Entropy: 0.20743779838085175, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 564,  Mean reward: -3.591549295774648, Mean Entropy: 0.2813935875892639, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 565,  Mean reward: -2.7794117647058822, Mean Entropy: 0.26397132873535156, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 566,  Mean reward: -1.028169014084507, Mean Entropy: 0.2512933909893036, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 567,  Mean reward: -1.9642857142857142, Mean Entropy: 0.2271559238433838, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 568,  Mean reward: -2.869565217391304, Mean Entropy: 0.19422218203544617, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 569,  Mean reward: -4.121428571428571, Mean Entropy: 0.21444933116436005, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 570,  Mean reward: -4.1, Mean Entropy: 0.2148052453994751, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 571,  Mean reward: -2.735294117647059, Mean Entropy: 0.1844627857208252, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 572,  Mean reward: -1.6126760563380282, Mean Entropy: 0.16712674498558044, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 573,  Mean reward: -3.063380281690141, Mean Entropy: 0.15348093211650848, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 574,  Mean reward: -1.8150684931506849, Mean Entropy: 0.1774752140045166, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 575,  Mean reward: -2.3835616438356166, Mean Entropy: 0.15531092882156372, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 576,  Mean reward: -3.8142857142857145, Mean Entropy: 0.18550479412078857, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 577,  Mean reward: -1.3732394366197183, Mean Entropy: 0.18197321891784668, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 578,  Mean reward: -3.4794520547945207, Mean Entropy: 0.17854216694831848, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 579,  Mean reward: -1.3309859154929577, Mean Entropy: 0.12863610684871674, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 580,  Mean reward: -1.5616438356164384, Mean Entropy: 0.1757570207118988, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 581,  Mean reward: -3.824324324324324, Mean Entropy: 0.16514013707637787, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 582,  Mean reward: -3.5704225352112675, Mean Entropy: 0.16701345145702362, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 583,  Mean reward: -3.732876712328767, Mean Entropy: 0.17133265733718872, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 584,  Mean reward: -2.8055555555555554, Mean Entropy: 0.18768803775310516, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 585,  Mean reward: -2.548611111111111, Mean Entropy: 0.13403929769992828, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 586,  Mean reward: -4.091549295774648, Mean Entropy: 0.18030290305614471, complete_episode_count: 71.0, Gather time: 0.71s, Train time: 0.74s
Iteration: 587,  Mean reward: -2.5902777777777777, Mean Entropy: 0.12442102283239365, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 588,  Mean reward: -1.6216216216216217, Mean Entropy: 0.11473836749792099, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 589,  Mean reward: -1.6418918918918919, Mean Entropy: 0.1533687561750412, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 590,  Mean reward: -1.1597222222222223, Mean Entropy: 0.10816846787929535, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 591,  Mean reward: -0.7133333333333334, Mean Entropy: 0.12746921181678772, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 592,  Mean reward: -3.423611111111111, Mean Entropy: 0.15412157773971558, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 593,  Mean reward: -1.3310810810810811, Mean Entropy: 0.13842108845710754, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 594,  Mean reward: -1.5205479452054795, Mean Entropy: 0.12580262124538422, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 595,  Mean reward: -2.0142857142857142, Mean Entropy: 0.11962132155895233, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 596,  Mean reward: 0.14383561643835616, Mean Entropy: 0.1244584172964096, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 597,  Mean reward: -3.26056338028169, Mean Entropy: 0.13116373121738434, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 598,  Mean reward: -1.7945205479452055, Mean Entropy: 0.1256093978881836, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 599,  Mean reward: -1.2465753424657535, Mean Entropy: 0.11989229917526245, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 600,  Mean reward: -1.0810810810810811, Mean Entropy: 0.16436295211315155, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -0.5405405405405406, Mean Entropy: 0.14805006980895996, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 602,  Mean reward: -4.429577464788732, Mean Entropy: 0.1325169801712036, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 603,  Mean reward: -2.1824324324324325, Mean Entropy: 0.16160443425178528, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 604,  Mean reward: -3.3028169014084505, Mean Entropy: 0.13053679466247559, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 605,  Mean reward: -2.952054794520548, Mean Entropy: 0.14969833195209503, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 606,  Mean reward: -3.184931506849315, Mean Entropy: 0.16000880300998688, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 607,  Mean reward: -3.3402777777777777, Mean Entropy: 0.1593109369277954, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 608,  Mean reward: -1.912162162162162, Mean Entropy: 0.15204095840454102, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 609,  Mean reward: -1.5410958904109588, Mean Entropy: 0.14505070447921753, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 610,  Mean reward: -1.3918918918918919, Mean Entropy: 0.16423572599887848, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 611,  Mean reward: -2.8666666666666667, Mean Entropy: 0.1781986951828003, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 612,  Mean reward: -1.9324324324324325, Mean Entropy: 0.14630089700222015, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 613,  Mean reward: -2.25, Mean Entropy: 0.14331750571727753, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 614,  Mean reward: -1.9722222222222223, Mean Entropy: 0.15340706706047058, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 615,  Mean reward: -3.8661971830985915, Mean Entropy: 0.1749286949634552, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 616,  Mean reward: -2.1301369863013697, Mean Entropy: 0.1741231381893158, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 617,  Mean reward: -1.2466666666666666, Mean Entropy: 0.17732948064804077, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 618,  Mean reward: -1.9527027027027026, Mean Entropy: 0.15064671635627747, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 619,  Mean reward: 0.10666666666666667, Mean Entropy: 0.14799268543720245, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 620,  Mean reward: -2.7635135135135136, Mean Entropy: 0.2073712944984436, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 621,  Mean reward: -1.2671232876712328, Mean Entropy: 0.16582125425338745, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 622,  Mean reward: -3.125, Mean Entropy: 0.18557259440422058, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -6.869047619047619, Mean Entropy: 0.9097556471824646, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.60s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -3.3, Mean Entropy: 0.9169758558273315, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -3.0, Mean Entropy: 0.9314146041870117, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 3,  Mean reward: -3.7738095238095237, Mean Entropy: 0.9458548426628113, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 4,  Mean reward: -6.902439024390244, Mean Entropy: 0.9169687032699585, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 5,  Mean reward: -3.2906976744186047, Mean Entropy: 0.9097459316253662, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 6,  Mean reward: -4.384615384615385, Mean Entropy: 0.9458159804344177, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 7,  Mean reward: -5.428571428571429, Mean Entropy: 0.9168431162834167, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 8,  Mean reward: -2.5930232558139537, Mean Entropy: 0.9962462782859802, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 9,  Mean reward: -6.475, Mean Entropy: 0.9312252402305603, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 10,  Mean reward: -3.4204545454545454, Mean Entropy: 0.9022808074951172, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 11,  Mean reward: -4.666666666666667, Mean Entropy: 0.9672080874443054, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 12,  Mean reward: -6.304878048780488, Mean Entropy: 0.8875492811203003, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 13,  Mean reward: -6.155555555555556, Mean Entropy: 0.9665888547897339, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 14,  Mean reward: -3.6704545454545454, Mean Entropy: 0.9521147608757019, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 15,  Mean reward: -5.023809523809524, Mean Entropy: 0.8727512359619141, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 16,  Mean reward: -3.9302325581395348, Mean Entropy: 0.9884657859802246, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 17,  Mean reward: -5.384615384615385, Mean Entropy: 1.023992896080017, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 18,  Mean reward: -2.6555555555555554, Mean Entropy: 0.9370555281639099, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 19,  Mean reward: -6.976744186046512, Mean Entropy: 1.0222738981246948, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 20,  Mean reward: -3.784090909090909, Mean Entropy: 0.9649922847747803, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 21,  Mean reward: -4.256410256410256, Mean Entropy: 0.9581934213638306, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 22,  Mean reward: -4.166666666666667, Mean Entropy: 0.9574486613273621, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.44s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 23,  Mean reward: -1.6777777777777778, Mean Entropy: 0.9801391363143921, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 24,  Mean reward: -4.5476190476190474, Mean Entropy: 0.9568578600883484, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 25,  Mean reward: -5.05, Mean Entropy: 0.9612940549850464, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 26,  Mean reward: -3.707317073170732, Mean Entropy: 0.9194297790527344, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 27,  Mean reward: -4.288888888888889, Mean Entropy: 0.9259557723999023, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 28,  Mean reward: -3.75531914893617, Mean Entropy: 0.9147118330001831, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 29,  Mean reward: -1.9183673469387754, Mean Entropy: 0.9678704142570496, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 30,  Mean reward: -4.6395348837209305, Mean Entropy: 0.9799680113792419, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 31,  Mean reward: -5.178571428571429, Mean Entropy: 0.9026007652282715, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 32,  Mean reward: -2.5425531914893615, Mean Entropy: 0.9930245280265808, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 33,  Mean reward: -3.4019607843137254, Mean Entropy: 0.9267441630363464, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 34,  Mean reward: -4.27906976744186, Mean Entropy: 0.9223310351371765, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.60s
Iteration: 35,  Mean reward: -4.104166666666667, Mean Entropy: 0.9563894271850586, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 36,  Mean reward: -4.202127659574468, Mean Entropy: 0.9304313659667969, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 37,  Mean reward: -5.090909090909091, Mean Entropy: 0.903471827507019, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 38,  Mean reward: -4.739130434782608, Mean Entropy: 0.885290265083313, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 39,  Mean reward: -2.0, Mean Entropy: 0.8884264826774597, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 40,  Mean reward: -1.7083333333333333, Mean Entropy: 0.9344128966331482, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 41,  Mean reward: -1.74, Mean Entropy: 0.8683264255523682, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 42,  Mean reward: -2.160377358490566, Mean Entropy: 0.8256842494010925, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 43,  Mean reward: -2.3137254901960786, Mean Entropy: 0.9000591039657593, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 44,  Mean reward: -4.96875, Mean Entropy: 0.8193590641021729, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 45,  Mean reward: -5.0, Mean Entropy: 0.7382097244262695, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 46,  Mean reward: -2.5789473684210527, Mean Entropy: 0.8062964677810669, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 47,  Mean reward: -3.1346153846153846, Mean Entropy: 0.65831458568573, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 48,  Mean reward: -3.9464285714285716, Mean Entropy: 0.6476647257804871, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 49,  Mean reward: -2.1979166666666665, Mean Entropy: 0.8539239764213562, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 50,  Mean reward: -5.33, Mean Entropy: 0.6853525638580322, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 51,  Mean reward: -3.1122448979591835, Mean Entropy: 0.9033769965171814, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 52,  Mean reward: -3.411111111111111, Mean Entropy: 0.8067449331283569, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 53,  Mean reward: -5.826923076923077, Mean Entropy: 0.8517634272575378, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 54,  Mean reward: -2.2551020408163267, Mean Entropy: 0.8142940402030945, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 55,  Mean reward: -5.28, Mean Entropy: 0.7555241584777832, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 56,  Mean reward: -3.87, Mean Entropy: 0.6628695726394653, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 57,  Mean reward: -2.0338983050847457, Mean Entropy: 0.7674689292907715, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 58,  Mean reward: -4.215686274509804, Mean Entropy: 0.7333536148071289, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 59,  Mean reward: -1.25, Mean Entropy: 0.8159420490264893, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 60,  Mean reward: -4.480392156862745, Mean Entropy: 0.6834391355514526, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 61,  Mean reward: -2.9732142857142856, Mean Entropy: 0.5378692150115967, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 62,  Mean reward: -1.7672413793103448, Mean Entropy: 0.5491943359375, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 63,  Mean reward: -2.95, Mean Entropy: 0.5709496140480042, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 64,  Mean reward: -3.467741935483871, Mean Entropy: 0.47826001048088074, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 65,  Mean reward: -2.6384615384615384, Mean Entropy: 0.3203302025794983, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 66,  Mean reward: -0.6575342465753424, Mean Entropy: 0.2376893013715744, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 67,  Mean reward: -0.3055555555555556, Mean Entropy: 0.41744741797447205, complete_episode_count: 72.0, Gather time: 0.73s, Train time: 0.74s
Iteration: 68,  Mean reward: -2.4224137931034484, Mean Entropy: 0.4384241998195648, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 69,  Mean reward: -3.3046875, Mean Entropy: 0.38063639402389526, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 70,  Mean reward: -3.1307692307692307, Mean Entropy: 0.45321911573410034, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 71,  Mean reward: -2.122950819672131, Mean Entropy: 0.4388698935508728, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 72,  Mean reward: -2.9921875, Mean Entropy: 0.42786741256713867, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 73,  Mean reward: -2.296875, Mean Entropy: 0.4223288893699646, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 74,  Mean reward: -1.8548387096774193, Mean Entropy: 0.3826778829097748, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 75,  Mean reward: -2.8968253968253967, Mean Entropy: 0.42491984367370605, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 76,  Mean reward: -2.5, Mean Entropy: 0.43135422468185425, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 77,  Mean reward: -0.22413793103448276, Mean Entropy: 0.34447914361953735, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 78,  Mean reward: -0.03968253968253968, Mean Entropy: 0.31103384494781494, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 79,  Mean reward: -4.578125, Mean Entropy: 0.4298205077648163, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 80,  Mean reward: -3.2549019607843137, Mean Entropy: 0.4893476665019989, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 81,  Mean reward: -3.8389830508474576, Mean Entropy: 0.5924032926559448, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 82,  Mean reward: -5.854166666666667, Mean Entropy: 0.47481569647789, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 83,  Mean reward: -3.5, Mean Entropy: 0.34858617186546326, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 84,  Mean reward: -3.0161290322580645, Mean Entropy: 0.22490590810775757, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 85,  Mean reward: -3.5454545454545454, Mean Entropy: 0.4058429002761841, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 86,  Mean reward: -0.8240740740740741, Mean Entropy: 0.5050822496414185, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 87,  Mean reward: -2.087719298245614, Mean Entropy: 0.5635254383087158, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 88,  Mean reward: -2.3333333333333335, Mean Entropy: 0.6140704154968262, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 89,  Mean reward: -3.8229166666666665, Mean Entropy: 0.573945164680481, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 90,  Mean reward: -2.6272727272727274, Mean Entropy: 0.6022195219993591, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 91,  Mean reward: -4.921568627450981, Mean Entropy: 0.5736136436462402, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 92,  Mean reward: -2.8703703703703702, Mean Entropy: 0.5346804857254028, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 93,  Mean reward: -3.1228070175438596, Mean Entropy: 0.5733334422111511, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 94,  Mean reward: -3.4444444444444446, Mean Entropy: 0.603562593460083, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 95,  Mean reward: -3.526315789473684, Mean Entropy: 0.5532271862030029, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 96,  Mean reward: -3.330357142857143, Mean Entropy: 0.5752279758453369, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 97,  Mean reward: -0.8365384615384616, Mean Entropy: 0.5960752964019775, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 98,  Mean reward: -2.3508771929824563, Mean Entropy: 0.4963049292564392, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 99,  Mean reward: -3.5660377358490565, Mean Entropy: 0.5309872627258301, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 100,  Mean reward: -2.611111111111111, Mean Entropy: 0.5711885690689087, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.59s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -3.0283018867924527, Mean Entropy: 0.6114307045936584, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 102,  Mean reward: -6.547169811320755, Mean Entropy: 0.5302815437316895, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 103,  Mean reward: -2.2280701754385963, Mean Entropy: 0.5403363704681396, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 104,  Mean reward: -3.574074074074074, Mean Entropy: 0.5144267082214355, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 105,  Mean reward: -1.736842105263158, Mean Entropy: 0.48847752809524536, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 106,  Mean reward: -4.3545454545454545, Mean Entropy: 0.4555705189704895, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 107,  Mean reward: -1.7338709677419355, Mean Entropy: 0.4077821671962738, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 108,  Mean reward: -2.9846153846153847, Mean Entropy: 0.3981937766075134, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 109,  Mean reward: -1.7416666666666667, Mean Entropy: 0.4642699658870697, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 110,  Mean reward: -4.057377049180328, Mean Entropy: 0.3883475065231323, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 111,  Mean reward: -2.1015625, Mean Entropy: 0.3000538647174835, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 112,  Mean reward: -3.640625, Mean Entropy: 0.1984187811613083, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 113,  Mean reward: -2.708955223880597, Mean Entropy: 0.16408488154411316, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 114,  Mean reward: -4.6461538461538465, Mean Entropy: 0.19025540351867676, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 115,  Mean reward: -3.0615384615384613, Mean Entropy: 0.17245525121688843, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 116,  Mean reward: 0.4861111111111111, Mean Entropy: 0.1556629091501236, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 117,  Mean reward: -1.4701492537313432, Mean Entropy: 0.20607835054397583, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 118,  Mean reward: -0.8731343283582089, Mean Entropy: 0.2039380520582199, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 119,  Mean reward: -1.5692307692307692, Mean Entropy: 0.20792922377586365, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 120,  Mean reward: -0.7272727272727273, Mean Entropy: 0.1601770669221878, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 121,  Mean reward: -2.590909090909091, Mean Entropy: 0.1888870894908905, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 122,  Mean reward: -4.544117647058823, Mean Entropy: 0.17463257908821106, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 123,  Mean reward: -2.3857142857142857, Mean Entropy: 0.11686619371175766, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 124,  Mean reward: -1.55, Mean Entropy: 0.10324456542730331, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 125,  Mean reward: -3.5597014925373136, Mean Entropy: 0.13827423751354218, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 126,  Mean reward: -3.1076923076923078, Mean Entropy: 0.08038689196109772, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 127,  Mean reward: -1.1304347826086956, Mean Entropy: 0.09897448122501373, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 128,  Mean reward: -2.0923076923076924, Mean Entropy: 0.09292314946651459, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 129,  Mean reward: 0.18571428571428572, Mean Entropy: 0.07581429183483124, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 130,  Mean reward: -1.6590909090909092, Mean Entropy: 0.10545311868190765, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 131,  Mean reward: -2.708955223880597, Mean Entropy: 0.10977670550346375, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 132,  Mean reward: -2.65625, Mean Entropy: 0.11469261348247528, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 133,  Mean reward: -2.9453125, Mean Entropy: 0.12092854827642441, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 134,  Mean reward: -4.076923076923077, Mean Entropy: 0.11402782052755356, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 135,  Mean reward: -4.095238095238095, Mean Entropy: 0.09745095670223236, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 136,  Mean reward: -1.5149253731343284, Mean Entropy: 0.10409262031316757, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 137,  Mean reward: -2.196969696969697, Mean Entropy: 0.08132082223892212, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 138,  Mean reward: -0.5507246376811594, Mean Entropy: 0.09916871786117554, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 139,  Mean reward: -1.9366197183098592, Mean Entropy: 0.1134091466665268, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 140,  Mean reward: -2.388059701492537, Mean Entropy: 0.09549576044082642, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 141,  Mean reward: -2.265151515151515, Mean Entropy: 0.11274445056915283, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 142,  Mean reward: -2.388059701492537, Mean Entropy: 0.11925562471151352, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 143,  Mean reward: -2.871212121212121, Mean Entropy: 0.121138796210289, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 144,  Mean reward: -4.666666666666667, Mean Entropy: 0.1264818161725998, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 145,  Mean reward: -1.9848484848484849, Mean Entropy: 0.08460856974124908, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 146,  Mean reward: -1.9191176470588236, Mean Entropy: 0.102932408452034, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 147,  Mean reward: -1.4701492537313432, Mean Entropy: 0.0963134691119194, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 148,  Mean reward: -1.6363636363636365, Mean Entropy: 0.10335355252027512, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 149,  Mean reward: -2.96875, Mean Entropy: 0.10972732305526733, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 150,  Mean reward: -4.21875, Mean Entropy: 0.12314435839653015, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 151,  Mean reward: -3.283582089552239, Mean Entropy: 0.1222553700208664, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 152,  Mean reward: -4.976923076923077, Mean Entropy: 0.13620659708976746, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 153,  Mean reward: -1.625, Mean Entropy: 0.08068449795246124, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 154,  Mean reward: -3.7, Mean Entropy: 0.11653643101453781, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 155,  Mean reward: -1.875, Mean Entropy: 0.10533111542463303, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 156,  Mean reward: 0.3591549295774648, Mean Entropy: 0.10410686582326889, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 157,  Mean reward: -0.9926470588235294, Mean Entropy: 0.08612533658742905, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 158,  Mean reward: -3.7, Mean Entropy: 0.1388918161392212, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 159,  Mean reward: -1.9848484848484849, Mean Entropy: 0.08746184408664703, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 160,  Mean reward: -3.4153846153846152, Mean Entropy: 0.09964917600154877, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 161,  Mean reward: -1.492537313432836, Mean Entropy: 0.0878281220793724, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 162,  Mean reward: -4.142857142857143, Mean Entropy: 0.10019664466381073, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 163,  Mean reward: -3.196969696969697, Mean Entropy: 0.1005825623869896, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 164,  Mean reward: -3.2196969696969697, Mean Entropy: 0.10052024573087692, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 165,  Mean reward: -4.363636363636363, Mean Entropy: 0.15041476488113403, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 166,  Mean reward: -0.65, Mean Entropy: 0.11288896203041077, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 167,  Mean reward: -1.7686567164179106, Mean Entropy: 0.126157745718956, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 168,  Mean reward: -2.142857142857143, Mean Entropy: 0.11546990275382996, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 169,  Mean reward: -2.847826086956522, Mean Entropy: 0.10873566567897797, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 170,  Mean reward: -4.484375, Mean Entropy: 0.15441538393497467, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 171,  Mean reward: -1.6136363636363635, Mean Entropy: 0.12860961258411407, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 172,  Mean reward: -0.9926470588235294, Mean Entropy: 0.11572244018316269, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 173,  Mean reward: -1.625, Mean Entropy: 0.12275861203670502, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 174,  Mean reward: -3.0615384615384613, Mean Entropy: 0.1167677566409111, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 175,  Mean reward: -2.388059701492537, Mean Entropy: 0.11746308207511902, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 176,  Mean reward: -1.5808823529411764, Mean Entropy: 0.09743520617485046, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 177,  Mean reward: -0.3602941176470588, Mean Entropy: 0.1173546314239502, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 178,  Mean reward: -2.0671641791044775, Mean Entropy: 0.11197708547115326, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 179,  Mean reward: -3.6838235294117645, Mean Entropy: 0.13857853412628174, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 180,  Mean reward: -1.9621212121212122, Mean Entropy: 0.11867757141590118, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 181,  Mean reward: -1.286764705882353, Mean Entropy: 0.11932939291000366, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 182,  Mean reward: -3.1159420289855073, Mean Entropy: 0.11281537264585495, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 183,  Mean reward: -1.4477611940298507, Mean Entropy: 0.1263905167579651, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 184,  Mean reward: -4.076923076923077, Mean Entropy: 0.12742269039154053, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 185,  Mean reward: -1.7462686567164178, Mean Entropy: 0.0798509269952774, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 186,  Mean reward: -1.5808823529411764, Mean Entropy: 0.10650645196437836, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 187,  Mean reward: -4.6893939393939394, Mean Entropy: 0.1272423267364502, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 188,  Mean reward: -3.7777777777777777, Mean Entropy: 0.1278170794248581, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 189,  Mean reward: -3.8358208955223883, Mean Entropy: 0.14088916778564453, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 190,  Mean reward: -2.41044776119403, Mean Entropy: 0.09449140727519989, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 191,  Mean reward: -3.3692307692307693, Mean Entropy: 0.09477667510509491, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 192,  Mean reward: -2.08955223880597, Mean Entropy: 0.10188299417495728, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 193,  Mean reward: -2.6714285714285713, Mean Entropy: 0.12887324392795563, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 194,  Mean reward: -2.1, Mean Entropy: 0.10240438580513, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 195,  Mean reward: -1.55, Mean Entropy: 0.10863043367862701, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 196,  Mean reward: -3.106060606060606, Mean Entropy: 0.15762761235237122, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 197,  Mean reward: -2.9402985074626864, Mean Entropy: 0.13690060377120972, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 198,  Mean reward: -3.411764705882353, Mean Entropy: 0.1439896523952484, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 199,  Mean reward: -2.0217391304347827, Mean Entropy: 0.10979440808296204, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 200,  Mean reward: -1.1044776119402986, Mean Entropy: 0.10304444283246994, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -0.9926470588235294, Mean Entropy: 0.08914093673229218, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 202,  Mean reward: -0.2391304347826087, Mean Entropy: 0.10324462503194809, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 203,  Mean reward: -4.076923076923077, Mean Entropy: 0.12330955266952515, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 204,  Mean reward: -1.5071428571428571, Mean Entropy: 0.1382245272397995, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 205,  Mean reward: -2.9357142857142855, Mean Entropy: 0.15853367745876312, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 206,  Mean reward: -2.536231884057971, Mean Entropy: 0.1246228814125061, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 207,  Mean reward: -2.579710144927536, Mean Entropy: 0.09734304249286652, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 208,  Mean reward: -3.6044776119402986, Mean Entropy: 0.0972275659441948, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 209,  Mean reward: -2.463235294117647, Mean Entropy: 0.11842872202396393, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 210,  Mean reward: -2.5454545454545454, Mean Entropy: 0.12454762309789658, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 211,  Mean reward: -2.08955223880597, Mean Entropy: 0.1669813096523285, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 212,  Mean reward: -4.134328358208955, Mean Entropy: 0.1609659343957901, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 213,  Mean reward: -4.201492537313433, Mean Entropy: 0.16063205897808075, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 214,  Mean reward: -3.7058823529411766, Mean Entropy: 0.14037677645683289, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 215,  Mean reward: -2.485294117647059, Mean Entropy: 0.1334519386291504, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 216,  Mean reward: -2.3970588235294117, Mean Entropy: 0.15378054976463318, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 217,  Mean reward: -2.536231884057971, Mean Entropy: 0.1330932378768921, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 218,  Mean reward: -3.582089552238806, Mean Entropy: 0.14053860306739807, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 219,  Mean reward: -1.9191176470588236, Mean Entropy: 0.11917243897914886, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 220,  Mean reward: -5.3283582089552235, Mean Entropy: 0.18251606822013855, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 221,  Mean reward: 0.8, Mean Entropy: 0.07052993029356003, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 222,  Mean reward: -3.13768115942029, Mean Entropy: 0.1262727528810501, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 223,  Mean reward: -1.2428571428571429, Mean Entropy: 0.11972443759441376, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 224,  Mean reward: -2.6285714285714286, Mean Entropy: 0.19665312767028809, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 225,  Mean reward: -1.3309859154929577, Mean Entropy: 0.13365277647972107, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 226,  Mean reward: -0.7971014492753623, Mean Entropy: 0.1403031051158905, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 227,  Mean reward: -3.073529411764706, Mean Entropy: 0.14019636809825897, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 228,  Mean reward: -1.2, Mean Entropy: 0.14063763618469238, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 229,  Mean reward: -1.3985507246376812, Mean Entropy: 0.14091509580612183, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 230,  Mean reward: -2.6641791044776117, Mean Entropy: 0.09858759492635727, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 231,  Mean reward: -3.760869565217391, Mean Entropy: 0.1837969720363617, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 232,  Mean reward: -4.121428571428571, Mean Entropy: 0.15590102970600128, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 233,  Mean reward: -1.8142857142857143, Mean Entropy: 0.11370597779750824, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 234,  Mean reward: -4.134328358208955, Mean Entropy: 0.15590420365333557, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 235,  Mean reward: -2.1691176470588234, Mean Entropy: 0.12782487273216248, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 236,  Mean reward: -1.4375, Mean Entropy: 0.13454172015190125, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 237,  Mean reward: -2.7794117647058822, Mean Entropy: 0.12758344411849976, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 238,  Mean reward: -0.09027777777777778, Mean Entropy: 0.12713640928268433, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 239,  Mean reward: -0.6928571428571428, Mean Entropy: 0.1268487125635147, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 240,  Mean reward: -2.407142857142857, Mean Entropy: 0.15589505434036255, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 241,  Mean reward: -3.507142857142857, Mean Entropy: 0.169651061296463, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 242,  Mean reward: -2.2183098591549295, Mean Entropy: 0.1487772911787033, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 243,  Mean reward: -2.869565217391304, Mean Entropy: 0.12773218750953674, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 244,  Mean reward: -4.623076923076923, Mean Entropy: 0.14203861355781555, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 245,  Mean reward: -2.1971830985915495, Mean Entropy: 0.15644322335720062, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 246,  Mean reward: -1.8571428571428572, Mean Entropy: 0.1206822395324707, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 247,  Mean reward: -3.0294117647058822, Mean Entropy: 0.11356665939092636, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 248,  Mean reward: -2.2681159420289854, Mean Entropy: 0.13520243763923645, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 249,  Mean reward: -3.2611940298507465, Mean Entropy: 0.13509856164455414, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 250,  Mean reward: -2.1470588235294117, Mean Entropy: 0.1210484728217125, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 251,  Mean reward: -2.485294117647059, Mean Entropy: 0.1494932770729065, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 252,  Mean reward: -3.4545454545454546, Mean Entropy: 0.14238345623016357, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 253,  Mean reward: -3.3676470588235294, Mean Entropy: 0.1566973179578781, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 254,  Mean reward: -2.1214285714285714, Mean Entropy: 0.12824448943138123, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 255,  Mean reward: -3.859375, Mean Entropy: 0.15644338726997375, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 256,  Mean reward: -1.2647058823529411, Mean Entropy: 0.1140282154083252, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 257,  Mean reward: 0.06164383561643835, Mean Entropy: 0.12813067436218262, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 258,  Mean reward: -1.0492957746478873, Mean Entropy: 0.09246357530355453, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 259,  Mean reward: 0.16666666666666666, Mean Entropy: 0.09268303960561752, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 260,  Mean reward: -1.676056338028169, Mean Entropy: 0.13533735275268555, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 261,  Mean reward: -1.7361111111111112, Mean Entropy: 0.15004976093769073, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 262,  Mean reward: -2.73943661971831, Mean Entropy: 0.1643788367509842, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 263,  Mean reward: 0.20833333333333334, Mean Entropy: 0.09357331693172455, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 264,  Mean reward: -2.176056338028169, Mean Entropy: 0.17872141301631927, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 265,  Mean reward: -1.875, Mean Entropy: 0.1219896525144577, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 266,  Mean reward: -2.692857142857143, Mean Entropy: 0.17181949317455292, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 267,  Mean reward: -0.6071428571428571, Mean Entropy: 0.12164081633090973, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 268,  Mean reward: -3.3676470588235294, Mean Entropy: 0.16423186659812927, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 269,  Mean reward: -0.9705882352941176, Mean Entropy: 0.12153353542089462, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 270,  Mean reward: -3.283582089552239, Mean Entropy: 0.15002915263175964, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 271,  Mean reward: -1.676056338028169, Mean Entropy: 0.11449622362852097, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 272,  Mean reward: -2.1470588235294117, Mean Entropy: 0.1286247819662094, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 273,  Mean reward: 0.5220588235294118, Mean Entropy: 0.11442693322896957, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 274,  Mean reward: -1.9782608695652173, Mean Entropy: 0.12160910665988922, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 275,  Mean reward: -4.753731343283582, Mean Entropy: 0.17892003059387207, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 276,  Mean reward: -3.5597014925373136, Mean Entropy: 0.12893879413604736, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 277,  Mean reward: -3.389705882352941, Mean Entropy: 0.1504456102848053, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 278,  Mean reward: -2.2681159420289854, Mean Entropy: 0.09318576753139496, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 279,  Mean reward: -4.477611940298507, Mean Entropy: 0.15049821138381958, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 280,  Mean reward: -1.2013888888888888, Mean Entropy: 0.10759131610393524, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 281,  Mean reward: -0.4657534246575342, Mean Entropy: 0.12181316316127777, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 282,  Mean reward: -2.0223880597014925, Mean Entropy: 0.14324039220809937, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 283,  Mean reward: -2.013888888888889, Mean Entropy: 0.12200972437858582, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 284,  Mean reward: -2.7573529411764706, Mean Entropy: 0.15796557068824768, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 285,  Mean reward: -2.213235294117647, Mean Entropy: 0.10814206302165985, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 286,  Mean reward: -1.9782608695652173, Mean Entropy: 0.13648545742034912, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.80s
Iteration: 287,  Mean reward: -2.7573529411764706, Mean Entropy: 0.1792333573102951, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 288,  Mean reward: -2.65, Mean Entropy: 0.13713538646697998, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 289,  Mean reward: -1.6549295774647887, Mean Entropy: 0.13696172833442688, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 290,  Mean reward: -0.9357142857142857, Mean Entropy: 0.08616133034229279, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 291,  Mean reward: -0.7205882352941176, Mean Entropy: 0.12903033196926117, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 292,  Mean reward: -2.76056338028169, Mean Entropy: 0.12975239753723145, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 293,  Mean reward: -1.108695652173913, Mean Entropy: 0.14555850625038147, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 294,  Mean reward: -0.9236111111111112, Mean Entropy: 0.09481142461299896, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 295,  Mean reward: -2.8260869565217392, Mean Entropy: 0.13730788230895996, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 296,  Mean reward: -3.13768115942029, Mean Entropy: 0.15207484364509583, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 297,  Mean reward: -2.0, Mean Entropy: 0.15265381336212158, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 298,  Mean reward: -2.289855072463768, Mean Entropy: 0.1312537044286728, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 299,  Mean reward: -0.9485294117647058, Mean Entropy: 0.1231941431760788, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 300,  Mean reward: -0.6714285714285714, Mean Entropy: 0.10798277705907822, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -1.2013888888888888, Mean Entropy: 0.1374388188123703, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 302,  Mean reward: -1.9154929577464788, Mean Entropy: 0.12984547019004822, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 303,  Mean reward: -4.072463768115942, Mean Entropy: 0.13688808679580688, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 304,  Mean reward: -3.3455882352941178, Mean Entropy: 0.13870158791542053, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 305,  Mean reward: -1.352112676056338, Mean Entropy: 0.11775647103786469, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 306,  Mean reward: -1.9366197183098592, Mean Entropy: 0.13708257675170898, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 307,  Mean reward: 0.07746478873239436, Mean Entropy: 0.10893329232931137, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 308,  Mean reward: -1.352112676056338, Mean Entropy: 0.150999516248703, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 309,  Mean reward: -0.2328767123287671, Mean Entropy: 0.08627563714981079, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 310,  Mean reward: -2.1691176470588234, Mean Entropy: 0.11500503867864609, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 311,  Mean reward: -3.661764705882353, Mean Entropy: 0.13668961822986603, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 312,  Mean reward: -3.7714285714285714, Mean Entropy: 0.17268693447113037, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 313,  Mean reward: -2.4788732394366195, Mean Entropy: 0.1368015706539154, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 314,  Mean reward: -2.847826086956522, Mean Entropy: 0.1512899398803711, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 315,  Mean reward: -1.75, Mean Entropy: 0.15135934948921204, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 316,  Mean reward: -0.9705882352941176, Mean Entropy: 0.12555187940597534, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 317,  Mean reward: -2.3642857142857143, Mean Entropy: 0.14494583010673523, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 318,  Mean reward: -1.8142857142857143, Mean Entropy: 0.12503862380981445, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 319,  Mean reward: -1.4583333333333333, Mean Entropy: 0.16137754917144775, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 320,  Mean reward: -2.548611111111111, Mean Entropy: 0.166707843542099, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 321,  Mean reward: -0.8098591549295775, Mean Entropy: 0.11911659687757492, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 322,  Mean reward: -3.3455882352941178, Mean Entropy: 0.14568160474300385, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 323,  Mean reward: -2.536231884057971, Mean Entropy: 0.15552003681659698, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 324,  Mean reward: -2.536231884057971, Mean Entropy: 0.15411801636219025, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 325,  Mean reward: -1.676056338028169, Mean Entropy: 0.09704084694385529, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 326,  Mean reward: -3.2642857142857142, Mean Entropy: 0.16683201491832733, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 327,  Mean reward: -1.9930555555555556, Mean Entropy: 0.16719728708267212, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 328,  Mean reward: -2.057142857142857, Mean Entropy: 0.13235896825790405, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 329,  Mean reward: -2.601449275362319, Mean Entropy: 0.16090798377990723, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 330,  Mean reward: -3.051470588235294, Mean Entropy: 0.13173827528953552, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 331,  Mean reward: -0.6285714285714286, Mean Entropy: 0.10347163677215576, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 332,  Mean reward: -1.3309859154929577, Mean Entropy: 0.14511486887931824, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 333,  Mean reward: -2.4788732394366195, Mean Entropy: 0.1320262849330902, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 334,  Mean reward: -1.9782608695652173, Mean Entropy: 0.1192702054977417, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 335,  Mean reward: -2.125, Mean Entropy: 0.12413560599088669, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 336,  Mean reward: -2.891304347826087, Mean Entropy: 0.12348799407482147, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 337,  Mean reward: -3.9338235294117645, Mean Entropy: 0.1444457769393921, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 338,  Mean reward: -2.713235294117647, Mean Entropy: 0.13752371072769165, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 339,  Mean reward: -1.6126760563380282, Mean Entropy: 0.13811977207660675, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 340,  Mean reward: -3.1159420289855073, Mean Entropy: 0.16593977808952332, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 341,  Mean reward: -1.4857142857142858, Mean Entropy: 0.13014981150627136, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 342,  Mean reward: -0.4859154929577465, Mean Entropy: 0.11692677438259125, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 343,  Mean reward: -6.246268656716418, Mean Entropy: 0.18762964010238647, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 344,  Mean reward: -1.2205882352941178, Mean Entropy: 0.13835813105106354, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 345,  Mean reward: -4.078571428571428, Mean Entropy: 0.15184205770492554, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 346,  Mean reward: -2.9402985074626864, Mean Entropy: 0.12693403661251068, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 347,  Mean reward: -4.544117647058823, Mean Entropy: 0.14508482813835144, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 348,  Mean reward: -1.7361111111111112, Mean Entropy: 0.1448984593153, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 349,  Mean reward: -2.0, Mean Entropy: 0.14866416156291962, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 350,  Mean reward: -2.463235294117647, Mean Entropy: 0.12516938149929047, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 351,  Mean reward: -1.6338028169014085, Mean Entropy: 0.1335667222738266, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 352,  Mean reward: -0.9027777777777778, Mean Entropy: 0.12527760863304138, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 353,  Mean reward: -0.5068493150684932, Mean Entropy: 0.08733883500099182, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 354,  Mean reward: -1.5285714285714285, Mean Entropy: 0.16697606444358826, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 355,  Mean reward: -3.3676470588235294, Mean Entropy: 0.12368343770503998, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 356,  Mean reward: -4.0606060606060606, Mean Entropy: 0.13810580968856812, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 357,  Mean reward: -1.6126760563380282, Mean Entropy: 0.1393033266067505, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 358,  Mean reward: -0.6714285714285714, Mean Entropy: 0.11891918629407883, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 359,  Mean reward: -2.1971830985915495, Mean Entropy: 0.15884992480278015, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 360,  Mean reward: -2.7816901408450705, Mean Entropy: 0.15862350165843964, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 361,  Mean reward: -2.692857142857143, Mean Entropy: 0.15860919654369354, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 362,  Mean reward: -0.9444444444444444, Mean Entropy: 0.1084585040807724, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 363,  Mean reward: -3.471014492753623, Mean Entropy: 0.1586729884147644, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 364,  Mean reward: -1.9577464788732395, Mean Entropy: 0.1446622908115387, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 365,  Mean reward: -0.65, Mean Entropy: 0.13826444745063782, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 366,  Mean reward: 0.9225352112676056, Mean Entropy: 0.10096922516822815, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 367,  Mean reward: -2.8680555555555554, Mean Entropy: 0.12977278232574463, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 368,  Mean reward: -1.4166666666666667, Mean Entropy: 0.14182430505752563, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 369,  Mean reward: -2.522727272727273, Mean Entropy: 0.1165362000465393, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 370,  Mean reward: -1.8943661971830985, Mean Entropy: 0.12282764911651611, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 371,  Mean reward: -0.17123287671232876, Mean Entropy: 0.10097286105155945, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 372,  Mean reward: -2.2183098591549295, Mean Entropy: 0.15892574191093445, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 373,  Mean reward: -5.360294117647059, Mean Entropy: 0.1601722240447998, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 374,  Mean reward: -2.985074626865672, Mean Entropy: 0.15664809942245483, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 375,  Mean reward: -0.4647887323943662, Mean Entropy: 0.1814299076795578, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 376,  Mean reward: -1.8357142857142856, Mean Entropy: 0.1492844521999359, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 377,  Mean reward: -5.2101449275362315, Mean Entropy: 0.27473968267440796, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 378,  Mean reward: -2.776923076923077, Mean Entropy: 0.39460527896881104, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 379,  Mean reward: -3.9836065573770494, Mean Entropy: 0.29188328981399536, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 380,  Mean reward: 0.45, Mean Entropy: 0.17866405844688416, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 381,  Mean reward: -1.6884057971014492, Mean Entropy: 0.31069415807724, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 382,  Mean reward: -0.5507246376811594, Mean Entropy: 0.3429378271102905, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 383,  Mean reward: -2.825, Mean Entropy: 0.3016239404678345, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 384,  Mean reward: -3.3692307692307693, Mean Entropy: 0.2618734538555145, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 385,  Mean reward: -2.3203125, Mean Entropy: 0.19540628790855408, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 386,  Mean reward: -2.1911764705882355, Mean Entropy: 0.292496919631958, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 387,  Mean reward: -2.3208955223880596, Mean Entropy: 0.23740403354167938, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 388,  Mean reward: -2.65, Mean Entropy: 0.27057522535324097, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 389,  Mean reward: -0.9357142857142857, Mean Entropy: 0.19868025183677673, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 390,  Mean reward: -3.073529411764706, Mean Entropy: 0.28448784351348877, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 391,  Mean reward: -0.08823529411764706, Mean Entropy: 0.2545202672481537, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 392,  Mean reward: -3.073529411764706, Mean Entropy: 0.1697659194469452, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 393,  Mean reward: -1.065217391304348, Mean Entropy: 0.20133042335510254, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 394,  Mean reward: -0.22535211267605634, Mean Entropy: 0.18840539455413818, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 395,  Mean reward: -0.9571428571428572, Mean Entropy: 0.15740013122558594, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 396,  Mean reward: -4.083333333333333, Mean Entropy: 0.16524213552474976, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 397,  Mean reward: -4.27536231884058, Mean Entropy: 0.15097180008888245, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 398,  Mean reward: -3.6597222222222223, Mean Entropy: 0.20236721634864807, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 399,  Mean reward: -2.2681159420289854, Mean Entropy: 0.20008406043052673, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 400,  Mean reward: -2.0, Mean Entropy: 0.1692633330821991, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.78s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -2.7794117647058822, Mean Entropy: 0.1521434634923935, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 402,  Mean reward: -2.692857142857143, Mean Entropy: 0.17403239011764526, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 403,  Mean reward: -3.537313432835821, Mean Entropy: 0.14061537384986877, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 404,  Mean reward: -1.091549295774648, Mean Entropy: 0.15484172105789185, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 405,  Mean reward: -2.692857142857143, Mean Entropy: 0.1652267873287201, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 406,  Mean reward: -2.3857142857142857, Mean Entropy: 0.18754366040229797, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 407,  Mean reward: -4.028985507246377, Mean Entropy: 0.18606412410736084, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 408,  Mean reward: -2.3115942028985508, Mean Entropy: 0.17220531404018402, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 409,  Mean reward: -1.173913043478261, Mean Entropy: 0.15749406814575195, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 410,  Mean reward: -5.5, Mean Entropy: 0.13793763518333435, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 411,  Mean reward: -0.060810810810810814, Mean Entropy: 0.12087957561016083, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 412,  Mean reward: -2.2028985507246377, Mean Entropy: 0.14854177832603455, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 413,  Mean reward: -0.9931506849315068, Mean Entropy: 0.1427488923072815, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.78s
Iteration: 414,  Mean reward: -1.8529411764705883, Mean Entropy: 0.16177409887313843, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 415,  Mean reward: -1.6338028169014085, Mean Entropy: 0.14032915234565735, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 416,  Mean reward: 0.1232876712328767, Mean Entropy: 0.13652944564819336, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 417,  Mean reward: -1.6126760563380282, Mean Entropy: 0.13084998726844788, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 418,  Mean reward: -1.2876712328767124, Mean Entropy: 0.1317415088415146, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 419,  Mean reward: -2.0785714285714287, Mean Entropy: 0.13861995935440063, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 420,  Mean reward: -3.584507042253521, Mean Entropy: 0.14607331156730652, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 421,  Mean reward: -2.5579710144927534, Mean Entropy: 0.14228956401348114, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 422,  Mean reward: -3.1940298507462686, Mean Entropy: 0.14672085642814636, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 423,  Mean reward: -3.717391304347826, Mean Entropy: 0.1576358675956726, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 424,  Mean reward: -2.4577464788732395, Mean Entropy: 0.14862370491027832, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 425,  Mean reward: -1.0704225352112675, Mean Entropy: 0.12620790302753448, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 426,  Mean reward: -2.1691176470588234, Mean Entropy: 0.14525386691093445, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 427,  Mean reward: -4.386363636363637, Mean Entropy: 0.14642031490802765, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 428,  Mean reward: -4.0606060606060606, Mean Entropy: 0.23639558255672455, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 429,  Mean reward: -0.6714285714285714, Mean Entropy: 0.1363297998905182, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 430,  Mean reward: -3.125, Mean Entropy: 0.18811562657356262, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 431,  Mean reward: -2.176056338028169, Mean Entropy: 0.2230374813079834, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 432,  Mean reward: -2.3970588235294117, Mean Entropy: 0.19547848403453827, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 433,  Mean reward: -2.057142857142857, Mean Entropy: 0.1819038838148117, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 434,  Mean reward: -1.9782608695652173, Mean Entropy: 0.23487168550491333, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 435,  Mean reward: -2.985074626865672, Mean Entropy: 0.2402946949005127, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 436,  Mean reward: -2.4923076923076923, Mean Entropy: 0.23055705428123474, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 437,  Mean reward: -4.36231884057971, Mean Entropy: 0.23155304789543152, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 438,  Mean reward: -2.9785714285714286, Mean Entropy: 0.18940548598766327, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 439,  Mean reward: -2.579710144927536, Mean Entropy: 0.23922547698020935, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 440,  Mean reward: -2.6231884057971016, Mean Entropy: 0.27133217453956604, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 441,  Mean reward: 0.38028169014084506, Mean Entropy: 0.1632545292377472, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 442,  Mean reward: -3.1594202898550723, Mean Entropy: 0.17614281177520752, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 443,  Mean reward: -0.7602739726027398, Mean Entropy: 0.14510774612426758, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 444,  Mean reward: -2.3208955223880596, Mean Entropy: 0.1393468677997589, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 445,  Mean reward: -3.3455882352941178, Mean Entropy: 0.1648065149784088, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 446,  Mean reward: -2.057142857142857, Mean Entropy: 0.1801137924194336, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 447,  Mean reward: -1.5588235294117647, Mean Entropy: 0.22682297229766846, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 448,  Mean reward: -5.110294117647059, Mean Entropy: 0.19546201825141907, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 449,  Mean reward: -2.246376811594203, Mean Entropy: 0.1820131540298462, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 450,  Mean reward: -2.2183098591549295, Mean Entropy: 0.2440948784351349, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 451,  Mean reward: -1.5588235294117647, Mean Entropy: 0.21416263282299042, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 452,  Mean reward: -3.5597014925373136, Mean Entropy: 0.20059925317764282, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 453,  Mean reward: -2.2246376811594204, Mean Entropy: 0.24533548951148987, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 454,  Mean reward: -3.4927536231884058, Mean Entropy: 0.18732187151908875, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 455,  Mean reward: -1.8142857142857143, Mean Entropy: 0.19670194387435913, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 456,  Mean reward: -1.5285714285714285, Mean Entropy: 0.18330958485603333, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 457,  Mean reward: -3.6171875, Mean Entropy: 0.18003365397453308, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 458,  Mean reward: -2.9357142857142855, Mean Entropy: 0.15328624844551086, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 459,  Mean reward: -2.3115942028985508, Mean Entropy: 0.15217863023281097, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 460,  Mean reward: -4.340579710144928, Mean Entropy: 0.19837293028831482, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 461,  Mean reward: -4.201492537313433, Mean Entropy: 0.1906414031982422, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 462,  Mean reward: -2.1, Mean Entropy: 0.1930328607559204, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 463,  Mean reward: -2.246376811594203, Mean Entropy: 0.17789563536643982, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 464,  Mean reward: -1.4701492537313432, Mean Entropy: 0.15892641246318817, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 465,  Mean reward: -2.289855072463768, Mean Entropy: 0.2247464507818222, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 466,  Mean reward: -3.661764705882353, Mean Entropy: 0.23019103705883026, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 467,  Mean reward: -3.887323943661972, Mean Entropy: 0.23686836659908295, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 468,  Mean reward: -1.6884057971014492, Mean Entropy: 0.20763705670833588, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 469,  Mean reward: -2.3115942028985508, Mean Entropy: 0.2403775304555893, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 470,  Mean reward: -1.6363636363636365, Mean Entropy: 0.28744029998779297, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 471,  Mean reward: -3.4603174603174605, Mean Entropy: 0.21281716227531433, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 472,  Mean reward: -3.9846153846153847, Mean Entropy: 0.19246236979961395, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 473,  Mean reward: -0.40714285714285714, Mean Entropy: 0.23695197701454163, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 474,  Mean reward: -1.4242424242424243, Mean Entropy: 0.2710920572280884, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 475,  Mean reward: -0.38571428571428573, Mean Entropy: 0.22940485179424286, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 476,  Mean reward: 1.0661764705882353, Mean Entropy: 0.17262355983257294, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 477,  Mean reward: -1.9782608695652173, Mean Entropy: 0.15900705754756927, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 478,  Mean reward: -2.801470588235294, Mean Entropy: 0.24519246816635132, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 479,  Mean reward: -3.13768115942029, Mean Entropy: 0.2460634559392929, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 480,  Mean reward: -3.196969696969697, Mean Entropy: 0.26261845231056213, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 481,  Mean reward: -3.0074626865671643, Mean Entropy: 0.2024933397769928, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 482,  Mean reward: -1.6338028169014085, Mean Entropy: 0.18683096766471863, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 483,  Mean reward: -2.801470588235294, Mean Entropy: 0.18974435329437256, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 484,  Mean reward: -1.8943661971830985, Mean Entropy: 0.21318137645721436, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 485,  Mean reward: -2.962686567164179, Mean Entropy: 0.1940787434577942, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 486,  Mean reward: -0.4859154929577465, Mean Entropy: 0.22097711265087128, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 487,  Mean reward: -2.6417910447761193, Mean Entropy: 0.20920991897583008, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 488,  Mean reward: -5.2846153846153845, Mean Entropy: 0.18349811434745789, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 489,  Mean reward: -3.242857142857143, Mean Entropy: 0.21645042300224304, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 490,  Mean reward: -2.242424242424242, Mean Entropy: 0.18048308789730072, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 491,  Mean reward: -1.9191176470588236, Mean Entropy: 0.13956773281097412, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.80s
Iteration: 492,  Mean reward: -1.8529411764705883, Mean Entropy: 0.1439918577671051, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 493,  Mean reward: -3.051470588235294, Mean Entropy: 0.14959584176540375, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 494,  Mean reward: -1.8943661971830985, Mean Entropy: 0.18669746816158295, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 495,  Mean reward: -3.128787878787879, Mean Entropy: 0.17773035168647766, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 496,  Mean reward: -2.4191176470588234, Mean Entropy: 0.1289239227771759, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 497,  Mean reward: -2.601449275362319, Mean Entropy: 0.15695366263389587, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 498,  Mean reward: -3.2, Mean Entropy: 0.15445148944854736, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 499,  Mean reward: -1.3732394366197183, Mean Entropy: 0.1255505532026291, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 500,  Mean reward: -1.0492957746478873, Mean Entropy: 0.12551617622375488, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.78s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -1.9154929577464788, Mean Entropy: 0.14544448256492615, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 502,  Mean reward: -0.7971014492753623, Mean Entropy: 0.12453677505254745, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 503,  Mean reward: -4.297101449275362, Mean Entropy: 0.17478281259536743, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 504,  Mean reward: -2.213235294117647, Mean Entropy: 0.13992534577846527, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 505,  Mean reward: -0.3680555555555556, Mean Entropy: 0.18263499438762665, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 506,  Mean reward: -1.8529411764705883, Mean Entropy: 0.17121653258800507, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 507,  Mean reward: 0.20833333333333334, Mean Entropy: 0.17492590844631195, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 508,  Mean reward: -2.9357142857142855, Mean Entropy: 0.20010529458522797, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 509,  Mean reward: -4.5661764705882355, Mean Entropy: 0.19222992658615112, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 510,  Mean reward: -2.7183098591549295, Mean Entropy: 0.16492970287799835, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 511,  Mean reward: -2.2183098591549295, Mean Entropy: 0.19473811984062195, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 512,  Mean reward: -0.3382352941176471, Mean Entropy: 0.19240891933441162, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 513,  Mean reward: -3.2028985507246377, Mean Entropy: 0.13583451509475708, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 514,  Mean reward: -1.6884057971014492, Mean Entropy: 0.12937353551387787, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 515,  Mean reward: -1.5285714285714285, Mean Entropy: 0.126667320728302, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 516,  Mean reward: -2.1, Mean Entropy: 0.15425479412078857, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 517,  Mean reward: -2.463235294117647, Mean Entropy: 0.1412544697523117, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 518,  Mean reward: -1.9930555555555556, Mean Entropy: 0.15545490384101868, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 519,  Mean reward: -1.9130434782608696, Mean Entropy: 0.22335772216320038, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 520,  Mean reward: -2.289855072463768, Mean Entropy: 0.2645226716995239, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 521,  Mean reward: -3.3059701492537314, Mean Entropy: 0.21801678836345673, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 522,  Mean reward: -4.6893939393939394, Mean Entropy: 0.21426092088222504, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 523,  Mean reward: -1.9930555555555556, Mean Entropy: 0.2583782374858856, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 524,  Mean reward: -3.073529411764706, Mean Entropy: 0.32840225100517273, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 525,  Mean reward: -1.835820895522388, Mean Entropy: 0.2698749601840973, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 526,  Mean reward: -3.6838235294117645, Mean Entropy: 0.23136496543884277, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 527,  Mean reward: 0.9246575342465754, Mean Entropy: 0.23814329504966736, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 528,  Mean reward: -3.5597014925373136, Mean Entropy: 0.2899119257926941, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 529,  Mean reward: -2.1691176470588234, Mean Entropy: 0.2608930468559265, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 530,  Mean reward: -1.1940298507462686, Mean Entropy: 0.2633749842643738, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 531,  Mean reward: -2.962686567164179, Mean Entropy: 0.2272103875875473, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 532,  Mean reward: -1.7361111111111112, Mean Entropy: 0.22282153367996216, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 533,  Mean reward: -2.917910447761194, Mean Entropy: 0.28392621874809265, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 534,  Mean reward: -3.7, Mean Entropy: 0.22307217121124268, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 535,  Mean reward: -4.227941176470588, Mean Entropy: 0.15761840343475342, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 536,  Mean reward: -2.0785714285714287, Mean Entropy: 0.1308155059814453, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 537,  Mean reward: -2.601449275362319, Mean Entropy: 0.15611153841018677, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 538,  Mean reward: -1.8529411764705883, Mean Entropy: 0.13925011456012726, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 539,  Mean reward: -2.2246376811594204, Mean Entropy: 0.15262766182422638, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 540,  Mean reward: -2.6865671641791047, Mean Entropy: 0.14050184190273285, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 541,  Mean reward: -1.6549295774647887, Mean Entropy: 0.09549566358327866, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 542,  Mean reward: -1.7714285714285714, Mean Entropy: 0.142637237906456, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 543,  Mean reward: -2.65, Mean Entropy: 0.14912524819374084, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 544,  Mean reward: -2.1971830985915495, Mean Entropy: 0.18015816807746887, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 545,  Mean reward: -3.2611940298507465, Mean Entropy: 0.15797899663448334, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 546,  Mean reward: -1.0492957746478873, Mean Entropy: 0.1632263958454132, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 547,  Mean reward: -3.8582089552238807, Mean Entropy: 0.1918296068906784, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 548,  Mean reward: -1.2214285714285715, Mean Entropy: 0.12628746032714844, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 549,  Mean reward: -1.6666666666666667, Mean Entropy: 0.12237416207790375, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 550,  Mean reward: -3.5285714285714285, Mean Entropy: 0.1247803121805191, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 551,  Mean reward: -2.6780821917808217, Mean Entropy: 0.16234949231147766, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 552,  Mean reward: 0.3356164383561644, Mean Entropy: 0.12963753938674927, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 553,  Mean reward: -1.2876712328767124, Mean Entropy: 0.13948342204093933, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 554,  Mean reward: -0.4859154929577465, Mean Entropy: 0.10140445083379745, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 555,  Mean reward: -2.1, Mean Entropy: 0.15191569924354553, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 556,  Mean reward: -0.8611111111111112, Mean Entropy: 0.13104280829429626, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 557,  Mean reward: -2.485294117647059, Mean Entropy: 0.1614348590373993, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 558,  Mean reward: -0.18309859154929578, Mean Entropy: 0.12643861770629883, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 559,  Mean reward: -2.9571428571428573, Mean Entropy: 0.18555593490600586, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 560,  Mean reward: -2.0, Mean Entropy: 0.11590864509344101, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 561,  Mean reward: -2.0, Mean Entropy: 0.13273011147975922, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 562,  Mean reward: -3.1159420289855073, Mean Entropy: 0.13907065987586975, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 563,  Mean reward: -2.213235294117647, Mean Entropy: 0.13872700929641724, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 564,  Mean reward: -4.338461538461538, Mean Entropy: 0.15854273736476898, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 565,  Mean reward: -1.0869565217391304, Mean Entropy: 0.167027086019516, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 566,  Mean reward: -2.962686567164179, Mean Entropy: 0.13162025809288025, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 567,  Mean reward: -4.3428571428571425, Mean Entropy: 0.1965722292661667, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 568,  Mean reward: -1.8732394366197183, Mean Entropy: 0.15518637001514435, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 569,  Mean reward: -3.6838235294117645, Mean Entropy: 0.14870212972164154, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 570,  Mean reward: -4.050724637681159, Mean Entropy: 0.15797434747219086, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 571,  Mean reward: -3.3676470588235294, Mean Entropy: 0.1444707214832306, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 572,  Mean reward: -2.871212121212121, Mean Entropy: 0.17278361320495605, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 573,  Mean reward: -1.7361111111111112, Mean Entropy: 0.1445595622062683, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 574,  Mean reward: -3.661764705882353, Mean Entropy: 0.23704811930656433, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 575,  Mean reward: -2.1911764705882355, Mean Entropy: 0.251709520816803, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 576,  Mean reward: -2.601449275362319, Mean Entropy: 0.2253790944814682, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 577,  Mean reward: -1.1940298507462686, Mean Entropy: 0.15444007515907288, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 578,  Mean reward: -1.6126760563380282, Mean Entropy: 0.18147200345993042, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 579,  Mean reward: -0.9485294117647058, Mean Entropy: 0.15696942806243896, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 580,  Mean reward: -1.3309859154929577, Mean Entropy: 0.1922624558210373, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 581,  Mean reward: -1.8308823529411764, Mean Entropy: 0.1187727078795433, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 582,  Mean reward: -1.5285714285714285, Mean Entropy: 0.13089901208877563, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 583,  Mean reward: -0.6285714285714286, Mean Entropy: 0.15028296411037445, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 584,  Mean reward: -2.5694444444444446, Mean Entropy: 0.18998169898986816, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 585,  Mean reward: -3.757575757575758, Mean Entropy: 0.18288908898830414, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 586,  Mean reward: -1.6884057971014492, Mean Entropy: 0.13383887708187103, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 587,  Mean reward: -2.4411764705882355, Mean Entropy: 0.1582547426223755, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 588,  Mean reward: -1.352112676056338, Mean Entropy: 0.17038634419441223, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 589,  Mean reward: -4.050724637681159, Mean Entropy: 0.16524483263492584, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 590,  Mean reward: -2.985074626865672, Mean Entropy: 0.14799371361732483, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 591,  Mean reward: 0.3767123287671233, Mean Entropy: 0.22028318047523499, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 592,  Mean reward: -1.9565217391304348, Mean Entropy: 0.15031705796718597, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 593,  Mean reward: -2.4788732394366195, Mean Entropy: 0.17193703353405, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 594,  Mean reward: 0.07746478873239436, Mean Entropy: 0.12939535081386566, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 595,  Mean reward: -3.6956521739130435, Mean Entropy: 0.17000053822994232, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 596,  Mean reward: -3.739130434782609, Mean Entropy: 0.1790597140789032, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 597,  Mean reward: -1.4857142857142858, Mean Entropy: 0.19925376772880554, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 598,  Mean reward: -0.7887323943661971, Mean Entropy: 0.14844223856925964, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 599,  Mean reward: -1.4375, Mean Entropy: 0.1602291762828827, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 600,  Mean reward: -5.029850746268656, Mean Entropy: 0.19570493698120117, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -4.544117647058823, Mean Entropy: 0.3586813807487488, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 602,  Mean reward: -3.30327868852459, Mean Entropy: 0.4617837369441986, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 603,  Mean reward: -1.8240740740740742, Mean Entropy: 0.5032241940498352, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.53s
Iteration: 604,  Mean reward: -4.304347826086956, Mean Entropy: 0.5553420186042786, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 605,  Mean reward: -3.418181818181818, Mean Entropy: 0.47728556394577026, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 606,  Mean reward: -6.074468085106383, Mean Entropy: 0.524761438369751, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 607,  Mean reward: -2.6956521739130435, Mean Entropy: 0.5184943675994873, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 608,  Mean reward: -3.8617021276595747, Mean Entropy: 0.4855365753173828, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 609,  Mean reward: -4.409090909090909, Mean Entropy: 0.5558409094810486, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 610,  Mean reward: -5.261904761904762, Mean Entropy: 0.5512961745262146, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 611,  Mean reward: -5.956521739130435, Mean Entropy: 0.5052709579467773, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 612,  Mean reward: -4.308510638297872, Mean Entropy: 0.4613445997238159, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 613,  Mean reward: -2.5595238095238093, Mean Entropy: 0.5157102346420288, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 614,  Mean reward: -3.9239130434782608, Mean Entropy: 0.5035665035247803, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 615,  Mean reward: -4.1976744186046515, Mean Entropy: 0.5736405253410339, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 616,  Mean reward: -3.0784313725490198, Mean Entropy: 0.5470064282417297, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.73s
Iteration: 617,  Mean reward: -2.75531914893617, Mean Entropy: 0.5580527186393738, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 618,  Mean reward: -4.5638297872340425, Mean Entropy: 0.592448353767395, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 619,  Mean reward: -3.952830188679245, Mean Entropy: 0.5692500472068787, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 620,  Mean reward: -3.7410714285714284, Mean Entropy: 0.5652382373809814, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 621,  Mean reward: -0.6509433962264151, Mean Entropy: 0.5983823537826538, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 622,  Mean reward: -4.1454545454545455, Mean Entropy: 0.5936790704727173, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 623,  Mean reward: -1.5188679245283019, Mean Entropy: 0.5649609565734863, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 624,  Mean reward: -1.2727272727272727, Mean Entropy: 0.5670287609100342, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.49s
Iteration: 625,  Mean reward: -2.875, Mean Entropy: 0.51116943359375, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 626,  Mean reward: -0.6721311475409836, Mean Entropy: 0.5389566421508789, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 627,  Mean reward: -3.9464285714285716, Mean Entropy: 0.5556625127792358, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 628,  Mean reward: -3.6052631578947367, Mean Entropy: 0.565689742565155, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 629,  Mean reward: -3.491228070175439, Mean Entropy: 0.5520063638687134, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 630,  Mean reward: -3.210526315789474, Mean Entropy: 0.49826547503471375, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 631,  Mean reward: -2.2542372881355934, Mean Entropy: 0.4836786687374115, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 632,  Mean reward: -2.7711864406779663, Mean Entropy: 0.538013219833374, complete_episode_count: 59.0, Gather time: 0.66s, Train time: 1.44s
Iteration: 633,  Mean reward: -3.9909090909090907, Mean Entropy: 0.5842990279197693, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 634,  Mean reward: -2.457627118644068, Mean Entropy: 0.5787836313247681, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 635,  Mean reward: -2.7711864406779663, Mean Entropy: 0.5296972990036011, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 636,  Mean reward: -0.9016393442622951, Mean Entropy: 0.4294710159301758, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 637,  Mean reward: -3.1451612903225805, Mean Entropy: 0.47749048471450806, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 638,  Mean reward: -2.1015625, Mean Entropy: 0.5255248546600342, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 639,  Mean reward: -3.6842105263157894, Mean Entropy: 0.47370657324790955, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 640,  Mean reward: -3.591666666666667, Mean Entropy: 0.5069562196731567, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 641,  Mean reward: -3.169491525423729, Mean Entropy: 0.4498310983181, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 642,  Mean reward: -3.8253968253968256, Mean Entropy: 0.5051627159118652, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 643,  Mean reward: -2.2916666666666665, Mean Entropy: 0.4672675132751465, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 644,  Mean reward: -2.5546875, Mean Entropy: 0.42167532444000244, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 645,  Mean reward: -4.942622950819672, Mean Entropy: 0.44357141852378845, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 646,  Mean reward: -0.15873015873015872, Mean Entropy: 0.4190022349357605, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 647,  Mean reward: -2.390625, Mean Entropy: 0.4512937068939209, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 648,  Mean reward: -1.2615384615384615, Mean Entropy: 0.5232978463172913, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 649,  Mean reward: -4.333333333333333, Mean Entropy: 0.5794267654418945, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 650,  Mean reward: -3.1271186440677967, Mean Entropy: 0.4770411252975464, complete_episode_count: 59.0, Gather time: 0.68s, Train time: 1.40s
Iteration: 651,  Mean reward: -4.967213114754099, Mean Entropy: 0.6322773694992065, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 652,  Mean reward: -3.0789473684210527, Mean Entropy: 0.633569598197937, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 653,  Mean reward: -2.591666666666667, Mean Entropy: 0.4014335870742798, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 654,  Mean reward: -4.492063492063492, Mean Entropy: 0.44278058409690857, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 655,  Mean reward: -3.873015873015873, Mean Entropy: 0.4226316809654236, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 656,  Mean reward: -1.356060606060606, Mean Entropy: 0.3792133927345276, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 657,  Mean reward: -1.7686567164179106, Mean Entropy: 0.4043682813644409, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 658,  Mean reward: -2.5681818181818183, Mean Entropy: 0.46815574169158936, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 659,  Mean reward: -2.1904761904761907, Mean Entropy: 0.4049198031425476, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 660,  Mean reward: -2.6136363636363638, Mean Entropy: 0.39295274019241333, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 661,  Mean reward: -1.5149253731343284, Mean Entropy: 0.39174574613571167, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 662,  Mean reward: -4.134328358208955, Mean Entropy: 0.42497581243515015, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 663,  Mean reward: -1.2878787878787878, Mean Entropy: 0.4103034436702728, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 664,  Mean reward: -2.507936507936508, Mean Entropy: 0.4079752564430237, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 665,  Mean reward: -2.169230769230769, Mean Entropy: 0.38478219509124756, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 666,  Mean reward: -3.2578125, Mean Entropy: 0.4227182865142822, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 667,  Mean reward: -2.6865671641791047, Mean Entropy: 0.3922298848628998, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 668,  Mean reward: -0.984375, Mean Entropy: 0.3428398072719574, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 669,  Mean reward: -1.7686567164179106, Mean Entropy: 0.3532862067222595, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 670,  Mean reward: -2.196969696969697, Mean Entropy: 0.3686152398586273, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 671,  Mean reward: -1.126865671641791, Mean Entropy: 0.39633652567863464, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 672,  Mean reward: 0.8046875, Mean Entropy: 0.31632864475250244, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 673,  Mean reward: -1.9191176470588236, Mean Entropy: 0.30063119530677795, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 674,  Mean reward: -0.4264705882352941, Mean Entropy: 0.3143138587474823, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 675,  Mean reward: -2.207692307692308, Mean Entropy: 0.3447805345058441, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 676,  Mean reward: -3.8492063492063493, Mean Entropy: 0.3959173858165741, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 677,  Mean reward: -2.403225806451613, Mean Entropy: 0.43938812613487244, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 678,  Mean reward: -4.387096774193548, Mean Entropy: 0.45477375388145447, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 679,  Mean reward: -2.1293103448275863, Mean Entropy: 0.292606920003891, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 680,  Mean reward: -1.8307692307692307, Mean Entropy: 0.29867976903915405, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 681,  Mean reward: -3.757575757575758, Mean Entropy: 0.3583259582519531, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 682,  Mean reward: -3.859375, Mean Entropy: 0.4135238528251648, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 683,  Mean reward: 0.5846153846153846, Mean Entropy: 0.35865867137908936, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 684,  Mean reward: -3.3461538461538463, Mean Entropy: 0.331999272108078, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 685,  Mean reward: -3.427536231884058, Mean Entropy: 0.3228929042816162, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 686,  Mean reward: 0.6739130434782609, Mean Entropy: 0.30026009678840637, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 687,  Mean reward: -4.166666666666667, Mean Entropy: 0.3208507001399994, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 688,  Mean reward: -2.962686567164179, Mean Entropy: 0.32213252782821655, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 689,  Mean reward: -3.4603174603174605, Mean Entropy: 0.3558858335018158, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 690,  Mean reward: -4.621212121212121, Mean Entropy: 0.4294130802154541, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 691,  Mean reward: -0.6590909090909091, Mean Entropy: 0.3889896273612976, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 692,  Mean reward: -6.346774193548387, Mean Entropy: 0.40172356367111206, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 693,  Mean reward: -2.6417910447761193, Mean Entropy: 0.3266962766647339, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 694,  Mean reward: -3.8828125, Mean Entropy: 0.28263846039772034, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 695,  Mean reward: -3.015625, Mean Entropy: 0.26510950922966003, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 696,  Mean reward: -3.9338235294117645, Mean Entropy: 0.29357442259788513, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 697,  Mean reward: -3.0846153846153848, Mean Entropy: 0.3314894437789917, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 698,  Mean reward: -1.8142857142857143, Mean Entropy: 0.24229323863983154, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 699,  Mean reward: -2.0434782608695654, Mean Entropy: 0.2464432418346405, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 700,  Mean reward: -2.1911764705882355, Mean Entropy: 0.31400448083877563, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -2.1, Mean Entropy: 0.35735708475112915, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 702,  Mean reward: -4.459677419354839, Mean Entropy: 0.34705743193626404, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 703,  Mean reward: -0.463768115942029, Mean Entropy: 0.28983891010284424, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 704,  Mean reward: -2.3208955223880596, Mean Entropy: 0.3036240041255951, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 705,  Mean reward: -1.8076923076923077, Mean Entropy: 0.3173823952674866, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 706,  Mean reward: -4.522058823529412, Mean Entropy: 0.3366334140300751, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 707,  Mean reward: -1.171641791044776, Mean Entropy: 0.31537580490112305, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 708,  Mean reward: -2.0671641791044775, Mean Entropy: 0.28833019733428955, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 709,  Mean reward: -1.6029411764705883, Mean Entropy: 0.2914290428161621, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 710,  Mean reward: -3.5597014925373136, Mean Entropy: 0.3326572775840759, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 711,  Mean reward: -2.4, Mean Entropy: 0.3123340308666229, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 712,  Mean reward: -4.363636363636363, Mean Entropy: 0.3118117153644562, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 713,  Mean reward: -5.074626865671642, Mean Entropy: 0.29551464319229126, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 714,  Mean reward: -2.4191176470588234, Mean Entropy: 0.32886889576911926, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 715,  Mean reward: -0.7971014492753623, Mean Entropy: 0.2855926752090454, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 716,  Mean reward: -1.036764705882353, Mean Entropy: 0.27469244599342346, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 717,  Mean reward: -2.6641791044776117, Mean Entropy: 0.2628360986709595, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 718,  Mean reward: -1.791044776119403, Mean Entropy: 0.27885550260543823, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 719,  Mean reward: -1.5285714285714285, Mean Entropy: 0.30129730701446533, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 720,  Mean reward: -1.9393939393939394, Mean Entropy: 0.3252508044242859, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 721,  Mean reward: -0.17391304347826086, Mean Entropy: 0.25387293100357056, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 722,  Mean reward: -2.0, Mean Entropy: 0.26084065437316895, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 723,  Mean reward: -4.586956521739131, Mean Entropy: 0.39325714111328125, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 724,  Mean reward: -4.119047619047619, Mean Entropy: 0.31013792753219604, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 725,  Mean reward: -0.9785714285714285, Mean Entropy: 0.238418310880661, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 726,  Mean reward: -2.3115942028985508, Mean Entropy: 0.2104642242193222, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 727,  Mean reward: -3.8358208955223883, Mean Entropy: 0.27622443437576294, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 728,  Mean reward: -1.9565217391304348, Mean Entropy: 0.26884937286376953, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 729,  Mean reward: -3.106060606060606, Mean Entropy: 0.31243187189102173, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 730,  Mean reward: 0.028985507246376812, Mean Entropy: 0.2841726839542389, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 731,  Mean reward: -2.6194029850746268, Mean Entropy: 0.30139872431755066, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 732,  Mean reward: -1.8307692307692307, Mean Entropy: 0.2842053174972534, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 733,  Mean reward: -2.213235294117647, Mean Entropy: 0.24042317271232605, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 734,  Mean reward: -0.4855072463768116, Mean Entropy: 0.19225403666496277, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 735,  Mean reward: 0.09859154929577464, Mean Entropy: 0.18821458518505096, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 736,  Mean reward: -3.0294117647058822, Mean Entropy: 0.2834813594818115, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 737,  Mean reward: -0.22535211267605634, Mean Entropy: 0.27200472354888916, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 738,  Mean reward: -2.246376811594203, Mean Entropy: 0.3026909828186035, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 739,  Mean reward: -3.5597014925373136, Mean Entropy: 0.3536648452281952, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 740,  Mean reward: -4.553846153846154, Mean Entropy: 0.35622888803482056, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 741,  Mean reward: -2.5, Mean Entropy: 0.3278302252292633, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 742,  Mean reward: -1.5, Mean Entropy: 0.26192840933799744, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 743,  Mean reward: -2.5073529411764706, Mean Entropy: 0.2684100270271301, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 744,  Mean reward: -5.142857142857143, Mean Entropy: 0.37655746936798096, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 745,  Mean reward: -0.75, Mean Entropy: 0.25515708327293396, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 746,  Mean reward: -2.8260869565217392, Mean Entropy: 0.2884392738342285, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 747,  Mean reward: -3.443548387096774, Mean Entropy: 0.30104100704193115, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 748,  Mean reward: -4.907692307692308, Mean Entropy: 0.3237903118133545, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 749,  Mean reward: -1.7794117647058822, Mean Entropy: 0.3036365509033203, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 750,  Mean reward: -1.5149253731343284, Mean Entropy: 0.2550516128540039, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 751,  Mean reward: -2.3857142857142857, Mean Entropy: 0.19208990037441254, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 752,  Mean reward: -1.1492537313432836, Mean Entropy: 0.19687877595424652, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 753,  Mean reward: -2.9357142857142855, Mean Entropy: 0.2486853301525116, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 754,  Mean reward: -2.5073529411764706, Mean Entropy: 0.25276970863342285, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 755,  Mean reward: -1.2642857142857142, Mean Entropy: 0.25198858976364136, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 756,  Mean reward: -2.013888888888889, Mean Entropy: 0.24239802360534668, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 757,  Mean reward: -1.1785714285714286, Mean Entropy: 0.26097652316093445, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 758,  Mean reward: -3.8142857142857145, Mean Entropy: 0.26578018069267273, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 759,  Mean reward: -3.723076923076923, Mean Entropy: 0.27163922786712646, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 760,  Mean reward: -2.8257575757575757, Mean Entropy: 0.23089227080345154, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 761,  Mean reward: -4.708955223880597, Mean Entropy: 0.26943811774253845, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 762,  Mean reward: -1.5588235294117647, Mean Entropy: 0.22822698950767517, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 763,  Mean reward: -2.0357142857142856, Mean Entropy: 0.2031671702861786, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 764,  Mean reward: -1.792857142857143, Mean Entropy: 0.24354588985443115, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 765,  Mean reward: -1.5588235294117647, Mean Entropy: 0.2609257400035858, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 766,  Mean reward: -2.7142857142857144, Mean Entropy: 0.2412959784269333, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 767,  Mean reward: -1.792857142857143, Mean Entropy: 0.1836366355419159, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 768,  Mean reward: -1.3805970149253732, Mean Entropy: 0.26204586029052734, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 769,  Mean reward: -3.7686567164179103, Mean Entropy: 0.3029521107673645, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 770,  Mean reward: -3.2214285714285715, Mean Entropy: 0.24478936195373535, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 771,  Mean reward: -2.9785714285714286, Mean Entropy: 0.22754359245300293, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 772,  Mean reward: -2.4191176470588234, Mean Entropy: 0.2254323959350586, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 773,  Mean reward: -1.091549295774648, Mean Entropy: 0.23901650309562683, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 774,  Mean reward: -1.3106060606060606, Mean Entropy: 0.23401443660259247, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 775,  Mean reward: -4.385714285714286, Mean Entropy: 0.29169148206710815, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 776,  Mean reward: -4.111940298507463, Mean Entropy: 0.314688503742218, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 777,  Mean reward: -3.073529411764706, Mean Entropy: 0.2709318995475769, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 778,  Mean reward: -2.2681159420289854, Mean Entropy: 0.23188087344169617, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 779,  Mean reward: -1.710144927536232, Mean Entropy: 0.26768064498901367, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 780,  Mean reward: -1.9930555555555556, Mean Entropy: 0.28646156191825867, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 781,  Mean reward: -2.111940298507463, Mean Entropy: 0.3042994439601898, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 782,  Mean reward: -2.1384615384615384, Mean Entropy: 0.31259140372276306, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 783,  Mean reward: -2.7794117647058822, Mean Entropy: 0.3975803554058075, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 784,  Mean reward: 0.5, Mean Entropy: 0.35782670974731445, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 785,  Mean reward: -0.5970149253731343, Mean Entropy: 0.33465301990509033, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 786,  Mean reward: -2.590909090909091, Mean Entropy: 0.2621898353099823, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 787,  Mean reward: -4.477611940298507, Mean Entropy: 0.2529962658882141, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 788,  Mean reward: 0.6521739130434783, Mean Entropy: 0.22052466869354248, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 789,  Mean reward: -1.9154929577464788, Mean Entropy: 0.22685770690441132, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 790,  Mean reward: -1.126865671641791, Mean Entropy: 0.2520279586315155, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 791,  Mean reward: -1.6338028169014085, Mean Entropy: 0.3064953088760376, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 792,  Mean reward: -1.5149253731343284, Mean Entropy: 0.2761481702327728, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 793,  Mean reward: -2.213235294117647, Mean Entropy: 0.33706772327423096, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 794,  Mean reward: -1.9, Mean Entropy: 0.2975093424320221, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 795,  Mean reward: -4.363636363636363, Mean Entropy: 0.32829520106315613, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 796,  Mean reward: -1.1044776119402986, Mean Entropy: 0.3198370337486267, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 797,  Mean reward: -1.6029411764705883, Mean Entropy: 0.26428917050361633, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 798,  Mean reward: -2.1343283582089554, Mean Entropy: 0.26404520869255066, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 799,  Mean reward: -1.0492957746478873, Mean Entropy: 0.21076719462871552, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 800,  Mean reward: -2.0347222222222223, Mean Entropy: 0.2129872739315033, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -1.4857142857142858, Mean Entropy: 0.192798912525177, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 802,  Mean reward: 0.7430555555555556, Mean Entropy: 0.16974693536758423, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 803,  Mean reward: -2.708955223880597, Mean Entropy: 0.20683424174785614, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 804,  Mean reward: -3.477272727272727, Mean Entropy: 0.23955874145030975, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 805,  Mean reward: -3.063380281690141, Mean Entropy: 0.2802303433418274, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 806,  Mean reward: -3.889705882352941, Mean Entropy: 0.3279827833175659, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 807,  Mean reward: -2.4, Mean Entropy: 0.21983717381954193, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 808,  Mean reward: -0.5289855072463768, Mean Entropy: 0.21549785137176514, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 809,  Mean reward: -3.582089552238806, Mean Entropy: 0.25066399574279785, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 810,  Mean reward: -4.106060606060606, Mean Entropy: 0.24998998641967773, complete_episode_count: 66.0, Gather time: 0.69s, Train time: 0.71s
Iteration: 811,  Mean reward: -2.893939393939394, Mean Entropy: 0.25340163707733154, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 812,  Mean reward: -1.355072463768116, Mean Entropy: 0.23043571412563324, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 813,  Mean reward: -1.355072463768116, Mean Entropy: 0.200344055891037, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 814,  Mean reward: -2.65, Mean Entropy: 0.1981489658355713, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 815,  Mean reward: -1.5808823529411764, Mean Entropy: 0.16657887399196625, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 816,  Mean reward: -1.6029411764705883, Mean Entropy: 0.1879948377609253, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 817,  Mean reward: -2.6641791044776117, Mean Entropy: 0.18453392386436462, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 818,  Mean reward: -1.710144927536232, Mean Entropy: 0.1798490732908249, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 819,  Mean reward: -3.427536231884058, Mean Entropy: 0.2246028184890747, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 820,  Mean reward: -1.5808823529411764, Mean Entropy: 0.20953118801116943, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 821,  Mean reward: -3.6956521739130435, Mean Entropy: 0.2718747556209564, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 822,  Mean reward: -2.485294117647059, Mean Entropy: 0.2665312886238098, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 823,  Mean reward: -1.6363636363636365, Mean Entropy: 0.21650569140911102, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 824,  Mean reward: -3.2196969696969697, Mean Entropy: 0.24346157908439636, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 825,  Mean reward: -3.5714285714285716, Mean Entropy: 0.23769763112068176, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 826,  Mean reward: -0.9926470588235294, Mean Entropy: 0.21065282821655273, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 827,  Mean reward: -4.363636363636363, Mean Entropy: 0.28644856810569763, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 828,  Mean reward: -1.108695652173913, Mean Entropy: 0.25024738907814026, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 829,  Mean reward: -0.9926470588235294, Mean Entropy: 0.23130953311920166, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 830,  Mean reward: -3.1076923076923078, Mean Entropy: 0.17661303281784058, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 831,  Mean reward: -1.2887323943661972, Mean Entropy: 0.21089652180671692, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 832,  Mean reward: -2.3208955223880596, Mean Entropy: 0.20750854909420013, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 833,  Mean reward: -1.55, Mean Entropy: 0.2287108600139618, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 834,  Mean reward: -3.9029850746268657, Mean Entropy: 0.21978077292442322, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 835,  Mean reward: -1.4857142857142858, Mean Entropy: 0.2192191332578659, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 836,  Mean reward: -1.5808823529411764, Mean Entropy: 0.18288174271583557, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 837,  Mean reward: 0.4652777777777778, Mean Entropy: 0.18077117204666138, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 838,  Mean reward: -0.04861111111111111, Mean Entropy: 0.1920662820339203, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 839,  Mean reward: -2.1470588235294117, Mean Entropy: 0.2314266860485077, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 840,  Mean reward: -3.323529411764706, Mean Entropy: 0.24035899341106415, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 841,  Mean reward: -1.7462686567164178, Mean Entropy: 0.24400298297405243, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 842,  Mean reward: -1.934782608695652, Mean Entropy: 0.23550239205360413, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 843,  Mean reward: -0.2391304347826087, Mean Entropy: 0.2059212625026703, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 844,  Mean reward: -2.0, Mean Entropy: 0.1898808777332306, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 845,  Mean reward: -1.7569444444444444, Mean Entropy: 0.14761295914649963, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 846,  Mean reward: -2.013888888888889, Mean Entropy: 0.22140567004680634, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 847,  Mean reward: -1.1304347826086956, Mean Entropy: 0.23556487262248993, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 848,  Mean reward: -1.9191176470588236, Mean Entropy: 0.2228519171476364, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 849,  Mean reward: -5.7164179104477615, Mean Entropy: 0.22248820960521698, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 850,  Mean reward: -1.3768115942028984, Mean Entropy: 0.18037346005439758, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 851,  Mean reward: -3.449275362318841, Mean Entropy: 0.1965862214565277, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 852,  Mean reward: -3.507142857142857, Mean Entropy: 0.24446117877960205, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 853,  Mean reward: -2.713235294117647, Mean Entropy: 0.3348730504512787, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 854,  Mean reward: -1.352112676056338, Mean Entropy: 0.315999835729599, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 855,  Mean reward: -1.523076923076923, Mean Entropy: 0.2997099757194519, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 856,  Mean reward: -3.427536231884058, Mean Entropy: 0.26084017753601074, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 857,  Mean reward: -3.216417910447761, Mean Entropy: 0.2874283492565155, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 858,  Mean reward: -2.1911764705882355, Mean Entropy: 0.24303185939788818, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 859,  Mean reward: -3.6267605633802815, Mean Entropy: 0.18766874074935913, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 860,  Mean reward: -5.154411764705882, Mean Entropy: 0.26046067476272583, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 861,  Mean reward: -2.601449275362319, Mean Entropy: 0.2513519525527954, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 862,  Mean reward: -3.537313432835821, Mean Entropy: 0.2440892904996872, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 863,  Mean reward: -0.5070422535211268, Mean Entropy: 0.23119740188121796, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 864,  Mean reward: -1.2428571428571429, Mean Entropy: 0.226463183760643, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 865,  Mean reward: -0.6458333333333334, Mean Entropy: 0.20558255910873413, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 866,  Mean reward: -1.6549295774647887, Mean Entropy: 0.2309253215789795, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 867,  Mean reward: -2.1214285714285714, Mean Entropy: 0.20491008460521698, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 868,  Mean reward: -1.3732394366197183, Mean Entropy: 0.2108069509267807, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 869,  Mean reward: -3.427536231884058, Mean Entropy: 0.23797163367271423, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 870,  Mean reward: -1.6136363636363635, Mean Entropy: 0.23668110370635986, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 871,  Mean reward: -2.485294117647059, Mean Entropy: 0.2521238923072815, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 872,  Mean reward: 0.20714285714285716, Mean Entropy: 0.2611023187637329, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 873,  Mean reward: -2.289855072463768, Mean Entropy: 0.3830993175506592, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 874,  Mean reward: -3.2142857142857144, Mean Entropy: 0.5184407830238342, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 875,  Mean reward: -3.2358490566037736, Mean Entropy: 0.5174015164375305, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 876,  Mean reward: -2.093220338983051, Mean Entropy: 0.2667843997478485, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 877,  Mean reward: -2.6641791044776117, Mean Entropy: 0.2882325053215027, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 878,  Mean reward: -0.5507246376811594, Mean Entropy: 0.2665589451789856, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 879,  Mean reward: -0.9357142857142857, Mean Entropy: 0.2863333225250244, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 880,  Mean reward: -2.287878787878788, Mean Entropy: 0.3175450563430786, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 881,  Mean reward: -1.6666666666666667, Mean Entropy: 0.35061466693878174, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 882,  Mean reward: -1.2230769230769232, Mean Entropy: 0.24713627994060516, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 883,  Mean reward: -1.536764705882353, Mean Entropy: 0.2645873427391052, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 884,  Mean reward: -2.057142857142857, Mean Entropy: 0.2668582797050476, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 885,  Mean reward: -4.106060606060606, Mean Entropy: 0.26233088970184326, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 886,  Mean reward: -2.985074626865672, Mean Entropy: 0.3040963411331177, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 887,  Mean reward: -1.0151515151515151, Mean Entropy: 0.3237324655056, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 888,  Mean reward: -3.640625, Mean Entropy: 0.3255804777145386, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 889,  Mean reward: -3.8492063492063493, Mean Entropy: 0.3257136940956116, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 890,  Mean reward: -3.9384615384615387, Mean Entropy: 0.2682475745677948, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 891,  Mean reward: -2.485294117647059, Mean Entropy: 0.24789588153362274, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 892,  Mean reward: -0.9571428571428572, Mean Entropy: 0.20769581198692322, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 893,  Mean reward: -3.3923076923076922, Mean Entropy: 0.2782360911369324, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 894,  Mean reward: -4.156716417910448, Mean Entropy: 0.272969126701355, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 895,  Mean reward: -1.9154929577464788, Mean Entropy: 0.2683377265930176, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 896,  Mean reward: -4.053846153846154, Mean Entropy: 0.2480183094739914, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 897,  Mean reward: -1.8357142857142856, Mean Entropy: 0.18968980014324188, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 898,  Mean reward: -0.8611111111111112, Mean Entropy: 0.24534854292869568, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 899,  Mean reward: -4.84375, Mean Entropy: 0.30269962549209595, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 900,  Mean reward: -1.5714285714285714, Mean Entropy: 0.2702459990978241, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -3.0294117647058822, Mean Entropy: 0.31237441301345825, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 902,  Mean reward: -1.710144927536232, Mean Entropy: 0.2849372625350952, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 903,  Mean reward: -2.485294117647059, Mean Entropy: 0.2853236198425293, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 904,  Mean reward: -2.4692307692307693, Mean Entropy: 0.2884555459022522, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 905,  Mean reward: -4.25, Mean Entropy: 0.49622535705566406, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 906,  Mean reward: -1.7796610169491525, Mean Entropy: 0.22605793178081512, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 907,  Mean reward: -3.9338235294117645, Mean Entropy: 0.29212215542793274, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 908,  Mean reward: -1.4857142857142858, Mean Entropy: 0.28243300318717957, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 909,  Mean reward: -2.388059701492537, Mean Entropy: 0.3151051700115204, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 910,  Mean reward: -0.9571428571428572, Mean Entropy: 0.2766483724117279, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 911,  Mean reward: -2.463235294117647, Mean Entropy: 0.2785317301750183, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 912,  Mean reward: -2.871212121212121, Mean Entropy: 0.281277060508728, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 913,  Mean reward: -4.588235294117647, Mean Entropy: 0.28835949301719666, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 914,  Mean reward: -2.463235294117647, Mean Entropy: 0.2575189769268036, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 915,  Mean reward: -3.2578125, Mean Entropy: 0.21960453689098358, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 916,  Mean reward: -3.051470588235294, Mean Entropy: 0.1927703619003296, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 917,  Mean reward: -1.3309859154929577, Mean Entropy: 0.23376989364624023, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 918,  Mean reward: -1.792857142857143, Mean Entropy: 0.2258899062871933, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 919,  Mean reward: -2.5, Mean Entropy: 0.24789981544017792, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 920,  Mean reward: -1.0704225352112675, Mean Entropy: 0.19320589303970337, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 921,  Mean reward: -3.739130434782609, Mean Entropy: 0.24572543799877167, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 922,  Mean reward: -2.111940298507463, Mean Entropy: 0.25015169382095337, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 923,  Mean reward: -3.3461538461538463, Mean Entropy: 0.25155529379844666, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 924,  Mean reward: -0.9705882352941176, Mean Entropy: 0.2093832939863205, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 925,  Mean reward: -0.5072463768115942, Mean Entropy: 0.19582895934581757, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 926,  Mean reward: -4.898550724637682, Mean Entropy: 0.264684796333313, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 927,  Mean reward: -5.649253731343284, Mean Entropy: 0.23197798430919647, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 928,  Mean reward: -2.5579710144927534, Mean Entropy: 0.20129643380641937, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 929,  Mean reward: -3.389705882352941, Mean Entropy: 0.19759324193000793, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 930,  Mean reward: -3.301470588235294, Mean Entropy: 0.21251356601715088, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 931,  Mean reward: -1.4420289855072463, Mean Entropy: 0.17192864418029785, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 932,  Mean reward: -3.889705882352941, Mean Entropy: 0.24788439273834229, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 933,  Mean reward: -3.242857142857143, Mean Entropy: 0.2306182086467743, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 934,  Mean reward: -1.3309859154929577, Mean Entropy: 0.2225610464811325, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 935,  Mean reward: -2.847826086956522, Mean Entropy: 0.2415250539779663, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 936,  Mean reward: -3.3059701492537314, Mean Entropy: 0.24669721722602844, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 937,  Mean reward: -3.073529411764706, Mean Entropy: 0.22622451186180115, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 938,  Mean reward: -0.8188405797101449, Mean Entropy: 0.20350655913352966, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 939,  Mean reward: -2.4191176470588234, Mean Entropy: 0.227853924036026, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 940,  Mean reward: -2.8484848484848486, Mean Entropy: 0.208257257938385, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 941,  Mean reward: -3.8805970149253732, Mean Entropy: 0.2302011251449585, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 942,  Mean reward: -1.5071428571428571, Mean Entropy: 0.18983781337738037, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 943,  Mean reward: -5.350746268656716, Mean Entropy: 0.24822477996349335, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 944,  Mean reward: -2.5902777777777777, Mean Entropy: 0.19857585430145264, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 945,  Mean reward: -2.7816901408450705, Mean Entropy: 0.2524706721305847, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 946,  Mean reward: -2.176056338028169, Mean Entropy: 0.1993468999862671, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 947,  Mean reward: -1.6884057971014492, Mean Entropy: 0.1950128972530365, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 948,  Mean reward: -3.0294117647058822, Mean Entropy: 0.23039484024047852, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 949,  Mean reward: -2.3115942028985508, Mean Entropy: 0.21219703555107117, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 950,  Mean reward: -4.776119402985074, Mean Entropy: 0.22905763983726501, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 951,  Mean reward: -2.985294117647059, Mean Entropy: 0.23247985541820526, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 952,  Mean reward: -2.8955223880597014, Mean Entropy: 0.22913450002670288, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 953,  Mean reward: -2.3115942028985508, Mean Entropy: 0.22588229179382324, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 954,  Mean reward: -1.7686567164179106, Mean Entropy: 0.20725581049919128, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 955,  Mean reward: -4.978571428571429, Mean Entropy: 0.23949775099754333, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 956,  Mean reward: -3.1742424242424243, Mean Entropy: 0.2231835573911667, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 957,  Mean reward: -2.013888888888889, Mean Entropy: 0.2358313500881195, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 958,  Mean reward: -3.2611940298507465, Mean Entropy: 0.19048643112182617, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 959,  Mean reward: -0.09027777777777778, Mean Entropy: 0.2055894136428833, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 960,  Mean reward: -1.0342465753424657, Mean Entropy: 0.22096262872219086, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 961,  Mean reward: -2.213235294117647, Mean Entropy: 0.20069977641105652, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 962,  Mean reward: -3.216417910447761, Mean Entropy: 0.16815678775310516, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 963,  Mean reward: -1.7238805970149254, Mean Entropy: 0.2012251317501068, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 964,  Mean reward: 0.3356164383561644, Mean Entropy: 0.16629673540592194, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 965,  Mean reward: -3.3455882352941178, Mean Entropy: 0.21807953715324402, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 966,  Mean reward: -3.639705882352941, Mean Entropy: 0.23579978942871094, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 967,  Mean reward: -2.5579710144927534, Mean Entropy: 0.25031691789627075, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 968,  Mean reward: -1.9191176470588236, Mean Entropy: 0.23002740740776062, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 969,  Mean reward: -3.323529411764706, Mean Entropy: 0.20547978579998016, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 970,  Mean reward: -0.4647887323943662, Mean Entropy: 0.20084676146507263, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 971,  Mean reward: -4.630434782608695, Mean Entropy: 0.231632262468338, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 972,  Mean reward: -3.4857142857142858, Mean Entropy: 0.2019195556640625, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 973,  Mean reward: -4.6, Mean Entropy: 0.22639715671539307, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 974,  Mean reward: -2.0434782608695654, Mean Entropy: 0.20984868705272675, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 975,  Mean reward: 0.6301369863013698, Mean Entropy: 0.21822583675384521, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 976,  Mean reward: -3.073529411764706, Mean Entropy: 0.1809254139661789, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.49it/s]100%|| 1/1 [00:00<00:00,  1.49it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.82it/s]100%|| 1/1 [00:00<00:00,  1.82it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.4 0.6 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 0.5 0.1 0.2 0.1 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.1 0.9 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 0.5 0.1 0.2 0.1 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.77it/s]100%|| 1/1 [00:00<00:00,  1.77it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 3 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.5 s_ (3, 1)
  s:             (3, 1) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 2)
  s:             (4, 2) a 2 action_probs [0.0 0.0 0.9 0.0 0.1 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 3 steps, Captured: True Reward: -13.5


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 0.9 0.0 0.1 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.50 +/- 0.50
   Lengths :[3 2]
Average return: -2.75 +/- 10.75
   Returns :[-13.5 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.75
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: -2.25
  std over seeds: 0.3535533905932738
  per seed: [-2.000 -2.000 -2.750]

success_rate.......
  avg over seeds: 0.5
  std over seeds: 0.0
  per seed: [0.500 0.500 0.500]

batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: EMB
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: q
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
node_dim: 7
lstm_on: True
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy_EMB_LSTM(
  (FE): FeatureExtractor(
    (gat): GATv2(7, 24, num_layers=5)
  )
  (LSTM): EMB_LSTM(
    (lstm): LSTM(24, 24)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (theta6_v): Linear(in_features=24, out_features=24, bias=True)
    (theta7_v): Linear(in_features=24, out_features=24, bias=True)
    (theta5_v): Linear(in_features=48, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with GATv2 feature extraction and LSTM applied on node embeddings
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
LSTM.lstm.weight_ih_l0   [96, 24]     requires_grad=True
LSTM.lstm.weight_hh_l0   [96, 24]     requires_grad=True
LSTM.lstm.bias_ih_l0     [96]         requires_grad=True
LSTM.lstm.bias_hh_l0     [96]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.theta6_v.weight        [24, 24]     requires_grad=True
V.theta6_v.bias          [24]         requires_grad=True
V.theta7_v.weight        [24, 24]     requires_grad=True
V.theta7_v.bias          [24]         requires_grad=True
V.theta5_v.weight        [1, 48]      requires_grad=True
V.theta5_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 12722
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -6.136363636363637, Mean Entropy: 0.960297703742981, complete_episode_count: 44.0, Gather time: 5.36s, Train time: 3.48s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -3.5875, Mean Entropy: 0.9891781806945801, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 2,  Mean reward: -4.023255813953488, Mean Entropy: 1.0036176443099976, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -2.3555555555555556, Mean Entropy: 0.8880933523178101, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 4,  Mean reward: -2.909090909090909, Mean Entropy: 0.9458551406860352, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 5,  Mean reward: -3.6707317073170733, Mean Entropy: 0.9241940379142761, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 6,  Mean reward: -5.785714285714286, Mean Entropy: 0.9386352300643921, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 7,  Mean reward: -5.328947368421052, Mean Entropy: 0.8664330840110779, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 8,  Mean reward: -6.744186046511628, Mean Entropy: 0.9097553491592407, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 9,  Mean reward: -3.6547619047619047, Mean Entropy: 0.938636302947998, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 10,  Mean reward: -5.151162790697675, Mean Entropy: 0.9025334119796753, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 11,  Mean reward: -5.761363636363637, Mean Entropy: 0.9386350512504578, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 12,  Mean reward: -4.6395348837209305, Mean Entropy: 0.9458567500114441, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 13,  Mean reward: -4.904761904761905, Mean Entropy: 0.945855975151062, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 14,  Mean reward: -5.921052631578948, Mean Entropy: 0.960294246673584, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 15,  Mean reward: -4.732558139534884, Mean Entropy: 0.9891771078109741, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 16,  Mean reward: -3.175, Mean Entropy: 1.0036184787750244, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.55s
Iteration: 17,  Mean reward: -6.659574468085107, Mean Entropy: 0.9747340083122253, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 18,  Mean reward: -5.526315789473684, Mean Entropy: 0.8880926966667175, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 19,  Mean reward: -4.353658536585366, Mean Entropy: 0.9602949619293213, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 20,  Mean reward: -3.227272727272727, Mean Entropy: 0.9530729651451111, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 21,  Mean reward: -3.197674418604651, Mean Entropy: 0.9386345148086548, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 22,  Mean reward: -6.045454545454546, Mean Entropy: 0.8880822658538818, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 23,  Mean reward: -5.226190476190476, Mean Entropy: 0.9891608953475952, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 24,  Mean reward: -3.532608695652174, Mean Entropy: 0.9314133524894714, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 25,  Mean reward: -3.5714285714285716, Mean Entropy: 0.9819557666778564, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 26,  Mean reward: -5.0625, Mean Entropy: 0.9530723094940186, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.74s
Iteration: 27,  Mean reward: -3.7195121951219514, Mean Entropy: 0.9314133524894714, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 28,  Mean reward: -6.313953488372093, Mean Entropy: 0.9819416403770447, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 29,  Mean reward: -2.2325581395348837, Mean Entropy: 0.9747235178947449, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 30,  Mean reward: -3.8372093023255816, Mean Entropy: 0.9314104914665222, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 31,  Mean reward: -4.348837209302325, Mean Entropy: 0.9819516539573669, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 32,  Mean reward: -3.573170731707317, Mean Entropy: 0.9530717730522156, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 33,  Mean reward: -5.340909090909091, Mean Entropy: 0.9747189283370972, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 34,  Mean reward: -3.625, Mean Entropy: 0.989168643951416, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 35,  Mean reward: -4.609756097560975, Mean Entropy: 0.9458529353141785, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.59s
Iteration: 36,  Mean reward: -3.8214285714285716, Mean Entropy: 0.9169637560844421, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.61s
Iteration: 37,  Mean reward: -5.585365853658536, Mean Entropy: 0.9458394050598145, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.60s
Iteration: 38,  Mean reward: -3.2209302325581395, Mean Entropy: 0.9602921009063721, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.58s
Iteration: 39,  Mean reward: -8.536585365853659, Mean Entropy: 0.9314116835594177, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.57s
Iteration: 40,  Mean reward: -4.174418604651163, Mean Entropy: 0.9097535014152527, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 41,  Mean reward: -5.559523809523809, Mean Entropy: 1.0036171674728394, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 42,  Mean reward: -3.9642857142857144, Mean Entropy: 0.9602941870689392, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 43,  Mean reward: -5.378048780487805, Mean Entropy: 0.8880891799926758, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.55s
Iteration: 44,  Mean reward: -5.487179487179487, Mean Entropy: 0.960281252861023, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 45,  Mean reward: -4.77027027027027, Mean Entropy: 0.9097516536712646, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 46,  Mean reward: -3.0875, Mean Entropy: 0.9819571375846863, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 47,  Mean reward: -5.4875, Mean Entropy: 0.9458563327789307, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 48,  Mean reward: -6.7439024390243905, Mean Entropy: 0.9747370481491089, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 49,  Mean reward: -7.55, Mean Entropy: 0.9169756770133972, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 50,  Mean reward: -4.204545454545454, Mean Entropy: 0.9025349617004395, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 51,  Mean reward: -3.616279069767442, Mean Entropy: 0.9241957664489746, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 52,  Mean reward: -7.0, Mean Entropy: 0.9675173759460449, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 53,  Mean reward: -4.27906976744186, Mean Entropy: 0.916975736618042, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 54,  Mean reward: -4.333333333333333, Mean Entropy: 0.8592135906219482, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 55,  Mean reward: -2.8, Mean Entropy: 0.924196183681488, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 56,  Mean reward: -5.45945945945946, Mean Entropy: 0.9386365413665771, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.57s
Iteration: 57,  Mean reward: -5.375, Mean Entropy: 0.9458562731742859, complete_episode_count: 36.0, Gather time: 0.54s, Train time: 1.55s
Iteration: 58,  Mean reward: -4.693181818181818, Mean Entropy: 0.9675167798995972, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 59,  Mean reward: -4.1477272727272725, Mean Entropy: 0.9675166010856628, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 60,  Mean reward: -4.916666666666667, Mean Entropy: 0.9386358261108398, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.56s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 61,  Mean reward: -1.8636363636363635, Mean Entropy: 0.9530758261680603, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 62,  Mean reward: -4.5, Mean Entropy: 0.8953135013580322, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 63,  Mean reward: -6.097560975609756, Mean Entropy: 0.9097476005554199, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 64,  Mean reward: -1.7391304347826086, Mean Entropy: 0.8953026533126831, complete_episode_count: 46.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 65,  Mean reward: -3.128205128205128, Mean Entropy: 0.9675021171569824, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.59s
Iteration: 66,  Mean reward: -4.666666666666667, Mean Entropy: 0.97471684217453, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.59s
Iteration: 67,  Mean reward: -5.130952380952381, Mean Entropy: 0.9602606296539307, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.56s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 68,  Mean reward: -1.6979166666666667, Mean Entropy: 0.9458131790161133, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 69,  Mean reward: -4.659090909090909, Mean Entropy: 1.0035589933395386, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 70,  Mean reward: -3.9875, Mean Entropy: 0.9458035826683044, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 71,  Mean reward: -6.181818181818182, Mean Entropy: 0.9169146418571472, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 72,  Mean reward: -1.7560975609756098, Mean Entropy: 0.9096915125846863, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.58s
Iteration: 73,  Mean reward: -2.3181818181818183, Mean Entropy: 0.9313222169876099, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 74,  Mean reward: -6.085365853658536, Mean Entropy: 0.9457377195358276, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 75,  Mean reward: -4.662790697674419, Mean Entropy: 0.8880070447921753, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 76,  Mean reward: -4.371794871794871, Mean Entropy: 0.9529129862785339, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 77,  Mean reward: -5.209302325581396, Mean Entropy: 0.9672946929931641, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 78,  Mean reward: -3.15, Mean Entropy: 0.9166535139083862, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 79,  Mean reward: -5.5, Mean Entropy: 0.9453555345535278, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 80,  Mean reward: -3.4358974358974357, Mean Entropy: 0.8444013595581055, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 81,  Mean reward: -1.6111111111111112, Mean Entropy: 0.9659785628318787, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 82,  Mean reward: -3.3658536585365852, Mean Entropy: 0.9296500086784363, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 83,  Mean reward: -5.681818181818182, Mean Entropy: 0.9301967024803162, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 84,  Mean reward: -7.304878048780488, Mean Entropy: 0.9313370585441589, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 85,  Mean reward: -3.872093023255814, Mean Entropy: 0.9168323278427124, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 86,  Mean reward: -4.142857142857143, Mean Entropy: 0.9221482276916504, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 87,  Mean reward: -4.1022727272727275, Mean Entropy: 0.9621884226799011, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 88,  Mean reward: -3.988095238095238, Mean Entropy: 0.9934663772583008, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 89,  Mean reward: -8.115384615384615, Mean Entropy: 0.935664176940918, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 90,  Mean reward: -4.4021739130434785, Mean Entropy: 0.9672101736068726, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 91,  Mean reward: -5.011627906976744, Mean Entropy: 0.9153591394424438, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 92,  Mean reward: -1.7934782608695652, Mean Entropy: 0.9280888438224792, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 93,  Mean reward: -3.2674418604651163, Mean Entropy: 0.8511624336242676, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 94,  Mean reward: -2.966666666666667, Mean Entropy: 0.9000608325004578, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 95,  Mean reward: -4.088888888888889, Mean Entropy: 0.8952943086624146, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 96,  Mean reward: -4.313953488372093, Mean Entropy: 0.8767763376235962, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 97,  Mean reward: -3.3137254901960786, Mean Entropy: 0.8423117399215698, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 98,  Mean reward: -5.73404255319149, Mean Entropy: 0.8808335661888123, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 99,  Mean reward: -3.4, Mean Entropy: 0.922290027141571, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.72s
Iteration: 100,  Mean reward: -4.888888888888889, Mean Entropy: 0.8770698308944702, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.53s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -2.9693877551020407, Mean Entropy: 0.8727129697799683, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 102,  Mean reward: -5.0, Mean Entropy: 0.9068691730499268, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 103,  Mean reward: -3.595744680851064, Mean Entropy: 0.8865002989768982, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 104,  Mean reward: -4.427083333333333, Mean Entropy: 0.8197534084320068, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 105,  Mean reward: -3.8260869565217392, Mean Entropy: 0.8366293907165527, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 106,  Mean reward: -2.53125, Mean Entropy: 0.8396133184432983, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 107,  Mean reward: -2.3295454545454546, Mean Entropy: 0.8708853721618652, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 108,  Mean reward: -7.488888888888889, Mean Entropy: 0.817347526550293, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 109,  Mean reward: -3.1203703703703702, Mean Entropy: 0.8586155772209167, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 110,  Mean reward: -3.951923076923077, Mean Entropy: 0.788578450679779, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 111,  Mean reward: -4.724489795918367, Mean Entropy: 0.7611955404281616, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 112,  Mean reward: -4.33, Mean Entropy: 0.7227255702018738, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 113,  Mean reward: -2.4479166666666665, Mean Entropy: 0.7151283025741577, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 114,  Mean reward: -6.078431372549019, Mean Entropy: 0.5881604552268982, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 115,  Mean reward: -3.6923076923076925, Mean Entropy: 0.7120794057846069, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 116,  Mean reward: -2.259259259259259, Mean Entropy: 0.6737193465232849, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 117,  Mean reward: -3.9444444444444446, Mean Entropy: 0.5207871198654175, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 118,  Mean reward: -3.473684210526316, Mean Entropy: 0.5752629637718201, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 119,  Mean reward: -1.9454545454545455, Mean Entropy: 0.7824766039848328, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 120,  Mean reward: -1.7547169811320755, Mean Entropy: 0.7666758298873901, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.55s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 121,  Mean reward: -1.5576923076923077, Mean Entropy: 0.5821086168289185, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 122,  Mean reward: -4.509803921568627, Mean Entropy: 0.6238574385643005, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 123,  Mean reward: -4.536363636363636, Mean Entropy: 0.5872105360031128, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 124,  Mean reward: -5.020833333333333, Mean Entropy: 0.7833843231201172, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 125,  Mean reward: -4.978260869565218, Mean Entropy: 0.7893244624137878, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 126,  Mean reward: -1.6944444444444444, Mean Entropy: 0.7660075426101685, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 127,  Mean reward: -2.202127659574468, Mean Entropy: 0.7721238136291504, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 128,  Mean reward: -1.9772727272727273, Mean Entropy: 0.7393495440483093, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 129,  Mean reward: -0.12962962962962962, Mean Entropy: 0.6795800924301147, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.53s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 130,  Mean reward: 1.69, Mean Entropy: 0.7443817853927612, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 131,  Mean reward: -0.11818181818181818, Mean Entropy: 0.6284266710281372, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 132,  Mean reward: -2.0545454545454547, Mean Entropy: 0.6259344816207886, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 133,  Mean reward: -0.0196078431372549, Mean Entropy: 0.7046718597412109, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.58s
Iteration: 134,  Mean reward: 0.8482142857142857, Mean Entropy: 0.8029507398605347, complete_episode_count: 56.0, Gather time: 0.64s, Train time: 1.74s
Iteration: 135,  Mean reward: 0.36904761904761907, Mean Entropy: 0.7059451341629028, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 136,  Mean reward: -3.0, Mean Entropy: 0.7505940794944763, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 137,  Mean reward: -0.7040816326530612, Mean Entropy: 0.7759735584259033, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 138,  Mean reward: 2.75, Mean Entropy: 0.8374396562576294, complete_episode_count: 50.0, Gather time: 0.59s, Train time: 1.53s
Iteration: 139,  Mean reward: -6.593023255813954, Mean Entropy: 0.7115909457206726, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 140,  Mean reward: -0.55, Mean Entropy: 0.6856640577316284, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 141,  Mean reward: 5.224489795918367, Mean Entropy: 0.7563818693161011, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 142,  Mean reward: -2.67, Mean Entropy: 0.7512949705123901, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 143,  Mean reward: -3.6363636363636362, Mean Entropy: 0.7143563628196716, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 144,  Mean reward: -3.28125, Mean Entropy: 0.74786776304245, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 145,  Mean reward: -2.53125, Mean Entropy: 0.6811367869377136, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 146,  Mean reward: -6.815217391304348, Mean Entropy: 0.7088200449943542, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 147,  Mean reward: -0.04, Mean Entropy: 0.6521137952804565, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.56s
Iteration: 148,  Mean reward: -4.283018867924528, Mean Entropy: 0.6948322653770447, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 149,  Mean reward: -0.7090909090909091, Mean Entropy: 0.6465235352516174, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 150,  Mean reward: 0.8365384615384616, Mean Entropy: 0.7036435008049011, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 151,  Mean reward: -0.40816326530612246, Mean Entropy: 0.6554485559463501, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.53s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 152,  Mean reward: 5.258620689655173, Mean Entropy: 0.6284239292144775, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.54s
Iteration: 153,  Mean reward: -4.83, Mean Entropy: 0.6137253642082214, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 154,  Mean reward: -3.3333333333333335, Mean Entropy: 0.6125662326812744, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 155,  Mean reward: 1.7450980392156863, Mean Entropy: 0.5948009490966797, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 156,  Mean reward: -1.537037037037037, Mean Entropy: 0.5930713415145874, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 157,  Mean reward: -3.24, Mean Entropy: 0.5874884128570557, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 158,  Mean reward: 4.0092592592592595, Mean Entropy: 0.6247509121894836, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 159,  Mean reward: -3.0510204081632653, Mean Entropy: 0.5849504470825195, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 160,  Mean reward: -2.209090909090909, Mean Entropy: 0.5956460237503052, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 161,  Mean reward: 3.57, Mean Entropy: 0.591023862361908, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 162,  Mean reward: -4.833333333333333, Mean Entropy: 0.5923680067062378, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 163,  Mean reward: -4.318181818181818, Mean Entropy: 0.6203442811965942, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 164,  Mean reward: 3.4074074074074074, Mean Entropy: 0.6256095170974731, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.56s
Iteration: 165,  Mean reward: -2.036363636363636, Mean Entropy: 0.5200111269950867, complete_episode_count: 55.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 166,  Mean reward: -5.46, Mean Entropy: 0.6092228293418884, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 167,  Mean reward: 2.9711538461538463, Mean Entropy: 0.5878572463989258, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 168,  Mean reward: -3.05, Mean Entropy: 0.5732447504997253, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 169,  Mean reward: -2.287037037037037, Mean Entropy: 0.5722609758377075, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.70s
Iteration: 170,  Mean reward: 0.8653846153846154, Mean Entropy: 0.5817711353302002, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 171,  Mean reward: -5.478260869565218, Mean Entropy: 0.5975667834281921, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 172,  Mean reward: -1.0648148148148149, Mean Entropy: 0.5564514398574829, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 173,  Mean reward: -2.1982758620689653, Mean Entropy: 0.6078975200653076, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 174,  Mean reward: 1.9313725490196079, Mean Entropy: 0.6040531992912292, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 175,  Mean reward: -4.166666666666667, Mean Entropy: 0.629044771194458, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 176,  Mean reward: 1.9, Mean Entropy: 0.6114242076873779, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 177,  Mean reward: 5.296296296296297, Mean Entropy: 0.8005160093307495, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 178,  Mean reward: -5.2444444444444445, Mean Entropy: 0.8398500680923462, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 179,  Mean reward: -5.065789473684211, Mean Entropy: 0.7659642100334167, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.58s
Iteration: 180,  Mean reward: -2.6707317073170733, Mean Entropy: 0.6762247085571289, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 181,  Mean reward: 4.98, Mean Entropy: 0.765892744064331, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 182,  Mean reward: -2.9166666666666665, Mean Entropy: 0.8287466764450073, complete_episode_count: 36.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 183,  Mean reward: -5.319444444444445, Mean Entropy: 0.8236109614372253, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 184,  Mean reward: -4.861111111111111, Mean Entropy: 0.8686707019805908, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 185,  Mean reward: -5.105263157894737, Mean Entropy: 0.8531116843223572, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 186,  Mean reward: -2.8125, Mean Entropy: 0.8521250486373901, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 187,  Mean reward: -6.35, Mean Entropy: 0.8618607521057129, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 188,  Mean reward: 0.925531914893617, Mean Entropy: 0.8256872892379761, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 189,  Mean reward: -0.8076923076923077, Mean Entropy: 0.7863575220108032, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 190,  Mean reward: -0.5208333333333334, Mean Entropy: 0.702889621257782, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 191,  Mean reward: 2.95, Mean Entropy: 0.6101664304733276, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 192,  Mean reward: -3.1132075471698113, Mean Entropy: 0.6338034868240356, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 193,  Mean reward: -1.9824561403508771, Mean Entropy: 0.6384137272834778, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 194,  Mean reward: -0.21153846153846154, Mean Entropy: 0.5866411328315735, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 195,  Mean reward: 4.951923076923077, Mean Entropy: 0.6699134707450867, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 196,  Mean reward: -3.7555555555555555, Mean Entropy: 0.7664312124252319, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 197,  Mean reward: -3.7804878048780486, Mean Entropy: 0.808024525642395, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 198,  Mean reward: -3.8513513513513513, Mean Entropy: 0.8657859563827515, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 199,  Mean reward: -4.2368421052631575, Mean Entropy: 0.8473936319351196, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 200,  Mean reward: -5.487179487179487, Mean Entropy: 0.8713776469230652, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.53s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -3.448717948717949, Mean Entropy: 0.7526353597640991, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 202,  Mean reward: -0.30612244897959184, Mean Entropy: 0.7029699087142944, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 203,  Mean reward: 0.9385964912280702, Mean Entropy: 0.6490437984466553, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 204,  Mean reward: -1.1944444444444444, Mean Entropy: 0.6662470698356628, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 205,  Mean reward: 1.4509803921568627, Mean Entropy: 0.6274782419204712, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.70s
Iteration: 206,  Mean reward: -2.4056603773584904, Mean Entropy: 0.5790212750434875, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 207,  Mean reward: -0.046296296296296294, Mean Entropy: 0.604312539100647, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 208,  Mean reward: 3.715686274509804, Mean Entropy: 0.7835204005241394, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 209,  Mean reward: -4.85, Mean Entropy: 0.8255280256271362, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 210,  Mean reward: -6.72972972972973, Mean Entropy: 0.7754027247428894, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 211,  Mean reward: -5.857142857142857, Mean Entropy: 0.8914036154747009, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 212,  Mean reward: -5.184210526315789, Mean Entropy: 0.8269729018211365, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 213,  Mean reward: -3.35, Mean Entropy: 0.858359694480896, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 214,  Mean reward: -0.9230769230769231, Mean Entropy: 0.8602796792984009, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 215,  Mean reward: 1.5108695652173914, Mean Entropy: 0.8371572494506836, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 216,  Mean reward: 0.20212765957446807, Mean Entropy: 0.8001880049705505, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 217,  Mean reward: 2.843137254901961, Mean Entropy: 0.7885774374008179, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 218,  Mean reward: 0.22093023255813954, Mean Entropy: 0.7541079521179199, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.55s
Iteration: 219,  Mean reward: -0.46938775510204084, Mean Entropy: 0.7468506097793579, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 220,  Mean reward: 0.10377358490566038, Mean Entropy: 0.7032658457756042, complete_episode_count: 53.0, Gather time: 0.72s, Train time: 1.51s
Iteration: 221,  Mean reward: 4.83, Mean Entropy: 0.7713009715080261, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 222,  Mean reward: -6.893939393939394, Mean Entropy: 0.7847497463226318, complete_episode_count: 33.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 223,  Mean reward: -5.716216216216216, Mean Entropy: 0.8805959224700928, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 224,  Mean reward: -5.838235294117647, Mean Entropy: 0.8922181129455566, complete_episode_count: 34.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 225,  Mean reward: -6.75, Mean Entropy: 0.8294856548309326, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 226,  Mean reward: -5.6125, Mean Entropy: 0.8679397106170654, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 227,  Mean reward: 0.7738095238095238, Mean Entropy: 0.9607593417167664, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 228,  Mean reward: 0.9777777777777777, Mean Entropy: 0.8321523070335388, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 229,  Mean reward: 1.7954545454545454, Mean Entropy: 0.77910315990448, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 230,  Mean reward: 0.13043478260869565, Mean Entropy: 0.7412125468254089, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 231,  Mean reward: 0.8888888888888888, Mean Entropy: 0.6916006803512573, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 232,  Mean reward: -0.4270833333333333, Mean Entropy: 0.6820586919784546, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 233,  Mean reward: 0.037037037037037035, Mean Entropy: 0.6220623254776001, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 234,  Mean reward: 1.5480769230769231, Mean Entropy: 0.5933519601821899, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 235,  Mean reward: 2.938775510204082, Mean Entropy: 0.6345497965812683, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 236,  Mean reward: 1.83, Mean Entropy: 0.5456428527832031, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 237,  Mean reward: 3.877551020408163, Mean Entropy: 0.6343669891357422, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 238,  Mean reward: 5.433333333333334, Mean Entropy: 0.6412923336029053, complete_episode_count: 45.0, Gather time: 0.67s, Train time: 1.50s
Iteration: 239,  Mean reward: 3.933333333333333, Mean Entropy: 0.5497897863388062, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 240,  Mean reward: 0.8362068965517241, Mean Entropy: 0.5291313529014587, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 241,  Mean reward: 5.615384615384615, Mean Entropy: 0.7383813858032227, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.65s
Iteration: 242,  Mean reward: 3.422222222222222, Mean Entropy: 0.7166266441345215, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 243,  Mean reward: 4.086956521739131, Mean Entropy: 0.8003290891647339, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 244,  Mean reward: 3.5930232558139537, Mean Entropy: 0.7246303558349609, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 245,  Mean reward: 3.82, Mean Entropy: 0.603310763835907, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 246,  Mean reward: 5.05, Mean Entropy: 0.5727578401565552, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 247,  Mean reward: 4.43, Mean Entropy: 0.5598140954971313, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 248,  Mean reward: 6.609090909090909, Mean Entropy: 0.5237747430801392, complete_episode_count: 55.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 249,  Mean reward: 6.472727272727273, Mean Entropy: 0.5426343679428101, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 250,  Mean reward: 5.798076923076923, Mean Entropy: 0.5484825372695923, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 251,  Mean reward: 6.296610169491525, Mean Entropy: 0.5644153356552124, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 252,  Mean reward: 5.872549019607843, Mean Entropy: 0.5833908915519714, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 253,  Mean reward: 5.21, Mean Entropy: 0.5710146427154541, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 254,  Mean reward: 5.921568627450981, Mean Entropy: 0.5403056144714355, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 255,  Mean reward: 4.945454545454545, Mean Entropy: 0.5583574771881104, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 256,  Mean reward: 6.661016949152542, Mean Entropy: 0.5206209421157837, complete_episode_count: 59.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 257,  Mean reward: 6.490909090909091, Mean Entropy: 0.4992179274559021, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 258,  Mean reward: 6.56140350877193, Mean Entropy: 0.5364928245544434, complete_episode_count: 57.0, Gather time: 0.59s, Train time: 1.53s
Iteration: 259,  Mean reward: -1.625, Mean Entropy: 0.5562638640403748, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 260,  Mean reward: -4.043859649122807, Mean Entropy: 0.611962080001831, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 261,  Mean reward: 1.543859649122807, Mean Entropy: 0.48619207739830017, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 262,  Mean reward: 5.510416666666667, Mean Entropy: 0.5274277925491333, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 263,  Mean reward: 6.37037037037037, Mean Entropy: 0.5321605205535889, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 264,  Mean reward: 6.154545454545454, Mean Entropy: 0.5663053393363953, complete_episode_count: 55.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 265,  Mean reward: 6.053571428571429, Mean Entropy: 0.5055549740791321, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 266,  Mean reward: 6.296296296296297, Mean Entropy: 0.5541505217552185, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 267,  Mean reward: 4.948979591836735, Mean Entropy: 0.5658570528030396, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 268,  Mean reward: 6.694444444444445, Mean Entropy: 0.5527639389038086, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 269,  Mean reward: 4.76595744680851, Mean Entropy: 0.5547336339950562, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 270,  Mean reward: 6.163793103448276, Mean Entropy: 0.5026066899299622, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 271,  Mean reward: 6.0, Mean Entropy: 0.5270451307296753, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 272,  Mean reward: 5.372727272727273, Mean Entropy: 0.554518461227417, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 273,  Mean reward: 6.473214285714286, Mean Entropy: 0.48097509145736694, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 274,  Mean reward: 6.553571428571429, Mean Entropy: 0.6117700338363647, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 275,  Mean reward: 5.469387755102041, Mean Entropy: 0.47855693101882935, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 276,  Mean reward: 6.545454545454546, Mean Entropy: 0.6260594129562378, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.67s
Iteration: 277,  Mean reward: 3.4272727272727272, Mean Entropy: 0.5399059057235718, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 278,  Mean reward: 6.705357142857143, Mean Entropy: 0.5236086249351501, complete_episode_count: 56.0, Gather time: 0.74s, Train time: 1.52s
Iteration: 279,  Mean reward: 6.558333333333334, Mean Entropy: 0.5470552444458008, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 280,  Mean reward: 6.544642857142857, Mean Entropy: 0.5366885662078857, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 281,  Mean reward: 5.666666666666667, Mean Entropy: 0.5356760621070862, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 282,  Mean reward: 6.166666666666667, Mean Entropy: 0.5129132270812988, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 283,  Mean reward: 6.473214285714286, Mean Entropy: 0.5519461035728455, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 284,  Mean reward: 6.651785714285714, Mean Entropy: 0.5403620004653931, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 285,  Mean reward: 5.12037037037037, Mean Entropy: 0.5074936151504517, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 286,  Mean reward: 6.027272727272727, Mean Entropy: 0.5189299583435059, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 287,  Mean reward: 5.72, Mean Entropy: 0.5482134222984314, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 288,  Mean reward: 5.759615384615385, Mean Entropy: 0.5778031349182129, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 289,  Mean reward: 5.990196078431373, Mean Entropy: 0.4899669587612152, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 290,  Mean reward: 6.098214285714286, Mean Entropy: 0.4922151267528534, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 291,  Mean reward: 5.2745098039215685, Mean Entropy: 0.5334526896476746, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 292,  Mean reward: 6.635593220338983, Mean Entropy: 0.4999549388885498, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.64s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 293,  Mean reward: 6.853448275862069, Mean Entropy: 0.5635452270507812, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 294,  Mean reward: 4.084905660377358, Mean Entropy: 0.5565423369407654, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 295,  Mean reward: 6.259615384615385, Mean Entropy: 0.5070472955703735, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 296,  Mean reward: 6.074074074074074, Mean Entropy: 0.5219483375549316, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 297,  Mean reward: 5.933962264150943, Mean Entropy: 0.5355188846588135, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 298,  Mean reward: 5.892857142857143, Mean Entropy: 0.5587615370750427, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 299,  Mean reward: 5.758620689655173, Mean Entropy: 0.5291605591773987, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 300,  Mean reward: 5.865384615384615, Mean Entropy: 0.5250060558319092, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 2.3627450980392157, Mean Entropy: 0.5827648639678955, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 302,  Mean reward: -1.8888888888888888, Mean Entropy: 0.6429073810577393, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 303,  Mean reward: -0.6111111111111112, Mean Entropy: 0.5383456945419312, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 304,  Mean reward: -3.980769230769231, Mean Entropy: 0.6817598342895508, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 305,  Mean reward: -2.6842105263157894, Mean Entropy: 0.5204839706420898, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 306,  Mean reward: -2.7037037037037037, Mean Entropy: 0.5363042950630188, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 307,  Mean reward: -2.2115384615384617, Mean Entropy: 0.5761203169822693, complete_episode_count: 52.0, Gather time: 0.72s, Train time: 1.51s
Iteration: 308,  Mean reward: -1.5727272727272728, Mean Entropy: 0.5591745376586914, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 309,  Mean reward: -2.4, Mean Entropy: 0.5670706629753113, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 310,  Mean reward: -5.388888888888889, Mean Entropy: 0.604816198348999, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.67s
Iteration: 311,  Mean reward: -1.790909090909091, Mean Entropy: 0.7111197710037231, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 312,  Mean reward: -1.27, Mean Entropy: 0.6836091876029968, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 313,  Mean reward: 0.7, Mean Entropy: 0.7442426681518555, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 314,  Mean reward: 1.7934782608695652, Mean Entropy: 0.6180087924003601, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 315,  Mean reward: 4.175925925925926, Mean Entropy: 0.7309249639511108, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 316,  Mean reward: -1.3918918918918919, Mean Entropy: 0.847386360168457, complete_episode_count: 37.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 317,  Mean reward: 0.30434782608695654, Mean Entropy: 0.838674783706665, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 318,  Mean reward: 1.4130434782608696, Mean Entropy: 0.6904730796813965, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 319,  Mean reward: 3.9711538461538463, Mean Entropy: 0.7368779182434082, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 320,  Mean reward: 4.042553191489362, Mean Entropy: 0.7324931025505066, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 321,  Mean reward: 3.843137254901961, Mean Entropy: 0.6381296515464783, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 322,  Mean reward: 5.80188679245283, Mean Entropy: 0.6526504755020142, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 323,  Mean reward: 4.329787234042553, Mean Entropy: 0.6632158160209656, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 324,  Mean reward: -2.732142857142857, Mean Entropy: 0.8963324427604675, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 325,  Mean reward: -3.369047619047619, Mean Entropy: 0.9965185523033142, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 326,  Mean reward: -5.381578947368421, Mean Entropy: 0.9237807989120483, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 327,  Mean reward: -6.2375, Mean Entropy: 0.8802983164787292, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 328,  Mean reward: -5.615384615384615, Mean Entropy: 0.9594849348068237, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 329,  Mean reward: -3.697674418604651, Mean Entropy: 0.9093844294548035, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 330,  Mean reward: -5.7375, Mean Entropy: 0.9385643005371094, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 331,  Mean reward: -3.15, Mean Entropy: 0.953044593334198, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 332,  Mean reward: -2.697674418604651, Mean Entropy: 0.9311904311180115, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 333,  Mean reward: -4.548780487804878, Mean Entropy: 0.9520761370658875, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 334,  Mean reward: -3.402439024390244, Mean Entropy: 0.9020168781280518, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 335,  Mean reward: -3.9204545454545454, Mean Entropy: 0.9120532274246216, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 336,  Mean reward: -3.5, Mean Entropy: 0.9190043807029724, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 337,  Mean reward: -4.96875, Mean Entropy: 0.871634840965271, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 338,  Mean reward: -4.444444444444445, Mean Entropy: 0.8320974707603455, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 339,  Mean reward: -2.9423076923076925, Mean Entropy: 0.7526795268058777, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 340,  Mean reward: -4.259615384615385, Mean Entropy: 0.8274500370025635, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 341,  Mean reward: -3.2346938775510203, Mean Entropy: 0.8333410024642944, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 342,  Mean reward: -3.63265306122449, Mean Entropy: 0.7974861860275269, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 343,  Mean reward: -2.4622641509433962, Mean Entropy: 0.8612766265869141, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 344,  Mean reward: -3.519607843137255, Mean Entropy: 0.7708733081817627, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 345,  Mean reward: -1.5392156862745099, Mean Entropy: 0.702037513256073, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 346,  Mean reward: 1.3392857142857142, Mean Entropy: 0.6317384243011475, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.68s
Iteration: 347,  Mean reward: 0.8461538461538461, Mean Entropy: 0.6154641509056091, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 348,  Mean reward: 0.2631578947368421, Mean Entropy: 0.5675877928733826, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 349,  Mean reward: 1.3076923076923077, Mean Entropy: 0.627591073513031, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 350,  Mean reward: 1.0961538461538463, Mean Entropy: 0.6085970997810364, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.65s
Iteration: 351,  Mean reward: 1.6929824561403508, Mean Entropy: 0.6037556529045105, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 352,  Mean reward: 0.5980392156862745, Mean Entropy: 0.6364835500717163, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 353,  Mean reward: -0.7058823529411765, Mean Entropy: 0.6362004280090332, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 354,  Mean reward: 2.087719298245614, Mean Entropy: 0.6637219190597534, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 355,  Mean reward: 4.480769230769231, Mean Entropy: 0.6745273470878601, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 356,  Mean reward: 2.9565217391304346, Mean Entropy: 0.7642259001731873, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 357,  Mean reward: -4.490909090909091, Mean Entropy: 0.7648307681083679, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 358,  Mean reward: -5.3431372549019605, Mean Entropy: 0.8445789217948914, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 359,  Mean reward: -5.01, Mean Entropy: 0.8915202617645264, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 360,  Mean reward: -6.544444444444444, Mean Entropy: 0.9199529886245728, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 361,  Mean reward: -4.1938775510204085, Mean Entropy: 0.8658299446105957, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 362,  Mean reward: -7.0978260869565215, Mean Entropy: 0.9007169604301453, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 363,  Mean reward: -5.476744186046512, Mean Entropy: 0.8880378603935242, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 364,  Mean reward: -3.622448979591837, Mean Entropy: 0.8907574415206909, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 365,  Mean reward: -4.058139534883721, Mean Entropy: 0.9093372225761414, complete_episode_count: 43.0, Gather time: 0.70s, Train time: 1.50s
Iteration: 366,  Mean reward: -2.7674418604651163, Mean Entropy: 0.9392848014831543, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 367,  Mean reward: -4.081632653061225, Mean Entropy: 0.8350200653076172, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 368,  Mean reward: -2.305084745762712, Mean Entropy: 0.7150923013687134, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 369,  Mean reward: -4.923076923076923, Mean Entropy: 0.7721465826034546, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 370,  Mean reward: -5.66, Mean Entropy: 0.8366332054138184, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 371,  Mean reward: -4.648936170212766, Mean Entropy: 0.8250269889831543, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 372,  Mean reward: -4.320754716981132, Mean Entropy: 0.7689160704612732, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 373,  Mean reward: -4.336538461538462, Mean Entropy: 0.7499346137046814, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 374,  Mean reward: -3.175925925925926, Mean Entropy: 0.7041494250297546, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 375,  Mean reward: -3.0714285714285716, Mean Entropy: 0.652842104434967, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 376,  Mean reward: 2.0754716981132075, Mean Entropy: 0.6210500001907349, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 377,  Mean reward: 5.112068965517241, Mean Entropy: 0.5925816297531128, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 378,  Mean reward: -0.25, Mean Entropy: 0.801555335521698, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 379,  Mean reward: 0.05813953488372093, Mean Entropy: 0.8081344962120056, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.64s
Iteration: 380,  Mean reward: 0.7906976744186046, Mean Entropy: 0.7747749090194702, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 381,  Mean reward: 2.802325581395349, Mean Entropy: 0.7218055725097656, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.69s
Iteration: 382,  Mean reward: 2.90625, Mean Entropy: 0.7179940938949585, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 383,  Mean reward: 5.30188679245283, Mean Entropy: 0.6105252504348755, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 384,  Mean reward: 5.471698113207547, Mean Entropy: 0.5561811923980713, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 385,  Mean reward: 4.673076923076923, Mean Entropy: 0.5598080158233643, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 386,  Mean reward: 5.9818181818181815, Mean Entropy: 0.5237051248550415, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 387,  Mean reward: 5.830357142857143, Mean Entropy: 0.5773701667785645, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 388,  Mean reward: 5.009615384615385, Mean Entropy: 0.6501004099845886, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 389,  Mean reward: 3.3425925925925926, Mean Entropy: 0.6363391280174255, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 390,  Mean reward: 6.296296296296297, Mean Entropy: 0.49598854780197144, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 391,  Mean reward: 5.913461538461538, Mean Entropy: 0.5392632484436035, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 392,  Mean reward: 6.196428571428571, Mean Entropy: 0.5106534957885742, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 393,  Mean reward: 5.125, Mean Entropy: 0.5579625964164734, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 394,  Mean reward: 6.268518518518518, Mean Entropy: 0.541777491569519, complete_episode_count: 54.0, Gather time: 0.71s, Train time: 1.49s
Iteration: 395,  Mean reward: 6.196428571428571, Mean Entropy: 0.5206420421600342, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 396,  Mean reward: 5.787037037037037, Mean Entropy: 0.5216554403305054, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 397,  Mean reward: 6.766666666666667, Mean Entropy: 0.47792038321495056, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 398,  Mean reward: 6.201754385964913, Mean Entropy: 0.4870356023311615, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 399,  Mean reward: 5.701923076923077, Mean Entropy: 0.5621216297149658, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 400,  Mean reward: 5.411764705882353, Mean Entropy: 0.5879582166671753, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 6.175925925925926, Mean Entropy: 0.5084972381591797, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 402,  Mean reward: 5.716981132075472, Mean Entropy: 0.5229693651199341, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 403,  Mean reward: 5.907407407407407, Mean Entropy: 0.4648779332637787, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 404,  Mean reward: 7.095238095238095, Mean Entropy: 0.42540210485458374, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 405,  Mean reward: 7.120967741935484, Mean Entropy: 0.44821280241012573, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 406,  Mean reward: 7.110169491525424, Mean Entropy: 0.34803709387779236, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 407,  Mean reward: 7.3671875, Mean Entropy: 0.2927345633506775, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 408,  Mean reward: 6.959016393442623, Mean Entropy: 0.40396618843078613, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 409,  Mean reward: 7.666666666666667, Mean Entropy: 0.23307639360427856, complete_episode_count: 69.0, Gather time: 0.61s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 410,  Mean reward: 7.732876712328767, Mean Entropy: 0.3650021255016327, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 411,  Mean reward: 7.17741935483871, Mean Entropy: 0.11281927675008774, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 412,  Mean reward: 7.8175675675675675, Mean Entropy: 0.10635006427764893, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 413,  Mean reward: 7.883116883116883, Mean Entropy: 0.06477197259664536, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 0.18848541378974915, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 415,  Mean reward: 7.0, Mean Entropy: 0.4307214617729187, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 416,  Mean reward: 6.7, Mean Entropy: 0.3605461120605469, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 417,  Mean reward: -2.3987341772151898, Mean Entropy: 0.3184472918510437, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.70s
Iteration: 418,  Mean reward: 2.6031746031746033, Mean Entropy: 0.4479448199272156, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 419,  Mean reward: 0.4365079365079365, Mean Entropy: 0.19879023730754852, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 420,  Mean reward: 0.8173076923076923, Mean Entropy: 0.3367304801940918, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 421,  Mean reward: -3.7580645161290325, Mean Entropy: 0.2886098325252533, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 422,  Mean reward: -1.2452830188679245, Mean Entropy: 0.3296782374382019, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 423,  Mean reward: -0.9074074074074074, Mean Entropy: 0.39151403307914734, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 424,  Mean reward: 4.5, Mean Entropy: 0.4550960958003998, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 425,  Mean reward: -3.4444444444444446, Mean Entropy: 0.4257015585899353, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 426,  Mean reward: 2.902173913043478, Mean Entropy: 0.6090648770332336, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 427,  Mean reward: -3.2295081967213113, Mean Entropy: 0.582350492477417, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 428,  Mean reward: 1.9836065573770492, Mean Entropy: 0.3305298686027527, complete_episode_count: 61.0, Gather time: 0.67s, Train time: 1.49s
Iteration: 429,  Mean reward: 6.0546875, Mean Entropy: 0.5518282055854797, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 430,  Mean reward: -2.735294117647059, Mean Entropy: 0.13629841804504395, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 431,  Mean reward: 3.1666666666666665, Mean Entropy: 0.43473494052886963, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 432,  Mean reward: 2.9574468085106385, Mean Entropy: 0.6238244771957397, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 433,  Mean reward: 3.5576923076923075, Mean Entropy: 0.3335992693901062, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 434,  Mean reward: 2.6, Mean Entropy: 0.2667226195335388, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 435,  Mean reward: 1.3333333333333333, Mean Entropy: 0.3082338571548462, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 436,  Mean reward: 2.7755102040816326, Mean Entropy: 0.3702232837677002, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 437,  Mean reward: 3.018867924528302, Mean Entropy: 0.35225409269332886, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 438,  Mean reward: 4.113207547169812, Mean Entropy: 0.4616795778274536, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 439,  Mean reward: -0.9897959183673469, Mean Entropy: 0.2898638844490051, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 440,  Mean reward: 1.5833333333333333, Mean Entropy: 0.37077975273132324, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 441,  Mean reward: 2.511111111111111, Mean Entropy: 0.48092350363731384, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 442,  Mean reward: -1.2291666666666667, Mean Entropy: 0.4703584611415863, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 443,  Mean reward: 2.875, Mean Entropy: 0.368122398853302, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 444,  Mean reward: -1.8653846153846154, Mean Entropy: 0.23851798474788666, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 445,  Mean reward: 3.814814814814815, Mean Entropy: 0.4190351963043213, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 446,  Mean reward: -0.5116279069767442, Mean Entropy: 0.31046146154403687, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 447,  Mean reward: 4.2407407407407405, Mean Entropy: 0.44526296854019165, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 448,  Mean reward: 1.5, Mean Entropy: 0.40546321868896484, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 449,  Mean reward: 3.24, Mean Entropy: 0.4075488746166229, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 450,  Mean reward: 1.8235294117647058, Mean Entropy: 0.3380095362663269, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 451,  Mean reward: 3.19, Mean Entropy: 0.4524260461330414, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 452,  Mean reward: -2.5851063829787235, Mean Entropy: 0.3882141709327698, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 453,  Mean reward: 4.803921568627451, Mean Entropy: 0.28992700576782227, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 454,  Mean reward: 5.3474576271186445, Mean Entropy: 0.40884584188461304, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 455,  Mean reward: 2.5416666666666665, Mean Entropy: 0.2906344532966614, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 456,  Mean reward: -2.7232142857142856, Mean Entropy: 0.4958551824092865, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 457,  Mean reward: 2.8333333333333335, Mean Entropy: 0.25108301639556885, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 458,  Mean reward: -4.872340425531915, Mean Entropy: 0.3637966513633728, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 459,  Mean reward: 2.659090909090909, Mean Entropy: 0.4150633215904236, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 460,  Mean reward: 2.9183673469387754, Mean Entropy: 0.3503990173339844, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 461,  Mean reward: 4.113207547169812, Mean Entropy: 0.3509231507778168, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 462,  Mean reward: 4.4818181818181815, Mean Entropy: 0.29723644256591797, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 463,  Mean reward: 2.85, Mean Entropy: 0.28115177154541016, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 464,  Mean reward: 3.2115384615384617, Mean Entropy: 0.24577151238918304, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 465,  Mean reward: 4.142857142857143, Mean Entropy: 0.3066341280937195, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 466,  Mean reward: 5.158333333333333, Mean Entropy: 0.28602632880210876, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 467,  Mean reward: 4.733333333333333, Mean Entropy: 0.2882572412490845, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 468,  Mean reward: 5.709677419354839, Mean Entropy: 0.3916301727294922, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 469,  Mean reward: -0.6111111111111112, Mean Entropy: 0.5075833797454834, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 470,  Mean reward: -4.867647058823529, Mean Entropy: 0.271982878446579, complete_episode_count: 34.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 471,  Mean reward: -3.4, Mean Entropy: 0.2563334107398987, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 472,  Mean reward: -3.5714285714285716, Mean Entropy: 0.24757152795791626, complete_episode_count: 35.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 473,  Mean reward: -2.1944444444444446, Mean Entropy: 0.33924323320388794, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 474,  Mean reward: -1.013157894736842, Mean Entropy: 0.29521235823631287, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 475,  Mean reward: -2.7916666666666665, Mean Entropy: 0.2886040210723877, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 476,  Mean reward: -3.5, Mean Entropy: 0.19117867946624756, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 477,  Mean reward: -1.4857142857142858, Mean Entropy: 0.2191278040409088, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 478,  Mean reward: -2.6857142857142855, Mean Entropy: 0.3070093095302582, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 479,  Mean reward: -5.25, Mean Entropy: 0.23461245000362396, complete_episode_count: 34.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 480,  Mean reward: -3.9705882352941178, Mean Entropy: 0.2281721830368042, complete_episode_count: 34.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 481,  Mean reward: -2.111111111111111, Mean Entropy: 0.21839945018291473, complete_episode_count: 36.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 482,  Mean reward: -1.5526315789473684, Mean Entropy: 0.20916986465454102, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 483,  Mean reward: -1.355263157894737, Mean Entropy: 0.20711874961853027, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 484,  Mean reward: -2.875, Mean Entropy: 0.2388598918914795, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 485,  Mean reward: -3.4027777777777777, Mean Entropy: 0.09280094504356384, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 486,  Mean reward: -1.6891891891891893, Mean Entropy: 0.2357238382101059, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 487,  Mean reward: -2.4722222222222223, Mean Entropy: 0.13055472075939178, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 488,  Mean reward: -3.4558823529411766, Mean Entropy: 0.15815886855125427, complete_episode_count: 34.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 489,  Mean reward: -1.513157894736842, Mean Entropy: 0.1712224781513214, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 490,  Mean reward: -2.7714285714285714, Mean Entropy: 0.2864900529384613, complete_episode_count: 35.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 491,  Mean reward: -0.9473684210526315, Mean Entropy: 0.19821584224700928, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 492,  Mean reward: -2.8, Mean Entropy: 0.20259463787078857, complete_episode_count: 35.0, Gather time: 0.52s, Train time: 1.64s
Iteration: 493,  Mean reward: -3.5142857142857142, Mean Entropy: 0.10598497837781906, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 494,  Mean reward: -1.8918918918918919, Mean Entropy: 0.15282708406448364, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 495,  Mean reward: -1.1973684210526316, Mean Entropy: 0.15110734105110168, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 496,  Mean reward: -0.8717948717948718, Mean Entropy: 0.22134006023406982, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 497,  Mean reward: -2.6666666666666665, Mean Entropy: 0.033066488802433014, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 498,  Mean reward: -2.8472222222222223, Mean Entropy: 0.10431081056594849, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 499,  Mean reward: -0.6125, Mean Entropy: 0.1897679567337036, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 500,  Mean reward: -1.5921052631578947, Mean Entropy: 0.2656632363796234, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.50s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -1.1538461538461537, Mean Entropy: 0.10262404382228851, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 502,  Mean reward: 0.4772727272727273, Mean Entropy: 0.19407936930656433, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 503,  Mean reward: 1.4222222222222223, Mean Entropy: 0.1855267882347107, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 504,  Mean reward: 0.46511627906976744, Mean Entropy: 0.1140357255935669, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 505,  Mean reward: -1.1125, Mean Entropy: 0.061252158135175705, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 506,  Mean reward: 1.6333333333333333, Mean Entropy: 0.4277532994747162, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 507,  Mean reward: 1.2608695652173914, Mean Entropy: 0.10574747622013092, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 508,  Mean reward: 7.753333333333333, Mean Entropy: 0.3174054026603699, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 509,  Mean reward: 6.1, Mean Entropy: 0.30768513679504395, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.52s
Iteration: 510,  Mean reward: 6.607692307692307, Mean Entropy: 0.25806093215942383, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 511,  Mean reward: 7.073529411764706, Mean Entropy: 0.2585544288158417, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 512,  Mean reward: 7.021739130434782, Mean Entropy: 0.20906347036361694, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 513,  Mean reward: 7.421428571428572, Mean Entropy: 0.1766299605369568, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 514,  Mean reward: 6.791044776119403, Mean Entropy: 0.21360862255096436, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 515,  Mean reward: 0.2857142857142857, Mean Entropy: 0.09100240468978882, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 516,  Mean reward: -0.175, Mean Entropy: 0.20047208666801453, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 517,  Mean reward: 1.6666666666666667, Mean Entropy: 0.2528843283653259, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 518,  Mean reward: -0.32142857142857145, Mean Entropy: 0.3188285827636719, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 519,  Mean reward: 2.2916666666666665, Mean Entropy: 0.32904574275016785, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 520,  Mean reward: 2.8229166666666665, Mean Entropy: 0.528892993927002, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 521,  Mean reward: 3.53, Mean Entropy: 0.6467859745025635, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 522,  Mean reward: 3.55, Mean Entropy: 0.5753579139709473, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 523,  Mean reward: 3.010204081632653, Mean Entropy: 0.6266794800758362, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 524,  Mean reward: 2.111111111111111, Mean Entropy: 0.2814044654369354, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 525,  Mean reward: 1.2173913043478262, Mean Entropy: 0.6073877215385437, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 526,  Mean reward: -1.1888888888888889, Mean Entropy: 0.6671684980392456, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 527,  Mean reward: 0.045454545454545456, Mean Entropy: 0.3845815062522888, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 528,  Mean reward: -0.32954545454545453, Mean Entropy: 0.7378067970275879, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 529,  Mean reward: 2.2058823529411766, Mean Entropy: 0.48972344398498535, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 530,  Mean reward: 2.7395833333333335, Mean Entropy: 0.7596240043640137, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.65s
Iteration: 531,  Mean reward: 3.488888888888889, Mean Entropy: 0.4381919801235199, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 532,  Mean reward: 0.36904761904761907, Mean Entropy: 0.10441490262746811, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 533,  Mean reward: 4.549019607843137, Mean Entropy: 0.6876181960105896, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 534,  Mean reward: -2.3636363636363638, Mean Entropy: 0.6815769076347351, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 535,  Mean reward: 3.9431818181818183, Mean Entropy: 0.5819244384765625, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 536,  Mean reward: 1.9901960784313726, Mean Entropy: 0.47111839056015015, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 537,  Mean reward: 3.5083333333333333, Mean Entropy: 0.3526763319969177, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 538,  Mean reward: 5.416666666666667, Mean Entropy: 0.32162874937057495, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 539,  Mean reward: 3.7661290322580645, Mean Entropy: 0.38641270995140076, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 540,  Mean reward: 2.3214285714285716, Mean Entropy: 0.3311738669872284, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 541,  Mean reward: 3.641509433962264, Mean Entropy: 0.4100727438926697, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 542,  Mean reward: -0.6090909090909091, Mean Entropy: 0.6423757076263428, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 543,  Mean reward: -0.5892857142857143, Mean Entropy: 0.5915842056274414, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 544,  Mean reward: 3.52, Mean Entropy: 0.5522111654281616, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 545,  Mean reward: 0.2222222222222222, Mean Entropy: 0.39652013778686523, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 546,  Mean reward: 5.125, Mean Entropy: 0.3862241208553314, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 547,  Mean reward: 4.211538461538462, Mean Entropy: 0.4896717667579651, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 548,  Mean reward: 2.9591836734693877, Mean Entropy: 0.48216795921325684, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 549,  Mean reward: 4.625, Mean Entropy: 0.49215230345726013, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 550,  Mean reward: 5.919354838709677, Mean Entropy: 0.5224126577377319, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 551,  Mean reward: 5.877049180327869, Mean Entropy: 0.5464349389076233, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 552,  Mean reward: 5.413793103448276, Mean Entropy: 0.5333237648010254, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 553,  Mean reward: 5.076923076923077, Mean Entropy: 0.47914251685142517, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 554,  Mean reward: 6.483050847457627, Mean Entropy: 0.4510751962661743, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 555,  Mean reward: 6.268656716417911, Mean Entropy: 0.4587824046611786, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 556,  Mean reward: 5.363636363636363, Mean Entropy: 0.5403098464012146, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 557,  Mean reward: 6.825396825396825, Mean Entropy: 0.45846813917160034, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 558,  Mean reward: 7.285714285714286, Mean Entropy: 0.42112764716148376, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 559,  Mean reward: 7.285714285714286, Mean Entropy: 0.2604079842567444, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 560,  Mean reward: 6.8515625, Mean Entropy: 0.25404974818229675, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 561,  Mean reward: 7.7, Mean Entropy: 0.1711367666721344, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.70s
Iteration: 562,  Mean reward: 7.922077922077922, Mean Entropy: 0.05631384253501892, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 563,  Mean reward: 8.0, Mean Entropy: 0.07504749298095703, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 564,  Mean reward: 6.923076923076923, Mean Entropy: 0.202137291431427, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 565,  Mean reward: 5.8478260869565215, Mean Entropy: 0.35315102338790894, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 566,  Mean reward: 4.4818181818181815, Mean Entropy: 0.4275939464569092, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 567,  Mean reward: 6.672131147540983, Mean Entropy: 0.354684442281723, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 568,  Mean reward: 6.7109375, Mean Entropy: 0.4360348582267761, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 569,  Mean reward: 6.161290322580645, Mean Entropy: 0.3264157474040985, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 570,  Mean reward: 7.635714285714286, Mean Entropy: 0.3386937975883484, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 571,  Mean reward: 4.087301587301587, Mean Entropy: 0.4025014340877533, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 572,  Mean reward: 0.7232142857142857, Mean Entropy: 0.5080606341362, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 573,  Mean reward: 6.7578125, Mean Entropy: 0.4416801631450653, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 574,  Mean reward: 7.423076923076923, Mean Entropy: 0.3918898105621338, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 575,  Mean reward: 4.741935483870968, Mean Entropy: 0.3557812571525574, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 576,  Mean reward: 7.323076923076923, Mean Entropy: 0.3502398431301117, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 577,  Mean reward: 7.063492063492063, Mean Entropy: 0.3834998607635498, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 578,  Mean reward: 6.358208955223881, Mean Entropy: 0.3807142972946167, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 579,  Mean reward: 7.360294117647059, Mean Entropy: 0.3262540400028229, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 580,  Mean reward: 6.739130434782608, Mean Entropy: 0.2428084909915924, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 581,  Mean reward: 7.8618421052631575, Mean Entropy: 0.1238144263625145, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 582,  Mean reward: 7.961538461538462, Mean Entropy: 0.05279801785945892, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 583,  Mean reward: 7.981012658227848, Mean Entropy: 0.0361296609044075, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 584,  Mean reward: 8.0, Mean Entropy: 0.192469522356987, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 585,  Mean reward: 4.7734375, Mean Entropy: 0.13555577397346497, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 586,  Mean reward: -0.21428571428571427, Mean Entropy: 0.18621325492858887, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 587,  Mean reward: 0.07, Mean Entropy: 0.7240546941757202, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 588,  Mean reward: -2.1931818181818183, Mean Entropy: 0.541890561580658, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 589,  Mean reward: -4.336734693877551, Mean Entropy: 0.4025588929653168, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 590,  Mean reward: -3.5859375, Mean Entropy: 0.2874419093132019, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 591,  Mean reward: -3.4166666666666665, Mean Entropy: 0.22945073246955872, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 592,  Mean reward: -2.3214285714285716, Mean Entropy: 0.3009173274040222, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 593,  Mean reward: -4.246153846153846, Mean Entropy: 0.43404731154441833, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 594,  Mean reward: -3.3442622950819674, Mean Entropy: 0.4541986882686615, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 595,  Mean reward: -3.8492063492063493, Mean Entropy: 0.4547250270843506, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 596,  Mean reward: -1.3333333333333333, Mean Entropy: 0.4999096393585205, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 597,  Mean reward: -0.4927536231884058, Mean Entropy: 0.38853615522384644, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 598,  Mean reward: -1.3169014084507042, Mean Entropy: 0.3344153165817261, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 599,  Mean reward: -0.43661971830985913, Mean Entropy: 0.4399663805961609, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 600,  Mean reward: 1.4104477611940298, Mean Entropy: 0.4902585744857788, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -0.15942028985507245, Mean Entropy: 0.4097243547439575, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 602,  Mean reward: 0.2727272727272727, Mean Entropy: 0.4392429292201996, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 603,  Mean reward: 0.09615384615384616, Mean Entropy: 0.3930046558380127, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 604,  Mean reward: 3.169230769230769, Mean Entropy: 0.3216666579246521, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 605,  Mean reward: 0.8333333333333334, Mean Entropy: 0.36265450716018677, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 606,  Mean reward: 0.3263888888888889, Mean Entropy: 0.3717976212501526, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 607,  Mean reward: 2.109375, Mean Entropy: 0.4222245216369629, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 608,  Mean reward: 1.8986486486486487, Mean Entropy: 0.44743913412094116, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 609,  Mean reward: 1.3829787234042554, Mean Entropy: 0.4707505702972412, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 610,  Mean reward: 0.017241379310344827, Mean Entropy: 0.3730490207672119, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.64s
Iteration: 611,  Mean reward: -1.3571428571428572, Mean Entropy: 0.48366665840148926, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 612,  Mean reward: -1.4253731343283582, Mean Entropy: 0.41948556900024414, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 613,  Mean reward: 2.0073529411764706, Mean Entropy: 0.3820837736129761, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 614,  Mean reward: -0.29411764705882354, Mean Entropy: 0.4880821704864502, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 615,  Mean reward: -0.14423076923076922, Mean Entropy: 0.2709375321865082, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 616,  Mean reward: 2.0307692307692307, Mean Entropy: 0.5714480876922607, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 617,  Mean reward: 1.2346938775510203, Mean Entropy: 0.3892706334590912, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 618,  Mean reward: 1.0918367346938775, Mean Entropy: 0.3767814636230469, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 619,  Mean reward: -4.347222222222222, Mean Entropy: 0.44919419288635254, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 620,  Mean reward: 2.8333333333333335, Mean Entropy: 0.4807354807853699, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 621,  Mean reward: -5.191489361702128, Mean Entropy: 0.5371556282043457, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 622,  Mean reward: 2.4361702127659575, Mean Entropy: 0.46043285727500916, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 623,  Mean reward: 4.745901639344262, Mean Entropy: 0.7062791585922241, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 624,  Mean reward: 2.4782608695652173, Mean Entropy: 0.8510397672653198, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 625,  Mean reward: -1.40625, Mean Entropy: 0.8310291171073914, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 626,  Mean reward: -8.0, Mean Entropy: 0.70695960521698, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 627,  Mean reward: -6.027272727272727, Mean Entropy: 0.5657033920288086, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 628,  Mean reward: -5.3545454545454545, Mean Entropy: 0.5945053100585938, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 629,  Mean reward: -5.278846153846154, Mean Entropy: 0.5588235259056091, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 630,  Mean reward: -4.173469387755102, Mean Entropy: 0.5491426587104797, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 631,  Mean reward: -3.5833333333333335, Mean Entropy: 0.5160617232322693, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 632,  Mean reward: -4.443396226415095, Mean Entropy: 0.5268387794494629, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 633,  Mean reward: -1.271186440677966, Mean Entropy: 0.5545307397842407, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 634,  Mean reward: -3.25, Mean Entropy: 0.5496119260787964, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 635,  Mean reward: -5.298245614035087, Mean Entropy: 0.5288709998130798, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 636,  Mean reward: -3.4051724137931036, Mean Entropy: 0.527846097946167, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 637,  Mean reward: -5.038461538461538, Mean Entropy: 0.5121250748634338, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 638,  Mean reward: -1.0625, Mean Entropy: 0.5435977578163147, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 639,  Mean reward: -1.9722222222222223, Mean Entropy: 0.5103811025619507, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 640,  Mean reward: -2.792452830188679, Mean Entropy: 0.5473262071609497, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 641,  Mean reward: -3.232142857142857, Mean Entropy: 0.5017275810241699, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 642,  Mean reward: -0.35294117647058826, Mean Entropy: 0.504524827003479, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 643,  Mean reward: -1.9576271186440677, Mean Entropy: 0.48246821761131287, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 644,  Mean reward: -0.6129032258064516, Mean Entropy: 0.5577554702758789, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 645,  Mean reward: -1.2295081967213115, Mean Entropy: 0.5031213760375977, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 646,  Mean reward: -5.151785714285714, Mean Entropy: 0.4793683886528015, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.65s
Iteration: 647,  Mean reward: -4.071428571428571, Mean Entropy: 0.4698527753353119, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 648,  Mean reward: -2.7916666666666665, Mean Entropy: 0.40843459963798523, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 649,  Mean reward: -3.835714285714286, Mean Entropy: 0.2459370344877243, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 650,  Mean reward: -1.9930555555555556, Mean Entropy: 0.20808546245098114, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 651,  Mean reward: -3.864864864864865, Mean Entropy: 0.09583981335163116, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 652,  Mean reward: -0.8987341772151899, Mean Entropy: 0.10099095106124878, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 653,  Mean reward: -4.384615384615385, Mean Entropy: 0.04531252011656761, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 654,  Mean reward: -1.1329113924050633, Mean Entropy: 0.015858737751841545, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 655,  Mean reward: -2.0, Mean Entropy: 0.015072542242705822, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 656,  Mean reward: -2.911392405063291, Mean Entropy: 0.008378907106816769, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 657,  Mean reward: -1.75, Mean Entropy: 0.006594721227884293, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 658,  Mean reward: -2.75, Mean Entropy: 0.006176793482154608, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 659,  Mean reward: -2.0, Mean Entropy: 0.006715881638228893, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 660,  Mean reward: -3.25, Mean Entropy: 0.005551529582589865, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 661,  Mean reward: -3.25, Mean Entropy: 0.006261768750846386, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 662,  Mean reward: -0.5, Mean Entropy: 0.0077065154910087585, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 663,  Mean reward: -4.170886075949367, Mean Entropy: 0.004503289237618446, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 664,  Mean reward: -3.0, Mean Entropy: 0.005061610601842403, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 665,  Mean reward: -3.1582278481012658, Mean Entropy: 0.0026504560373723507, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 666,  Mean reward: -1.1329113924050633, Mean Entropy: 0.0016346847405657172, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 667,  Mean reward: -1.0, Mean Entropy: 0.001148564973846078, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 668,  Mean reward: 0.0, Mean Entropy: 0.0010473460424691439, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 669,  Mean reward: -3.75, Mean Entropy: 0.0006883626338094473, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 670,  Mean reward: -2.5, Mean Entropy: 0.0009582490893080831, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 671,  Mean reward: -3.0, Mean Entropy: 0.0007415722939185798, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 672,  Mean reward: 0.0, Mean Entropy: 0.0009965564822778106, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 673,  Mean reward: -1.75, Mean Entropy: 0.0008064472931437194, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 674,  Mean reward: -1.5, Mean Entropy: 0.0008253766573034227, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 675,  Mean reward: -1.0, Mean Entropy: 0.0010128768626600504, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 676,  Mean reward: -1.75, Mean Entropy: 0.0010684578446671367, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 677,  Mean reward: -1.75, Mean Entropy: 0.001112980768084526, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 678,  Mean reward: -3.75, Mean Entropy: 0.0009371063206344843, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 679,  Mean reward: -2.25, Mean Entropy: 0.0011083068093284965, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 680,  Mean reward: -2.0, Mean Entropy: 0.0014895384665578604, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 681,  Mean reward: -3.0, Mean Entropy: 0.0012887570774182677, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 682,  Mean reward: -1.5, Mean Entropy: 0.0012205815874040127, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 683,  Mean reward: -1.75, Mean Entropy: 0.0013895072042942047, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 684,  Mean reward: -0.75, Mean Entropy: 0.0017416005721315742, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 685,  Mean reward: -1.5, Mean Entropy: 0.0016287260223180056, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 686,  Mean reward: -1.0, Mean Entropy: 0.0014380629872903228, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 687,  Mean reward: -4.5, Mean Entropy: 0.0010600745445117354, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 688,  Mean reward: -1.0, Mean Entropy: 0.0015640833880752325, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 689,  Mean reward: -0.5, Mean Entropy: 0.0015663643134757876, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 690,  Mean reward: -2.75, Mean Entropy: 0.0013074608286842704, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 691,  Mean reward: -3.75, Mean Entropy: 0.001140077132731676, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 692,  Mean reward: -3.25, Mean Entropy: 0.0012713349424302578, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 693,  Mean reward: -2.0, Mean Entropy: 0.0012826224556192756, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 694,  Mean reward: -2.75, Mean Entropy: 0.0012566042132675648, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 695,  Mean reward: -0.75, Mean Entropy: 0.0013187023578211665, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 696,  Mean reward: -4.25, Mean Entropy: 0.0010253735817968845, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 697,  Mean reward: -2.75, Mean Entropy: 0.0010331812081858516, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 698,  Mean reward: -0.75, Mean Entropy: 0.001221210928633809, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 699,  Mean reward: -3.75, Mean Entropy: 0.0010153863113373518, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 700,  Mean reward: -0.25, Mean Entropy: 0.0014206657651811838, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -2.25, Mean Entropy: 0.0013968420680612326, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 702,  Mean reward: -2.5, Mean Entropy: 0.0016137845814228058, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 703,  Mean reward: -0.5, Mean Entropy: 0.002232747618108988, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 704,  Mean reward: -2.0, Mean Entropy: 0.0019921171478927135, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 705,  Mean reward: -4.0, Mean Entropy: 0.0012500740122050047, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 706,  Mean reward: -2.5, Mean Entropy: 0.0011831946903839707, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 707,  Mean reward: -0.75, Mean Entropy: 0.0012201035860925913, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 708,  Mean reward: -2.0, Mean Entropy: 0.0008892783662304282, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 709,  Mean reward: -2.25, Mean Entropy: 0.0010447734966874123, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 710,  Mean reward: -1.25, Mean Entropy: 0.0012424716260284185, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 711,  Mean reward: -2.5, Mean Entropy: 0.0009552111732773483, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 712,  Mean reward: -2.0, Mean Entropy: 0.0010747162159532309, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 713,  Mean reward: -1.5, Mean Entropy: 0.0010881577618420124, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 714,  Mean reward: -1.25, Mean Entropy: 0.0011719546746462584, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 715,  Mean reward: -3.5, Mean Entropy: 0.000916767749004066, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 716,  Mean reward: -0.25, Mean Entropy: 0.0010574774350970984, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 717,  Mean reward: -0.25, Mean Entropy: 0.0010944780660793185, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 718,  Mean reward: -2.5, Mean Entropy: 0.0009078631410375237, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 719,  Mean reward: -1.0, Mean Entropy: 0.0009913062676787376, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 720,  Mean reward: -0.25, Mean Entropy: 0.0012206236133351922, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 721,  Mean reward: -4.5, Mean Entropy: 0.0008270088583230972, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 722,  Mean reward: -1.25, Mean Entropy: 0.0012050489895045757, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 723,  Mean reward: -1.5, Mean Entropy: 0.0008693422423675656, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 724,  Mean reward: -3.0, Mean Entropy: 0.0006769490428268909, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 725,  Mean reward: -1.75, Mean Entropy: 0.0008922995766624808, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 726,  Mean reward: -1.5, Mean Entropy: 0.0009273102041333914, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 727,  Mean reward: -2.5, Mean Entropy: 0.0007806167704984546, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 728,  Mean reward: -1.75, Mean Entropy: 0.0009034177637659013, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 729,  Mean reward: -3.75, Mean Entropy: 0.0007342260796576738, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 730,  Mean reward: -3.0, Mean Entropy: 0.0008889903547242284, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 731,  Mean reward: -1.25, Mean Entropy: 0.0009194479207508266, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 732,  Mean reward: -2.0, Mean Entropy: 0.0008151062647812068, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 733,  Mean reward: -2.3987341772151898, Mean Entropy: 0.0007029877742752433, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 734,  Mean reward: -0.25, Mean Entropy: 0.0009450153447687626, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 735,  Mean reward: -1.75, Mean Entropy: 0.0008472631452605128, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 736,  Mean reward: -2.75, Mean Entropy: 0.0008314022561535239, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 737,  Mean reward: -2.5, Mean Entropy: 0.0008441979880444705, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 738,  Mean reward: -2.25, Mean Entropy: 0.0007429234101437032, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 739,  Mean reward: -1.75, Mean Entropy: 0.0007571777678094804, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 740,  Mean reward: -1.75, Mean Entropy: 0.0009163783397525549, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 741,  Mean reward: -1.75, Mean Entropy: 0.0009768547024577856, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 742,  Mean reward: -1.75, Mean Entropy: 0.0008271450060419738, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 743,  Mean reward: -2.75, Mean Entropy: 0.0007984903641045094, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 1.09s
Iteration: 744,  Mean reward: -2.25, Mean Entropy: 0.000814579427242279, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 745,  Mean reward: -1.5, Mean Entropy: 0.00069710350362584, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 746,  Mean reward: -3.0, Mean Entropy: 0.0006529713282361627, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 747,  Mean reward: -3.5, Mean Entropy: 0.0006882016896270216, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.87s
Iteration: 748,  Mean reward: -1.75, Mean Entropy: 0.0006798057584092021, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 749,  Mean reward: -0.5, Mean Entropy: 0.0008556251414120197, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 750,  Mean reward: -3.75, Mean Entropy: 0.0004679822304751724, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 751,  Mean reward: -0.75, Mean Entropy: 0.0006643002270720899, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 752,  Mean reward: -1.75, Mean Entropy: 0.000692113651894033, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 753,  Mean reward: -2.0, Mean Entropy: 0.0006663352251052856, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 754,  Mean reward: 0.75, Mean Entropy: 0.0007167432340793312, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 755,  Mean reward: -1.0, Mean Entropy: 0.0007592265028506517, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 756,  Mean reward: -2.5, Mean Entropy: 0.0006077000871300697, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 757,  Mean reward: -2.25, Mean Entropy: 0.0007100707734934986, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 758,  Mean reward: -0.75, Mean Entropy: 0.0006833141087554395, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 759,  Mean reward: -3.0, Mean Entropy: 0.0006679943762719631, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 760,  Mean reward: -1.75, Mean Entropy: 0.0005584830651059747, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 761,  Mean reward: -1.5, Mean Entropy: 0.0006058561848476529, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 762,  Mean reward: -1.25, Mean Entropy: 0.0005838089273311198, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 763,  Mean reward: -0.75, Mean Entropy: 0.0006744943093508482, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 764,  Mean reward: -2.25, Mean Entropy: 0.0005036607617512345, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 765,  Mean reward: -1.5, Mean Entropy: 0.0005360022187232971, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 766,  Mean reward: -4.0, Mean Entropy: 0.00039333925815299153, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 767,  Mean reward: -3.0, Mean Entropy: 0.00042811757884919643, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 768,  Mean reward: -0.5, Mean Entropy: 0.0004680203855969012, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 769,  Mean reward: -2.0, Mean Entropy: 0.0004459419287741184, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.90s
Iteration: 770,  Mean reward: -3.25, Mean Entropy: 0.0003795787342824042, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.79s
Iteration: 771,  Mean reward: -0.25, Mean Entropy: 0.000566322123631835, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 772,  Mean reward: -1.75, Mean Entropy: 0.00043700647074729204, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 773,  Mean reward: -2.25, Mean Entropy: 0.0004930419381707907, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 774,  Mean reward: -3.25, Mean Entropy: 0.0004081049410160631, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 775,  Mean reward: -1.0, Mean Entropy: 0.00044358623563311994, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 776,  Mean reward: -0.75, Mean Entropy: 0.0005834531038999557, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 777,  Mean reward: -2.5, Mean Entropy: 0.00048608583165332675, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 778,  Mean reward: -3.75, Mean Entropy: 0.0003905111807398498, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 779,  Mean reward: -1.0, Mean Entropy: 0.0004870341799687594, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 780,  Mean reward: -3.75, Mean Entropy: 0.00036418912350200117, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 781,  Mean reward: -2.25, Mean Entropy: 0.0004282994195818901, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 782,  Mean reward: -2.5, Mean Entropy: 0.00041119172237813473, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 783,  Mean reward: -0.5, Mean Entropy: 0.00044534087646752596, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 784,  Mean reward: -3.0, Mean Entropy: 0.00037663005059584975, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 785,  Mean reward: -1.0, Mean Entropy: 0.0005119742127135396, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 786,  Mean reward: -2.25, Mean Entropy: 0.00045511647476814687, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 787,  Mean reward: -2.25, Mean Entropy: 0.00046234449837356806, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 788,  Mean reward: -3.75, Mean Entropy: 0.00039027942693792284, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 789,  Mean reward: -3.0, Mean Entropy: 0.0003760490217246115, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 790,  Mean reward: -3.5, Mean Entropy: 0.00040551985148340464, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 791,  Mean reward: -3.25, Mean Entropy: 0.00041129018063656986, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 792,  Mean reward: -2.25, Mean Entropy: 0.0003902023599948734, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.90s
Iteration: 793,  Mean reward: -4.0, Mean Entropy: 0.00032948440639302135, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 794,  Mean reward: -1.25, Mean Entropy: 0.00044283244642429054, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 795,  Mean reward: -2.5, Mean Entropy: 0.0003648675628937781, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 796,  Mean reward: -2.5, Mean Entropy: 0.0003823787847068161, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 797,  Mean reward: -0.5, Mean Entropy: 0.000407918356359005, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 798,  Mean reward: -2.25, Mean Entropy: 0.00038405958912335336, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 799,  Mean reward: -2.75, Mean Entropy: 0.0003712436882779002, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 800,  Mean reward: -1.0, Mean Entropy: 0.00043723685666918755, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -3.75, Mean Entropy: 0.00028928075334988534, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 802,  Mean reward: -2.25, Mean Entropy: 0.00041623375727795064, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 803,  Mean reward: -0.5, Mean Entropy: 0.0004720660799648613, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 804,  Mean reward: -1.0, Mean Entropy: 0.00042601849418133497, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 805,  Mean reward: -1.75, Mean Entropy: 0.0003775007789954543, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 806,  Mean reward: -2.75, Mean Entropy: 0.00035081556416116655, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 807,  Mean reward: -2.0, Mean Entropy: 0.00042653270065784454, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 808,  Mean reward: -1.25, Mean Entropy: 0.0003721590037457645, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 809,  Mean reward: 0.0, Mean Entropy: 0.00045481743291020393, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 810,  Mean reward: -3.75, Mean Entropy: 0.0002684046921785921, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 811,  Mean reward: -1.5, Mean Entropy: 0.0004520689544733614, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 812,  Mean reward: -1.25, Mean Entropy: 0.00033586774952709675, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 813,  Mean reward: -0.75, Mean Entropy: 0.000346253247698769, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 814,  Mean reward: -1.0, Mean Entropy: 0.0003160200431011617, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 815,  Mean reward: -2.5, Mean Entropy: 0.00032084365375339985, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 816,  Mean reward: -2.5, Mean Entropy: 0.00031466057407669723, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 817,  Mean reward: -1.5, Mean Entropy: 0.0003449009673204273, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 818,  Mean reward: -1.5, Mean Entropy: 0.0003528526285663247, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 819,  Mean reward: -3.25, Mean Entropy: 0.00029857060872018337, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 820,  Mean reward: -4.0, Mean Entropy: 0.0002929750771727413, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 821,  Mean reward: -2.75, Mean Entropy: 0.00031057954765856266, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 822,  Mean reward: -2.75, Mean Entropy: 0.00032588449539616704, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 823,  Mean reward: -3.0, Mean Entropy: 0.00026450282894074917, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 824,  Mean reward: -3.5, Mean Entropy: 0.0002916931116487831, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 825,  Mean reward: -2.25, Mean Entropy: 0.00028925639344379306, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 826,  Mean reward: -3.0, Mean Entropy: 0.0002775700413621962, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 827,  Mean reward: -2.0, Mean Entropy: 0.00031020896858535707, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 828,  Mean reward: -1.75, Mean Entropy: 0.00034781225258484483, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 829,  Mean reward: -2.75, Mean Entropy: 0.0003578092437237501, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 830,  Mean reward: -3.25, Mean Entropy: 0.00032465733238495886, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 831,  Mean reward: 0.5, Mean Entropy: 0.0003558924363460392, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 832,  Mean reward: -0.75, Mean Entropy: 0.0003316251968499273, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 833,  Mean reward: -2.25, Mean Entropy: 0.00031033993582241237, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 834,  Mean reward: -2.5, Mean Entropy: 0.00027186417719349265, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 835,  Mean reward: -3.0, Mean Entropy: 0.00026976969093084335, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 836,  Mean reward: -1.0, Mean Entropy: 0.0002779162023216486, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 837,  Mean reward: -0.25, Mean Entropy: 0.00029941261163912714, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 838,  Mean reward: -3.75, Mean Entropy: 0.00023767078528180718, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 839,  Mean reward: -1.5, Mean Entropy: 0.0002714783768169582, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 840,  Mean reward: -1.5, Mean Entropy: 0.00027933978708460927, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 841,  Mean reward: -0.5, Mean Entropy: 0.00027368078008294106, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 842,  Mean reward: -0.75, Mean Entropy: 0.00027637192397378385, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 843,  Mean reward: -1.25, Mean Entropy: 0.0002679775934666395, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 844,  Mean reward: -2.75, Mean Entropy: 0.00023656937992200255, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 845,  Mean reward: -2.25, Mean Entropy: 0.0002467305166646838, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 846,  Mean reward: -1.75, Mean Entropy: 0.00024817135999910533, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 847,  Mean reward: -2.25, Mean Entropy: 0.00022628861188422889, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 848,  Mean reward: -2.75, Mean Entropy: 0.0002274153957841918, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 849,  Mean reward: -2.25, Mean Entropy: 0.00023579131811857224, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 850,  Mean reward: -0.75, Mean Entropy: 0.000225682626478374, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 851,  Mean reward: -3.5, Mean Entropy: 0.00021632466814480722, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 852,  Mean reward: -1.5, Mean Entropy: 0.00023774213332217187, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 853,  Mean reward: 0.0, Mean Entropy: 0.0002504945732653141, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 854,  Mean reward: -1.25, Mean Entropy: 0.0002324895467609167, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 855,  Mean reward: -4.0, Mean Entropy: 0.00021291177836246789, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 856,  Mean reward: -0.75, Mean Entropy: 0.0002496764063835144, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 857,  Mean reward: -2.75, Mean Entropy: 0.00024900809512473643, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 858,  Mean reward: -1.75, Mean Entropy: 0.00029081152752041817, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 859,  Mean reward: -1.75, Mean Entropy: 0.0003183399676345289, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 860,  Mean reward: -1.5, Mean Entropy: 0.0003492206451483071, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 861,  Mean reward: -1.25, Mean Entropy: 0.00036597810685634613, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 862,  Mean reward: -2.25, Mean Entropy: 0.0003218029742129147, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 863,  Mean reward: -3.0, Mean Entropy: 0.00025345291942358017, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 864,  Mean reward: -2.25, Mean Entropy: 0.00025108217960223556, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 865,  Mean reward: -0.75, Mean Entropy: 0.00022855422866996378, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 866,  Mean reward: -2.25, Mean Entropy: 0.00022424478083848953, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 867,  Mean reward: -2.0, Mean Entropy: 0.0002514135558158159, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 868,  Mean reward: -0.5, Mean Entropy: 0.0002631314273457974, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 869,  Mean reward: -1.75, Mean Entropy: 0.00026256823912262917, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 870,  Mean reward: -1.25, Mean Entropy: 0.00030602928018197417, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 871,  Mean reward: -3.5, Mean Entropy: 0.00029673095559701324, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 872,  Mean reward: -2.0, Mean Entropy: 0.00032584284781478345, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 873,  Mean reward: -2.0, Mean Entropy: 0.00037315126974135637, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 874,  Mean reward: -0.5, Mean Entropy: 0.0003868884814437479, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 875,  Mean reward: -3.75, Mean Entropy: 0.0004071560688316822, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 876,  Mean reward: -1.75, Mean Entropy: 0.00041397777386009693, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 877,  Mean reward: -1.75, Mean Entropy: 0.00041799675091169775, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 878,  Mean reward: -1.25, Mean Entropy: 0.00038076945929788053, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 879,  Mean reward: -2.75, Mean Entropy: 0.0003417191037442535, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 880,  Mean reward: 0.5, Mean Entropy: 0.00041203838190995157, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 881,  Mean reward: -2.0, Mean Entropy: 0.0005256377044133842, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 882,  Mean reward: -2.25, Mean Entropy: 0.0007112291641533375, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 883,  Mean reward: -0.75, Mean Entropy: 0.0006320172105915844, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 884,  Mean reward: -3.0, Mean Entropy: 0.0003982476773671806, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 885,  Mean reward: -3.75, Mean Entropy: 0.0002903086715377867, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 886,  Mean reward: -3.25, Mean Entropy: 0.0002662639017216861, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 887,  Mean reward: -2.75, Mean Entropy: 0.00029465320403687656, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 888,  Mean reward: 0.0, Mean Entropy: 0.0003211934817954898, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 889,  Mean reward: 0.0, Mean Entropy: 0.0002899656246881932, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 890,  Mean reward: -2.5, Mean Entropy: 0.00023221182345878333, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 891,  Mean reward: -1.0, Mean Entropy: 0.0002782702213153243, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 892,  Mean reward: -0.5, Mean Entropy: 0.00025688132154755294, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 893,  Mean reward: -3.5, Mean Entropy: 0.00021589951938949525, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 894,  Mean reward: -1.5, Mean Entropy: 0.0002291513083036989, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 895,  Mean reward: -1.25, Mean Entropy: 0.0002515188534744084, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 896,  Mean reward: -3.0, Mean Entropy: 0.00024137126456480473, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 897,  Mean reward: -3.0, Mean Entropy: 0.00023606966715306044, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 898,  Mean reward: 0.5, Mean Entropy: 0.0002935293014161289, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 899,  Mean reward: -1.75, Mean Entropy: 0.0002290574775543064, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 900,  Mean reward: -0.25, Mean Entropy: 0.00027020485140383244, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -2.25, Mean Entropy: 0.00023124381550587714, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 902,  Mean reward: -4.75, Mean Entropy: 0.00020120553381275386, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 903,  Mean reward: -3.75, Mean Entropy: 0.000185732074896805, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 904,  Mean reward: -1.25, Mean Entropy: 0.0002214063861174509, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 905,  Mean reward: 0.25, Mean Entropy: 0.0002759150811471045, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 906,  Mean reward: -1.5, Mean Entropy: 0.0002389874280197546, complete_episode_count: 80.0, Gather time: 0.66s, Train time: 0.71s
Iteration: 907,  Mean reward: -2.75, Mean Entropy: 0.0002107391192112118, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 908,  Mean reward: -1.25, Mean Entropy: 0.00024918123381212354, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 909,  Mean reward: -1.25, Mean Entropy: 0.00022991918376646936, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 910,  Mean reward: -3.0, Mean Entropy: 0.00021313512115739286, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 911,  Mean reward: -1.5, Mean Entropy: 0.000261011446127668, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 912,  Mean reward: -3.0, Mean Entropy: 0.00022758086561225355, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 913,  Mean reward: -0.75, Mean Entropy: 0.000263539666775614, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 914,  Mean reward: -1.0, Mean Entropy: 0.0002529180783312768, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -3.352272727272727, Mean Entropy: 0.9386367797851562, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.47s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.440476190476191, Mean Entropy: 0.9602975845336914, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 2,  Mean reward: -4.230769230769231, Mean Entropy: 0.9025354385375977, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 3,  Mean reward: -5.6, Mean Entropy: 0.8375529050827026, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 4,  Mean reward: -6.0, Mean Entropy: 0.9386367797851562, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 5,  Mean reward: -4.802631578947368, Mean Entropy: 0.9530774354934692, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 6,  Mean reward: -4.9523809523809526, Mean Entropy: 0.9458571672439575, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 7,  Mean reward: -2.8292682926829267, Mean Entropy: 0.960297703742981, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 8,  Mean reward: -3.573170731707317, Mean Entropy: 0.8736542463302612, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.58s
Iteration: 9,  Mean reward: -5.177777777777778, Mean Entropy: 0.9458562135696411, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 10,  Mean reward: -4.329545454545454, Mean Entropy: 0.9241957664489746, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 11,  Mean reward: -5.273809523809524, Mean Entropy: 0.9891778826713562, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 12,  Mean reward: -3.1341463414634148, Mean Entropy: 0.8592137098312378, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 13,  Mean reward: -3.9285714285714284, Mean Entropy: 0.9241937398910522, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 14,  Mean reward: -5.813953488372093, Mean Entropy: 0.9241950511932373, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 15,  Mean reward: -5.121951219512195, Mean Entropy: 0.9602975249290466, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 16,  Mean reward: -5.059523809523809, Mean Entropy: 0.9241956472396851, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.68s
Iteration: 17,  Mean reward: -6.8, Mean Entropy: 0.9458556175231934, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 18,  Mean reward: -4.443181818181818, Mean Entropy: 0.9097554087638855, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 19,  Mean reward: -4.7682926829268295, Mean Entropy: 0.9819573163986206, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 20,  Mean reward: -6.325, Mean Entropy: 0.916973352432251, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 21,  Mean reward: -4.297872340425532, Mean Entropy: 0.9891777038574219, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 22,  Mean reward: -3.9166666666666665, Mean Entropy: 0.9025343656539917, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 23,  Mean reward: -5.127906976744186, Mean Entropy: 0.9819507598876953, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.62s
Iteration: 24,  Mean reward: -3.86046511627907, Mean Entropy: 0.9314149618148804, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 25,  Mean reward: -3.7261904761904763, Mean Entropy: 0.9674968719482422, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 26,  Mean reward: -6.372093023255814, Mean Entropy: 0.9819571375846863, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 27,  Mean reward: -3.341463414634146, Mean Entropy: 0.9458563327789307, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 28,  Mean reward: -3.104651162790698, Mean Entropy: 0.9530739784240723, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 29,  Mean reward: -3.9, Mean Entropy: 0.9314165115356445, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 30,  Mean reward: -4.034883720930233, Mean Entropy: 1.003614068031311, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 31,  Mean reward: -3.268292682926829, Mean Entropy: 0.960259199142456, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 32,  Mean reward: -4.621951219512195, Mean Entropy: 0.9241865873336792, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 33,  Mean reward: -4.72093023255814, Mean Entropy: 0.9530526995658875, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 34,  Mean reward: -4.304878048780488, Mean Entropy: 0.9386172294616699, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 35,  Mean reward: -5.670731707317073, Mean Entropy: 0.8736345171928406, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 36,  Mean reward: -3.2375, Mean Entropy: 0.9891664981842041, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 37,  Mean reward: -3.227272727272727, Mean Entropy: 0.9819387793540955, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 38,  Mean reward: -5.7272727272727275, Mean Entropy: 0.9241915941238403, complete_episode_count: 44.0, Gather time: 0.69s, Train time: 1.51s
Iteration: 39,  Mean reward: -3.425, Mean Entropy: 0.8736240267753601, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 40,  Mean reward: -4.293478260869565, Mean Entropy: 0.9169675707817078, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 41,  Mean reward: -3.7439024390243905, Mean Entropy: 0.9241905212402344, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 42,  Mean reward: -4.511627906976744, Mean Entropy: 0.9747244715690613, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 43,  Mean reward: -5.556818181818182, Mean Entropy: 0.9457241892814636, complete_episode_count: 44.0, Gather time: 0.71s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 44,  Mean reward: -2.511627906976744, Mean Entropy: 0.9602965116500854, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 45,  Mean reward: -4.136363636363637, Mean Entropy: 1.010839581489563, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 46,  Mean reward: -7.414634146341464, Mean Entropy: 0.9097552299499512, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 47,  Mean reward: -4.858974358974359, Mean Entropy: 0.8375524282455444, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 48,  Mean reward: -5.975609756097561, Mean Entropy: 0.960295557975769, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 49,  Mean reward: -2.7625, Mean Entropy: 0.9675168991088867, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 50,  Mean reward: -3.3125, Mean Entropy: 0.9530757665634155, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 51,  Mean reward: -3.302325581395349, Mean Entropy: 0.9963918924331665, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 52,  Mean reward: -3.5125, Mean Entropy: 0.8952841758728027, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.60s
Iteration: 53,  Mean reward: -4.0131578947368425, Mean Entropy: 0.9313647150993347, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.64s
Iteration: 54,  Mean reward: -5.878048780487805, Mean Entropy: 0.9675101637840271, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 55,  Mean reward: -5.3875, Mean Entropy: 0.8880891799926758, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 56,  Mean reward: -2.4634146341463414, Mean Entropy: 0.916965126991272, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 57,  Mean reward: -3.2325581395348837, Mean Entropy: 0.945831298828125, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 58,  Mean reward: -5.75, Mean Entropy: 0.9241538047790527, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 59,  Mean reward: -4.8625, Mean Entropy: 1.0396116971969604, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 60,  Mean reward: -5.273809523809524, Mean Entropy: 0.9240597486495972, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 61,  Mean reward: -4.891891891891892, Mean Entropy: 0.9097390174865723, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 62,  Mean reward: -3.8181818181818183, Mean Entropy: 0.9530744552612305, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 63,  Mean reward: -3.717948717948718, Mean Entropy: 1.003610372543335, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 64,  Mean reward: -4.523809523809524, Mean Entropy: 0.9747207164764404, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 65,  Mean reward: -4.829268292682927, Mean Entropy: 0.9169427156448364, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 66,  Mean reward: -2.72972972972973, Mean Entropy: 0.9096909761428833, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 67,  Mean reward: -4.560975609756097, Mean Entropy: 0.8805932998657227, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.59s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 68,  Mean reward: -1.9, Mean Entropy: 0.9674903750419617, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 69,  Mean reward: -3.269230769230769, Mean Entropy: 0.9819518327713013, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 70,  Mean reward: -5.777777777777778, Mean Entropy: 0.9241752624511719, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 71,  Mean reward: -3.3372093023255816, Mean Entropy: 0.9530715942382812, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 72,  Mean reward: -2.451219512195122, Mean Entropy: 0.9458509683609009, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 73,  Mean reward: -2.761904761904762, Mean Entropy: 0.9169726371765137, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 74,  Mean reward: -5.7555555555555555, Mean Entropy: 0.9458538889884949, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 75,  Mean reward: -4.913043478260869, Mean Entropy: 0.9386305809020996, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 76,  Mean reward: -6.142857142857143, Mean Entropy: 0.9386281371116638, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 77,  Mean reward: -5.416666666666667, Mean Entropy: 0.9458259344100952, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 78,  Mean reward: -4.568181818181818, Mean Entropy: 0.8664169311523438, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 79,  Mean reward: -4.2625, Mean Entropy: 0.9385991096496582, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 80,  Mean reward: -7.465909090909091, Mean Entropy: 0.9097253680229187, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 81,  Mean reward: -3.116279069767442, Mean Entropy: 0.9530684351921082, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 82,  Mean reward: -4.625, Mean Entropy: 0.9602259397506714, complete_episode_count: 40.0, Gather time: 0.66s, Train time: 1.50s
Iteration: 83,  Mean reward: -2.616279069767442, Mean Entropy: 0.9891447424888611, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 84,  Mean reward: -5.476190476190476, Mean Entropy: 0.9457688331604004, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 85,  Mean reward: -6.024390243902439, Mean Entropy: 0.9527837038040161, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 86,  Mean reward: -1.6744186046511629, Mean Entropy: 0.9819467067718506, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 87,  Mean reward: -3.9615384615384617, Mean Entropy: 0.8953012228012085, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 88,  Mean reward: -5.627906976744186, Mean Entropy: 0.8952224254608154, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 89,  Mean reward: -4.4625, Mean Entropy: 0.9530555009841919, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 90,  Mean reward: -4.372093023255814, Mean Entropy: 0.996292769908905, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 91,  Mean reward: -7.395348837209302, Mean Entropy: 0.8446671366691589, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 92,  Mean reward: -5.075, Mean Entropy: 0.938535213470459, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 93,  Mean reward: -5.588888888888889, Mean Entropy: 0.9818629026412964, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 94,  Mean reward: -4.6477272727272725, Mean Entropy: 1.0324829816818237, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 95,  Mean reward: -5.67948717948718, Mean Entropy: 0.8230141401290894, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 96,  Mean reward: -5.402439024390244, Mean Entropy: 0.9891101717948914, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 97,  Mean reward: -3.5609756097560976, Mean Entropy: 0.9386197924613953, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 98,  Mean reward: -7.440476190476191, Mean Entropy: 0.9529697895050049, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 99,  Mean reward: -3.095744680851064, Mean Entropy: 1.003595232963562, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 100,  Mean reward: -4.261904761904762, Mean Entropy: 0.9458421468734741, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -6.170731707317073, Mean Entropy: 0.9022914171218872, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 102,  Mean reward: -3.4204545454545454, Mean Entropy: 0.9890555739402771, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 103,  Mean reward: -7.25, Mean Entropy: 0.9457439184188843, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 104,  Mean reward: -3.3214285714285716, Mean Entropy: 0.9528228640556335, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 105,  Mean reward: -6.1375, Mean Entropy: 0.9240458011627197, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 106,  Mean reward: -3.292682926829268, Mean Entropy: 0.909675121307373, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 107,  Mean reward: -5.6, Mean Entropy: 0.9601854085922241, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 108,  Mean reward: -6.524390243902439, Mean Entropy: 0.8878898620605469, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 109,  Mean reward: -5.055555555555555, Mean Entropy: 0.9241721034049988, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 110,  Mean reward: -5.064102564102564, Mean Entropy: 0.9457501173019409, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 111,  Mean reward: -5.813953488372093, Mean Entropy: 0.9384192824363708, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 112,  Mean reward: -6.5813953488372094, Mean Entropy: 0.9313575625419617, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 113,  Mean reward: -3.9047619047619047, Mean Entropy: 0.9312704801559448, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 114,  Mean reward: -4.1375, Mean Entropy: 0.9241223335266113, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 115,  Mean reward: -4.159090909090909, Mean Entropy: 0.9313570857048035, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 116,  Mean reward: -3.2375, Mean Entropy: 0.8735779523849487, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 117,  Mean reward: -4.2272727272727275, Mean Entropy: 0.9745867252349854, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 118,  Mean reward: -5.559523809523809, Mean Entropy: 0.9385101795196533, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 119,  Mean reward: -6.083333333333333, Mean Entropy: 0.9168758392333984, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 120,  Mean reward: -5.441860465116279, Mean Entropy: 0.9457449913024902, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 121,  Mean reward: -5.283783783783784, Mean Entropy: 0.8879746794700623, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 122,  Mean reward: -6.175, Mean Entropy: 0.9526156187057495, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 123,  Mean reward: -3.941860465116279, Mean Entropy: 0.9594211578369141, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 124,  Mean reward: -5.0, Mean Entropy: 0.9600273966789246, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 125,  Mean reward: -4.576923076923077, Mean Entropy: 0.9673118591308594, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 126,  Mean reward: -5.9125, Mean Entropy: 0.9384199976921082, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 127,  Mean reward: -3.75, Mean Entropy: 0.9671816229820251, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.69s
Iteration: 128,  Mean reward: -3.5853658536585367, Mean Entropy: 0.8873709440231323, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 129,  Mean reward: -3.2790697674418605, Mean Entropy: 0.9801005125045776, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 130,  Mean reward: -4.178571428571429, Mean Entropy: 0.9810940027236938, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 131,  Mean reward: -4.222222222222222, Mean Entropy: 0.9233665466308594, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 132,  Mean reward: -4.222222222222222, Mean Entropy: 0.9513565897941589, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 133,  Mean reward: -6.9523809523809526, Mean Entropy: 1.0025954246520996, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 134,  Mean reward: -2.0795454545454546, Mean Entropy: 0.9384249448776245, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 135,  Mean reward: -4.085106382978723, Mean Entropy: 0.9233435392379761, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 136,  Mean reward: -2.388888888888889, Mean Entropy: 0.9814943075180054, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 137,  Mean reward: -5.630952380952381, Mean Entropy: 0.9094879031181335, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 138,  Mean reward: -3.5, Mean Entropy: 0.9456478953361511, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 139,  Mean reward: -4.1875, Mean Entropy: 0.938506007194519, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 140,  Mean reward: -4.181818181818182, Mean Entropy: 0.8952252268791199, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 141,  Mean reward: -5.0, Mean Entropy: 0.9313688278198242, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 142,  Mean reward: -5.025, Mean Entropy: 0.902493953704834, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 143,  Mean reward: -5.426829268292683, Mean Entropy: 0.9097123146057129, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 144,  Mean reward: -3.9743589743589745, Mean Entropy: 0.8880350589752197, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 145,  Mean reward: -1.6547619047619047, Mean Entropy: 0.9819371700286865, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 146,  Mean reward: -1.4204545454545454, Mean Entropy: 0.9602693319320679, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 147,  Mean reward: -5.2375, Mean Entropy: 0.8736297488212585, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 148,  Mean reward: -3.1219512195121952, Mean Entropy: 0.9458363056182861, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 149,  Mean reward: -4.159090909090909, Mean Entropy: 0.9674885272979736, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 150,  Mean reward: -2.825, Mean Entropy: 0.9385942816734314, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 151,  Mean reward: -1.630952380952381, Mean Entropy: 0.9819200038909912, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 152,  Mean reward: -2.7777777777777777, Mean Entropy: 0.9241632223129272, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 153,  Mean reward: -4.232558139534884, Mean Entropy: 0.9458247423171997, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 154,  Mean reward: -6.420454545454546, Mean Entropy: 0.9457840919494629, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 155,  Mean reward: -2.6025641025641026, Mean Entropy: 0.9674930572509766, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 156,  Mean reward: -6.605263157894737, Mean Entropy: 0.9819329977035522, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 157,  Mean reward: -4.2682926829268295, Mean Entropy: 0.9169613718986511, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 158,  Mean reward: -4.988095238095238, Mean Entropy: 0.895298957824707, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 159,  Mean reward: -7.947368421052632, Mean Entropy: 0.8880648612976074, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 160,  Mean reward: -4.092105263157895, Mean Entropy: 0.9458322525024414, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 161,  Mean reward: -5.0625, Mean Entropy: 0.9313947558403015, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 162,  Mean reward: -5.523255813953488, Mean Entropy: 0.9169365167617798, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 163,  Mean reward: -4.357142857142857, Mean Entropy: 0.8952826261520386, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 164,  Mean reward: -5.1, Mean Entropy: 0.9386000633239746, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.70s
Iteration: 165,  Mean reward: -2.6341463414634148, Mean Entropy: 0.9385876059532166, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 166,  Mean reward: -0.9, Mean Entropy: 0.9458119869232178, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 167,  Mean reward: -5.865853658536586, Mean Entropy: 0.9602513313293457, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 168,  Mean reward: -4.654761904761905, Mean Entropy: 0.9674737453460693, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 169,  Mean reward: -3.7888888888888888, Mean Entropy: 0.8664075136184692, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 170,  Mean reward: -3.7142857142857144, Mean Entropy: 0.9746960401535034, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 171,  Mean reward: -4.3375, Mean Entropy: 0.9674543738365173, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 172,  Mean reward: -4.780487804878049, Mean Entropy: 0.8808199763298035, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 173,  Mean reward: -6.488636363636363, Mean Entropy: 0.909652590751648, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 174,  Mean reward: -6.0875, Mean Entropy: 0.8951983451843262, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 175,  Mean reward: -4.975, Mean Entropy: 0.9024352431297302, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 176,  Mean reward: -3.5, Mean Entropy: 0.9312488436698914, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 177,  Mean reward: -5.121951219512195, Mean Entropy: 0.9240058660507202, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 178,  Mean reward: -4.761363636363637, Mean Entropy: 0.9601578712463379, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 179,  Mean reward: -2.4761904761904763, Mean Entropy: 0.9457286596298218, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 180,  Mean reward: -3.1153846153846154, Mean Entropy: 0.9312557578086853, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 181,  Mean reward: -5.684210526315789, Mean Entropy: 0.9168566465377808, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 182,  Mean reward: -5.232558139534884, Mean Entropy: 0.9456018209457397, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 183,  Mean reward: -2.0789473684210527, Mean Entropy: 0.959874153137207, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 184,  Mean reward: -5.2894736842105265, Mean Entropy: 0.93077152967453, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 185,  Mean reward: -6.214285714285714, Mean Entropy: 0.9520348310470581, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 186,  Mean reward: -4.545454545454546, Mean Entropy: 0.9655501246452332, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 187,  Mean reward: -3.825, Mean Entropy: 0.9012801051139832, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 188,  Mean reward: -4.630434782608695, Mean Entropy: 0.9225696921348572, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 189,  Mean reward: -4.2875, Mean Entropy: 0.9276609420776367, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 190,  Mean reward: -2.933333333333333, Mean Entropy: 0.900537371635437, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 191,  Mean reward: -3.9615384615384617, Mean Entropy: 0.9621578454971313, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 192,  Mean reward: -2.9431818181818183, Mean Entropy: 0.9491530656814575, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 193,  Mean reward: -1.475609756097561, Mean Entropy: 0.9762320518493652, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 194,  Mean reward: -3.1354166666666665, Mean Entropy: 0.9188001155853271, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 195,  Mean reward: -2.1847826086956523, Mean Entropy: 0.9585587382316589, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 196,  Mean reward: -4.670731707317073, Mean Entropy: 0.8705053329467773, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 197,  Mean reward: -0.5952380952380952, Mean Entropy: 0.8658514022827148, complete_episode_count: 42.0, Gather time: 0.65s, Train time: 1.48s
Iteration: 198,  Mean reward: -4.543478260869565, Mean Entropy: 0.9566023349761963, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 199,  Mean reward: -4.666666666666667, Mean Entropy: 0.9531762003898621, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 200,  Mean reward: -2.8625, Mean Entropy: 0.8728547096252441, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.49s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -5.1022727272727275, Mean Entropy: 0.8814539909362793, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.69s
Iteration: 202,  Mean reward: -5.068181818181818, Mean Entropy: 0.8983657360076904, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 203,  Mean reward: -1.630952380952381, Mean Entropy: 0.9026861190795898, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 204,  Mean reward: -1.5543478260869565, Mean Entropy: 0.9054824113845825, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 205,  Mean reward: -4.1375, Mean Entropy: 1.0083587169647217, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 206,  Mean reward: -5.719512195121951, Mean Entropy: 0.9451786875724792, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 207,  Mean reward: -4.943181818181818, Mean Entropy: 0.8779975175857544, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 208,  Mean reward: -3.011111111111111, Mean Entropy: 0.9269798994064331, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 209,  Mean reward: -6.636363636363637, Mean Entropy: 0.9152547717094421, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 210,  Mean reward: -3.9239130434782608, Mean Entropy: 0.9362909197807312, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 211,  Mean reward: -3.606382978723404, Mean Entropy: 0.8642692565917969, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 212,  Mean reward: -3.3684210526315788, Mean Entropy: 0.9392906427383423, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 213,  Mean reward: -3.488372093023256, Mean Entropy: 0.8895419836044312, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 214,  Mean reward: -1.2826086956521738, Mean Entropy: 0.9139954447746277, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 215,  Mean reward: -4.973684210526316, Mean Entropy: 0.9033428430557251, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 216,  Mean reward: -5.5875, Mean Entropy: 0.8964404463768005, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 217,  Mean reward: -4.914634146341464, Mean Entropy: 0.9480026960372925, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 218,  Mean reward: -2.7439024390243905, Mean Entropy: 0.8641892671585083, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 219,  Mean reward: -0.5408163265306123, Mean Entropy: 0.9523429870605469, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 220,  Mean reward: -6.2875, Mean Entropy: 0.9000978469848633, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 221,  Mean reward: -3.3068181818181817, Mean Entropy: 0.948138415813446, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 222,  Mean reward: -2.2127659574468086, Mean Entropy: 0.8903498649597168, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 223,  Mean reward: -2.6794871794871793, Mean Entropy: 0.892120361328125, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 224,  Mean reward: 2.1595744680851063, Mean Entropy: 0.8791416883468628, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 225,  Mean reward: -6.397435897435898, Mean Entropy: 0.8709556460380554, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 226,  Mean reward: -2.7282608695652173, Mean Entropy: 0.8234538435935974, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 227,  Mean reward: -0.9803921568627451, Mean Entropy: 0.9165539741516113, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 228,  Mean reward: -2.84, Mean Entropy: 0.9293516874313354, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 229,  Mean reward: -1.697674418604651, Mean Entropy: 0.8849133253097534, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 230,  Mean reward: -1.9387755102040816, Mean Entropy: 0.8172685503959656, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 231,  Mean reward: 4.028846153846154, Mean Entropy: 0.8841807246208191, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 232,  Mean reward: -2.76, Mean Entropy: 0.6145994067192078, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 233,  Mean reward: 2.19, Mean Entropy: 0.9144879579544067, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 234,  Mean reward: 2.077777777777778, Mean Entropy: 0.8759535551071167, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 235,  Mean reward: -5.313725490196078, Mean Entropy: 0.7403959631919861, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 236,  Mean reward: 5.175438596491228, Mean Entropy: 0.8964292407035828, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 237,  Mean reward: 0.6976744186046512, Mean Entropy: 0.7777494192123413, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.66s
Iteration: 238,  Mean reward: 2.647727272727273, Mean Entropy: 0.8225057125091553, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 239,  Mean reward: 1.2954545454545454, Mean Entropy: 0.8036677241325378, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 240,  Mean reward: 2.4767441860465116, Mean Entropy: 0.7718583345413208, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 241,  Mean reward: -0.0425531914893617, Mean Entropy: 0.740060031414032, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 242,  Mean reward: 2.4166666666666665, Mean Entropy: 0.7191057205200195, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 243,  Mean reward: 2.22, Mean Entropy: 0.6863489151000977, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 244,  Mean reward: 3.5849056603773586, Mean Entropy: 0.702257513999939, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 245,  Mean reward: 4.469387755102041, Mean Entropy: 0.5212132930755615, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 246,  Mean reward: -2.0153846153846153, Mean Entropy: 0.5927533507347107, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 247,  Mean reward: 3.1607142857142856, Mean Entropy: 0.7411470413208008, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 248,  Mean reward: 4.25, Mean Entropy: 0.6285011768341064, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 249,  Mean reward: 5.268518518518518, Mean Entropy: 0.48687005043029785, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 250,  Mean reward: 4.9, Mean Entropy: 0.5512761473655701, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 251,  Mean reward: 5.87962962962963, Mean Entropy: 0.4093132019042969, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.45s
Iteration: 252,  Mean reward: -3.13768115942029, Mean Entropy: 0.3608151376247406, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 253,  Mean reward: 5.509433962264151, Mean Entropy: 0.5484051704406738, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 254,  Mean reward: 4.4411764705882355, Mean Entropy: 0.47927582263946533, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 255,  Mean reward: 5.153846153846154, Mean Entropy: 0.4819587171077728, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 256,  Mean reward: 2.1206896551724137, Mean Entropy: 0.49661678075790405, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 257,  Mean reward: 2.9375, Mean Entropy: 0.6107866764068604, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 258,  Mean reward: 5.892857142857143, Mean Entropy: 0.44039225578308105, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 259,  Mean reward: 5.375, Mean Entropy: 0.32479333877563477, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 260,  Mean reward: -3.5, Mean Entropy: 0.43245524168014526, complete_episode_count: 73.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 261,  Mean reward: 4.918181818181818, Mean Entropy: 0.47712135314941406, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 262,  Mean reward: 6.366071428571429, Mean Entropy: 0.38465675711631775, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 263,  Mean reward: -2.3220338983050848, Mean Entropy: 0.4627876281738281, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 264,  Mean reward: -5.349056603773585, Mean Entropy: 0.5467292070388794, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 265,  Mean reward: -2.707317073170732, Mean Entropy: 0.7143845558166504, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 266,  Mean reward: -4.853658536585366, Mean Entropy: 0.8180524706840515, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 267,  Mean reward: -3.8513513513513513, Mean Entropy: 0.7663063406944275, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 268,  Mean reward: -5.884615384615385, Mean Entropy: 0.7732057571411133, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 269,  Mean reward: -1.2093023255813953, Mean Entropy: 0.8041015863418579, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 270,  Mean reward: -4.217948717948718, Mean Entropy: 0.8132981657981873, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 271,  Mean reward: -4.461538461538462, Mean Entropy: 0.8419572710990906, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 272,  Mean reward: -1.8289473684210527, Mean Entropy: 0.8641036748886108, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 273,  Mean reward: -3.8289473684210527, Mean Entropy: 0.7885361909866333, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 274,  Mean reward: -3.6951219512195124, Mean Entropy: 0.870142936706543, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.64s
Iteration: 275,  Mean reward: -1.7804878048780488, Mean Entropy: 0.8893362879753113, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 276,  Mean reward: -3.9146341463414633, Mean Entropy: 0.8018075823783875, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 277,  Mean reward: -0.05813953488372093, Mean Entropy: 0.8680126070976257, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 278,  Mean reward: 0.6022727272727273, Mean Entropy: 0.8285671472549438, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 279,  Mean reward: -2.902439024390244, Mean Entropy: 0.7951762080192566, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 280,  Mean reward: 0.10465116279069768, Mean Entropy: 0.781967043876648, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 281,  Mean reward: -3.369047619047619, Mean Entropy: 0.5765827894210815, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 282,  Mean reward: 5.47457627118644, Mean Entropy: 0.4441194534301758, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 283,  Mean reward: 4.723214285714286, Mean Entropy: 0.7211477160453796, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 284,  Mean reward: -0.8953488372093024, Mean Entropy: 0.5979602336883545, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 285,  Mean reward: 4.894230769230769, Mean Entropy: 0.6931967735290527, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 286,  Mean reward: 4.042553191489362, Mean Entropy: 0.6637046933174133, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 287,  Mean reward: 4.591836734693878, Mean Entropy: 0.5457046627998352, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 288,  Mean reward: 0.8064516129032258, Mean Entropy: 0.4022875726222992, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 289,  Mean reward: 1.7647058823529411, Mean Entropy: 0.517810583114624, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 290,  Mean reward: 5.018867924528302, Mean Entropy: 0.6354140043258667, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 291,  Mean reward: 4.010416666666667, Mean Entropy: 0.5361303091049194, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 292,  Mean reward: 5.845454545454546, Mean Entropy: 0.5912060737609863, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 293,  Mean reward: 4.744897959183674, Mean Entropy: 0.5019047260284424, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 294,  Mean reward: 6.483606557377049, Mean Entropy: 0.4876931607723236, complete_episode_count: 61.0, Gather time: 0.77s, Train time: 0.72s
Iteration: 295,  Mean reward: 3.4210526315789473, Mean Entropy: 0.3598765730857849, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 296,  Mean reward: 5.5964912280701755, Mean Entropy: 0.5705103874206543, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 297,  Mean reward: 5.028846153846154, Mean Entropy: 0.47289666533470154, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 298,  Mean reward: 5.852941176470588, Mean Entropy: 0.4820498824119568, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 299,  Mean reward: 5.361111111111111, Mean Entropy: 0.37998104095458984, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 300,  Mean reward: 6.163793103448276, Mean Entropy: 0.446952760219574, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.48s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 1.0508474576271187, Mean Entropy: 0.4332220256328583, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 302,  Mean reward: 5.9035087719298245, Mean Entropy: 0.5005306601524353, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 303,  Mean reward: 4.574468085106383, Mean Entropy: 0.41893261671066284, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 304,  Mean reward: 5.536363636363636, Mean Entropy: 0.42788344621658325, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 305,  Mean reward: 5.3076923076923075, Mean Entropy: 0.38626378774642944, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 306,  Mean reward: 0.38461538461538464, Mean Entropy: 0.4297298192977905, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 307,  Mean reward: 7.0, Mean Entropy: 0.3494345545768738, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 308,  Mean reward: 6.419354838709677, Mean Entropy: 0.4034593999385834, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 309,  Mean reward: 5.580357142857143, Mean Entropy: 0.34610438346862793, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 310,  Mean reward: 6.9375, Mean Entropy: 0.3070845305919647, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 311,  Mean reward: 6.69672131147541, Mean Entropy: 0.42764386534690857, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 312,  Mean reward: 5.528301886792453, Mean Entropy: 0.361126184463501, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 313,  Mean reward: 6.466101694915254, Mean Entropy: 0.35222434997558594, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 314,  Mean reward: 7.46969696969697, Mean Entropy: 0.40903839468955994, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 315,  Mean reward: 6.286885245901639, Mean Entropy: 0.29576000571250916, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 316,  Mean reward: 7.358208955223881, Mean Entropy: 0.2230362892150879, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 317,  Mean reward: 7.507462686567164, Mean Entropy: 0.24877455830574036, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.73s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 318,  Mean reward: 7.681159420289855, Mean Entropy: 0.2059488594532013, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 319,  Mean reward: 7.608695652173913, Mean Entropy: 0.2578616142272949, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 320,  Mean reward: 7.507462686567164, Mean Entropy: 0.25982406735420227, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 321,  Mean reward: 7.367647058823529, Mean Entropy: 0.21921217441558838, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 322,  Mean reward: 7.767123287671233, Mean Entropy: 0.12537044286727905, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 323,  Mean reward: 7.8175675675675675, Mean Entropy: 0.08303277939558029, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 324,  Mean reward: 7.981012658227848, Mean Entropy: 0.0967879518866539, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 325,  Mean reward: 7.921052631578948, Mean Entropy: 0.050568342208862305, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 326,  Mean reward: 7.955128205128205, Mean Entropy: 0.029088741168379784, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 327,  Mean reward: 7.981012658227848, Mean Entropy: 0.022879276424646378, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 328,  Mean reward: 7.981012658227848, Mean Entropy: 0.027224663645029068, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 329,  Mean reward: 7.981012658227848, Mean Entropy: 0.017223913222551346, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 330,  Mean reward: 8.0, Mean Entropy: 0.013070372864603996, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 331,  Mean reward: 8.0, Mean Entropy: 0.01950940303504467, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 332,  Mean reward: 7.974683544303797, Mean Entropy: 0.011791731230914593, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 333,  Mean reward: 8.0, Mean Entropy: 0.014548331499099731, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 334,  Mean reward: 8.0, Mean Entropy: 0.013299141079187393, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 335,  Mean reward: 7.981012658227848, Mean Entropy: 0.0059232087805867195, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 336,  Mean reward: 8.0, Mean Entropy: 0.004704995080828667, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 337,  Mean reward: 8.0, Mean Entropy: 0.004787039943039417, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 338,  Mean reward: 8.0, Mean Entropy: 0.004229219164699316, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 339,  Mean reward: 8.0, Mean Entropy: 0.004304344765841961, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 340,  Mean reward: 8.0, Mean Entropy: 0.1351507008075714, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 341,  Mean reward: 7.405797101449275, Mean Entropy: 0.22913095355033875, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 342,  Mean reward: 6.516129032258065, Mean Entropy: 0.24405109882354736, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 343,  Mean reward: 6.838709677419355, Mean Entropy: 0.30068695545196533, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 344,  Mean reward: 7.204545454545454, Mean Entropy: 0.28765571117401123, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 345,  Mean reward: 7.3768115942028984, Mean Entropy: 0.027079565450549126, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 346,  Mean reward: 8.0, Mean Entropy: 0.00041222941945306957, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 347,  Mean reward: -4.25, Mean Entropy: 0.0003605246893130243, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 348,  Mean reward: -3.5, Mean Entropy: 0.1515098363161087, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 349,  Mean reward: 7.8533333333333335, Mean Entropy: 0.11663840711116791, complete_episode_count: 75.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 350,  Mean reward: 7.901315789473684, Mean Entropy: 0.01068690326064825, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 351,  Mean reward: 7.961538461538462, Mean Entropy: 0.015185253694653511, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 352,  Mean reward: 7.948717948717949, Mean Entropy: 0.008216528221964836, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 353,  Mean reward: 8.0, Mean Entropy: 0.007230136543512344, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 354,  Mean reward: 8.0, Mean Entropy: 0.0076533472165465355, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.91s
Iteration: 355,  Mean reward: 8.0, Mean Entropy: 0.021553508937358856, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 356,  Mean reward: 7.981012658227848, Mean Entropy: 0.009892020374536514, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 357,  Mean reward: 8.0, Mean Entropy: 0.006413164548575878, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 358,  Mean reward: 8.0, Mean Entropy: 0.006858260370790958, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 359,  Mean reward: 8.0, Mean Entropy: 0.005794533994048834, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 360,  Mean reward: 8.0, Mean Entropy: 0.004801994655281305, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 361,  Mean reward: 8.0, Mean Entropy: 0.0047276923432946205, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 362,  Mean reward: 8.0, Mean Entropy: 0.003632158041000366, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 363,  Mean reward: 8.0, Mean Entropy: 0.004906967747956514, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 364,  Mean reward: 7.981012658227848, Mean Entropy: 0.0146395368501544, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 365,  Mean reward: 7.974683544303797, Mean Entropy: 0.0018335743807256222, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 366,  Mean reward: 8.0, Mean Entropy: 0.001502762082964182, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 367,  Mean reward: 8.0, Mean Entropy: 0.0017249255906790495, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 368,  Mean reward: 8.0, Mean Entropy: 0.0015386230079457164, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 369,  Mean reward: 8.0, Mean Entropy: 0.0016073004808276892, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 370,  Mean reward: 8.0, Mean Entropy: 0.001664196141064167, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 371,  Mean reward: 8.0, Mean Entropy: 0.0015613895375281572, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 372,  Mean reward: 8.0, Mean Entropy: 0.0015987458173185587, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 373,  Mean reward: 8.0, Mean Entropy: 0.0014900624519214034, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 374,  Mean reward: 8.0, Mean Entropy: 0.0015278698410838842, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 375,  Mean reward: 7.801282051282051, Mean Entropy: 0.0017489977180957794, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 376,  Mean reward: 8.0, Mean Entropy: 0.001468715607188642, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 377,  Mean reward: 8.0, Mean Entropy: 0.001394397346302867, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 378,  Mean reward: 8.0, Mean Entropy: 0.0014040720416232944, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 379,  Mean reward: 8.0, Mean Entropy: 0.0015789041062816978, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 380,  Mean reward: 8.0, Mean Entropy: 0.0014813663437962532, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 381,  Mean reward: 8.0, Mean Entropy: 0.0014252127148211002, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 382,  Mean reward: 8.0, Mean Entropy: 0.001100861351005733, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 383,  Mean reward: 8.0, Mean Entropy: 0.0014082560082897544, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 384,  Mean reward: 8.0, Mean Entropy: 0.0012799532851204276, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 385,  Mean reward: 8.0, Mean Entropy: 0.0010373282711952925, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 386,  Mean reward: 8.0, Mean Entropy: 0.001147158443927765, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 0.0011127934558317065, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 0.0011901225661858916, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 389,  Mean reward: 8.0, Mean Entropy: 0.0011579229030758142, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 390,  Mean reward: 8.0, Mean Entropy: 0.0011715221917256713, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 0.0010607921285554767, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 392,  Mean reward: 7.981012658227848, Mean Entropy: 0.0007956202025525272, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 0.0009460852015763521, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 0.000974235706962645, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 395,  Mean reward: 8.0, Mean Entropy: 0.0010853999992832541, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 396,  Mean reward: 8.0, Mean Entropy: 0.0012804183643311262, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 0.0014659682055935264, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 0.0016827501822263002, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 0.002722201868891716, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 0.018947632983326912, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 8.0, Mean Entropy: 0.2441478669643402, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 402,  Mean reward: 6.474137931034483, Mean Entropy: 0.4550880193710327, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 403,  Mean reward: 6.390625, Mean Entropy: 0.4423447847366333, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 404,  Mean reward: 5.782258064516129, Mean Entropy: 0.37572407722473145, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 405,  Mean reward: 5.598214285714286, Mean Entropy: 0.42645716667175293, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 406,  Mean reward: 6.237288135593221, Mean Entropy: 0.17996548116207123, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 407,  Mean reward: -2.75, Mean Entropy: 0.3933755159378052, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 408,  Mean reward: 5.663636363636364, Mean Entropy: 0.17759482562541962, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 409,  Mean reward: 7.76056338028169, Mean Entropy: 0.15441784262657166, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 410,  Mean reward: 7.157894736842105, Mean Entropy: 0.03469808027148247, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 411,  Mean reward: 8.0, Mean Entropy: 0.0007724687457084656, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 412,  Mean reward: -2.5, Mean Entropy: 0.006706029176712036, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 413,  Mean reward: -0.25, Mean Entropy: 0.011813437566161156, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 0.05741444230079651, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 415,  Mean reward: 7.922077922077922, Mean Entropy: 0.02982604131102562, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 416,  Mean reward: 7.626666666666667, Mean Entropy: 0.044669218361377716, complete_episode_count: 75.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 417,  Mean reward: 7.474683544303797, Mean Entropy: 0.004876548424363136, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 418,  Mean reward: 8.0, Mean Entropy: 0.03727754205465317, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 419,  Mean reward: 8.0, Mean Entropy: 0.031233083456754684, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 0.02618454396724701, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 421,  Mean reward: 7.962025316455696, Mean Entropy: 0.02415926568210125, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.004099048674106598, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 423,  Mean reward: -2.5, Mean Entropy: 0.0004863234353251755, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 424,  Mean reward: -1.75, Mean Entropy: 4.1678440538817085e-06, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 425,  Mean reward: -3.0, Mean Entropy: 0.0014987739268690348, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 426,  Mean reward: -3.75, Mean Entropy: 4.606362836057087e-06, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 427,  Mean reward: -1.75, Mean Entropy: 0.0003348800237290561, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 428,  Mean reward: -3.0, Mean Entropy: 0.0015804920112714171, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 429,  Mean reward: -3.5, Mean Entropy: 0.05726950615644455, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 430,  Mean reward: -0.9090909090909091, Mean Entropy: 0.11707828938961029, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 431,  Mean reward: -0.8026315789473685, Mean Entropy: 0.014104321599006653, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 432,  Mean reward: -0.75, Mean Entropy: 0.0036863076966255903, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 433,  Mean reward: -2.75, Mean Entropy: 0.005422742106020451, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 434,  Mean reward: -3.25, Mean Entropy: 0.02288452535867691, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 435,  Mean reward: -2.2948717948717947, Mean Entropy: 0.029735257849097252, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.91s
Iteration: 436,  Mean reward: -2.651898734177215, Mean Entropy: 0.03148603439331055, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 437,  Mean reward: -3.1582278481012658, Mean Entropy: 0.0368460938334465, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 438,  Mean reward: -0.7756410256410257, Mean Entropy: 0.04381513595581055, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 439,  Mean reward: -3.8333333333333335, Mean Entropy: 0.07423143833875656, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 440,  Mean reward: -2.9675324675324677, Mean Entropy: 0.08828622102737427, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 441,  Mean reward: -1.9113924050632911, Mean Entropy: 0.0337311290204525, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 442,  Mean reward: -1.1518987341772151, Mean Entropy: 0.0052545866928994656, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 443,  Mean reward: -1.25, Mean Entropy: 0.007729006465524435, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 444,  Mean reward: -2.25, Mean Entropy: 0.013990136794745922, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 445,  Mean reward: -2.1455696202531644, Mean Entropy: 0.0030637967865914106, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 446,  Mean reward: -1.5, Mean Entropy: 0.0023143987637013197, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 447,  Mean reward: -1.75, Mean Entropy: 0.0019670194014906883, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 448,  Mean reward: -3.0, Mean Entropy: 0.0014365104725584388, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 449,  Mean reward: -1.0, Mean Entropy: 0.0012712146854028106, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 450,  Mean reward: -2.0, Mean Entropy: 0.0012526310747489333, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 451,  Mean reward: -3.5, Mean Entropy: 0.0009301730315200984, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 452,  Mean reward: 0.25, Mean Entropy: 0.00128340779338032, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 453,  Mean reward: -1.0, Mean Entropy: 0.000804188079200685, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 454,  Mean reward: 0.0, Mean Entropy: 0.0011744238436222076, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 455,  Mean reward: -2.75, Mean Entropy: 0.0010081981308758259, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 456,  Mean reward: -0.75, Mean Entropy: 0.0010042418725788593, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 457,  Mean reward: -1.0, Mean Entropy: 0.0011516099330037832, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.86s
Iteration: 458,  Mean reward: -2.0, Mean Entropy: 0.0012853584485128522, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 459,  Mean reward: -4.25, Mean Entropy: 0.000525843701325357, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 460,  Mean reward: -1.75, Mean Entropy: 0.0009899900760501623, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 461,  Mean reward: -3.25, Mean Entropy: 0.0010600006207823753, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 462,  Mean reward: -1.0, Mean Entropy: 0.0010495702736079693, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 463,  Mean reward: -0.75, Mean Entropy: 0.0012709613656625152, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 464,  Mean reward: -2.25, Mean Entropy: 0.0011996281100437045, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 465,  Mean reward: -2.75, Mean Entropy: 0.001116658910177648, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 466,  Mean reward: -2.0, Mean Entropy: 0.0013170517049729824, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 467,  Mean reward: -2.5, Mean Entropy: 0.0010547189740464091, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 468,  Mean reward: 0.25, Mean Entropy: 0.0013695345260202885, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 469,  Mean reward: -3.0, Mean Entropy: 0.0008969306945800781, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 470,  Mean reward: -1.25, Mean Entropy: 0.0008470294997096062, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 471,  Mean reward: -0.75, Mean Entropy: 0.0007146596908569336, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 472,  Mean reward: -1.75, Mean Entropy: 0.000497744360473007, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 473,  Mean reward: -1.0, Mean Entropy: 0.0005319201154634356, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 474,  Mean reward: -3.5, Mean Entropy: 0.000280145148281008, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 475,  Mean reward: -3.0, Mean Entropy: 0.0003990227705799043, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 476,  Mean reward: -1.75, Mean Entropy: 0.00035227544140070677, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 477,  Mean reward: -2.75, Mean Entropy: 0.00028811267111450434, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 478,  Mean reward: -2.75, Mean Entropy: 0.0002945974702015519, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 479,  Mean reward: -3.0, Mean Entropy: 0.00031967496033757925, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 480,  Mean reward: -1.0, Mean Entropy: 0.0002982687728945166, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.87s
Iteration: 481,  Mean reward: -2.25, Mean Entropy: 0.0003051938838325441, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 482,  Mean reward: -3.25, Mean Entropy: 0.0002603637403808534, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 483,  Mean reward: -3.5, Mean Entropy: 0.0002451809123158455, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 484,  Mean reward: -2.5, Mean Entropy: 0.00026404831442050636, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 485,  Mean reward: -2.25, Mean Entropy: 0.00025712716160342097, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 486,  Mean reward: -3.0, Mean Entropy: 0.00026391432038508356, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 487,  Mean reward: -1.75, Mean Entropy: 0.0002656422439031303, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 488,  Mean reward: -2.75, Mean Entropy: 0.00022316478134598583, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 489,  Mean reward: -3.25, Mean Entropy: 0.00021517137065529823, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 490,  Mean reward: -1.5, Mean Entropy: 0.000261468201642856, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 491,  Mean reward: -2.5, Mean Entropy: 0.00023378882906399667, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 492,  Mean reward: -0.75, Mean Entropy: 0.00022680932306684554, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 493,  Mean reward: -2.0, Mean Entropy: 0.00020003078680019826, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 494,  Mean reward: -0.75, Mean Entropy: 0.00027722353115677834, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 495,  Mean reward: -1.0, Mean Entropy: 0.00024856586242094636, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 496,  Mean reward: -4.25, Mean Entropy: 0.00017515102808829397, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 497,  Mean reward: -2.5, Mean Entropy: 0.00018345288117416203, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 498,  Mean reward: -3.25, Mean Entropy: 0.00012992018309887499, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 499,  Mean reward: -1.75, Mean Entropy: 0.0001587739825481549, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 500,  Mean reward: -2.5, Mean Entropy: 0.0001569305022712797, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -0.5, Mean Entropy: 0.00019060063641518354, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 502,  Mean reward: -2.75, Mean Entropy: 0.0001451180869480595, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 503,  Mean reward: -0.5, Mean Entropy: 0.00016724260058254004, complete_episode_count: 80.0, Gather time: 0.70s, Train time: 0.73s
Iteration: 504,  Mean reward: -2.25, Mean Entropy: 0.00015128718223422766, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 505,  Mean reward: 0.25, Mean Entropy: 0.00019547229749150574, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 506,  Mean reward: -1.75, Mean Entropy: 0.000139409996336326, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 507,  Mean reward: -0.75, Mean Entropy: 0.00012985800276510417, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 508,  Mean reward: -2.0, Mean Entropy: 0.0001537789066787809, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 509,  Mean reward: -2.5, Mean Entropy: 0.00013723863230552524, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 510,  Mean reward: -2.25, Mean Entropy: 0.00011248274677200243, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 511,  Mean reward: -1.25, Mean Entropy: 0.00011364431702531874, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 512,  Mean reward: -1.5, Mean Entropy: 0.00012006979522993788, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 513,  Mean reward: -1.75, Mean Entropy: 0.00011671689571812749, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 514,  Mean reward: -1.75, Mean Entropy: 0.00011713674030033872, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 515,  Mean reward: -1.0, Mean Entropy: 0.00011818633356597275, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 516,  Mean reward: -1.25, Mean Entropy: 0.00011402428935980424, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 517,  Mean reward: -1.0, Mean Entropy: 9.471028170082718e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 518,  Mean reward: -2.5, Mean Entropy: 9.638989286031574e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 519,  Mean reward: -2.75, Mean Entropy: 8.624274050816894e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 520,  Mean reward: -3.25, Mean Entropy: 9.103452612180263e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 521,  Mean reward: -3.25, Mean Entropy: 7.060629286570475e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 522,  Mean reward: -3.25, Mean Entropy: 6.90364686306566e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 523,  Mean reward: -3.25, Mean Entropy: 7.450606062775478e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 524,  Mean reward: -0.25, Mean Entropy: 9.678199421614408e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 525,  Mean reward: -1.75, Mean Entropy: 7.687218021601439e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 526,  Mean reward: -3.5, Mean Entropy: 6.236525950953364e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 527,  Mean reward: -1.5, Mean Entropy: 8.039581007324159e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 528,  Mean reward: -1.5, Mean Entropy: 8.193711983039975e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 529,  Mean reward: -2.0, Mean Entropy: 7.71735722082667e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 530,  Mean reward: -0.75, Mean Entropy: 7.151911268010736e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 531,  Mean reward: -1.75, Mean Entropy: 7.597038347739726e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 532,  Mean reward: -3.0, Mean Entropy: 6.028590723872185e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 533,  Mean reward: -2.0, Mean Entropy: 6.780996773159131e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 534,  Mean reward: -2.5, Mean Entropy: 5.558507837122306e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 535,  Mean reward: -0.5, Mean Entropy: 7.36286528990604e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 536,  Mean reward: -2.75, Mean Entropy: 6.162842328194529e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 537,  Mean reward: -2.0, Mean Entropy: 6.576973828487098e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 538,  Mean reward: -3.25, Mean Entropy: 5.341006180969998e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 539,  Mean reward: -0.75, Mean Entropy: 6.998608296271414e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 540,  Mean reward: -1.75, Mean Entropy: 5.164472895557992e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 541,  Mean reward: -2.25, Mean Entropy: 6.533812847919762e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 542,  Mean reward: -3.0, Mean Entropy: 5.4851057939231396e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 543,  Mean reward: -2.5, Mean Entropy: 6.115028372732922e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 544,  Mean reward: -2.25, Mean Entropy: 5.3298303100746125e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 545,  Mean reward: -3.0, Mean Entropy: 4.7987705329433084e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 546,  Mean reward: -3.75, Mean Entropy: 3.839591227006167e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 547,  Mean reward: -2.25, Mean Entropy: 5.3523177484748885e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 548,  Mean reward: -1.25, Mean Entropy: 5.720689659938216e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 549,  Mean reward: -1.5, Mean Entropy: 5.421494643087499e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 550,  Mean reward: -1.0, Mean Entropy: 5.466117727337405e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 551,  Mean reward: -1.75, Mean Entropy: 5.8203811931889504e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 552,  Mean reward: -1.5, Mean Entropy: 5.7531477068550885e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 553,  Mean reward: -1.25, Mean Entropy: 5.085493830847554e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 554,  Mean reward: -1.0, Mean Entropy: 5.4418396757682785e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 555,  Mean reward: -3.0, Mean Entropy: 3.79693228751421e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 556,  Mean reward: -0.5, Mean Entropy: 6.119061436038464e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 557,  Mean reward: -1.5, Mean Entropy: 4.681172140408307e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 558,  Mean reward: -4.0, Mean Entropy: 4.0441620512865484e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 559,  Mean reward: -1.5, Mean Entropy: 5.3319930884754285e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 560,  Mean reward: -3.0, Mean Entropy: 3.582981298677623e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 561,  Mean reward: -3.25, Mean Entropy: 3.3645424991846085e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 562,  Mean reward: -3.5, Mean Entropy: 3.516829019645229e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 563,  Mean reward: -1.75, Mean Entropy: 4.76360073662363e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.81s
Iteration: 564,  Mean reward: -0.75, Mean Entropy: 4.899759005638771e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 565,  Mean reward: -0.5, Mean Entropy: 5.0242953875567764e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 566,  Mean reward: -2.5, Mean Entropy: 3.55478041456081e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 567,  Mean reward: -2.25, Mean Entropy: 3.701734749483876e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 568,  Mean reward: -2.75, Mean Entropy: 4.191194238956086e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 569,  Mean reward: -3.25, Mean Entropy: 3.814646333921701e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 570,  Mean reward: 0.0, Mean Entropy: 4.878702020505443e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 571,  Mean reward: -2.75, Mean Entropy: 4.509456266532652e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 572,  Mean reward: -1.5, Mean Entropy: 3.992362326243892e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 573,  Mean reward: 0.25, Mean Entropy: 4.968545908923261e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 574,  Mean reward: -2.75, Mean Entropy: 3.6181027098791674e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 575,  Mean reward: 0.0, Mean Entropy: 4.57704154541716e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 576,  Mean reward: -3.75, Mean Entropy: 3.086807191721164e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 577,  Mean reward: -3.5, Mean Entropy: 3.064922202611342e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 578,  Mean reward: -1.5, Mean Entropy: 4.476917092688382e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 579,  Mean reward: -1.25, Mean Entropy: 3.804916559602134e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 580,  Mean reward: -2.25, Mean Entropy: 3.308810119051486e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 581,  Mean reward: -1.5, Mean Entropy: 3.9081063732737675e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 582,  Mean reward: -1.25, Mean Entropy: 4.191623520455323e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 583,  Mean reward: -2.5, Mean Entropy: 3.553592978278175e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 584,  Mean reward: -3.0, Mean Entropy: 3.232930976082571e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 585,  Mean reward: -2.25, Mean Entropy: 4.447831815923564e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 586,  Mean reward: -3.0, Mean Entropy: 3.2108757295645773e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 587,  Mean reward: -3.25, Mean Entropy: 2.8977803594898432e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 588,  Mean reward: -2.0, Mean Entropy: 3.496471981634386e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 589,  Mean reward: -3.25, Mean Entropy: 3.1776955438544974e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 590,  Mean reward: -2.25, Mean Entropy: 3.6185840144753456e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 591,  Mean reward: -2.0, Mean Entropy: 4.0650345908943564e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 592,  Mean reward: -1.0, Mean Entropy: 3.913821274181828e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 593,  Mean reward: -3.5, Mean Entropy: 2.5555433239787817e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 594,  Mean reward: -2.75, Mean Entropy: 3.60151898348704e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 595,  Mean reward: -2.5, Mean Entropy: 2.6980978873325512e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 596,  Mean reward: -1.5, Mean Entropy: 3.734579877345823e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 597,  Mean reward: -2.25, Mean Entropy: 3.5657511034514755e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 598,  Mean reward: -1.5, Mean Entropy: 3.701333116623573e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 599,  Mean reward: -4.0, Mean Entropy: 2.6722824259195477e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.91s
Iteration: 600,  Mean reward: -2.5, Mean Entropy: 3.992029087385163e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.75, Mean Entropy: 3.251431189710274e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 602,  Mean reward: -0.75, Mean Entropy: 4.1582778067095205e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 603,  Mean reward: -2.75, Mean Entropy: 3.193048178218305e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 604,  Mean reward: -2.5, Mean Entropy: 3.21088737109676e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 605,  Mean reward: -3.25, Mean Entropy: 3.2763149647507817e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 606,  Mean reward: -2.75, Mean Entropy: 3.357311652507633e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 607,  Mean reward: -1.25, Mean Entropy: 4.0951337723527104e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 608,  Mean reward: -4.5, Mean Entropy: 2.799167668854352e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 609,  Mean reward: -3.0, Mean Entropy: 2.8991034923819825e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 610,  Mean reward: -2.5, Mean Entropy: 3.5853852750733495e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 611,  Mean reward: -2.0, Mean Entropy: 3.0179955501807854e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 612,  Mean reward: -0.5, Mean Entropy: 4.2628595110727474e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 613,  Mean reward: -1.0, Mean Entropy: 4.0864935726858675e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 614,  Mean reward: -2.5, Mean Entropy: 3.829213892458938e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 615,  Mean reward: -0.75, Mean Entropy: 4.2968662455677986e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 616,  Mean reward: -1.5, Mean Entropy: 3.762488631764427e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 617,  Mean reward: -2.0, Mean Entropy: 3.774688229896128e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 618,  Mean reward: -1.0, Mean Entropy: 3.9286125684157014e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 619,  Mean reward: -2.5, Mean Entropy: 3.444055619183928e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 620,  Mean reward: -1.75, Mean Entropy: 3.7756984966108575e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 621,  Mean reward: -1.25, Mean Entropy: 3.9240367186721414e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 622,  Mean reward: -0.75, Mean Entropy: 4.825529322260991e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 623,  Mean reward: -1.75, Mean Entropy: 3.7030564271844923e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 624,  Mean reward: -3.25, Mean Entropy: 3.6903395084664226e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 625,  Mean reward: -2.5, Mean Entropy: 3.217742414562963e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 626,  Mean reward: -2.75, Mean Entropy: 3.946115975850262e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 627,  Mean reward: -3.0, Mean Entropy: 3.2986688893288374e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 628,  Mean reward: -3.25, Mean Entropy: 2.6698375222622417e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 629,  Mean reward: -2.0, Mean Entropy: 3.67678003385663e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 630,  Mean reward: 1.5, Mean Entropy: 4.6696171921212226e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 631,  Mean reward: -3.0, Mean Entropy: 3.3283515222137794e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 632,  Mean reward: -1.75, Mean Entropy: 3.5834062146022916e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 633,  Mean reward: -2.5, Mean Entropy: 3.4219076042063534e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 634,  Mean reward: -1.75, Mean Entropy: 3.536840813467279e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 635,  Mean reward: -3.0, Mean Entropy: 2.6648605853552e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 636,  Mean reward: -2.25, Mean Entropy: 2.785158358165063e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 637,  Mean reward: -1.25, Mean Entropy: 3.18204183713533e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 638,  Mean reward: -1.0, Mean Entropy: 3.71474125131499e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 639,  Mean reward: -1.0, Mean Entropy: 3.412954538362101e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 640,  Mean reward: -1.5, Mean Entropy: 3.5288147046230733e-05, complete_episode_count: 80.0, Gather time: 0.70s, Train time: 0.91s
Iteration: 641,  Mean reward: -1.25, Mean Entropy: 3.506276698317379e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 642,  Mean reward: -3.25, Mean Entropy: 2.8084668883820996e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 643,  Mean reward: -0.5, Mean Entropy: 3.181491047143936e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 644,  Mean reward: -0.75, Mean Entropy: 3.421392466407269e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 645,  Mean reward: -3.0, Mean Entropy: 3.0071618311922066e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 646,  Mean reward: -1.25, Mean Entropy: 4.033339791931212e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 647,  Mean reward: -0.5, Mean Entropy: 4.011105193058029e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 648,  Mean reward: -3.5, Mean Entropy: 2.4445373128401116e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 649,  Mean reward: -1.25, Mean Entropy: 3.458416540524922e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 650,  Mean reward: -0.75, Mean Entropy: 3.702469257405028e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 651,  Mean reward: -0.5, Mean Entropy: 3.0820188840152696e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 652,  Mean reward: -2.5, Mean Entropy: 2.826601848937571e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 653,  Mean reward: -1.75, Mean Entropy: 2.812740785884671e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 654,  Mean reward: -0.5, Mean Entropy: 3.406440373510122e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 655,  Mean reward: -0.25, Mean Entropy: 3.0258175684139132e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 656,  Mean reward: -2.0, Mean Entropy: 3.126296360278502e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 657,  Mean reward: -1.75, Mean Entropy: 2.6316960429539904e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 658,  Mean reward: -1.0, Mean Entropy: 3.338904571137391e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 659,  Mean reward: -0.5, Mean Entropy: 3.9235725125763565e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 660,  Mean reward: -2.75, Mean Entropy: 2.7257467081653886e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 661,  Mean reward: -4.25, Mean Entropy: 2.3628663257113658e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 662,  Mean reward: -1.25, Mean Entropy: 2.8380607545841485e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 663,  Mean reward: -2.0, Mean Entropy: 2.95629542961251e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 664,  Mean reward: -1.0, Mean Entropy: 3.4214484912808985e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 665,  Mean reward: -2.25, Mean Entropy: 2.828654760378413e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 666,  Mean reward: -3.0, Mean Entropy: 2.5879451641230844e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 667,  Mean reward: -3.0, Mean Entropy: 1.9950264686485752e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 668,  Mean reward: -1.25, Mean Entropy: 3.17563972203061e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 669,  Mean reward: -1.75, Mean Entropy: 2.5911529519362375e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 670,  Mean reward: -2.5, Mean Entropy: 2.351556031499058e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 671,  Mean reward: -1.5, Mean Entropy: 2.9406317480606958e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 672,  Mean reward: -0.25, Mean Entropy: 3.657004708657041e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 673,  Mean reward: -1.75, Mean Entropy: 3.305355494376272e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 674,  Mean reward: -1.5, Mean Entropy: 3.656479748315178e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 675,  Mean reward: -2.0, Mean Entropy: 2.83456283796113e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 676,  Mean reward: -0.5, Mean Entropy: 3.3124211768154055e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 677,  Mean reward: -2.5, Mean Entropy: 3.197048863512464e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 678,  Mean reward: -0.75, Mean Entropy: 3.082717739744112e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 679,  Mean reward: -3.25, Mean Entropy: 2.7227133614360355e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 680,  Mean reward: -0.25, Mean Entropy: 3.4268261515535414e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 681,  Mean reward: -1.75, Mean Entropy: 2.6028352294815704e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 682,  Mean reward: -2.75, Mean Entropy: 2.8358646886772476e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 683,  Mean reward: -3.0, Mean Entropy: 2.241207221231889e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 684,  Mean reward: -4.25, Mean Entropy: 2.484466676833108e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 685,  Mean reward: -2.5, Mean Entropy: 2.4956081688287668e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 686,  Mean reward: -3.0, Mean Entropy: 2.518373730708845e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 687,  Mean reward: -1.5, Mean Entropy: 3.389212361071259e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 688,  Mean reward: -0.75, Mean Entropy: 3.523545456118882e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 689,  Mean reward: 0.5, Mean Entropy: 3.776714584091678e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 690,  Mean reward: -1.5, Mean Entropy: 3.074607957387343e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 691,  Mean reward: -3.5, Mean Entropy: 2.4332428438356146e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 692,  Mean reward: -0.75, Mean Entropy: 3.6140896554570645e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 693,  Mean reward: -2.0, Mean Entropy: 2.980761018989142e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 694,  Mean reward: -2.5, Mean Entropy: 3.228137211408466e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 695,  Mean reward: -1.0, Mean Entropy: 3.8688667700625956e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 696,  Mean reward: -1.0, Mean Entropy: 3.109630415565334e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 697,  Mean reward: -2.75, Mean Entropy: 2.4664343072799966e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 698,  Mean reward: -2.5, Mean Entropy: 3.1158022466115654e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 699,  Mean reward: -2.25, Mean Entropy: 3.622521398938261e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 700,  Mean reward: -1.75, Mean Entropy: 2.8355110771371983e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -1.25, Mean Entropy: 3.6727695260196924e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 702,  Mean reward: -0.75, Mean Entropy: 4.225933662382886e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 703,  Mean reward: -1.75, Mean Entropy: 3.0141178285703063e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 704,  Mean reward: -2.75, Mean Entropy: 2.6000092475442216e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 705,  Mean reward: -1.75, Mean Entropy: 3.3443902793806046e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 706,  Mean reward: -1.75, Mean Entropy: 2.682470585568808e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 707,  Mean reward: -2.75, Mean Entropy: 2.69108477368718e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 708,  Mean reward: -3.25, Mean Entropy: 2.6070925741805695e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 709,  Mean reward: -2.25, Mean Entropy: 3.011241278727539e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 710,  Mean reward: -1.75, Mean Entropy: 3.284191188868135e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 711,  Mean reward: -1.75, Mean Entropy: 3.526288492139429e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 712,  Mean reward: -1.5, Mean Entropy: 3.515503340167925e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 713,  Mean reward: -2.5, Mean Entropy: 3.461142478045076e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 714,  Mean reward: -3.5, Mean Entropy: 3.0176750442478806e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 715,  Mean reward: -0.75, Mean Entropy: 2.9110500690876506e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 716,  Mean reward: -2.75, Mean Entropy: 2.9365121008595452e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 717,  Mean reward: -2.75, Mean Entropy: 3.029516665264964e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 718,  Mean reward: -1.0, Mean Entropy: 3.305787322460674e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 719,  Mean reward: -1.25, Mean Entropy: 3.2374497095588595e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 720,  Mean reward: 0.75, Mean Entropy: 4.3097832531202585e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 721,  Mean reward: -2.0, Mean Entropy: 3.1953280995367095e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 722,  Mean reward: -0.75, Mean Entropy: 3.7178593629505485e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 723,  Mean reward: -2.25, Mean Entropy: 2.81777793134097e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 724,  Mean reward: -3.5, Mean Entropy: 3.0203504138626158e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 725,  Mean reward: -1.0, Mean Entropy: 4.0528269892092794e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 726,  Mean reward: -1.25, Mean Entropy: 3.929526428692043e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 727,  Mean reward: -0.75, Mean Entropy: 4.226819146424532e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 728,  Mean reward: -1.5, Mean Entropy: 3.243580795242451e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 729,  Mean reward: -0.5, Mean Entropy: 3.7484187487279996e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 730,  Mean reward: -1.25, Mean Entropy: 3.6989695217926055e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 731,  Mean reward: -4.25, Mean Entropy: 2.8395295885275118e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 732,  Mean reward: -1.75, Mean Entropy: 3.751113763428293e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 733,  Mean reward: -1.0, Mean Entropy: 4.0943628846434876e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 734,  Mean reward: -2.25, Mean Entropy: 2.9140426704543643e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 735,  Mean reward: -2.0, Mean Entropy: 3.802268111030571e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 736,  Mean reward: -2.0, Mean Entropy: 2.9822633223375306e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 737,  Mean reward: -3.5, Mean Entropy: 2.6846819309866987e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 738,  Mean reward: -1.75, Mean Entropy: 3.6232111597200856e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 739,  Mean reward: -0.25, Mean Entropy: 4.039576379000209e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 740,  Mean reward: -2.75, Mean Entropy: 3.380021735210903e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 741,  Mean reward: -2.25, Mean Entropy: 3.138526153634302e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 742,  Mean reward: 1.5, Mean Entropy: 4.3936172005487606e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 743,  Mean reward: -1.0, Mean Entropy: 3.232069138903171e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 744,  Mean reward: -2.5, Mean Entropy: 3.1118368497118354e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 745,  Mean reward: -1.5, Mean Entropy: 3.369263140484691e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 746,  Mean reward: -2.5, Mean Entropy: 3.117046435363591e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 747,  Mean reward: -0.75, Mean Entropy: 3.528273373376578e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 748,  Mean reward: -0.25, Mean Entropy: 4.0688748413231224e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 749,  Mean reward: -1.5, Mean Entropy: 2.6102998162969016e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 750,  Mean reward: -1.0, Mean Entropy: 3.909929364454001e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 751,  Mean reward: -2.25, Mean Entropy: 3.1012328690849245e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 752,  Mean reward: -1.5, Mean Entropy: 3.0852050258545205e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 753,  Mean reward: -0.75, Mean Entropy: 2.94450182991568e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 754,  Mean reward: -2.25, Mean Entropy: 2.568913623690605e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 755,  Mean reward: -3.0, Mean Entropy: 2.6653109671315178e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 756,  Mean reward: -3.0, Mean Entropy: 3.12859847326763e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 757,  Mean reward: -1.5, Mean Entropy: 3.2596242817817256e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 758,  Mean reward: 0.0, Mean Entropy: 3.74161172658205e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 759,  Mean reward: -2.25, Mean Entropy: 2.767363184830174e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 760,  Mean reward: -1.75, Mean Entropy: 3.109832323389128e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 761,  Mean reward: -1.75, Mean Entropy: 3.0809980671619996e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 762,  Mean reward: -0.75, Mean Entropy: 3.285001730546355e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 763,  Mean reward: -1.0, Mean Entropy: 3.0511146178469062e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.90s
Iteration: 764,  Mean reward: -0.25, Mean Entropy: 3.182666114298627e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 765,  Mean reward: -2.5, Mean Entropy: 2.6980420443578623e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 766,  Mean reward: -0.25, Mean Entropy: 3.272428148193285e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 767,  Mean reward: -2.75, Mean Entropy: 3.1434832635568455e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 768,  Mean reward: -0.75, Mean Entropy: 3.3722168154781684e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 769,  Mean reward: -0.75, Mean Entropy: 2.9148839530535042e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 770,  Mean reward: -2.75, Mean Entropy: 2.6910751330433413e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 771,  Mean reward: -1.0, Mean Entropy: 3.2761799957370386e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 772,  Mean reward: -1.0, Mean Entropy: 3.384603405720554e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 773,  Mean reward: -3.5, Mean Entropy: 2.563254201959353e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 774,  Mean reward: -1.25, Mean Entropy: 3.267927240813151e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 775,  Mean reward: -1.75, Mean Entropy: 2.9267028367030434e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 776,  Mean reward: -2.0, Mean Entropy: 3.049593578907661e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 777,  Mean reward: -3.0, Mean Entropy: 2.4653327272972092e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 778,  Mean reward: -0.25, Mean Entropy: 3.154149453621358e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 779,  Mean reward: -2.25, Mean Entropy: 2.6782217901200056e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 780,  Mean reward: -3.5, Mean Entropy: 2.2039201212464832e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 781,  Mean reward: -2.25, Mean Entropy: 3.0239356419770047e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 782,  Mean reward: -2.25, Mean Entropy: 2.7451298592495732e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 783,  Mean reward: -1.25, Mean Entropy: 2.62756111624185e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 784,  Mean reward: -2.75, Mean Entropy: 2.5031080440385267e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 785,  Mean reward: -3.75, Mean Entropy: 2.379375473537948e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 786,  Mean reward: -2.5, Mean Entropy: 2.3588756448589265e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 787,  Mean reward: -2.0, Mean Entropy: 2.84862908301875e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 788,  Mean reward: -1.75, Mean Entropy: 2.8530475901789032e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 789,  Mean reward: -2.0, Mean Entropy: 2.966034662676975e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 790,  Mean reward: -2.25, Mean Entropy: 2.119148484780453e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 791,  Mean reward: -0.5, Mean Entropy: 3.618055416154675e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 792,  Mean reward: -0.75, Mean Entropy: 3.3254982554353774e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 793,  Mean reward: -1.5, Mean Entropy: 2.4950457373051904e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 794,  Mean reward: -2.25, Mean Entropy: 3.079333691857755e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 795,  Mean reward: -2.0, Mean Entropy: 3.164177906000987e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 796,  Mean reward: -4.25, Mean Entropy: 1.8340666429139674e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 797,  Mean reward: -1.0, Mean Entropy: 3.616795584093779e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 798,  Mean reward: -4.25, Mean Entropy: 2.4525968910893425e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 799,  Mean reward: -1.5, Mean Entropy: 3.2607764296699315e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.85s
Iteration: 800,  Mean reward: -3.0, Mean Entropy: 3.406254108995199e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -2.5, Mean Entropy: 2.650605165399611e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 802,  Mean reward: -1.25, Mean Entropy: 4.101717422599904e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 803,  Mean reward: -1.75, Mean Entropy: 3.876859773299657e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 804,  Mean reward: -1.5, Mean Entropy: 3.2336054573534057e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.91s
Iteration: 805,  Mean reward: 0.25, Mean Entropy: 4.134401388000697e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 806,  Mean reward: -1.5, Mean Entropy: 3.5623459552880377e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 807,  Mean reward: 0.0, Mean Entropy: 3.900480442098342e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 808,  Mean reward: -1.0, Mean Entropy: 3.488526999717578e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 809,  Mean reward: -1.5, Mean Entropy: 3.416368053876795e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 810,  Mean reward: -1.5, Mean Entropy: 3.708209260366857e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 811,  Mean reward: -0.75, Mean Entropy: 4.052429358125664e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 812,  Mean reward: -2.75, Mean Entropy: 2.9017566703259945e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.79s
Iteration: 813,  Mean reward: -2.75, Mean Entropy: 2.7736357878893614e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 814,  Mean reward: -2.5, Mean Entropy: 2.9801309210597537e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 815,  Mean reward: -1.75, Mean Entropy: 3.571677007130347e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 816,  Mean reward: -1.25, Mean Entropy: 2.767877231235616e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 817,  Mean reward: -2.5, Mean Entropy: 2.9932627512607723e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 818,  Mean reward: -3.5, Mean Entropy: 1.9768394849961624e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 819,  Mean reward: -1.5, Mean Entropy: 2.9370201445999555e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 820,  Mean reward: -1.5, Mean Entropy: 3.270151501055807e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 821,  Mean reward: -2.25, Mean Entropy: 2.5025414288393222e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 822,  Mean reward: -2.25, Mean Entropy: 3.291839675512165e-05, complete_episode_count: 80.0, Gather time: 0.70s, Train time: 0.73s
Iteration: 823,  Mean reward: -1.75, Mean Entropy: 3.0234165024012327e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 824,  Mean reward: -0.75, Mean Entropy: 3.110627949354239e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 825,  Mean reward: -1.5, Mean Entropy: 3.189122071489692e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 826,  Mean reward: -1.25, Mean Entropy: 2.4721643058001064e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 827,  Mean reward: -1.0, Mean Entropy: 2.9983213607920334e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 828,  Mean reward: -1.25, Mean Entropy: 2.7471469365991652e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 829,  Mean reward: -1.75, Mean Entropy: 2.825953924912028e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 830,  Mean reward: -4.75, Mean Entropy: 1.6138255887199193e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.902439024390244, Mean Entropy: 0.8664339780807495, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.159090909090909, Mean Entropy: 0.9675179719924927, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 2,  Mean reward: -5.052631578947368, Mean Entropy: 1.0036194324493408, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -3.0930232558139537, Mean Entropy: 0.960297703742981, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 4,  Mean reward: -5.848837209302325, Mean Entropy: 0.9675178527832031, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 5,  Mean reward: -4.2727272727272725, Mean Entropy: 0.9458569884300232, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 6,  Mean reward: -4.9605263157894735, Mean Entropy: 1.0036194324493408, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 7,  Mean reward: -4.7875, Mean Entropy: 1.0036194324493408, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 8,  Mean reward: -5.3, Mean Entropy: 0.960297703742981, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.56s
Iteration: 9,  Mean reward: -4.6395348837209305, Mean Entropy: 0.9675177931785583, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 10,  Mean reward: -2.9743589743589745, Mean Entropy: 0.9386367201805115, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 11,  Mean reward: -4.4743589743589745, Mean Entropy: 0.9386367797851562, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 12,  Mean reward: -4.2682926829268295, Mean Entropy: 0.9819583892822266, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.64s
Iteration: 13,  Mean reward: -4.866666666666666, Mean Entropy: 0.9602975845336914, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 14,  Mean reward: -3.106382978723404, Mean Entropy: 0.9891786575317383, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 15,  Mean reward: -2.9069767441860463, Mean Entropy: 0.9891787767410278, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 16,  Mean reward: -4.0, Mean Entropy: 0.9241962432861328, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 17,  Mean reward: -3.3658536585365852, Mean Entropy: 0.9602975845336914, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 18,  Mean reward: -3.117021276595745, Mean Entropy: 0.9241961240768433, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 19,  Mean reward: -4.025, Mean Entropy: 0.9314165115356445, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 20,  Mean reward: -5.4875, Mean Entropy: 0.9675173759460449, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 21,  Mean reward: -4.709302325581396, Mean Entropy: 0.9602968096733093, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 22,  Mean reward: -4.7439024390243905, Mean Entropy: 0.9458554983139038, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 23,  Mean reward: -4.166666666666667, Mean Entropy: 0.9530772566795349, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 24,  Mean reward: -4.092105263157895, Mean Entropy: 0.938636302947998, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 25,  Mean reward: -5.615384615384615, Mean Entropy: 0.9458541870117188, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 26,  Mean reward: -5.341463414634147, Mean Entropy: 0.9675158262252808, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 27,  Mean reward: -3.5454545454545454, Mean Entropy: 0.9097552299499512, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 28,  Mean reward: -5.579545454545454, Mean Entropy: 0.9241954684257507, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 29,  Mean reward: -3.8536585365853657, Mean Entropy: 0.9891781806945801, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 30,  Mean reward: -5.441860465116279, Mean Entropy: 0.9675171375274658, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 31,  Mean reward: -3.875, Mean Entropy: 0.9386361837387085, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 32,  Mean reward: -5.671052631578948, Mean Entropy: 0.9241933822631836, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 33,  Mean reward: -3.717948717948718, Mean Entropy: 0.9314138889312744, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 34,  Mean reward: -6.328947368421052, Mean Entropy: 0.9386252760887146, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 35,  Mean reward: -5.166666666666667, Mean Entropy: 0.9314097166061401, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 36,  Mean reward: -5.951219512195122, Mean Entropy: 0.9025313258171082, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 37,  Mean reward: -5.861111111111111, Mean Entropy: 0.9097540378570557, complete_episode_count: 36.0, Gather time: 0.52s, Train time: 1.49s
Iteration: 38,  Mean reward: -4.678571428571429, Mean Entropy: 0.9241940379142761, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 39,  Mean reward: -2.675, Mean Entropy: 0.9747381210327148, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 40,  Mean reward: -7.511111111111111, Mean Entropy: 1.0397205352783203, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 41,  Mean reward: -5.678571428571429, Mean Entropy: 0.9530673027038574, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 42,  Mean reward: -4.476744186046512, Mean Entropy: 0.8808739185333252, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 43,  Mean reward: -4.428571428571429, Mean Entropy: 0.8953145742416382, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 44,  Mean reward: -2.3333333333333335, Mean Entropy: 0.9241959452629089, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 45,  Mean reward: -1.3888888888888888, Mean Entropy: 0.9458569288253784, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 46,  Mean reward: -5.321428571428571, Mean Entropy: 0.9602975249290466, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 47,  Mean reward: -4.512195121951219, Mean Entropy: 0.9386364817619324, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 48,  Mean reward: -6.083333333333333, Mean Entropy: 0.9675116539001465, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 49,  Mean reward: -2.3684210526315788, Mean Entropy: 0.8519925475120544, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.66s
Iteration: 50,  Mean reward: -4.142857142857143, Mean Entropy: 0.9097552299499512, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 51,  Mean reward: -3.7209302325581395, Mean Entropy: 0.9025346636772156, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 52,  Mean reward: -3.2195121951219514, Mean Entropy: 0.9241954684257507, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 53,  Mean reward: -5.841463414634147, Mean Entropy: 0.8664281368255615, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 54,  Mean reward: -3.769230769230769, Mean Entropy: 1.0108366012573242, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 55,  Mean reward: -4.0256410256410255, Mean Entropy: 0.9097546339035034, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 56,  Mean reward: -4.337209302325581, Mean Entropy: 0.938629150390625, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 57,  Mean reward: -6.329268292682927, Mean Entropy: 0.9602564573287964, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 58,  Mean reward: -4.825581395348837, Mean Entropy: 0.9602758288383484, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 59,  Mean reward: -4.776315789473684, Mean Entropy: 0.9314138889312744, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 60,  Mean reward: -5.2375, Mean Entropy: 0.8664318919181824, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 61,  Mean reward: -4.7375, Mean Entropy: 0.9602925181388855, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 62,  Mean reward: -3.3026315789473686, Mean Entropy: 0.9314022064208984, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 63,  Mean reward: -3.3048780487804876, Mean Entropy: 0.8953141570091248, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 64,  Mean reward: -3.358974358974359, Mean Entropy: 0.9169667959213257, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 65,  Mean reward: -4.941860465116279, Mean Entropy: 0.8664337992668152, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 66,  Mean reward: -2.9166666666666665, Mean Entropy: 0.9097539782524109, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 67,  Mean reward: -3.0238095238095237, Mean Entropy: 0.8806657791137695, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 68,  Mean reward: -3.4523809523809526, Mean Entropy: 0.9013748168945312, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 69,  Mean reward: -6.095238095238095, Mean Entropy: 0.8798014521598816, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 70,  Mean reward: -3.268292682926829, Mean Entropy: 0.9312130212783813, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 71,  Mean reward: -3.4523809523809526, Mean Entropy: 0.9530331492424011, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 72,  Mean reward: -5.8023255813953485, Mean Entropy: 0.9313241243362427, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 73,  Mean reward: -4.864864864864865, Mean Entropy: 0.9097199440002441, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 74,  Mean reward: -4.615384615384615, Mean Entropy: 0.8952829241752625, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 75,  Mean reward: -4.813953488372093, Mean Entropy: 0.996370792388916, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 76,  Mean reward: -6.076923076923077, Mean Entropy: 0.9602932929992676, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 77,  Mean reward: -3.380434782608696, Mean Entropy: 0.9458518028259277, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 78,  Mean reward: -6.476190476190476, Mean Entropy: 0.9530715942382812, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 79,  Mean reward: -4.743589743589744, Mean Entropy: 0.9530690908432007, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 80,  Mean reward: -2.2906976744186047, Mean Entropy: 1.0179996490478516, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 81,  Mean reward: -3.058139534883721, Mean Entropy: 0.9458117485046387, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 82,  Mean reward: -6.193181818181818, Mean Entropy: 0.9963716268539429, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 83,  Mean reward: -7.255813953488372, Mean Entropy: 0.9819552302360535, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 84,  Mean reward: -3.1538461538461537, Mean Entropy: 0.9314143061637878, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 85,  Mean reward: -3.7125, Mean Entropy: 0.8592049479484558, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 86,  Mean reward: -5.2023809523809526, Mean Entropy: 0.967496395111084, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 87,  Mean reward: -2.5795454545454546, Mean Entropy: 0.9385753870010376, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.65s
Iteration: 88,  Mean reward: -5.573170731707317, Mean Entropy: 0.8375257849693298, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 89,  Mean reward: -4.7875, Mean Entropy: 0.9385910034179688, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 90,  Mean reward: -3.566666666666667, Mean Entropy: 0.9457422494888306, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 91,  Mean reward: -4.175, Mean Entropy: 0.8952159881591797, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 92,  Mean reward: -3.2093023255813953, Mean Entropy: 0.9168899059295654, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 93,  Mean reward: -2.875, Mean Entropy: 0.902514636516571, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 94,  Mean reward: -3.05, Mean Entropy: 0.9240826368331909, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 95,  Mean reward: -2.7435897435897436, Mean Entropy: 0.9169567823410034, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 96,  Mean reward: -5.6, Mean Entropy: 0.902501106262207, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 97,  Mean reward: -2.6951219512195124, Mean Entropy: 0.8735872507095337, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 98,  Mean reward: -3.1219512195121952, Mean Entropy: 0.9313973784446716, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 99,  Mean reward: -7.093023255813954, Mean Entropy: 0.9022638201713562, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 100,  Mean reward: -4.523809523809524, Mean Entropy: 0.9741985201835632, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.46s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -4.366666666666666, Mean Entropy: 0.9672629237174988, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 102,  Mean reward: -4.833333333333333, Mean Entropy: 0.8735169172286987, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 103,  Mean reward: -2.011111111111111, Mean Entropy: 0.9094385504722595, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 104,  Mean reward: -3.6136363636363638, Mean Entropy: 0.8663336634635925, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 105,  Mean reward: -5.321428571428571, Mean Entropy: 0.9671728610992432, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 106,  Mean reward: -3.388888888888889, Mean Entropy: 0.9527666568756104, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 107,  Mean reward: -5.559523809523809, Mean Entropy: 0.8734807968139648, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 108,  Mean reward: -3.046511627906977, Mean Entropy: 0.9743816256523132, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 109,  Mean reward: -4.611111111111111, Mean Entropy: 0.9313859939575195, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 110,  Mean reward: -4.380952380952381, Mean Entropy: 0.9530084133148193, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 111,  Mean reward: -4.364864864864865, Mean Entropy: 0.8944172859191895, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 112,  Mean reward: -5.2439024390243905, Mean Entropy: 0.9386293888092041, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 113,  Mean reward: -5.358974358974359, Mean Entropy: 0.9819580316543579, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 114,  Mean reward: -4.593023255813954, Mean Entropy: 0.9530665874481201, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 115,  Mean reward: -4.2625, Mean Entropy: 0.960284411907196, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 116,  Mean reward: -2.573170731707317, Mean Entropy: 0.9241213798522949, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 117,  Mean reward: -3.6794871794871793, Mean Entropy: 0.9236987829208374, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 118,  Mean reward: -3.1931818181818183, Mean Entropy: 0.8589533567428589, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 119,  Mean reward: -3.7625, Mean Entropy: 0.938208281993866, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 120,  Mean reward: -3.7375, Mean Entropy: 0.9583837985992432, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 121,  Mean reward: -5.357142857142857, Mean Entropy: 0.8941210508346558, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 122,  Mean reward: -3.7023809523809526, Mean Entropy: 0.9366463422775269, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 123,  Mean reward: -5.488095238095238, Mean Entropy: 0.9377058744430542, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 124,  Mean reward: -3.6463414634146343, Mean Entropy: 0.9441683292388916, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.64s
Iteration: 125,  Mean reward: -5.085365853658536, Mean Entropy: 0.9071957468986511, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 126,  Mean reward: -4.511363636363637, Mean Entropy: 0.9417878985404968, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 127,  Mean reward: -1.4787234042553192, Mean Entropy: 0.922814130783081, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 128,  Mean reward: -3.8222222222222224, Mean Entropy: 0.9436920881271362, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 129,  Mean reward: -2.9302325581395348, Mean Entropy: 0.9424095153808594, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 130,  Mean reward: -6.0227272727272725, Mean Entropy: 0.8820449709892273, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 131,  Mean reward: -3.978723404255319, Mean Entropy: 0.9432284235954285, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 132,  Mean reward: -6.7444444444444445, Mean Entropy: 0.9808529615402222, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 133,  Mean reward: -3.619047619047619, Mean Entropy: 0.9378981590270996, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 134,  Mean reward: -5.182926829268292, Mean Entropy: 0.9017874598503113, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 135,  Mean reward: -1.8295454545454546, Mean Entropy: 0.9058833122253418, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 136,  Mean reward: -4.348837209302325, Mean Entropy: 0.9037175178527832, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 137,  Mean reward: -5.875, Mean Entropy: 0.9384149312973022, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 138,  Mean reward: -4.390243902439025, Mean Entropy: 0.9130069613456726, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 139,  Mean reward: -4.130434782608695, Mean Entropy: 0.9034767150878906, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 140,  Mean reward: -5.681818181818182, Mean Entropy: 0.9591975212097168, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 141,  Mean reward: -1.4666666666666666, Mean Entropy: 0.8891264796257019, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 142,  Mean reward: -2.5444444444444443, Mean Entropy: 0.9647445678710938, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 143,  Mean reward: -5.837209302325581, Mean Entropy: 0.8669527769088745, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 144,  Mean reward: -5.837209302325581, Mean Entropy: 0.805965781211853, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 145,  Mean reward: -3.8043478260869565, Mean Entropy: 0.8729784488677979, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 146,  Mean reward: -5.0476190476190474, Mean Entropy: 0.907754123210907, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 147,  Mean reward: -4.512195121951219, Mean Entropy: 0.8468181490898132, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 148,  Mean reward: -3.0, Mean Entropy: 0.9068871736526489, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 149,  Mean reward: -4.404761904761905, Mean Entropy: 0.8507797718048096, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 150,  Mean reward: -3.8617021276595747, Mean Entropy: 0.8451677560806274, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 151,  Mean reward: -5.085106382978723, Mean Entropy: 0.769554615020752, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 152,  Mean reward: -4.144230769230769, Mean Entropy: 0.7855588793754578, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 153,  Mean reward: -4.969387755102041, Mean Entropy: 0.8141579031944275, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 154,  Mean reward: -3.377551020408163, Mean Entropy: 0.7032464742660522, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 155,  Mean reward: -3.0961538461538463, Mean Entropy: 0.7188891172409058, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 156,  Mean reward: -4.8478260869565215, Mean Entropy: 0.6964475512504578, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 157,  Mean reward: -4.666666666666667, Mean Entropy: 0.8124833106994629, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 158,  Mean reward: -3.0384615384615383, Mean Entropy: 0.6029500961303711, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 159,  Mean reward: -3.7884615384615383, Mean Entropy: 0.6426076292991638, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 160,  Mean reward: -3.547169811320755, Mean Entropy: 0.6384166479110718, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.66s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 161,  Mean reward: -0.9649122807017544, Mean Entropy: 0.6739844083786011, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 162,  Mean reward: -3.375, Mean Entropy: 0.6203736066818237, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 163,  Mean reward: -5.018867924528302, Mean Entropy: 0.583751380443573, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 164,  Mean reward: -1.1071428571428572, Mean Entropy: 0.581345796585083, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 165,  Mean reward: -5.232142857142857, Mean Entropy: 0.5396971106529236, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 166,  Mean reward: -3.425925925925926, Mean Entropy: 0.568128228187561, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 167,  Mean reward: -4.146551724137931, Mean Entropy: 0.6257392168045044, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 168,  Mean reward: -5.245283018867925, Mean Entropy: 0.6227986812591553, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 169,  Mean reward: -2.816326530612245, Mean Entropy: 0.5871073007583618, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 170,  Mean reward: -2.25, Mean Entropy: 0.5526120066642761, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 171,  Mean reward: -3.6982758620689653, Mean Entropy: 0.5447625517845154, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 172,  Mean reward: -1.5636363636363637, Mean Entropy: 0.5906848907470703, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 173,  Mean reward: -3.24468085106383, Mean Entropy: 0.6233001947402954, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 174,  Mean reward: -3.3773584905660377, Mean Entropy: 0.5623569488525391, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 175,  Mean reward: -4.481132075471698, Mean Entropy: 0.7091757655143738, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 176,  Mean reward: -3.510869565217391, Mean Entropy: 0.6299659609794617, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 177,  Mean reward: -4.958333333333333, Mean Entropy: 0.6903553009033203, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 178,  Mean reward: -4.8979591836734695, Mean Entropy: 0.7951549291610718, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 179,  Mean reward: -4.705882352941177, Mean Entropy: 0.6981310844421387, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 180,  Mean reward: -6.367924528301887, Mean Entropy: 0.8467671871185303, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 181,  Mean reward: -3.0232558139534884, Mean Entropy: 0.6722080111503601, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 182,  Mean reward: -4.0576923076923075, Mean Entropy: 0.75245201587677, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 183,  Mean reward: -4.086956521739131, Mean Entropy: 0.7829129099845886, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 184,  Mean reward: -3.147727272727273, Mean Entropy: 0.7450873255729675, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 185,  Mean reward: -5.297872340425532, Mean Entropy: 0.7227438688278198, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 186,  Mean reward: -6.490196078431373, Mean Entropy: 0.7860469222068787, complete_episode_count: 51.0, Gather time: 0.68s, Train time: 1.50s
Iteration: 187,  Mean reward: -2.7058823529411766, Mean Entropy: 0.6973894834518433, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 188,  Mean reward: -5.739583333333333, Mean Entropy: 0.8313068747520447, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 189,  Mean reward: -6.135416666666667, Mean Entropy: 0.8702199459075928, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 190,  Mean reward: -5.390243902439025, Mean Entropy: 0.6933190226554871, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 191,  Mean reward: -5.170454545454546, Mean Entropy: 0.6951614618301392, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 192,  Mean reward: -4.545454545454546, Mean Entropy: 0.7093721628189087, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 193,  Mean reward: -5.465909090909091, Mean Entropy: 0.7494245767593384, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 194,  Mean reward: -2.888888888888889, Mean Entropy: 0.7434514760971069, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 195,  Mean reward: -4.361702127659575, Mean Entropy: 0.7551805377006531, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 196,  Mean reward: -2.8369565217391304, Mean Entropy: 0.742492139339447, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.66s
Iteration: 197,  Mean reward: -2.6702127659574466, Mean Entropy: 0.6996927261352539, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 198,  Mean reward: -3.13, Mean Entropy: 0.7136303782463074, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 199,  Mean reward: -5.571428571428571, Mean Entropy: 0.7575026750564575, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 200,  Mean reward: -3.8469387755102042, Mean Entropy: 0.6722879409790039, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.64s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.390909090909091, Mean Entropy: 0.707913875579834, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 202,  Mean reward: -5.066666666666666, Mean Entropy: 0.944671094417572, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 203,  Mean reward: -2.7222222222222223, Mean Entropy: 0.6939064860343933, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 204,  Mean reward: -6.930232558139535, Mean Entropy: 0.9527162909507751, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 205,  Mean reward: -3.7708333333333335, Mean Entropy: 0.9552773237228394, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 206,  Mean reward: -4.825581395348837, Mean Entropy: 0.8918049335479736, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 207,  Mean reward: -5.690476190476191, Mean Entropy: 0.9111997485160828, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 208,  Mean reward: -5.706521739130435, Mean Entropy: 0.933661699295044, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 209,  Mean reward: -2.9318181818181817, Mean Entropy: 1.0028258562088013, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 210,  Mean reward: -1.6888888888888889, Mean Entropy: 0.9515039324760437, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 211,  Mean reward: -3.0454545454545454, Mean Entropy: 0.9685542583465576, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 212,  Mean reward: -5.8604651162790695, Mean Entropy: 0.8879952430725098, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 213,  Mean reward: -6.325, Mean Entropy: 1.0324535369873047, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 214,  Mean reward: -5.128205128205129, Mean Entropy: 0.9092296361923218, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 215,  Mean reward: -5.421052631578948, Mean Entropy: 0.916872501373291, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.62s
Iteration: 216,  Mean reward: -5.182926829268292, Mean Entropy: 0.9663087725639343, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 217,  Mean reward: -3.2857142857142856, Mean Entropy: 0.9169508218765259, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 218,  Mean reward: -2.86046511627907, Mean Entropy: 0.9675166606903076, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 219,  Mean reward: -4.048780487804878, Mean Entropy: 0.9241946935653687, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.53s
Iteration: 220,  Mean reward: -4.290697674418604, Mean Entropy: 0.9530683755874634, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 221,  Mean reward: -2.317073170731707, Mean Entropy: 0.9602932929992676, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 222,  Mean reward: -3.9186046511627906, Mean Entropy: 0.9530709981918335, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 223,  Mean reward: -4.6022727272727275, Mean Entropy: 0.9669109582901001, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 224,  Mean reward: -4.6375, Mean Entropy: 0.8675033450126648, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 225,  Mean reward: -1.686046511627907, Mean Entropy: 0.89531409740448, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 226,  Mean reward: -3.902439024390244, Mean Entropy: 0.9675110578536987, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 227,  Mean reward: -4.833333333333333, Mean Entropy: 0.9443482160568237, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 228,  Mean reward: -6.1, Mean Entropy: 0.9505788087844849, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 229,  Mean reward: -4.8478260869565215, Mean Entropy: 0.8936294317245483, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 230,  Mean reward: -4.0813953488372094, Mean Entropy: 0.8822815418243408, complete_episode_count: 43.0, Gather time: 0.66s, Train time: 1.49s
Iteration: 231,  Mean reward: -3.7386363636363638, Mean Entropy: 0.9295346140861511, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 232,  Mean reward: -3.5238095238095237, Mean Entropy: 0.8894726634025574, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 233,  Mean reward: -5.188888888888889, Mean Entropy: 0.9102108478546143, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 234,  Mean reward: -2.465909090909091, Mean Entropy: 0.9223121404647827, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 235,  Mean reward: -6.351063829787234, Mean Entropy: 0.8542532324790955, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 236,  Mean reward: -3.7551020408163267, Mean Entropy: 0.9083495140075684, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 237,  Mean reward: -4.918367346938775, Mean Entropy: 0.6730040311813354, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 238,  Mean reward: -3.375, Mean Entropy: 0.7725388407707214, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 239,  Mean reward: -6.988095238095238, Mean Entropy: 0.9314737319946289, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 240,  Mean reward: -4.215909090909091, Mean Entropy: 0.8923258781433105, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 241,  Mean reward: -4.642857142857143, Mean Entropy: 0.9157413244247437, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 242,  Mean reward: -6.941860465116279, Mean Entropy: 0.8943871259689331, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 243,  Mean reward: -4.902439024390244, Mean Entropy: 0.872828483581543, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 244,  Mean reward: -6.5, Mean Entropy: 0.8588689565658569, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.65s
Iteration: 245,  Mean reward: -5.3604651162790695, Mean Entropy: 0.845406711101532, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 246,  Mean reward: -4.566666666666666, Mean Entropy: 0.7683390974998474, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 247,  Mean reward: -5.372549019607843, Mean Entropy: 0.7544819712638855, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 248,  Mean reward: -3.0652173913043477, Mean Entropy: 0.7657374739646912, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 249,  Mean reward: -3.38, Mean Entropy: 0.8356903791427612, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 250,  Mean reward: -6.127659574468085, Mean Entropy: 0.9667816162109375, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 251,  Mean reward: -4.892857142857143, Mean Entropy: 0.9386357069015503, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 252,  Mean reward: -3.95, Mean Entropy: 0.9169761538505554, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 253,  Mean reward: -4.878048780487805, Mean Entropy: 0.8808748126029968, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 254,  Mean reward: -4.9, Mean Entropy: 0.9530776739120483, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 255,  Mean reward: -3.5, Mean Entropy: 0.9169760942459106, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 256,  Mean reward: -3.9285714285714284, Mean Entropy: 0.9097557067871094, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 257,  Mean reward: -5.65, Mean Entropy: 0.9314166903495789, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 258,  Mean reward: -3.761904761904762, Mean Entropy: 0.880874752998352, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 259,  Mean reward: -3.6547619047619047, Mean Entropy: 0.9025356769561768, complete_episode_count: 42.0, Gather time: 0.61s, Train time: 1.54s
Iteration: 260,  Mean reward: -4.890243902439025, Mean Entropy: 0.9386370182037354, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 261,  Mean reward: -2.9375, Mean Entropy: 0.9386367797851562, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 262,  Mean reward: -5.105263157894737, Mean Entropy: 0.9097557067871094, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 263,  Mean reward: -3.073170731707317, Mean Entropy: 0.9097555875778198, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 264,  Mean reward: -4.325581395348837, Mean Entropy: 0.9025346040725708, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 265,  Mean reward: -3.9347826086956523, Mean Entropy: 0.9524440765380859, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 266,  Mean reward: -2.7439024390243905, Mean Entropy: 0.9231854677200317, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 267,  Mean reward: -5.219512195121951, Mean Entropy: 0.9242233037948608, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 268,  Mean reward: -5.27906976744186, Mean Entropy: 0.9250144958496094, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 269,  Mean reward: -5.036585365853658, Mean Entropy: 0.9109842777252197, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.69s
Iteration: 270,  Mean reward: -5.536585365853658, Mean Entropy: 0.87992924451828, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 271,  Mean reward: -4.3478260869565215, Mean Entropy: 0.8939847946166992, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 272,  Mean reward: -5.7, Mean Entropy: 0.9065431356430054, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 273,  Mean reward: -5.380434782608695, Mean Entropy: 1.0135678052902222, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 274,  Mean reward: -5.321428571428571, Mean Entropy: 0.9386231899261475, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 275,  Mean reward: -2.9431818181818183, Mean Entropy: 0.9746297597885132, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 276,  Mean reward: -4.464285714285714, Mean Entropy: 0.9235461950302124, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.55s
Iteration: 277,  Mean reward: -3.197674418604651, Mean Entropy: 0.9311225414276123, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 278,  Mean reward: -5.011904761904762, Mean Entropy: 0.9169758558273315, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 279,  Mean reward: -4.0375, Mean Entropy: 1.0108397006988525, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 280,  Mean reward: -4.428571428571429, Mean Entropy: 0.953077495098114, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 281,  Mean reward: -5.280487804878049, Mean Entropy: 0.9675177931785583, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 282,  Mean reward: -6.058139534883721, Mean Entropy: 0.9747123718261719, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 283,  Mean reward: -3.0375, Mean Entropy: 0.9097295999526978, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 284,  Mean reward: -4.487804878048781, Mean Entropy: 0.8873203992843628, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 285,  Mean reward: -5.5256410256410255, Mean Entropy: 0.9540469646453857, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 286,  Mean reward: -2.933333333333333, Mean Entropy: 0.9803635478019714, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 287,  Mean reward: -3.215909090909091, Mean Entropy: 0.9310051798820496, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 288,  Mean reward: -3.5813953488372094, Mean Entropy: 0.9169518947601318, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 289,  Mean reward: -3.7142857142857144, Mean Entropy: 0.9314161539077759, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 290,  Mean reward: -2.5, Mean Entropy: 0.9675178527832031, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.56s
Iteration: 291,  Mean reward: -3.625, Mean Entropy: 0.9025354385375977, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.56s
Iteration: 292,  Mean reward: -4.2555555555555555, Mean Entropy: 0.9530772566795349, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.54s
Iteration: 293,  Mean reward: -4.890243902439025, Mean Entropy: 0.8808748126029968, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 294,  Mean reward: -2.0454545454545454, Mean Entropy: 1.0180597305297852, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 295,  Mean reward: -4.914634146341464, Mean Entropy: 0.9819587469100952, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 296,  Mean reward: -5.75, Mean Entropy: 0.9241964817047119, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 297,  Mean reward: -3.4545454545454546, Mean Entropy: 0.9530771970748901, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 298,  Mean reward: -2.7023809523809526, Mean Entropy: 0.9241955280303955, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 299,  Mean reward: -3.45, Mean Entropy: 0.9819583892822266, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 300,  Mean reward: -4.5813953488372094, Mean Entropy: 0.9530758857727051, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -4.939024390243903, Mean Entropy: 0.9818944931030273, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 302,  Mean reward: -7.404761904761905, Mean Entropy: 0.9602922797203064, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 303,  Mean reward: -5.630952380952381, Mean Entropy: 0.9486242532730103, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 304,  Mean reward: -6.023255813953488, Mean Entropy: 0.908515453338623, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 305,  Mean reward: -1.75, Mean Entropy: 0.920525074005127, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 306,  Mean reward: -6.372340425531915, Mean Entropy: 0.9807724952697754, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.67s
Iteration: 307,  Mean reward: -5.659090909090909, Mean Entropy: 0.8882918357849121, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 308,  Mean reward: -4.625, Mean Entropy: 0.8853935599327087, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 309,  Mean reward: -5.841463414634147, Mean Entropy: 0.9169110655784607, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 310,  Mean reward: -2.882978723404255, Mean Entropy: 0.9029802083969116, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 311,  Mean reward: -6.76595744680851, Mean Entropy: 0.9442598223686218, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 312,  Mean reward: -6.170731707317073, Mean Entropy: 0.8333959579467773, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 313,  Mean reward: -3.7448979591836733, Mean Entropy: 0.8234531283378601, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 314,  Mean reward: -6.469387755102041, Mean Entropy: 0.8133801221847534, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 315,  Mean reward: -4.023809523809524, Mean Entropy: 0.7412754893302917, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 316,  Mean reward: -2.2142857142857144, Mean Entropy: 0.6269422769546509, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 317,  Mean reward: -2.576923076923077, Mean Entropy: 0.7895568609237671, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 318,  Mean reward: -2.67, Mean Entropy: 0.6911845207214355, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 319,  Mean reward: -1.9411764705882353, Mean Entropy: 0.7643904685974121, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 320,  Mean reward: -0.8723404255319149, Mean Entropy: 0.7211908102035522, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 321,  Mean reward: -4.14, Mean Entropy: 0.852411150932312, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 322,  Mean reward: -2.0, Mean Entropy: 0.839024543762207, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 323,  Mean reward: -4.6, Mean Entropy: 0.8380429744720459, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 324,  Mean reward: -4.333333333333333, Mean Entropy: 0.8299722671508789, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 325,  Mean reward: -2.1041666666666665, Mean Entropy: 0.6625472903251648, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 326,  Mean reward: -4.324074074074074, Mean Entropy: 0.8511859178543091, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 327,  Mean reward: -2.8020833333333335, Mean Entropy: 0.7633640170097351, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 328,  Mean reward: -3.326923076923077, Mean Entropy: 0.7632550597190857, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 329,  Mean reward: -4.729166666666667, Mean Entropy: 0.8082389831542969, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 330,  Mean reward: -4.163265306122449, Mean Entropy: 0.8377096652984619, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 331,  Mean reward: -4.911111111111111, Mean Entropy: 0.8341225385665894, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 332,  Mean reward: -4.010204081632653, Mean Entropy: 0.7102674245834351, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 333,  Mean reward: -3.4130434782608696, Mean Entropy: 0.8588736057281494, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 334,  Mean reward: -5.326086956521739, Mean Entropy: 0.8438743948936462, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 335,  Mean reward: -2.8229166666666665, Mean Entropy: 0.8181084394454956, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 336,  Mean reward: -2.9705882352941178, Mean Entropy: 0.8367959856987, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 337,  Mean reward: -2.84375, Mean Entropy: 0.7719323039054871, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 338,  Mean reward: -5.811320754716981, Mean Entropy: 0.857877254486084, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 339,  Mean reward: -5.072916666666667, Mean Entropy: 0.8910738229751587, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 340,  Mean reward: -4.170454545454546, Mean Entropy: 0.8974908590316772, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 341,  Mean reward: -2.7560975609756095, Mean Entropy: 0.9525820016860962, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 342,  Mean reward: -4.573170731707317, Mean Entropy: 0.9318060874938965, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.60s
Iteration: 343,  Mean reward: -2.9761904761904763, Mean Entropy: 0.8837723135948181, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 344,  Mean reward: -4.571428571428571, Mean Entropy: 0.9038139581680298, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 345,  Mean reward: -4.163043478260869, Mean Entropy: 0.9254167079925537, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 346,  Mean reward: -4.3061224489795915, Mean Entropy: 0.8990408778190613, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 347,  Mean reward: -5.989130434782608, Mean Entropy: 0.9066479206085205, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 348,  Mean reward: -3.8152173913043477, Mean Entropy: 0.8814138770103455, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 349,  Mean reward: -6.536585365853658, Mean Entropy: 0.9136370420455933, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 350,  Mean reward: -3.302325581395349, Mean Entropy: 0.8388419151306152, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 351,  Mean reward: -4.293478260869565, Mean Entropy: 0.9266949892044067, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 352,  Mean reward: -5.443181818181818, Mean Entropy: 0.876442551612854, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 353,  Mean reward: -5.614583333333333, Mean Entropy: 0.9025750160217285, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 354,  Mean reward: -3.5531914893617023, Mean Entropy: 0.8544149398803711, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 355,  Mean reward: -2.2058823529411766, Mean Entropy: 0.8318328857421875, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 356,  Mean reward: -3.282608695652174, Mean Entropy: 0.7611464262008667, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 357,  Mean reward: -2.2708333333333335, Mean Entropy: 0.7185777425765991, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 358,  Mean reward: -3.0, Mean Entropy: 0.7384971380233765, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 359,  Mean reward: -5.0, Mean Entropy: 0.8602337837219238, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 360,  Mean reward: -3.8, Mean Entropy: 0.7548675537109375, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 361,  Mean reward: -1.4285714285714286, Mean Entropy: 0.7024377584457397, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 362,  Mean reward: -2.97, Mean Entropy: 0.7462668418884277, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 363,  Mean reward: -3.6222222222222222, Mean Entropy: 0.7721676826477051, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 364,  Mean reward: -5.145833333333333, Mean Entropy: 0.8485218286514282, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 365,  Mean reward: -3.316326530612245, Mean Entropy: 0.7731143832206726, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 366,  Mean reward: -3.5096153846153846, Mean Entropy: 0.7669298648834229, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 367,  Mean reward: -5.787234042553192, Mean Entropy: 0.8695168495178223, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 368,  Mean reward: -2.734042553191489, Mean Entropy: 0.868069052696228, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 369,  Mean reward: -3.5, Mean Entropy: 0.8717419505119324, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 370,  Mean reward: -2.9901960784313726, Mean Entropy: 0.7851704359054565, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 371,  Mean reward: -4.457446808510638, Mean Entropy: 0.7542616724967957, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 372,  Mean reward: -4.884615384615385, Mean Entropy: 0.689685583114624, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 373,  Mean reward: -4.866071428571429, Mean Entropy: 0.7202125787734985, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 374,  Mean reward: -4.132075471698113, Mean Entropy: 0.7394418716430664, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 375,  Mean reward: -4.180851063829787, Mean Entropy: 0.7110031843185425, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 376,  Mean reward: -4.69, Mean Entropy: 0.6653133630752563, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 377,  Mean reward: -4.1875, Mean Entropy: 0.6165955066680908, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.58s
Iteration: 378,  Mean reward: -4.84, Mean Entropy: 0.688841700553894, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 379,  Mean reward: -3.7127659574468086, Mean Entropy: 0.6807107925415039, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 380,  Mean reward: -3.076923076923077, Mean Entropy: 0.6483137011528015, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 381,  Mean reward: -2.74, Mean Entropy: 0.6486773490905762, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 382,  Mean reward: -4.719298245614035, Mean Entropy: 0.7524166107177734, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 383,  Mean reward: -1.7777777777777777, Mean Entropy: 0.7077978849411011, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 384,  Mean reward: -2.330357142857143, Mean Entropy: 0.8106254935264587, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 385,  Mean reward: -2.5, Mean Entropy: 0.7861685156822205, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 386,  Mean reward: -5.09, Mean Entropy: 0.7921656966209412, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 387,  Mean reward: -4.3173076923076925, Mean Entropy: 0.7681441307067871, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 388,  Mean reward: -3.8979591836734695, Mean Entropy: 0.7479547262191772, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 389,  Mean reward: -4.127659574468085, Mean Entropy: 0.726262092590332, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 390,  Mean reward: -5.783018867924528, Mean Entropy: 0.7357036471366882, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 391,  Mean reward: -1.1634615384615385, Mean Entropy: 0.5792086720466614, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 392,  Mean reward: -1.4074074074074074, Mean Entropy: 0.6389276385307312, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 393,  Mean reward: -2.209090909090909, Mean Entropy: 0.7414803504943848, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 394,  Mean reward: -5.953703703703703, Mean Entropy: 0.8152544498443604, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 395,  Mean reward: -4.145833333333333, Mean Entropy: 0.7621573209762573, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 396,  Mean reward: -3.2244897959183674, Mean Entropy: 0.6606836915016174, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 397,  Mean reward: -2.3461538461538463, Mean Entropy: 0.6073468923568726, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 398,  Mean reward: -1.6442307692307692, Mean Entropy: 0.6199155449867249, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 399,  Mean reward: -4.25, Mean Entropy: 0.6254664659500122, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 400,  Mean reward: -3.5754716981132075, Mean Entropy: 0.7268897294998169, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.41s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -2.3846153846153846, Mean Entropy: 0.6805533170700073, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 402,  Mean reward: -3.423076923076923, Mean Entropy: 0.6672362685203552, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 403,  Mean reward: -3.9814814814814814, Mean Entropy: 0.6569346785545349, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 404,  Mean reward: -1.2549019607843137, Mean Entropy: 0.5711272954940796, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 405,  Mean reward: -1.1637931034482758, Mean Entropy: 0.7004315257072449, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 406,  Mean reward: -2.590909090909091, Mean Entropy: 0.6518850326538086, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 407,  Mean reward: -4.519230769230769, Mean Entropy: 0.6667732000350952, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 408,  Mean reward: -4.862068965517241, Mean Entropy: 0.5444747805595398, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 409,  Mean reward: -4.872549019607843, Mean Entropy: 0.5963027477264404, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 410,  Mean reward: -4.212962962962963, Mean Entropy: 0.761418342590332, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 411,  Mean reward: -4.673913043478261, Mean Entropy: 0.8305907249450684, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 412,  Mean reward: -3.4148936170212765, Mean Entropy: 0.7520238757133484, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.60s
Iteration: 413,  Mean reward: -4.033333333333333, Mean Entropy: 0.7168254852294922, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 414,  Mean reward: -3.806122448979592, Mean Entropy: 0.6451978087425232, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 415,  Mean reward: -0.47368421052631576, Mean Entropy: 0.6001412868499756, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.40s
Iteration: 416,  Mean reward: -2.0288461538461537, Mean Entropy: 0.625008225440979, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 417,  Mean reward: -2.53125, Mean Entropy: 0.6696900129318237, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 418,  Mean reward: -3.0392156862745097, Mean Entropy: 0.6642672419548035, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 419,  Mean reward: -4.583333333333333, Mean Entropy: 0.6620289087295532, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 420,  Mean reward: -4.221153846153846, Mean Entropy: 0.5381815433502197, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 421,  Mean reward: -5.083333333333333, Mean Entropy: 0.5627660751342773, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 422,  Mean reward: -4.0636363636363635, Mean Entropy: 0.5699512958526611, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 423,  Mean reward: -3.5, Mean Entropy: 0.6080007553100586, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 424,  Mean reward: -4.535714285714286, Mean Entropy: 0.5787537097930908, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 425,  Mean reward: -3.0272727272727273, Mean Entropy: 0.5429191589355469, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 426,  Mean reward: -2.725, Mean Entropy: 0.584775447845459, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 427,  Mean reward: -3.201923076923077, Mean Entropy: 0.6060491800308228, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 428,  Mean reward: -3.2962962962962963, Mean Entropy: 0.6718236804008484, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 429,  Mean reward: -3.5208333333333335, Mean Entropy: 0.6403334140777588, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 430,  Mean reward: -3.290909090909091, Mean Entropy: 0.5910961627960205, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 431,  Mean reward: -4.098214285714286, Mean Entropy: 0.624665379524231, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 432,  Mean reward: -3.173469387755102, Mean Entropy: 0.6156243085861206, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 433,  Mean reward: -1.4017857142857142, Mean Entropy: 0.5969403386116028, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 434,  Mean reward: -4.19, Mean Entropy: 0.6152060031890869, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 435,  Mean reward: -4.472727272727273, Mean Entropy: 0.6367141008377075, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 436,  Mean reward: -1.537037037037037, Mean Entropy: 0.6347848176956177, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 437,  Mean reward: -5.392156862745098, Mean Entropy: 0.638654351234436, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 438,  Mean reward: -3.5849056603773586, Mean Entropy: 0.6619736552238464, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 439,  Mean reward: -4.185185185185185, Mean Entropy: 0.6007044315338135, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 440,  Mean reward: -1.7818181818181817, Mean Entropy: 0.5379190444946289, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 441,  Mean reward: -4.675925925925926, Mean Entropy: 0.6007533073425293, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 442,  Mean reward: -3.1826923076923075, Mean Entropy: 0.5871392488479614, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 443,  Mean reward: -3.3421052631578947, Mean Entropy: 0.597754180431366, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 444,  Mean reward: -2.6481481481481484, Mean Entropy: 0.6014112234115601, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 445,  Mean reward: -4.071428571428571, Mean Entropy: 0.5935102701187134, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 446,  Mean reward: -0.8448275862068966, Mean Entropy: 0.5415142774581909, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 447,  Mean reward: -3.2, Mean Entropy: 0.5901031494140625, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.59s
Iteration: 448,  Mean reward: -3.0625, Mean Entropy: 0.6296207904815674, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 449,  Mean reward: -4.845454545454546, Mean Entropy: 0.5931562781333923, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 450,  Mean reward: -2.8796296296296298, Mean Entropy: 0.5743932723999023, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 451,  Mean reward: -3.701923076923077, Mean Entropy: 0.5307227969169617, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 452,  Mean reward: -3.2982456140350878, Mean Entropy: 0.5707192420959473, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 453,  Mean reward: -4.629310344827586, Mean Entropy: 0.5259755849838257, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 454,  Mean reward: -4.394736842105263, Mean Entropy: 0.5308312177658081, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 455,  Mean reward: -1.6886792452830188, Mean Entropy: 0.5998691320419312, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 456,  Mean reward: -4.640350877192983, Mean Entropy: 0.5165833234786987, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 457,  Mean reward: -5.254716981132075, Mean Entropy: 0.5623077750205994, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 458,  Mean reward: -2.8684210526315788, Mean Entropy: 0.5450396537780762, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 459,  Mean reward: -2.890909090909091, Mean Entropy: 0.5236654877662659, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 460,  Mean reward: -3.705357142857143, Mean Entropy: 0.668498694896698, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 461,  Mean reward: -3.5714285714285716, Mean Entropy: 0.9097131490707397, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 462,  Mean reward: -4.151162790697675, Mean Entropy: 0.9386228919029236, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 463,  Mean reward: -3.595744680851064, Mean Entropy: 0.931411862373352, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 464,  Mean reward: -3.6222222222222222, Mean Entropy: 0.9963946342468262, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 465,  Mean reward: -4.564102564102564, Mean Entropy: 0.9241907000541687, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 466,  Mean reward: -5.25, Mean Entropy: 0.895309329032898, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 467,  Mean reward: -5.9523809523809526, Mean Entropy: 0.9097463488578796, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 468,  Mean reward: -1.5520833333333333, Mean Entropy: 0.8953051567077637, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 469,  Mean reward: -4.121951219512195, Mean Entropy: 0.9169611930847168, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 470,  Mean reward: -6.22972972972973, Mean Entropy: 0.8735978007316589, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 471,  Mean reward: -4.463414634146342, Mean Entropy: 0.9457699656486511, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 472,  Mean reward: -5.7682926829268295, Mean Entropy: 0.9161961078643799, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 473,  Mean reward: -5.115384615384615, Mean Entropy: 0.8804500102996826, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 474,  Mean reward: -5.119047619047619, Mean Entropy: 0.9458453059196472, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 475,  Mean reward: -5.4125, Mean Entropy: 0.9241943359375, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 476,  Mean reward: -2.8214285714285716, Mean Entropy: 0.9602912664413452, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 477,  Mean reward: -2.9625, Mean Entropy: 0.9241792559623718, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 478,  Mean reward: -3.7209302325581395, Mean Entropy: 0.8952462077140808, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 479,  Mean reward: -4.7073170731707314, Mean Entropy: 0.9530330896377563, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 480,  Mean reward: -2.975, Mean Entropy: 0.9385344386100769, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 481,  Mean reward: -3.792682926829268, Mean Entropy: 0.8951883316040039, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 482,  Mean reward: -4.595238095238095, Mean Entropy: 0.9863573908805847, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 483,  Mean reward: -4.878048780487805, Mean Entropy: 0.8203252553939819, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.59s
Iteration: 484,  Mean reward: -2.3958333333333335, Mean Entropy: 0.9022425413131714, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 485,  Mean reward: -3.85, Mean Entropy: 0.9439868927001953, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 486,  Mean reward: -3.3181818181818183, Mean Entropy: 0.8812991380691528, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 487,  Mean reward: -2.882978723404255, Mean Entropy: 0.8320462703704834, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 488,  Mean reward: -1.375, Mean Entropy: 0.9301384687423706, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 489,  Mean reward: -5.9375, Mean Entropy: 0.9063250422477722, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 490,  Mean reward: -5.371794871794871, Mean Entropy: 0.9982120394706726, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 491,  Mean reward: -3.088888888888889, Mean Entropy: 0.9457889795303345, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 492,  Mean reward: -6.355555555555555, Mean Entropy: 0.8952653408050537, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 493,  Mean reward: -4.27906976744186, Mean Entropy: 0.9164780974388123, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 494,  Mean reward: -3.784090909090909, Mean Entropy: 0.9061012268066406, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 495,  Mean reward: -2.7717391304347827, Mean Entropy: 0.9517722129821777, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 496,  Mean reward: -5.177083333333333, Mean Entropy: 0.9391640424728394, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 497,  Mean reward: -2.658536585365854, Mean Entropy: 0.8427873849868774, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 498,  Mean reward: -1.1875, Mean Entropy: 0.9523087739944458, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 499,  Mean reward: -3.5609756097560976, Mean Entropy: 0.9672656059265137, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 500,  Mean reward: -5.353658536585366, Mean Entropy: 0.9962170124053955, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.42s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -6.988372093023256, Mean Entropy: 0.866165816783905, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 502,  Mean reward: -6.809523809523809, Mean Entropy: 0.7924408912658691, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 503,  Mean reward: -3.816326530612245, Mean Entropy: 0.8165030479431152, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 504,  Mean reward: -4.554347826086956, Mean Entropy: 0.8026533126831055, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 505,  Mean reward: -4.75, Mean Entropy: 0.5213524103164673, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 506,  Mean reward: -4.907407407407407, Mean Entropy: 0.553957462310791, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 507,  Mean reward: -5.781818181818182, Mean Entropy: 0.5671637058258057, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 508,  Mean reward: -2.5272727272727273, Mean Entropy: 0.5426559448242188, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 509,  Mean reward: -2.8823529411764706, Mean Entropy: 0.5449173450469971, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 510,  Mean reward: -1.9636363636363636, Mean Entropy: 0.569273054599762, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 511,  Mean reward: -3.5847457627118646, Mean Entropy: 0.5340014696121216, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 512,  Mean reward: -5.3584905660377355, Mean Entropy: 0.6397499442100525, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 513,  Mean reward: -4.0754716981132075, Mean Entropy: 0.7989334464073181, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 514,  Mean reward: -4.6875, Mean Entropy: 0.7724310755729675, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 515,  Mean reward: -1.7755102040816326, Mean Entropy: 0.8822892904281616, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 516,  Mean reward: -1.25, Mean Entropy: 0.9290539622306824, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 517,  Mean reward: -4.977777777777778, Mean Entropy: 0.5038007497787476, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 518,  Mean reward: -1.8269230769230769, Mean Entropy: 0.716817319393158, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.59s
Iteration: 519,  Mean reward: -4.948979591836735, Mean Entropy: 0.866401195526123, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 520,  Mean reward: -4.825, Mean Entropy: 0.4764629304409027, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 521,  Mean reward: -3.669811320754717, Mean Entropy: 0.5804831981658936, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 522,  Mean reward: -3.611111111111111, Mean Entropy: 0.5305665731430054, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 523,  Mean reward: -2.3333333333333335, Mean Entropy: 0.5858079791069031, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 524,  Mean reward: -4.452830188679245, Mean Entropy: 0.587774932384491, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 525,  Mean reward: -4.051020408163265, Mean Entropy: 0.604412317276001, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 526,  Mean reward: -2.809090909090909, Mean Entropy: 0.583573579788208, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 527,  Mean reward: -2.8846153846153846, Mean Entropy: 0.6834192276000977, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 528,  Mean reward: -2.38, Mean Entropy: 0.6054445505142212, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 529,  Mean reward: -3.3365384615384617, Mean Entropy: 0.7128271460533142, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 530,  Mean reward: -1.45, Mean Entropy: 0.683320939540863, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 531,  Mean reward: -4.448979591836735, Mean Entropy: 0.6694868803024292, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 532,  Mean reward: -3.4, Mean Entropy: 0.6052488088607788, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 533,  Mean reward: -1.7, Mean Entropy: 0.6015040874481201, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 534,  Mean reward: -2.209090909090909, Mean Entropy: 0.5906956195831299, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 535,  Mean reward: -2.3947368421052633, Mean Entropy: 0.5414823293685913, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 536,  Mean reward: -4.5, Mean Entropy: 0.43503254652023315, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 537,  Mean reward: -5.8559322033898304, Mean Entropy: 0.4149516820907593, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 538,  Mean reward: -3.5793650793650795, Mean Entropy: 0.3798777759075165, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 539,  Mean reward: -3.8253968253968256, Mean Entropy: 0.48098716139793396, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 540,  Mean reward: -3.566666666666667, Mean Entropy: 0.4039074182510376, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 541,  Mean reward: -3.418181818181818, Mean Entropy: 0.34136995673179626, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 542,  Mean reward: -3.2416666666666667, Mean Entropy: 0.48967188596725464, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 543,  Mean reward: -3.4310344827586206, Mean Entropy: 0.40059328079223633, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 544,  Mean reward: -2.7155172413793105, Mean Entropy: 0.4068562388420105, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 545,  Mean reward: -4.95, Mean Entropy: 0.4370051622390747, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 546,  Mean reward: -2.483050847457627, Mean Entropy: 0.38167262077331543, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 547,  Mean reward: -4.254385964912281, Mean Entropy: 0.2929891347885132, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 548,  Mean reward: -2.0901639344262297, Mean Entropy: 0.39208436012268066, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 549,  Mean reward: -5.875, Mean Entropy: 0.23329207301139832, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 550,  Mean reward: -1.359375, Mean Entropy: 0.308420330286026, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 551,  Mean reward: -3.7661290322580645, Mean Entropy: 0.2601887583732605, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 552,  Mean reward: -3.2388059701492535, Mean Entropy: 0.2515830397605896, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 553,  Mean reward: -2.098684210526316, Mean Entropy: 0.2123490273952484, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 554,  Mean reward: 0.10273972602739725, Mean Entropy: 0.1462114453315735, complete_episode_count: 73.0, Gather time: 0.57s, Train time: 0.89s
Iteration: 555,  Mean reward: -1.821917808219178, Mean Entropy: 0.024292074143886566, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 556,  Mean reward: -2.0, Mean Entropy: 0.012284904718399048, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 557,  Mean reward: -1.25, Mean Entropy: 0.010359266772866249, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 558,  Mean reward: 0.0, Mean Entropy: 0.0057530151680111885, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 559,  Mean reward: -1.25, Mean Entropy: 0.005067544989287853, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 560,  Mean reward: -0.75, Mean Entropy: 0.005047648213803768, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 561,  Mean reward: -2.75, Mean Entropy: 0.0049761440604925156, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 562,  Mean reward: -2.5, Mean Entropy: 0.004552047234028578, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 563,  Mean reward: -3.25, Mean Entropy: 0.004190491046756506, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 564,  Mean reward: -3.75, Mean Entropy: 0.004664105363190174, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 565,  Mean reward: -2.75, Mean Entropy: 0.004565093666315079, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 566,  Mean reward: -1.0, Mean Entropy: 0.005943463649600744, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 567,  Mean reward: -1.25, Mean Entropy: 0.004354288801550865, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 568,  Mean reward: -4.75, Mean Entropy: 0.005469940602779388, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 569,  Mean reward: -2.0, Mean Entropy: 0.0052251061424613, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 570,  Mean reward: -2.551282051282051, Mean Entropy: 0.006601361092180014, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 571,  Mean reward: -1.5, Mean Entropy: 0.00387563812546432, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 572,  Mean reward: -1.75, Mean Entropy: 0.002275420818477869, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 573,  Mean reward: -1.75, Mean Entropy: 0.0024545625783503056, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 574,  Mean reward: -1.5, Mean Entropy: 0.0023319076281040907, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 575,  Mean reward: -1.75, Mean Entropy: 0.0023959516547620296, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 576,  Mean reward: -2.75, Mean Entropy: 0.0023116599768400192, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 577,  Mean reward: -2.5, Mean Entropy: 0.0024712751619517803, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 578,  Mean reward: -2.651898734177215, Mean Entropy: 0.002825690433382988, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 579,  Mean reward: -2.0, Mean Entropy: 0.005285905208438635, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 580,  Mean reward: -0.5, Mean Entropy: 0.006559628527611494, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 581,  Mean reward: -2.5, Mean Entropy: 0.0033360919915139675, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 582,  Mean reward: 0.0, Mean Entropy: 0.0037340286653488874, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 583,  Mean reward: -3.25, Mean Entropy: 0.003064542543143034, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 584,  Mean reward: -2.25, Mean Entropy: 0.003019468393176794, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 585,  Mean reward: -1.25, Mean Entropy: 0.0033783032558858395, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 586,  Mean reward: -4.25, Mean Entropy: 0.0028974395245313644, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 587,  Mean reward: -1.25, Mean Entropy: 0.0035137447994202375, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 588,  Mean reward: -1.75, Mean Entropy: 0.0033759779762476683, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 589,  Mean reward: -2.5, Mean Entropy: 0.0038116173818707466, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 590,  Mean reward: -2.0, Mean Entropy: 0.005168578587472439, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 591,  Mean reward: -3.0, Mean Entropy: 0.007535096257925034, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 592,  Mean reward: -1.5, Mean Entropy: 0.009138310328125954, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 593,  Mean reward: -0.8607594936708861, Mean Entropy: 0.007161588873714209, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 594,  Mean reward: -4.0, Mean Entropy: 0.004948618821799755, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 595,  Mean reward: -1.5, Mean Entropy: 0.005150300450623035, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 596,  Mean reward: -2.9050632911392404, Mean Entropy: 0.0040381895378232, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 597,  Mean reward: -3.5, Mean Entropy: 0.0030723612289875746, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 598,  Mean reward: -1.25, Mean Entropy: 0.003906011115759611, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 599,  Mean reward: -2.0, Mean Entropy: 0.0038466881960630417, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 600,  Mean reward: -2.0, Mean Entropy: 0.0037123248912394047, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.5, Mean Entropy: 0.0030133510008454323, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 602,  Mean reward: -3.0, Mean Entropy: 0.002611040137708187, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 603,  Mean reward: -2.9050632911392404, Mean Entropy: 0.002795345615595579, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 604,  Mean reward: -2.5, Mean Entropy: 0.002304261550307274, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 605,  Mean reward: -1.0, Mean Entropy: 0.00206797756254673, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 606,  Mean reward: -3.0, Mean Entropy: 0.002216299995779991, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 607,  Mean reward: -2.651898734177215, Mean Entropy: 0.0018418066902086139, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 608,  Mean reward: 0.25, Mean Entropy: 0.0019408351508900523, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 609,  Mean reward: -0.5, Mean Entropy: 0.001989100594073534, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 610,  Mean reward: 0.75, Mean Entropy: 0.002288208343088627, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 611,  Mean reward: -2.0, Mean Entropy: 0.0023686846252530813, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 612,  Mean reward: -1.75, Mean Entropy: 0.002563427435234189, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 613,  Mean reward: -3.5, Mean Entropy: 0.0028143245726823807, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 614,  Mean reward: -1.75, Mean Entropy: 0.0029988873284310102, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 615,  Mean reward: -1.75, Mean Entropy: 0.0026405786629766226, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 616,  Mean reward: -2.75, Mean Entropy: 0.0023774257861077785, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 617,  Mean reward: -1.75, Mean Entropy: 0.002286133822053671, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 618,  Mean reward: -2.0, Mean Entropy: 0.002209324389696121, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 619,  Mean reward: -0.75, Mean Entropy: 0.0022146846167743206, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 620,  Mean reward: -3.25, Mean Entropy: 0.0018920309375971556, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 621,  Mean reward: -2.5, Mean Entropy: 0.0018225668463855982, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 622,  Mean reward: -1.5, Mean Entropy: 0.0018099334556609392, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 623,  Mean reward: -0.75, Mean Entropy: 0.0018274562899023294, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 624,  Mean reward: -1.75, Mean Entropy: 0.0018210455309599638, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 625,  Mean reward: -1.75, Mean Entropy: 0.0018135076388716698, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 626,  Mean reward: -2.25, Mean Entropy: 0.001754929544404149, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 627,  Mean reward: -2.75, Mean Entropy: 0.0017129031475633383, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 628,  Mean reward: -2.0, Mean Entropy: 0.0017761082854121923, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 629,  Mean reward: -3.0, Mean Entropy: 0.0016610098537057638, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 630,  Mean reward: -1.25, Mean Entropy: 0.001692350720986724, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 631,  Mean reward: -0.5, Mean Entropy: 0.0017080012476071715, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 632,  Mean reward: -1.5, Mean Entropy: 0.0016314275562763214, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 633,  Mean reward: -2.75, Mean Entropy: 0.0016107404371723533, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 634,  Mean reward: -2.25, Mean Entropy: 0.0016011139377951622, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 635,  Mean reward: -1.75, Mean Entropy: 0.0016331797232851386, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 636,  Mean reward: -1.0, Mean Entropy: 0.0015836794627830386, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 637,  Mean reward: 0.5, Mean Entropy: 0.0016262035351246595, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 638,  Mean reward: -3.5, Mean Entropy: 0.0015551483957096934, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 639,  Mean reward: -2.5, Mean Entropy: 0.001552702859044075, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 640,  Mean reward: -2.5, Mean Entropy: 0.0015801995759829879, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 641,  Mean reward: -1.75, Mean Entropy: 0.0015731601743027568, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 642,  Mean reward: -1.25, Mean Entropy: 0.0015794287901371717, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 643,  Mean reward: -0.5, Mean Entropy: 0.0015906508779153228, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 644,  Mean reward: -1.0, Mean Entropy: 0.0015972391702234745, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 645,  Mean reward: -1.75, Mean Entropy: 0.0015491088852286339, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 646,  Mean reward: -1.5, Mean Entropy: 0.001561636570841074, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 647,  Mean reward: -2.5, Mean Entropy: 0.001530746929347515, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 648,  Mean reward: -2.25, Mean Entropy: 0.0014906579162925482, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 649,  Mean reward: -2.0, Mean Entropy: 0.0014990310883149505, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 650,  Mean reward: -4.0, Mean Entropy: 0.0014719727914780378, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 651,  Mean reward: -2.75, Mean Entropy: 0.0014618998393416405, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 652,  Mean reward: -2.25, Mean Entropy: 0.0014252928085625172, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 653,  Mean reward: -1.75, Mean Entropy: 0.0014318901812657714, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 654,  Mean reward: -1.5, Mean Entropy: 0.001397749176248908, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 655,  Mean reward: -1.5, Mean Entropy: 0.0013415783178061247, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 656,  Mean reward: -1.5, Mean Entropy: 0.001322766300290823, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 657,  Mean reward: -0.5, Mean Entropy: 0.0014196459669619799, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 658,  Mean reward: -2.25, Mean Entropy: 0.0014134699013084173, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 659,  Mean reward: -1.75, Mean Entropy: 0.0013084926176816225, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 660,  Mean reward: -4.677215189873418, Mean Entropy: 0.000766669400036335, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 661,  Mean reward: -3.25, Mean Entropy: 0.0008127129403874278, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 662,  Mean reward: -1.5, Mean Entropy: 0.0008854755433276296, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 663,  Mean reward: -2.5, Mean Entropy: 0.0008196592680178583, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 664,  Mean reward: -2.25, Mean Entropy: 0.0008891382021829486, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 665,  Mean reward: -2.0, Mean Entropy: 0.0008949986658990383, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 666,  Mean reward: -2.0, Mean Entropy: 0.0009048883803188801, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 667,  Mean reward: -1.0, Mean Entropy: 0.0009310523746535182, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 668,  Mean reward: -2.25, Mean Entropy: 0.0009661989752203226, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 669,  Mean reward: -2.0, Mean Entropy: 0.0009836647659540176, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 670,  Mean reward: -2.5, Mean Entropy: 0.000922848004847765, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 671,  Mean reward: -1.0, Mean Entropy: 0.000921157596167177, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 672,  Mean reward: -2.75, Mean Entropy: 0.000859635416418314, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 673,  Mean reward: -1.1139240506329113, Mean Entropy: 0.0006773126078769565, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 674,  Mean reward: -2.0, Mean Entropy: 0.0006249194266274571, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 675,  Mean reward: -2.25, Mean Entropy: 0.0006239420035853982, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 676,  Mean reward: -0.75, Mean Entropy: 0.0006649700808338821, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 677,  Mean reward: -1.25, Mean Entropy: 0.0006617058534175158, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 678,  Mean reward: -0.5, Mean Entropy: 0.0006627218681387603, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 679,  Mean reward: -1.5, Mean Entropy: 0.0006361127598211169, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 680,  Mean reward: -0.75, Mean Entropy: 0.0006686404813081026, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 681,  Mean reward: -1.0, Mean Entropy: 0.0007040644413791597, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 682,  Mean reward: -2.0, Mean Entropy: 0.0006952867843210697, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 683,  Mean reward: -1.25, Mean Entropy: 0.0006914031691849232, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 684,  Mean reward: -1.25, Mean Entropy: 0.0006607418181374669, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 685,  Mean reward: -3.0, Mean Entropy: 0.0006015425315126777, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 686,  Mean reward: -1.75, Mean Entropy: 0.0006328708841465414, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 687,  Mean reward: -2.5, Mean Entropy: 0.0006291671306826174, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 688,  Mean reward: -2.0, Mean Entropy: 0.0006052919197827578, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 689,  Mean reward: -3.5, Mean Entropy: 0.0005867565050721169, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 690,  Mean reward: -1.0, Mean Entropy: 0.0005892862100154161, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 691,  Mean reward: -3.0, Mean Entropy: 0.0005712747224606574, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 692,  Mean reward: -3.5, Mean Entropy: 0.0005773457232862711, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 693,  Mean reward: -2.0, Mean Entropy: 0.0005834712646901608, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 694,  Mean reward: -2.5, Mean Entropy: 0.0005848190630786121, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 695,  Mean reward: -2.25, Mean Entropy: 0.0005702177295461297, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 696,  Mean reward: -3.0, Mean Entropy: 0.0005744537338614464, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 697,  Mean reward: -2.25, Mean Entropy: 0.0005880750832147896, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 698,  Mean reward: -1.25, Mean Entropy: 0.000593407719861716, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 699,  Mean reward: -2.0, Mean Entropy: 0.0005840974627062678, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 700,  Mean reward: -1.25, Mean Entropy: 0.0005881082033738494, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 701,  Mean reward: 1.5, Mean Entropy: 0.0005994248203933239, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 702,  Mean reward: -3.0, Mean Entropy: 0.000589024624787271, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 703,  Mean reward: -0.5, Mean Entropy: 0.0005840212106704712, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 704,  Mean reward: -2.75, Mean Entropy: 0.0005804793327115476, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 705,  Mean reward: -3.0, Mean Entropy: 0.0005878558731637895, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 706,  Mean reward: -1.0, Mean Entropy: 0.0005820521619170904, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 707,  Mean reward: -1.75, Mean Entropy: 0.0005683887284249067, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 708,  Mean reward: -0.75, Mean Entropy: 0.0005783404922112823, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 709,  Mean reward: -3.0, Mean Entropy: 0.000575090991333127, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 710,  Mean reward: -0.5, Mean Entropy: 0.0005831296439282596, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 711,  Mean reward: -1.25, Mean Entropy: 0.0005688040982931852, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 712,  Mean reward: -2.0, Mean Entropy: 0.0005659013404510915, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 713,  Mean reward: -0.5, Mean Entropy: 0.0005631274543702602, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 714,  Mean reward: -4.5, Mean Entropy: 0.0005512087955139577, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 715,  Mean reward: -1.5, Mean Entropy: 0.0005551434005610645, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 716,  Mean reward: 0.0, Mean Entropy: 0.0005628712242469192, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 717,  Mean reward: -1.0, Mean Entropy: 0.0005583858583122492, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 718,  Mean reward: -3.75, Mean Entropy: 0.0005527400644496083, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.89s
Iteration: 719,  Mean reward: -4.0, Mean Entropy: 0.0005454883212223649, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 720,  Mean reward: -2.0, Mean Entropy: 0.0005548499757423997, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 721,  Mean reward: -1.5, Mean Entropy: 0.0005557195981964469, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 722,  Mean reward: -2.75, Mean Entropy: 0.000556577870156616, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 723,  Mean reward: -1.25, Mean Entropy: 0.0005752173019573092, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 724,  Mean reward: -2.75, Mean Entropy: 0.000587595859542489, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 725,  Mean reward: -3.25, Mean Entropy: 0.0005974412197247148, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 726,  Mean reward: -1.5, Mean Entropy: 0.0006046259077265859, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 727,  Mean reward: -1.25, Mean Entropy: 0.0005930790794081986, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 728,  Mean reward: -1.75, Mean Entropy: 0.0005711296107620001, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 729,  Mean reward: -1.25, Mean Entropy: 0.0005468687741085887, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 730,  Mean reward: -2.25, Mean Entropy: 0.0005124328890815377, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 731,  Mean reward: -2.25, Mean Entropy: 0.0005011432804167271, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 732,  Mean reward: -0.75, Mean Entropy: 0.0004901605425402522, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 733,  Mean reward: -1.75, Mean Entropy: 0.0004766976344399154, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 734,  Mean reward: -0.75, Mean Entropy: 0.0004895146703347564, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 735,  Mean reward: -0.5, Mean Entropy: 0.00048673985293135047, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 736,  Mean reward: -2.75, Mean Entropy: 0.0004817119042854756, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 737,  Mean reward: -2.75, Mean Entropy: 0.00047552777687087655, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 738,  Mean reward: -2.25, Mean Entropy: 0.0004938420606777072, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 739,  Mean reward: -1.75, Mean Entropy: 0.0004855016013607383, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 740,  Mean reward: -1.75, Mean Entropy: 0.0004913611337542534, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 741,  Mean reward: -2.0, Mean Entropy: 0.0004944524262100458, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 742,  Mean reward: -1.25, Mean Entropy: 0.0005062611307948828, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 743,  Mean reward: -2.25, Mean Entropy: 0.0004997440846636891, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 744,  Mean reward: -1.25, Mean Entropy: 0.0005142166628502309, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 745,  Mean reward: -2.5, Mean Entropy: 0.000502261274959892, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 746,  Mean reward: -2.0, Mean Entropy: 0.0005069387261755764, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 747,  Mean reward: -2.5, Mean Entropy: 0.0005045633879490197, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 748,  Mean reward: -2.0, Mean Entropy: 0.0005049007013440132, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 749,  Mean reward: -3.25, Mean Entropy: 0.0005034442292526364, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 750,  Mean reward: -1.0, Mean Entropy: 0.0005044670542702079, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 751,  Mean reward: -1.0, Mean Entropy: 0.0005058294045738876, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 752,  Mean reward: -1.25, Mean Entropy: 0.0005026497528888285, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 753,  Mean reward: -2.0, Mean Entropy: 0.0005097988760098815, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 754,  Mean reward: -4.25, Mean Entropy: 0.0004833571729250252, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 755,  Mean reward: -2.75, Mean Entropy: 0.00048407440772280097, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 756,  Mean reward: -2.5, Mean Entropy: 0.0004965145490132272, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 757,  Mean reward: -3.5, Mean Entropy: 0.0004946854314766824, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 758,  Mean reward: -1.25, Mean Entropy: 0.0005117951659485698, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 759,  Mean reward: -1.5, Mean Entropy: 0.0005197620484977961, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 760,  Mean reward: -0.5, Mean Entropy: 0.000537608633749187, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.89s
Iteration: 761,  Mean reward: -3.0, Mean Entropy: 0.0005420168745331466, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 762,  Mean reward: -0.25, Mean Entropy: 0.0005643804906867445, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 763,  Mean reward: -3.0, Mean Entropy: 0.0005671554827131331, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 764,  Mean reward: -1.75, Mean Entropy: 0.0005848277942277491, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 765,  Mean reward: -0.75, Mean Entropy: 0.0006109130335971713, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 766,  Mean reward: -4.25, Mean Entropy: 0.0005978905828669667, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 767,  Mean reward: -1.5, Mean Entropy: 0.0006267625140026212, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 768,  Mean reward: -0.5, Mean Entropy: 0.0006615875754505396, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 769,  Mean reward: -1.5, Mean Entropy: 0.000647742475848645, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 770,  Mean reward: -0.25, Mean Entropy: 0.0006647924892604351, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 771,  Mean reward: 0.25, Mean Entropy: 0.0006582082132808864, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 772,  Mean reward: -2.0, Mean Entropy: 0.0006321075488813221, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 773,  Mean reward: -0.5, Mean Entropy: 0.0006266221171244979, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 774,  Mean reward: -2.0, Mean Entropy: 0.0006129269604571164, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 775,  Mean reward: -2.5, Mean Entropy: 0.0006293096230365336, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 776,  Mean reward: -2.75, Mean Entropy: 0.0006434137467294931, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 777,  Mean reward: -2.0, Mean Entropy: 0.00066710589453578, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 778,  Mean reward: -2.25, Mean Entropy: 0.0006541486363857985, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 779,  Mean reward: -1.75, Mean Entropy: 0.0006352521013468504, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 780,  Mean reward: -1.0, Mean Entropy: 0.0006280489033088088, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 781,  Mean reward: -2.0, Mean Entropy: 0.0005987046170048416, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 782,  Mean reward: -1.75, Mean Entropy: 0.0005876057548448443, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 783,  Mean reward: -1.5, Mean Entropy: 0.0005806037224829197, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 784,  Mean reward: -2.25, Mean Entropy: 0.0005831439048051834, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 785,  Mean reward: -3.5, Mean Entropy: 0.0005912454216741025, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 786,  Mean reward: -3.75, Mean Entropy: 0.0005901601398363709, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 787,  Mean reward: -2.25, Mean Entropy: 0.0006107386434450746, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 788,  Mean reward: -3.0, Mean Entropy: 0.0006219245260581374, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 789,  Mean reward: -2.75, Mean Entropy: 0.0006496942369267344, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 790,  Mean reward: -3.0, Mean Entropy: 0.0006567953969351947, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 791,  Mean reward: -3.25, Mean Entropy: 0.0006195792811922729, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 792,  Mean reward: -2.0, Mean Entropy: 0.0003767132293432951, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 793,  Mean reward: -2.0, Mean Entropy: 0.0006440550205297768, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 794,  Mean reward: -1.5, Mean Entropy: 0.0008807961130514741, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 795,  Mean reward: -1.25, Mean Entropy: 0.0009750979370437562, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 796,  Mean reward: -3.75, Mean Entropy: 0.0009490097872912884, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 797,  Mean reward: -0.5, Mean Entropy: 0.0011225584894418716, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 798,  Mean reward: -1.75, Mean Entropy: 0.0010616413783282042, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 799,  Mean reward: -3.25, Mean Entropy: 0.0011044832644984126, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 800,  Mean reward: -1.0, Mean Entropy: 0.0011634000111371279, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -1.5, Mean Entropy: 0.0011333269067108631, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 802,  Mean reward: -1.25, Mean Entropy: 0.001157281338237226, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 803,  Mean reward: -1.5, Mean Entropy: 0.0011500081745907664, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 804,  Mean reward: -3.5, Mean Entropy: 0.0011059384560212493, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 805,  Mean reward: -5.25, Mean Entropy: 0.0010505699319764972, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 806,  Mean reward: -2.0, Mean Entropy: 0.001087856711819768, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 807,  Mean reward: 0.0, Mean Entropy: 0.0011227828217670321, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 808,  Mean reward: -1.0, Mean Entropy: 0.0011478452943265438, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 809,  Mean reward: -1.5, Mean Entropy: 0.0011163580929860473, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 810,  Mean reward: -2.1455696202531644, Mean Entropy: 0.0006760417018085718, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 811,  Mean reward: -0.25, Mean Entropy: 0.000773492269217968, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 812,  Mean reward: -1.75, Mean Entropy: 0.00045071644126437604, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 813,  Mean reward: -1.5, Mean Entropy: 0.0004079314530827105, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 814,  Mean reward: -3.75, Mean Entropy: 0.0004189510364085436, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 815,  Mean reward: -1.5, Mean Entropy: 0.0004329638322815299, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 816,  Mean reward: -2.25, Mean Entropy: 0.00044335611164569855, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 817,  Mean reward: -2.5, Mean Entropy: 0.0004381125036161393, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 818,  Mean reward: -2.25, Mean Entropy: 0.00045219770981930196, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 819,  Mean reward: -2.0, Mean Entropy: 0.00045255967415869236, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 820,  Mean reward: -0.75, Mean Entropy: 0.00045743476948700845, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 821,  Mean reward: -0.5, Mean Entropy: 0.00045388503349386156, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 822,  Mean reward: -1.5, Mean Entropy: 0.00044801924377679825, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 823,  Mean reward: -0.25, Mean Entropy: 0.0004645877634175122, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 824,  Mean reward: -0.5, Mean Entropy: 0.00046780877164565027, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 825,  Mean reward: -2.25, Mean Entropy: 0.00044990243623033166, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 826,  Mean reward: 0.0, Mean Entropy: 0.0004571438767015934, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 827,  Mean reward: -1.5, Mean Entropy: 0.0004388968809507787, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 828,  Mean reward: -4.75, Mean Entropy: 0.0004237661778461188, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 829,  Mean reward: -2.75, Mean Entropy: 0.00042411324102431536, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 830,  Mean reward: -1.25, Mean Entropy: 0.0004306129121687263, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 831,  Mean reward: -2.75, Mean Entropy: 0.000418597599491477, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 832,  Mean reward: -3.0, Mean Entropy: 0.00041652689105831087, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 833,  Mean reward: -0.25, Mean Entropy: 0.00042012683115899563, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 834,  Mean reward: -4.25, Mean Entropy: 0.00040454030386172235, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 835,  Mean reward: -3.75, Mean Entropy: 0.0003952598199248314, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 836,  Mean reward: -2.0, Mean Entropy: 0.00039926133467815816, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 837,  Mean reward: -1.0, Mean Entropy: 0.00039857905358076096, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 838,  Mean reward: -1.0, Mean Entropy: 0.00039432404446415603, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 839,  Mean reward: -2.25, Mean Entropy: 0.00039106360054574907, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 840,  Mean reward: -1.75, Mean Entropy: 0.00039178517181426287, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 841,  Mean reward: -2.0, Mean Entropy: 0.0003899206349160522, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 842,  Mean reward: -1.75, Mean Entropy: 0.00038107423461042345, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 843,  Mean reward: -1.25, Mean Entropy: 0.00037985650124028325, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 844,  Mean reward: -3.0, Mean Entropy: 0.0003693828475661576, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 845,  Mean reward: -3.0, Mean Entropy: 0.0003696183848660439, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 846,  Mean reward: -3.25, Mean Entropy: 0.00036442812415771186, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 847,  Mean reward: -3.5, Mean Entropy: 0.00036304278182797134, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 848,  Mean reward: -1.25, Mean Entropy: 0.0003650032158475369, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 849,  Mean reward: -1.75, Mean Entropy: 0.00036377989454194903, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 850,  Mean reward: -2.25, Mean Entropy: 0.0003578951582312584, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 851,  Mean reward: -0.5, Mean Entropy: 0.0003565132792573422, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 852,  Mean reward: -2.0, Mean Entropy: 0.0003529326932039112, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 853,  Mean reward: -1.75, Mean Entropy: 0.0003539745230227709, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 854,  Mean reward: -2.75, Mean Entropy: 0.00035027225385420024, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 855,  Mean reward: -2.75, Mean Entropy: 0.0003489640657790005, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 856,  Mean reward: -2.5, Mean Entropy: 0.0003461411688476801, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 857,  Mean reward: -0.5, Mean Entropy: 0.0003458901774138212, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 858,  Mean reward: -1.0, Mean Entropy: 0.0003443686000537127, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 859,  Mean reward: -3.5, Mean Entropy: 0.0003429090138524771, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 860,  Mean reward: -3.0, Mean Entropy: 0.0003415974206291139, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 861,  Mean reward: -2.5, Mean Entropy: 0.0003370250342413783, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 862,  Mean reward: -0.75, Mean Entropy: 0.000340007187332958, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 863,  Mean reward: -2.25, Mean Entropy: 0.0003348586324136704, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 864,  Mean reward: -1.25, Mean Entropy: 0.0003320230753161013, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 865,  Mean reward: -2.0, Mean Entropy: 0.00032855855533853173, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 866,  Mean reward: -0.75, Mean Entropy: 0.0003327362355776131, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 867,  Mean reward: 0.0, Mean Entropy: 0.00033133983379229903, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 868,  Mean reward: -2.25, Mean Entropy: 0.00032525608548894525, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 869,  Mean reward: -3.0, Mean Entropy: 0.0003212338197045028, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 870,  Mean reward: 0.5, Mean Entropy: 0.0003220320213586092, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 871,  Mean reward: -1.5, Mean Entropy: 0.00031210758606903255, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 872,  Mean reward: -1.25, Mean Entropy: 0.00030704354867339134, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 873,  Mean reward: -0.5, Mean Entropy: 0.00030302454251796007, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 874,  Mean reward: -1.0, Mean Entropy: 0.00030669927946291864, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 875,  Mean reward: -2.75, Mean Entropy: 0.00029753625858575106, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 876,  Mean reward: -1.75, Mean Entropy: 0.0002994648239109665, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 877,  Mean reward: -0.5, Mean Entropy: 0.00030108491773717105, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 878,  Mean reward: -2.0, Mean Entropy: 0.00029342062771320343, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 879,  Mean reward: -0.75, Mean Entropy: 0.00029917829670011997, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 880,  Mean reward: -1.75, Mean Entropy: 0.0002973988885059953, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 881,  Mean reward: -0.75, Mean Entropy: 0.00030255410820245743, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 882,  Mean reward: 1.0, Mean Entropy: 0.0003081680915784091, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 883,  Mean reward: -3.25, Mean Entropy: 0.0003017184208147228, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 884,  Mean reward: -3.75, Mean Entropy: 0.0003037895658053458, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 885,  Mean reward: -3.0, Mean Entropy: 0.0003125446382910013, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 886,  Mean reward: -2.0, Mean Entropy: 0.00031848702928982675, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 887,  Mean reward: -0.5, Mean Entropy: 0.00031574530294165015, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 888,  Mean reward: -4.5, Mean Entropy: 0.00030294840689748526, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 889,  Mean reward: -2.5, Mean Entropy: 0.00031255194335244596, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 890,  Mean reward: -0.25, Mean Entropy: 0.0003134742146357894, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.88s
Iteration: 891,  Mean reward: -2.0, Mean Entropy: 0.0003102714545093477, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 892,  Mean reward: -2.5, Mean Entropy: 0.0003059952869080007, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 893,  Mean reward: -1.25, Mean Entropy: 0.0003081808390561491, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 894,  Mean reward: -2.75, Mean Entropy: 0.0003048364305868745, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 895,  Mean reward: 0.0, Mean Entropy: 0.00030549021903425455, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 896,  Mean reward: -2.25, Mean Entropy: 0.0002983839949592948, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 897,  Mean reward: -2.0, Mean Entropy: 0.0003036069683730602, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 898,  Mean reward: -3.75, Mean Entropy: 0.0003118686145171523, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 899,  Mean reward: -2.0, Mean Entropy: 0.00031319574918597937, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 900,  Mean reward: -0.5, Mean Entropy: 0.0003283428377471864, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -4.0, Mean Entropy: 0.0003037502756342292, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 902,  Mean reward: -1.0, Mean Entropy: 0.00032090902095660567, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 903,  Mean reward: -3.0, Mean Entropy: 0.0003132416750304401, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 904,  Mean reward: -0.5, Mean Entropy: 0.00032506653224118054, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 905,  Mean reward: -2.0, Mean Entropy: 0.0003221638617105782, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 906,  Mean reward: -0.5, Mean Entropy: 0.0003232625313103199, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 907,  Mean reward: -2.0, Mean Entropy: 0.00031993474112823606, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 908,  Mean reward: -2.0, Mean Entropy: 0.0003178353072144091, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 909,  Mean reward: -3.0, Mean Entropy: 0.0003173741861246526, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 910,  Mean reward: -1.0, Mean Entropy: 0.0003205185930710286, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 911,  Mean reward: -1.0, Mean Entropy: 0.00032567803282290697, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 912,  Mean reward: 2.0, Mean Entropy: 0.000344224477885291, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.70s
Iteration: 913,  Mean reward: -0.5, Mean Entropy: 0.00033948873169720173, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 914,  Mean reward: -4.0, Mean Entropy: 0.0003176175814587623, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 915,  Mean reward: -0.5, Mean Entropy: 0.00032423625816591084, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 916,  Mean reward: -1.25, Mean Entropy: 0.0003182293730787933, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 917,  Mean reward: -0.5, Mean Entropy: 0.0003200943465344608, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 918,  Mean reward: -2.5, Mean Entropy: 0.00030692253494635224, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 919,  Mean reward: 1.25, Mean Entropy: 0.00031380605651065707, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 920,  Mean reward: -1.75, Mean Entropy: 0.00030482231522910297, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 921,  Mean reward: -1.75, Mean Entropy: 0.0003040901792701334, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 922,  Mean reward: -3.5, Mean Entropy: 0.0002929839538410306, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 923,  Mean reward: -1.75, Mean Entropy: 0.00030332658207044005, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 924,  Mean reward: -2.25, Mean Entropy: 0.0003054420230910182, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 925,  Mean reward: -2.75, Mean Entropy: 0.00029910041484981775, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 926,  Mean reward: -2.25, Mean Entropy: 0.0002982977603096515, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 927,  Mean reward: -1.5, Mean Entropy: 0.00029499776428565383, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 928,  Mean reward: -2.0, Mean Entropy: 0.00029608234763145447, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 929,  Mean reward: -2.0, Mean Entropy: 0.0002903885906562209, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 930,  Mean reward: -2.0, Mean Entropy: 0.0002966770261991769, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 931,  Mean reward: -2.5, Mean Entropy: 0.00029240810545161366, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 932,  Mean reward: -3.25, Mean Entropy: 0.00028604554245248437, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 933,  Mean reward: -2.0, Mean Entropy: 0.00030755612533539534, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 934,  Mean reward: -1.0, Mean Entropy: 0.00030833747587166727, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 935,  Mean reward: -1.25, Mean Entropy: 0.0003147829556837678, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 936,  Mean reward: -0.75, Mean Entropy: 0.00031002293690107763, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 937,  Mean reward: -1.25, Mean Entropy: 0.00031593433232046664, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 938,  Mean reward: -1.25, Mean Entropy: 0.00031842809403315187, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 939,  Mean reward: -1.0, Mean Entropy: 0.0003131869016215205, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 940,  Mean reward: -4.0, Mean Entropy: 0.00029552908381447196, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 941,  Mean reward: -1.5, Mean Entropy: 0.00031542390934191644, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 942,  Mean reward: -0.25, Mean Entropy: 0.00031306722667068243, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 943,  Mean reward: -1.25, Mean Entropy: 0.0003133350401185453, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 944,  Mean reward: -2.5, Mean Entropy: 0.0003052890533581376, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 945,  Mean reward: -0.5, Mean Entropy: 0.0003183566441293806, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 946,  Mean reward: -2.0, Mean Entropy: 0.00031433929689228535, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 947,  Mean reward: 0.0, Mean Entropy: 0.00033449759939685464, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 948,  Mean reward: -1.25, Mean Entropy: 0.00033129064831882715, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 949,  Mean reward: -1.25, Mean Entropy: 0.0003480674931779504, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 950,  Mean reward: -1.25, Mean Entropy: 0.00033407341106794775, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 951,  Mean reward: -1.0, Mean Entropy: 0.00035128596937283874, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 952,  Mean reward: -1.75, Mean Entropy: 0.00033686839742586017, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 953,  Mean reward: -3.0, Mean Entropy: 0.0003294192720204592, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 954,  Mean reward: -3.0, Mean Entropy: 0.00032364396611228585, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 955,  Mean reward: -1.0, Mean Entropy: 0.0003530439571477473, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 956,  Mean reward: -3.25, Mean Entropy: 0.0003400886198505759, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 957,  Mean reward: -3.25, Mean Entropy: 0.0003364912117831409, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 958,  Mean reward: -1.5, Mean Entropy: 0.000353573530446738, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 959,  Mean reward: -1.25, Mean Entropy: 0.00035909772850573063, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.73s
Iteration: 960,  Mean reward: -3.0, Mean Entropy: 0.00034584919922053814, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 961,  Mean reward: -2.25, Mean Entropy: 0.0003461108426563442, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 962,  Mean reward: -3.5, Mean Entropy: 0.0003438441490288824, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 963,  Mean reward: -0.75, Mean Entropy: 0.000363688770448789, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 964,  Mean reward: -0.75, Mean Entropy: 0.0003504658816382289, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 965,  Mean reward: -2.75, Mean Entropy: 0.0003575792070478201, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 966,  Mean reward: -2.25, Mean Entropy: 0.0003772517084144056, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 967,  Mean reward: -2.25, Mean Entropy: 0.0003969612007495016, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 968,  Mean reward: -1.5, Mean Entropy: 0.0003929056110791862, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 969,  Mean reward: -1.0, Mean Entropy: 0.000450136954896152, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 970,  Mean reward: -3.25, Mean Entropy: 0.0004185182915534824, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 971,  Mean reward: -2.25, Mean Entropy: 0.0004146769642829895, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 972,  Mean reward: -2.25, Mean Entropy: 0.0004102823731955141, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 973,  Mean reward: -2.25, Mean Entropy: 0.0004114509210921824, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 974,  Mean reward: -0.75, Mean Entropy: 0.00043289884342812, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 975,  Mean reward: -1.75, Mean Entropy: 0.0003916541754733771, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 976,  Mean reward: -2.75, Mean Entropy: 0.000404984166380018, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 977,  Mean reward: -3.75, Mean Entropy: 0.0003953393315896392, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 978,  Mean reward: -0.6265822784810127, Mean Entropy: 0.00028606000705622137, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 979,  Mean reward: -3.75, Mean Entropy: 0.00026131427148357034, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 980,  Mean reward: -1.75, Mean Entropy: 0.0003434797399677336, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 981,  Mean reward: -1.5, Mean Entropy: 0.0003737742663361132, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 982,  Mean reward: -2.0, Mean Entropy: 0.0004026817623525858, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 983,  Mean reward: -1.75, Mean Entropy: 0.0004575538623612374, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 984,  Mean reward: -3.0, Mean Entropy: 0.0004545017145574093, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 985,  Mean reward: -1.0, Mean Entropy: 0.0004925624234601855, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 986,  Mean reward: -1.75, Mean Entropy: 0.00047921459190547466, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 987,  Mean reward: -0.5, Mean Entropy: 0.0004973554750904441, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 988,  Mean reward: -4.25, Mean Entropy: 0.00046016633859835565, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 989,  Mean reward: -4.5, Mean Entropy: 0.00047545437701046467, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 990,  Mean reward: -2.25, Mean Entropy: 0.00047457878827117383, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 991,  Mean reward: -2.75, Mean Entropy: 0.0004816408036276698, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 992,  Mean reward: -1.0, Mean Entropy: 0.0004891431890428066, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 993,  Mean reward: -3.75, Mean Entropy: 0.0004617761296685785, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 994,  Mean reward: -3.25, Mean Entropy: 0.0004736564587801695, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 995,  Mean reward: -0.5, Mean Entropy: 0.00048341439105570316, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 996,  Mean reward: -4.0, Mean Entropy: 0.0004455484449863434, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 997,  Mean reward: -1.5, Mean Entropy: 0.0004697598924394697, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 998,  Mean reward: -1.0, Mean Entropy: 0.0004835196305066347, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 999,  Mean reward: -1.75, Mean Entropy: 0.0004937652847729623, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1000,  Mean reward: -2.75, Mean Entropy: 0.0005048533785156906, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: -3.5, Mean Entropy: 0.0005271260743029416, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1002,  Mean reward: -1.75, Mean Entropy: 0.0005132112419232726, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1003,  Mean reward: -3.25, Mean Entropy: 0.00048007522127591074, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1004,  Mean reward: -3.5, Mean Entropy: 0.0004841440240852535, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.71s
Iteration: 1005,  Mean reward: -2.5, Mean Entropy: 0.0004843473434448242, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1006,  Mean reward: -1.25, Mean Entropy: 0.0004706540494225919, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1007,  Mean reward: -2.25, Mean Entropy: 0.0004858802421949804, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1008,  Mean reward: -3.75, Mean Entropy: 0.00047678383998572826, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1009,  Mean reward: -3.25, Mean Entropy: 0.0004812304978258908, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1010,  Mean reward: -3.0, Mean Entropy: 0.00048609403893351555, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1011,  Mean reward: -1.75, Mean Entropy: 0.0004898422048427165, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1012,  Mean reward: -1.75, Mean Entropy: 0.0004894989542663097, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1013,  Mean reward: -4.25, Mean Entropy: 0.00048647518269717693, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1014,  Mean reward: -3.0, Mean Entropy: 0.0005087521858513355, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1015,  Mean reward: -2.5, Mean Entropy: 0.0005166399641893804, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1016,  Mean reward: -1.5, Mean Entropy: 0.0005345034296624362, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 1017,  Mean reward: -1.25, Mean Entropy: 0.0005384886171668768, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1018,  Mean reward: 0.25, Mean Entropy: 0.000528601580299437, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1019,  Mean reward: -2.25, Mean Entropy: 0.0004884544177912176, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1020,  Mean reward: -1.5, Mean Entropy: 0.0004709794884547591, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1021,  Mean reward: -0.25, Mean Entropy: 0.0004675275122281164, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1022,  Mean reward: -2.75, Mean Entropy: 0.0004703413578681648, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1023,  Mean reward: -3.5, Mean Entropy: 0.0004905156674794853, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1024,  Mean reward: -0.5, Mean Entropy: 0.0005473758792504668, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1025,  Mean reward: -3.5, Mean Entropy: 0.0005653435946442187, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1026,  Mean reward: -0.5, Mean Entropy: 0.0005895562935620546, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1027,  Mean reward: -3.75, Mean Entropy: 0.0005734977312386036, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1028,  Mean reward: -4.5, Mean Entropy: 0.0005568488850258291, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1029,  Mean reward: -3.0, Mean Entropy: 0.0005887986626476049, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1030,  Mean reward: 0.5, Mean Entropy: 0.0005974017549306154, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1031,  Mean reward: -1.0, Mean Entropy: 0.0005594769027084112, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1032,  Mean reward: -3.0, Mean Entropy: 0.0005489374743774533, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1033,  Mean reward: -2.5, Mean Entropy: 0.0005501648993231356, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1034,  Mean reward: -1.0, Mean Entropy: 0.0005867598229087889, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1035,  Mean reward: -2.5, Mean Entropy: 0.0005963958683423698, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1036,  Mean reward: -2.5, Mean Entropy: 0.0006137522868812084, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1037,  Mean reward: -2.25, Mean Entropy: 0.0006407368346117437, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1038,  Mean reward: -1.25, Mean Entropy: 0.0006855509127490222, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1039,  Mean reward: -3.25, Mean Entropy: 0.000678970362059772, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1040,  Mean reward: -3.25, Mean Entropy: 0.0006381290731951594, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1041,  Mean reward: -0.5, Mean Entropy: 0.0006573321297764778, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1042,  Mean reward: -2.0, Mean Entropy: 0.0006228329730220139, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1043,  Mean reward: -4.25, Mean Entropy: 0.0005730804987251759, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1044,  Mean reward: -3.0, Mean Entropy: 0.0006034675752744079, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1045,  Mean reward: -1.0, Mean Entropy: 0.0006369715556502342, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1046,  Mean reward: -1.25, Mean Entropy: 0.0006833525258116424, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1047,  Mean reward: -2.0, Mean Entropy: 0.0007193153724074364, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.89s
Iteration: 1048,  Mean reward: 0.25, Mean Entropy: 0.0007382313488051295, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1049,  Mean reward: -2.75, Mean Entropy: 0.000715136993676424, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1050,  Mean reward: 0.75, Mean Entropy: 0.0007280860445462167, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1051,  Mean reward: -3.5, Mean Entropy: 0.0007250503986142576, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1052,  Mean reward: -0.25, Mean Entropy: 0.0007535998593084514, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1053,  Mean reward: -1.5, Mean Entropy: 0.0007329969666898251, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1054,  Mean reward: -3.25, Mean Entropy: 0.0007154054474085569, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1055,  Mean reward: -1.75, Mean Entropy: 0.000713981338776648, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1056,  Mean reward: -1.75, Mean Entropy: 0.0007259504636749625, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1057,  Mean reward: -0.6265822784810127, Mean Entropy: 0.00024779472732916474, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1058,  Mean reward: -2.25, Mean Entropy: 0.000841770670376718, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1059,  Mean reward: -1.25, Mean Entropy: 0.00028396843117661774, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1060,  Mean reward: -2.5, Mean Entropy: 0.0002641843748278916, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1061,  Mean reward: -3.5, Mean Entropy: 0.0002793324529193342, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1062,  Mean reward: -3.25, Mean Entropy: 0.0003330510517116636, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1063,  Mean reward: -2.75, Mean Entropy: 0.0003339952672831714, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1064,  Mean reward: -3.0, Mean Entropy: 0.00035543186822906137, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1065,  Mean reward: -2.25, Mean Entropy: 0.00034826359478756785, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1066,  Mean reward: -2.5, Mean Entropy: 0.00036724269739352167, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1067,  Mean reward: -2.25, Mean Entropy: 0.00039180717431008816, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1068,  Mean reward: -1.75, Mean Entropy: 0.0003911695384886116, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1069,  Mean reward: -3.0, Mean Entropy: 0.0003880587173625827, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1070,  Mean reward: -1.5, Mean Entropy: 0.00039957891567610204, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1071,  Mean reward: -2.0, Mean Entropy: 0.00039103731978684664, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1072,  Mean reward: -1.75, Mean Entropy: 0.00039528400520794094, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1073,  Mean reward: -1.25, Mean Entropy: 0.00038571865297853947, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1074,  Mean reward: -1.75, Mean Entropy: 0.0003950766986235976, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1075,  Mean reward: -1.5, Mean Entropy: 0.0003968020319007337, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1076,  Mean reward: -2.25, Mean Entropy: 0.00038680643774569035, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1077,  Mean reward: -2.0, Mean Entropy: 0.00037889578379690647, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1078,  Mean reward: -4.0, Mean Entropy: 0.00038536437205038965, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1079,  Mean reward: -2.75, Mean Entropy: 0.00039773466414771974, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1080,  Mean reward: -1.75, Mean Entropy: 0.000402782519813627, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1081,  Mean reward: -2.25, Mean Entropy: 0.0003985533257946372, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1082,  Mean reward: -1.25, Mean Entropy: 0.0004096437187399715, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1083,  Mean reward: -1.25, Mean Entropy: 0.0003958420129492879, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1084,  Mean reward: -0.25, Mean Entropy: 0.00040800365968607366, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1085,  Mean reward: -2.5, Mean Entropy: 0.0003963300259783864, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1086,  Mean reward: -1.25, Mean Entropy: 0.00039244521758519113, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1087,  Mean reward: -3.75, Mean Entropy: 0.00038471445441246033, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1088,  Mean reward: -1.5, Mean Entropy: 0.0004026623209938407, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1089,  Mean reward: -2.25, Mean Entropy: 0.00040628513670526445, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1090,  Mean reward: -1.5, Mean Entropy: 0.00039685703814029694, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1091,  Mean reward: -2.25, Mean Entropy: 0.00038917490746825933, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.88s
Iteration: 1092,  Mean reward: -2.5, Mean Entropy: 0.0003946878423448652, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1093,  Mean reward: -1.0, Mean Entropy: 0.0004072020237799734, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1094,  Mean reward: -2.1455696202531644, Mean Entropy: 0.00013803623733110726, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1095,  Mean reward: -1.0, Mean Entropy: 0.00011032845213776454, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1096,  Mean reward: -1.5, Mean Entropy: 0.00010562170064076781, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1097,  Mean reward: 0.25, Mean Entropy: 0.0001309331419179216, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1098,  Mean reward: -3.25, Mean Entropy: 0.00013542397937271744, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1099,  Mean reward: -3.25, Mean Entropy: 0.00014221864694263786, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1100,  Mean reward: -2.5, Mean Entropy: 0.00013843446504324675, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: -2.75, Mean Entropy: 0.00014002808893565089, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1102,  Mean reward: -1.5, Mean Entropy: 0.0001446669193683192, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1103,  Mean reward: -0.25, Mean Entropy: 0.00014904557610861957, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1104,  Mean reward: -2.0, Mean Entropy: 0.00014448781439568847, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1105,  Mean reward: -0.25, Mean Entropy: 0.00015091357636265457, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1106,  Mean reward: -1.0, Mean Entropy: 0.00014775684394408017, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1107,  Mean reward: -0.25, Mean Entropy: 0.0001501278020441532, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1108,  Mean reward: -2.25, Mean Entropy: 0.00014604764874093235, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1109,  Mean reward: -2.0, Mean Entropy: 0.00014840715448372066, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1110,  Mean reward: -1.5, Mean Entropy: 0.00014964256843086332, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1111,  Mean reward: -1.5, Mean Entropy: 0.00014971043856348842, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1112,  Mean reward: -3.0, Mean Entropy: 0.0001477084297221154, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1113,  Mean reward: -2.75, Mean Entropy: 0.000148522958625108, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1114,  Mean reward: -0.5, Mean Entropy: 0.0001511828158982098, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1115,  Mean reward: -1.5, Mean Entropy: 0.0001522574748378247, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1116,  Mean reward: -2.75, Mean Entropy: 0.00015067124331835657, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1117,  Mean reward: -3.25, Mean Entropy: 0.00015046080807223916, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1118,  Mean reward: -0.5, Mean Entropy: 0.00015292811440303922, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1119,  Mean reward: -3.75, Mean Entropy: 0.00015105106285773218, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1120,  Mean reward: -1.0, Mean Entropy: 0.00015319693193305284, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1121,  Mean reward: -1.0, Mean Entropy: 0.00015412889479193836, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1122,  Mean reward: -1.25, Mean Entropy: 0.00015033081581350416, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1123,  Mean reward: -3.25, Mean Entropy: 0.0001471333671361208, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1124,  Mean reward: -4.5, Mean Entropy: 0.0001437094178982079, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1125,  Mean reward: -1.25, Mean Entropy: 0.00014370997087098658, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1126,  Mean reward: -2.0, Mean Entropy: 0.00014398153871297836, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1127,  Mean reward: 0.25, Mean Entropy: 0.00014661470777355134, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1128,  Mean reward: -1.75, Mean Entropy: 0.00014570097846444696, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1129,  Mean reward: -2.0, Mean Entropy: 0.0001443666114937514, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1130,  Mean reward: -1.0, Mean Entropy: 0.00014507897139992565, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1131,  Mean reward: -2.5, Mean Entropy: 0.0001415039732819423, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1132,  Mean reward: -1.25, Mean Entropy: 0.00013937918993178755, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1133,  Mean reward: -0.25, Mean Entropy: 0.00013866141671314836, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1134,  Mean reward: -1.5, Mean Entropy: 0.00013889004185330123, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1135,  Mean reward: -2.5, Mean Entropy: 0.00013840127212461084, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1136,  Mean reward: -2.75, Mean Entropy: 0.00013845246576238424, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1137,  Mean reward: -2.25, Mean Entropy: 0.00013611969188787043, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1138,  Mean reward: 1.5, Mean Entropy: 0.0001380034809699282, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1139,  Mean reward: -1.75, Mean Entropy: 0.00013207380834501237, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1140,  Mean reward: -1.25, Mean Entropy: 0.00013060818309895694, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1141,  Mean reward: -1.75, Mean Entropy: 0.0001293066015932709, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1142,  Mean reward: -1.0, Mean Entropy: 0.0001337279682047665, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1143,  Mean reward: -3.5, Mean Entropy: 0.00013380451127886772, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1144,  Mean reward: -2.5, Mean Entropy: 0.00013304721505846828, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1145,  Mean reward: -0.5, Mean Entropy: 0.00013529995339922607, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1146,  Mean reward: -4.0, Mean Entropy: 0.00013215412036515772, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1147,  Mean reward: -1.75, Mean Entropy: 0.00013336668780539185, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1148,  Mean reward: -2.5, Mean Entropy: 0.00012947960931342095, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1149,  Mean reward: -4.25, Mean Entropy: 0.00012814387446269393, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1150,  Mean reward: -1.25, Mean Entropy: 0.00012940958549734205, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1151,  Mean reward: -1.25, Mean Entropy: 0.000131989800138399, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1152,  Mean reward: -2.75, Mean Entropy: 0.00013091738219372928, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1153,  Mean reward: -1.75, Mean Entropy: 0.0001309252984356135, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1154,  Mean reward: -3.25, Mean Entropy: 0.00013025451335124671, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1155,  Mean reward: -2.5, Mean Entropy: 0.00013503953232429922, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1156,  Mean reward: -2.5, Mean Entropy: 0.00013663212303072214, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1157,  Mean reward: -2.0, Mean Entropy: 0.00013177251094020903, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1158,  Mean reward: -4.5, Mean Entropy: 0.00012376645463518798, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1159,  Mean reward: -1.75, Mean Entropy: 0.00011979413102380931, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1160,  Mean reward: -1.5, Mean Entropy: 0.00011859920050483197, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1161,  Mean reward: -2.75, Mean Entropy: 0.00011768487456720322, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.71s
Iteration: 1162,  Mean reward: -3.5, Mean Entropy: 0.00011807164264610037, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1163,  Mean reward: -3.25, Mean Entropy: 0.00011941955744987354, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1164,  Mean reward: -0.5, Mean Entropy: 0.00012315074854996055, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1165,  Mean reward: -2.25, Mean Entropy: 0.00012233536108396947, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1166,  Mean reward: -1.5, Mean Entropy: 0.0001244549930561334, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1167,  Mean reward: -2.25, Mean Entropy: 0.00012298120418563485, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1168,  Mean reward: -1.0, Mean Entropy: 0.00012679824430961162, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1169,  Mean reward: -1.25, Mean Entropy: 0.00012872443767264485, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1170,  Mean reward: -3.0, Mean Entropy: 0.00012999377213418484, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1171,  Mean reward: -3.0, Mean Entropy: 0.00013819258310832083, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1172,  Mean reward: -1.75, Mean Entropy: 0.0001424976362613961, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1173,  Mean reward: -1.75, Mean Entropy: 0.00014153812662698328, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1174,  Mean reward: -3.5, Mean Entropy: 0.00014390316209755838, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 1175,  Mean reward: -0.75, Mean Entropy: 0.00015098844596650451, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1176,  Mean reward: -0.75, Mean Entropy: 0.00015500813606195152, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1177,  Mean reward: -1.25, Mean Entropy: 0.00015973583504091948, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1178,  Mean reward: -1.25, Mean Entropy: 0.00016203282575588673, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1179,  Mean reward: -1.75, Mean Entropy: 0.00016291902284137905, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1180,  Mean reward: -1.0, Mean Entropy: 0.0001629747566767037, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1181,  Mean reward: -3.0, Mean Entropy: 0.0001599053357494995, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1182,  Mean reward: -2.75, Mean Entropy: 0.00016233851783908904, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1183,  Mean reward: -2.5, Mean Entropy: 0.000166115234605968, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1184,  Mean reward: -2.0, Mean Entropy: 0.00016690618940629065, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1185,  Mean reward: -3.75, Mean Entropy: 0.00016628261073492467, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1186,  Mean reward: -2.0, Mean Entropy: 0.00017058910452760756, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1187,  Mean reward: -2.5, Mean Entropy: 0.00016765632608439773, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1188,  Mean reward: -1.5, Mean Entropy: 0.00016973318997770548, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1189,  Mean reward: -1.0, Mean Entropy: 0.00016672624042257667, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1190,  Mean reward: -2.25, Mean Entropy: 0.00016821682220324874, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1191,  Mean reward: -0.25, Mean Entropy: 0.00016871960542630404, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1192,  Mean reward: -3.25, Mean Entropy: 0.0001642753486521542, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1193,  Mean reward: -3.75, Mean Entropy: 0.00016982952365651727, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1194,  Mean reward: -3.0, Mean Entropy: 0.00017040118109434843, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1195,  Mean reward: 0.0, Mean Entropy: 0.000176877569174394, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1196,  Mean reward: 0.25, Mean Entropy: 0.00017861554806586355, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1197,  Mean reward: -1.5, Mean Entropy: 0.00017554216901771724, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1198,  Mean reward: -1.0, Mean Entropy: 0.0001758839061949402, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1199,  Mean reward: -2.0, Mean Entropy: 0.00017469882732257247, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1200,  Mean reward: -1.5, Mean Entropy: 0.00017848335846792907, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: -1.5, Mean Entropy: 0.00017795973690226674, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1202,  Mean reward: -1.75, Mean Entropy: 0.00017477232904639095, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1203,  Mean reward: -1.75, Mean Entropy: 0.0001748446811689064, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1204,  Mean reward: -0.75, Mean Entropy: 0.00017515543731860816, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1205,  Mean reward: -0.5, Mean Entropy: 0.00017126265447586775, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1206,  Mean reward: -1.75, Mean Entropy: 0.00017490950995124876, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1207,  Mean reward: 0.75, Mean Entropy: 0.00017740041948854923, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1208,  Mean reward: -1.25, Mean Entropy: 0.00017638845019973814, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.71s
Iteration: 1209,  Mean reward: -0.5, Mean Entropy: 0.00017916239448823035, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1210,  Mean reward: -2.5, Mean Entropy: 0.00017870296142064035, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1211,  Mean reward: -0.25, Mean Entropy: 0.00018157681915909052, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1212,  Mean reward: -0.25, Mean Entropy: 0.00017983307770919055, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1213,  Mean reward: -2.25, Mean Entropy: 0.00017691524408292025, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1214,  Mean reward: -4.25, Mean Entropy: 0.00017520433175377548, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1215,  Mean reward: -1.0, Mean Entropy: 0.0001811346592148766, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1216,  Mean reward: -1.75, Mean Entropy: 0.0001905200188048184, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1217,  Mean reward: 0.0, Mean Entropy: 0.00019205643911845982, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1218,  Mean reward: -1.75, Mean Entropy: 0.0001861570926848799, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1219,  Mean reward: -2.25, Mean Entropy: 0.00018420148990117013, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1220,  Mean reward: -2.75, Mean Entropy: 0.00018708599964156747, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1221,  Mean reward: -1.75, Mean Entropy: 0.0001835123694036156, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1222,  Mean reward: -1.75, Mean Entropy: 0.0001829158718464896, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1223,  Mean reward: -2.0, Mean Entropy: 0.00018325010023545474, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1224,  Mean reward: -0.75, Mean Entropy: 0.00018408990581519902, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1225,  Mean reward: -2.25, Mean Entropy: 0.00018118994194082916, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1226,  Mean reward: -1.25, Mean Entropy: 0.0001823892816901207, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1227,  Mean reward: -2.0, Mean Entropy: 0.00018144019122701138, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1228,  Mean reward: -4.75, Mean Entropy: 0.00017860718071460724, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1229,  Mean reward: -2.25, Mean Entropy: 0.00018663918308448046, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1230,  Mean reward: -3.5, Mean Entropy: 0.00018954297411255538, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1231,  Mean reward: -2.0, Mean Entropy: 0.00019551014702301472, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1232,  Mean reward: -2.0, Mean Entropy: 0.00019218679517507553, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1233,  Mean reward: -1.25, Mean Entropy: 0.00019628994050435722, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1234,  Mean reward: -2.25, Mean Entropy: 0.00019968414562754333, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1235,  Mean reward: -3.5, Mean Entropy: 0.00019645893189590424, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1236,  Mean reward: -1.25, Mean Entropy: 0.0001979636144824326, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1237,  Mean reward: -3.0, Mean Entropy: 0.0001937255437951535, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1238,  Mean reward: -1.75, Mean Entropy: 0.00018997263396158814, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1239,  Mean reward: -1.5, Mean Entropy: 0.00018029901548288763, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1240,  Mean reward: -1.25, Mean Entropy: 0.00017391813162248582, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1241,  Mean reward: -2.25, Mean Entropy: 0.00017297026352025568, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1242,  Mean reward: -2.5, Mean Entropy: 0.00017885763372760266, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1243,  Mean reward: -1.75, Mean Entropy: 0.00019115593750029802, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1244,  Mean reward: -2.0, Mean Entropy: 0.00019537864136509597, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1245,  Mean reward: -1.75, Mean Entropy: 0.000198294292204082, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1246,  Mean reward: -2.25, Mean Entropy: 0.00018360320245847106, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1247,  Mean reward: -2.0, Mean Entropy: 0.00017839302017819136, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1248,  Mean reward: -2.5, Mean Entropy: 0.00016849795065354556, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1249,  Mean reward: -3.5, Mean Entropy: 0.00015490622899960726, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1250,  Mean reward: -1.25, Mean Entropy: 0.00015451502986252308, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1251,  Mean reward: -3.25, Mean Entropy: 0.000142798395245336, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.88s
Iteration: 1252,  Mean reward: -2.25, Mean Entropy: 0.0001382283226121217, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1253,  Mean reward: -1.25, Mean Entropy: 0.000138465067720972, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1254,  Mean reward: -2.0, Mean Entropy: 0.00013936453615315259, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1255,  Mean reward: -3.75, Mean Entropy: 0.0001346399076282978, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1256,  Mean reward: -3.0, Mean Entropy: 0.00013566907728090882, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1257,  Mean reward: -2.25, Mean Entropy: 0.00013748655328527093, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1258,  Mean reward: -2.25, Mean Entropy: 0.00013944998499937356, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1259,  Mean reward: 0.0, Mean Entropy: 0.0001425357477273792, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1260,  Mean reward: -0.75, Mean Entropy: 0.00014413479948416352, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1261,  Mean reward: -1.75, Mean Entropy: 0.0001401947665726766, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1262,  Mean reward: 0.75, Mean Entropy: 0.0001445049128960818, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1263,  Mean reward: -2.25, Mean Entropy: 0.00014210864901542664, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1264,  Mean reward: -3.75, Mean Entropy: 0.00014065155119169503, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1265,  Mean reward: -2.5, Mean Entropy: 0.0001437914907000959, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1266,  Mean reward: -3.5, Mean Entropy: 0.00014203391037881374, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1267,  Mean reward: -1.75, Mean Entropy: 0.00014456028293352574, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1268,  Mean reward: -4.5, Mean Entropy: 0.00014351820573210716, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1269,  Mean reward: -0.25, Mean Entropy: 0.0001583749835845083, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1270,  Mean reward: -3.25, Mean Entropy: 0.0001809757959563285, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1271,  Mean reward: 0.0, Mean Entropy: 0.0002081246202578768, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1272,  Mean reward: 0.25, Mean Entropy: 0.00020077734370715916, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1273,  Mean reward: -2.75, Mean Entropy: 0.00019130411965306848, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1274,  Mean reward: -4.25, Mean Entropy: 0.00017754570581018925, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1275,  Mean reward: -0.5, Mean Entropy: 0.00016617441724520177, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1276,  Mean reward: -3.5, Mean Entropy: 0.00016249532927758992, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1277,  Mean reward: -1.0, Mean Entropy: 0.0001633977226447314, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1278,  Mean reward: -0.75, Mean Entropy: 0.00016061877249740064, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1279,  Mean reward: 0.75, Mean Entropy: 0.00016305793542414904, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1280,  Mean reward: -3.25, Mean Entropy: 0.00014584844757337123, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1281,  Mean reward: -2.25, Mean Entropy: 0.0001451689749956131, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1282,  Mean reward: -4.25, Mean Entropy: 0.0001291143853450194, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1283,  Mean reward: -3.0, Mean Entropy: 0.00012967045768164098, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1284,  Mean reward: -2.75, Mean Entropy: 0.00013055131421424448, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1285,  Mean reward: -1.25, Mean Entropy: 0.00013932347064837813, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1286,  Mean reward: -3.0, Mean Entropy: 0.00013418187154456973, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1287,  Mean reward: -2.5, Mean Entropy: 0.00013482710346579552, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1288,  Mean reward: -2.75, Mean Entropy: 0.00012802486889995635, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1289,  Mean reward: -0.75, Mean Entropy: 0.00013472688442561775, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1290,  Mean reward: -1.25, Mean Entropy: 0.00013152147585060447, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1291,  Mean reward: -2.5, Mean Entropy: 0.0001368014345644042, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1292,  Mean reward: -1.0, Mean Entropy: 0.00013891614798922092, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1293,  Mean reward: -2.75, Mean Entropy: 0.00014815034228377044, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1294,  Mean reward: -1.75, Mean Entropy: 0.00016156182391569018, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1295,  Mean reward: -1.25, Mean Entropy: 0.0001728712668409571, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 1296,  Mean reward: -4.25, Mean Entropy: 0.00018166918016504496, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1297,  Mean reward: -1.5, Mean Entropy: 0.00019408359366934747, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1298,  Mean reward: -1.75, Mean Entropy: 0.0001861552009359002, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1299,  Mean reward: -3.25, Mean Entropy: 0.00017742736963555217, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1300,  Mean reward: -4.0, Mean Entropy: 0.00018010723579209298, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 1301,  Mean reward: -3.5, Mean Entropy: 0.00019206645083613694, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1302,  Mean reward: -1.75, Mean Entropy: 0.00020705093629658222, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1303,  Mean reward: -1.0, Mean Entropy: 0.00020959005632903427, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1304,  Mean reward: -2.5, Mean Entropy: 0.0001859067560872063, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1305,  Mean reward: -2.0, Mean Entropy: 0.0001775196724338457, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1306,  Mean reward: -3.25, Mean Entropy: 0.000175359527929686, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1307,  Mean reward: -2.25, Mean Entropy: 0.00017388470587320626, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1308,  Mean reward: -2.75, Mean Entropy: 0.00018227408872917295, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1309,  Mean reward: -1.75, Mean Entropy: 0.0001972606114577502, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1310,  Mean reward: -3.5, Mean Entropy: 0.000202800496481359, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1311,  Mean reward: -0.75, Mean Entropy: 0.00021603777713607997, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1312,  Mean reward: -0.75, Mean Entropy: 0.00022394761617761105, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1313,  Mean reward: -2.25, Mean Entropy: 0.00021793160703964531, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1314,  Mean reward: -2.75, Mean Entropy: 0.00021020090207457542, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1315,  Mean reward: -1.0, Mean Entropy: 0.00021519811707548797, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1316,  Mean reward: -2.0, Mean Entropy: 0.00021266061230562627, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1317,  Mean reward: -3.5, Mean Entropy: 0.00021854537772014737, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1318,  Mean reward: -3.5, Mean Entropy: 0.00022598086798097938, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1319,  Mean reward: -2.25, Mean Entropy: 0.0002188298385590315, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1320,  Mean reward: -1.5, Mean Entropy: 0.00021381959959398955, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1321,  Mean reward: -1.75, Mean Entropy: 0.00020966549345757812, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1322,  Mean reward: -2.0, Mean Entropy: 0.00020696762658189982, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1323,  Mean reward: -1.0, Mean Entropy: 0.00019356678240001202, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1324,  Mean reward: -2.75, Mean Entropy: 0.00017431416199542582, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1325,  Mean reward: -2.75, Mean Entropy: 0.00016869232058525085, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1326,  Mean reward: -1.0, Mean Entropy: 0.00016980344662442803, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1327,  Mean reward: 0.25, Mean Entropy: 0.00017624282918404788, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1328,  Mean reward: -1.75, Mean Entropy: 0.00017284229397773743, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1329,  Mean reward: -2.0, Mean Entropy: 0.00017031424795277417, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1330,  Mean reward: -1.5, Mean Entropy: 0.00016172844334505498, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1331,  Mean reward: -3.75, Mean Entropy: 0.0001547412248328328, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1332,  Mean reward: -2.25, Mean Entropy: 0.0001574168272782117, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1333,  Mean reward: -2.25, Mean Entropy: 0.00015493968385271728, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1334,  Mean reward: -2.25, Mean Entropy: 0.00015624139632564038, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1335,  Mean reward: -1.25, Mean Entropy: 0.00015927260392345488, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1336,  Mean reward: 0.0, Mean Entropy: 0.00015636008174624294, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1337,  Mean reward: -1.5, Mean Entropy: 0.00014185810869093984, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1338,  Mean reward: -1.75, Mean Entropy: 0.00014196241681929678, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1339,  Mean reward: -2.5, Mean Entropy: 0.00013896840391680598, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1340,  Mean reward: -3.0, Mean Entropy: 0.00014890333113726228, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1341,  Mean reward: -1.25, Mean Entropy: 0.00016614874766673893, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1342,  Mean reward: -2.75, Mean Entropy: 0.0001852904533734545, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1343,  Mean reward: -1.5, Mean Entropy: 0.00017716414004098624, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1344,  Mean reward: -2.0, Mean Entropy: 0.00015726283891126513, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1345,  Mean reward: -2.25, Mean Entropy: 0.00015231079305522144, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1346,  Mean reward: -3.25, Mean Entropy: 0.00014828033454250544, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1347,  Mean reward: -2.75, Mean Entropy: 0.00015287652786355466, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1348,  Mean reward: -3.0, Mean Entropy: 0.00015382148558273911, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1349,  Mean reward: -2.5, Mean Entropy: 0.0001571514003444463, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1350,  Mean reward: -2.0, Mean Entropy: 0.00014945630391594023, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1351,  Mean reward: -0.5, Mean Entropy: 0.0001525525003671646, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1352,  Mean reward: -3.0, Mean Entropy: 0.00014094950165599585, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1353,  Mean reward: -1.5, Mean Entropy: 0.00015865072782617062, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1354,  Mean reward: -3.0, Mean Entropy: 0.00014676769205834717, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1355,  Mean reward: -3.0, Mean Entropy: 0.00014880698290653527, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1356,  Mean reward: -1.0, Mean Entropy: 0.00015671020082663745, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1357,  Mean reward: -3.0, Mean Entropy: 0.0001487811969127506, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1358,  Mean reward: -2.25, Mean Entropy: 0.00015209491539280862, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1359,  Mean reward: -2.25, Mean Entropy: 0.00015333332703448832, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1360,  Mean reward: -2.5, Mean Entropy: 0.0001552931498736143, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1361,  Mean reward: -1.5, Mean Entropy: 0.0001538493379484862, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1362,  Mean reward: -3.5, Mean Entropy: 0.0001489708520239219, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1363,  Mean reward: -2.5, Mean Entropy: 0.00014511245535686612, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1364,  Mean reward: -2.5, Mean Entropy: 0.00015106667706277221, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1365,  Mean reward: -0.5, Mean Entropy: 0.00015202430950012058, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1366,  Mean reward: -2.75, Mean Entropy: 0.00014913355698809028, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1367,  Mean reward: -3.0, Mean Entropy: 0.00043812530930154026, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1368,  Mean reward: -1.0, Mean Entropy: 0.0018407662864774466, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.73s
Iteration: 1369,  Mean reward: -2.25, Mean Entropy: 0.005569417029619217, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1370,  Mean reward: -2.0, Mean Entropy: 0.1990170180797577, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1371,  Mean reward: -3.0135135135135136, Mean Entropy: 0.032749734818935394, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1372,  Mean reward: -1.75, Mean Entropy: 0.002602321095764637, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1373,  Mean reward: -1.0, Mean Entropy: 0.0012947650393471122, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1374,  Mean reward: -1.75, Mean Entropy: 0.0023123559076339006, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1375,  Mean reward: -2.5, Mean Entropy: 0.0014513435307890177, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1376,  Mean reward: -2.5, Mean Entropy: 0.001005669473670423, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1377,  Mean reward: -2.75, Mean Entropy: 0.0011120870476588607, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1378,  Mean reward: -1.25, Mean Entropy: 0.0009923388715833426, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1379,  Mean reward: -2.75, Mean Entropy: 0.0007572592003270984, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1380,  Mean reward: -3.25, Mean Entropy: 0.0008088956819847226, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1381,  Mean reward: -1.75, Mean Entropy: 0.0009636919712647796, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1382,  Mean reward: -1.75, Mean Entropy: 0.0012248815037310123, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1383,  Mean reward: -2.75, Mean Entropy: 0.0019402119796723127, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1384,  Mean reward: -2.25, Mean Entropy: 0.014682198874652386, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1385,  Mean reward: -3.5, Mean Entropy: 0.17852966487407684, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1386,  Mean reward: -0.22535211267605634, Mean Entropy: 0.02543237805366516, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 1387,  Mean reward: -1.25, Mean Entropy: 0.009649823419749737, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1388,  Mean reward: 0.25, Mean Entropy: 0.027330882847309113, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1389,  Mean reward: -1.5, Mean Entropy: 0.0029940977692604065, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1390,  Mean reward: -2.0, Mean Entropy: 0.004172211047261953, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1391,  Mean reward: -0.5, Mean Entropy: 0.0022162869572639465, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1392,  Mean reward: -3.5, Mean Entropy: 0.0012179699260741472, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1393,  Mean reward: -1.75, Mean Entropy: 0.0011069681495428085, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1394,  Mean reward: -3.25, Mean Entropy: 0.0009159037144854665, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1395,  Mean reward: -1.25, Mean Entropy: 0.0009361325646750629, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1396,  Mean reward: -2.9050632911392404, Mean Entropy: 0.000226410455070436, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1397,  Mean reward: -1.25, Mean Entropy: 0.00016603518452029675, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1398,  Mean reward: -1.25, Mean Entropy: 0.00017376747564412653, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1399,  Mean reward: -2.5, Mean Entropy: 0.00017557688988745213, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1400,  Mean reward: -3.5, Mean Entropy: 0.00018068122153636068, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 1401,  Mean reward: -2.75, Mean Entropy: 0.00017738858878146857, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1402,  Mean reward: -0.25, Mean Entropy: 0.00017370819114148617, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 1403,  Mean reward: -2.0, Mean Entropy: 0.0001769119844539091, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1404,  Mean reward: -2.5, Mean Entropy: 0.0001771289244061336, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1405,  Mean reward: -0.75, Mean Entropy: 0.00017558748368173838, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1406,  Mean reward: -1.75, Mean Entropy: 0.0001771600218489766, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1407,  Mean reward: -0.5, Mean Entropy: 0.00017735250003170222, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1408,  Mean reward: -0.5, Mean Entropy: 0.0001722705055726692, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1409,  Mean reward: -2.0, Mean Entropy: 0.00017848331481218338, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1410,  Mean reward: -2.0, Mean Entropy: 0.00017627263150643557, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1411,  Mean reward: -0.25, Mean Entropy: 0.0001658550463616848, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1412,  Mean reward: -0.5, Mean Entropy: 0.00016172815230675042, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.19it/s]100%|| 1/1 [00:00<00:00,  1.18it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.2 0.8 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.1 0.2 0.2 0.5 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.2 0.8 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 5 action_probs [0.0 0.0 0.1 0.2 0.2 0.5 0.0 0.0] r -11.0 s_ (5, 5)
  Done after 2 steps, Captured: True Reward: -12.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[0 1]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[8.0 -12.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type EMB
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.86it/s]100%|| 1/1 [00:00<00:00,  1.86it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type EMB
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.86it/s]100%|| 1/1 [00:00<00:00,  1.86it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type EMB
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: -2.0
  std over seeds: 0.0
  per seed: [-2.000 -2.000 -2.000]

success_rate.......
  avg over seeds: 0.5
  std over seeds: 0.0
  per seed: [0.500 0.500 0.500]

batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: FE
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: q
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
node_dim: 7
lstm_on: True
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy_FE_LSTM(
  (FE): FeatureExtractor_LSTM(
    (lstm): LSTM(7, 24)
    (gat): GATv2(24, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (theta6_v): Linear(in_features=24, out_features=24, bias=True)
    (theta7_v): Linear(in_features=24, out_features=24, bias=True)
    (theta5_v): Linear(in_features=48, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with LSTM (on node features) + GATv2 feature extraction
------------------------------------------
FE.lstm.weight_ih_l0     [96, 7]      requires_grad=True
FE.lstm.weight_hh_l0     [96, 24]     requires_grad=True
FE.lstm.bias_ih_l0       [96]         requires_grad=True
FE.lstm.bias_hh_l0       [96]         requires_grad=True
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.theta6_v.weight        [24, 24]     requires_grad=True
V.theta6_v.bias          [24]         requires_grad=True
V.theta7_v.weight        [24, 24]     requires_grad=True
V.theta7_v.bias          [24]         requires_grad=True
V.theta5_v.weight        [1, 48]      requires_grad=True
V.theta5_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 11906
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -6.136363636363637, Mean Entropy: 0.9602975845336914, complete_episode_count: 44.0, Gather time: 5.35s, Train time: 3.87s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -3.5875, Mean Entropy: 0.9891787767410278, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.88s
Iteration: 2,  Mean reward: -4.023255813953488, Mean Entropy: 1.0036194324493408, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.90s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -2.3555555555555556, Mean Entropy: 0.8880947828292847, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.96s
Iteration: 4,  Mean reward: -2.909090909090909, Mean Entropy: 0.9458569288253784, complete_episode_count: 44.0, Gather time: 0.61s, Train time: 1.90s
Iteration: 5,  Mean reward: -3.6707317073170733, Mean Entropy: 0.9241955280303955, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.95s
Iteration: 6,  Mean reward: -5.785714285714286, Mean Entropy: 0.938636302947998, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 7,  Mean reward: -5.328947368421052, Mean Entropy: 0.8664328455924988, complete_episode_count: 38.0, Gather time: 0.55s, Train time: 1.94s
Iteration: 8,  Mean reward: -6.744186046511628, Mean Entropy: 0.9097525477409363, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.96s
Iteration: 9,  Mean reward: -3.6547619047619047, Mean Entropy: 0.938629150390625, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.95s
Iteration: 10,  Mean reward: -5.151162790697675, Mean Entropy: 0.9025024175643921, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.95s
Iteration: 11,  Mean reward: -6.193181818181818, Mean Entropy: 0.9313451051712036, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 12,  Mean reward: -4.329545454545454, Mean Entropy: 0.9602274894714355, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 13,  Mean reward: -4.392857142857143, Mean Entropy: 0.9674489498138428, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 14,  Mean reward: -4.052631578947368, Mean Entropy: 1.0107930898666382, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 15,  Mean reward: -3.9047619047619047, Mean Entropy: 0.9747081995010376, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 16,  Mean reward: -3.6341463414634148, Mean Entropy: 0.9891669154167175, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 17,  Mean reward: -4.73404255319149, Mean Entropy: 0.9313945770263672, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 18,  Mean reward: -5.675675675675675, Mean Entropy: 0.9096670150756836, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 19,  Mean reward: -5.097560975609756, Mean Entropy: 0.9096970558166504, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 20,  Mean reward: -3.227272727272727, Mean Entropy: 0.9025055766105652, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 21,  Mean reward: -3.25, Mean Entropy: 0.9169744253158569, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 22,  Mean reward: -3.872093023255814, Mean Entropy: 0.9530606269836426, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 23,  Mean reward: -4.75, Mean Entropy: 0.9890815019607544, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 24,  Mean reward: -2.6630434782608696, Mean Entropy: 0.9457964897155762, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 2.12s
Iteration: 25,  Mean reward: -3.5714285714285716, Mean Entropy: 0.9386029243469238, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 26,  Mean reward: -3.0625, Mean Entropy: 0.9241669178009033, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 27,  Mean reward: -3.231707317073171, Mean Entropy: 0.9674836993217468, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 28,  Mean reward: -6.0, Mean Entropy: 0.9819062948226929, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 29,  Mean reward: -4.558139534883721, Mean Entropy: 0.924118161201477, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 30,  Mean reward: -4.058139534883721, Mean Entropy: 0.9891025424003601, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.93s
Iteration: 31,  Mean reward: -4.813953488372093, Mean Entropy: 1.0395350456237793, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 32,  Mean reward: -3.0853658536585367, Mean Entropy: 0.9168149828910828, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 33,  Mean reward: -3.0681818181818183, Mean Entropy: 0.9601098895072937, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 34,  Mean reward: -4.761363636363637, Mean Entropy: 0.9816905856132507, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 35,  Mean reward: -2.9146341463414633, Mean Entropy: 0.9528059959411621, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 36,  Mean reward: -3.409090909090909, Mean Entropy: 0.9167572855949402, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 37,  Mean reward: -3.1463414634146343, Mean Entropy: 0.8948401212692261, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 38,  Mean reward: -2.9651162790697674, Mean Entropy: 0.9882310628890991, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 39,  Mean reward: -5.833333333333333, Mean Entropy: 0.9587144255638123, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.87s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 40,  Mean reward: -2.0813953488372094, Mean Entropy: 0.9226311445236206, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 41,  Mean reward: -5.1976744186046515, Mean Entropy: 0.9365876913070679, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 42,  Mean reward: -6.333333333333333, Mean Entropy: 0.9495219588279724, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 43,  Mean reward: -2.453488372093023, Mean Entropy: 0.8714476823806763, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 44,  Mean reward: -4.05, Mean Entropy: 0.9569648504257202, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 45,  Mean reward: -3.9615384615384617, Mean Entropy: 0.9485787153244019, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 46,  Mean reward: -4.524390243902439, Mean Entropy: 0.9623382091522217, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 47,  Mean reward: -2.7125, Mean Entropy: 0.9408721923828125, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 48,  Mean reward: -5.337209302325581, Mean Entropy: 0.9408328533172607, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 49,  Mean reward: -4.7727272727272725, Mean Entropy: 0.9475147128105164, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 50,  Mean reward: -5.604651162790698, Mean Entropy: 0.918174147605896, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 51,  Mean reward: -6.163043478260869, Mean Entropy: 0.885687530040741, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 52,  Mean reward: -3.510869565217391, Mean Entropy: 0.991280734539032, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 53,  Mean reward: -4.2073170731707314, Mean Entropy: 0.9797266721725464, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 54,  Mean reward: -3.9318181818181817, Mean Entropy: 0.9232408404350281, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 55,  Mean reward: -3.6627906976744184, Mean Entropy: 0.9275860786437988, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.93s
Iteration: 56,  Mean reward: -6.342105263157895, Mean Entropy: 0.8900132179260254, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.92s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 57,  Mean reward: -1.8604651162790697, Mean Entropy: 0.9131265878677368, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.90s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 58,  Mean reward: -1.7291666666666667, Mean Entropy: 0.9952632784843445, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 59,  Mean reward: -3.0652173913043477, Mean Entropy: 0.9159021377563477, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 2.14s
Iteration: 60,  Mean reward: -7.476744186046512, Mean Entropy: 0.9140843749046326, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 61,  Mean reward: -6.8, Mean Entropy: 0.9107288122177124, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 62,  Mean reward: -4.196078431372549, Mean Entropy: 0.8412169218063354, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 63,  Mean reward: -4.6477272727272725, Mean Entropy: 0.9195996522903442, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.92s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 64,  Mean reward: -1.6224489795918366, Mean Entropy: 0.8985292315483093, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 65,  Mean reward: -2.046511627906977, Mean Entropy: 0.9361385703086853, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 66,  Mean reward: -4.777777777777778, Mean Entropy: 0.8946396708488464, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 67,  Mean reward: -4.214285714285714, Mean Entropy: 0.8350853323936462, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 68,  Mean reward: -3.34375, Mean Entropy: 0.8890346884727478, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 69,  Mean reward: -3.8125, Mean Entropy: 0.8457803726196289, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 70,  Mean reward: -2.033333333333333, Mean Entropy: 0.904178261756897, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 71,  Mean reward: -3.5784313725490198, Mean Entropy: 0.8191112875938416, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 72,  Mean reward: -3.7, Mean Entropy: 0.7607192993164062, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 73,  Mean reward: -4.211538461538462, Mean Entropy: 0.7208724021911621, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 74,  Mean reward: -2.8135593220338984, Mean Entropy: 0.819920003414154, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 75,  Mean reward: -4.427083333333333, Mean Entropy: 0.7966400980949402, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 76,  Mean reward: -2.5096153846153846, Mean Entropy: 0.5836589336395264, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 77,  Mean reward: -3.3524590163934427, Mean Entropy: 0.5332436561584473, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 78,  Mean reward: -1.9385964912280702, Mean Entropy: 0.5239759087562561, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 79,  Mean reward: -3.0, Mean Entropy: 0.49095940589904785, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 80,  Mean reward: -3.046153846153846, Mean Entropy: 0.4462905526161194, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 81,  Mean reward: -2.1746031746031744, Mean Entropy: 0.5398894548416138, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 82,  Mean reward: -2.1048387096774195, Mean Entropy: 0.7981860637664795, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 83,  Mean reward: -3.353448275862069, Mean Entropy: 0.8059309720993042, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 84,  Mean reward: -2.8488372093023258, Mean Entropy: 0.9455289244651794, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 85,  Mean reward: -4.722222222222222, Mean Entropy: 0.8572913408279419, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.88s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 86,  Mean reward: 1.8839285714285714, Mean Entropy: 0.9946605563163757, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.91s
Iteration: 87,  Mean reward: -5.5978260869565215, Mean Entropy: 0.927889347076416, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 88,  Mean reward: -0.031914893617021274, Mean Entropy: 1.0021837949752808, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 89,  Mean reward: -6.756410256410256, Mean Entropy: 1.0222786664962769, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 90,  Mean reward: -2.488888888888889, Mean Entropy: 0.914733350276947, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 91,  Mean reward: -2.902173913043478, Mean Entropy: 0.9051872491836548, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 92,  Mean reward: -2.130434782608696, Mean Entropy: 0.6400139331817627, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 93,  Mean reward: -0.06862745098039216, Mean Entropy: 0.8587946891784668, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.87s
Iteration: 94,  Mean reward: -2.09375, Mean Entropy: 0.6956062316894531, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 2.13s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 95,  Mean reward: 2.1037735849056602, Mean Entropy: 0.7536996603012085, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 96,  Mean reward: 0.967391304347826, Mean Entropy: 0.7196432948112488, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.94s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 97,  Mean reward: 3.9537037037037037, Mean Entropy: 0.8518027663230896, complete_episode_count: 54.0, Gather time: 0.60s, Train time: 1.93s
Iteration: 98,  Mean reward: 1.9705882352941178, Mean Entropy: 0.6620712876319885, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.93s
Iteration: 99,  Mean reward: 3.1517857142857144, Mean Entropy: 0.7334122657775879, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.94s
Iteration: 100,  Mean reward: 2.6203703703703702, Mean Entropy: 0.7029141187667847, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.93s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 2.9245283018867925, Mean Entropy: 0.6564571261405945, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 102,  Mean reward: 1.2115384615384615, Mean Entropy: 0.7543025016784668, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 103,  Mean reward: 1.0588235294117647, Mean Entropy: 0.7021721601486206, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 104,  Mean reward: 0.41509433962264153, Mean Entropy: 0.6892433166503906, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 105,  Mean reward: 1.9528301886792452, Mean Entropy: 0.659963846206665, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 106,  Mean reward: 2.25, Mean Entropy: 0.6518828272819519, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 107,  Mean reward: 1.8981481481481481, Mean Entropy: 0.6785868406295776, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 108,  Mean reward: 0.7549019607843137, Mean Entropy: 0.7911064028739929, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 109,  Mean reward: 2.6293103448275863, Mean Entropy: 0.6845272779464722, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 110,  Mean reward: 2.0677966101694913, Mean Entropy: 0.6353408098220825, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 111,  Mean reward: 0.9210526315789473, Mean Entropy: 0.5541955232620239, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 112,  Mean reward: -0.8365384615384616, Mean Entropy: 0.7824532389640808, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 113,  Mean reward: 1.1415094339622642, Mean Entropy: 0.6982682943344116, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 114,  Mean reward: 0.01020408163265306, Mean Entropy: 0.8403772115707397, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 115,  Mean reward: 1.4375, Mean Entropy: 0.5744602680206299, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 116,  Mean reward: 2.7416666666666667, Mean Entropy: 0.6308223605155945, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.90s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 117,  Mean reward: 4.533333333333333, Mean Entropy: 0.6744112372398376, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 118,  Mean reward: 3.5, Mean Entropy: 0.6799501776695251, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 119,  Mean reward: 0.9051724137931034, Mean Entropy: 0.7305051684379578, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 120,  Mean reward: 0.42592592592592593, Mean Entropy: 0.6650216579437256, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.92s
Iteration: 121,  Mean reward: -0.43, Mean Entropy: 0.7873190641403198, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 122,  Mean reward: 1.3679245283018868, Mean Entropy: 0.7157851457595825, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 123,  Mean reward: 2.7410714285714284, Mean Entropy: 0.7218289375305176, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 124,  Mean reward: 2.1320754716981134, Mean Entropy: 0.6107473373413086, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 125,  Mean reward: 2.3839285714285716, Mean Entropy: 0.5063793063163757, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 126,  Mean reward: 1.4509803921568627, Mean Entropy: 0.8792204856872559, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 127,  Mean reward: 1.9636363636363636, Mean Entropy: 0.618943452835083, complete_episode_count: 55.0, Gather time: 0.57s, Train time: 2.14s
Iteration: 128,  Mean reward: 3.0344827586206895, Mean Entropy: 0.6828822493553162, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.92s
Iteration: 129,  Mean reward: 2.7037037037037037, Mean Entropy: 0.6259689927101135, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.94s
Iteration: 130,  Mean reward: 1.6545454545454545, Mean Entropy: 0.7305840253829956, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.95s
Iteration: 131,  Mean reward: 3.6454545454545455, Mean Entropy: 0.6486864686012268, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 132,  Mean reward: 3.5508474576271185, Mean Entropy: 0.6168875694274902, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 133,  Mean reward: 1.1153846153846154, Mean Entropy: 0.685302734375, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 134,  Mean reward: 1.2980769230769231, Mean Entropy: 0.7241301536560059, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 135,  Mean reward: 2.9537037037037037, Mean Entropy: 0.6332306265830994, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.93s
Iteration: 136,  Mean reward: 3.526315789473684, Mean Entropy: 0.5715629458427429, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.90s
Iteration: 137,  Mean reward: 0.2777777777777778, Mean Entropy: 0.6090116500854492, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.94s
Iteration: 138,  Mean reward: 2.294642857142857, Mean Entropy: 0.7045325636863708, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.94s
Iteration: 139,  Mean reward: 2.482142857142857, Mean Entropy: 0.6018074750900269, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.93s
Iteration: 140,  Mean reward: 0.5775862068965517, Mean Entropy: 0.6025393009185791, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.91s
Iteration: 141,  Mean reward: 2.49, Mean Entropy: 0.8135857582092285, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.93s
Iteration: 142,  Mean reward: 1.7019230769230769, Mean Entropy: 0.7978054881095886, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 143,  Mean reward: 1.6636363636363636, Mean Entropy: 0.7052994966506958, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 144,  Mean reward: 0.9423076923076923, Mean Entropy: 0.6462621688842773, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 145,  Mean reward: 2.2033898305084745, Mean Entropy: 0.6182513236999512, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 146,  Mean reward: 2.1545454545454548, Mean Entropy: 0.7667995691299438, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 147,  Mean reward: 2.5, Mean Entropy: 0.705233097076416, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 148,  Mean reward: 0.3392857142857143, Mean Entropy: 0.7315467596054077, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 149,  Mean reward: 1.25, Mean Entropy: 0.6818265914916992, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 150,  Mean reward: 1.3113207547169812, Mean Entropy: 0.6817303895950317, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 151,  Mean reward: 0.10185185185185185, Mean Entropy: 0.6691562533378601, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 152,  Mean reward: 2.459016393442623, Mean Entropy: 0.6148492693901062, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 153,  Mean reward: 1.8508771929824561, Mean Entropy: 0.7939554452896118, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 154,  Mean reward: 0.91, Mean Entropy: 0.761192262172699, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.93s
Iteration: 155,  Mean reward: 1.6442307692307692, Mean Entropy: 0.7160469889640808, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 156,  Mean reward: 1.45, Mean Entropy: 0.613539457321167, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 157,  Mean reward: 2.23, Mean Entropy: 0.6925886869430542, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 158,  Mean reward: -0.08035714285714286, Mean Entropy: 0.7134873270988464, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 159,  Mean reward: 0.30357142857142855, Mean Entropy: 0.7210019826889038, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 2.12s
Iteration: 160,  Mean reward: 2.8363636363636364, Mean Entropy: 0.622821569442749, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 161,  Mean reward: 1.462962962962963, Mean Entropy: 0.6126514077186584, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 162,  Mean reward: 3.7338709677419355, Mean Entropy: 0.5526190996170044, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 163,  Mean reward: 4.058333333333334, Mean Entropy: 0.6820497512817383, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 164,  Mean reward: 0.8877551020408163, Mean Entropy: 0.6896674036979675, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 165,  Mean reward: 0.6415094339622641, Mean Entropy: 0.7728990316390991, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 166,  Mean reward: 0.7264150943396226, Mean Entropy: 0.6497253179550171, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 167,  Mean reward: 2.4074074074074074, Mean Entropy: 0.7119255065917969, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 168,  Mean reward: 2.190909090909091, Mean Entropy: 0.7601665258407593, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 169,  Mean reward: -1.4803921568627452, Mean Entropy: 0.8065426349639893, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 170,  Mean reward: -1.663265306122449, Mean Entropy: 0.767910897731781, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 171,  Mean reward: 3.30188679245283, Mean Entropy: 0.6568667888641357, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 172,  Mean reward: 1.7589285714285714, Mean Entropy: 0.623675525188446, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 173,  Mean reward: 2.590909090909091, Mean Entropy: 0.771216094493866, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 174,  Mean reward: 1.6140350877192982, Mean Entropy: 0.6745787858963013, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 175,  Mean reward: 0.6372549019607843, Mean Entropy: 0.7001356482505798, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 176,  Mean reward: 3.111111111111111, Mean Entropy: 0.6150566935539246, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 177,  Mean reward: 1.6929824561403508, Mean Entropy: 0.729034423828125, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 178,  Mean reward: 2.3461538461538463, Mean Entropy: 0.6809201240539551, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 179,  Mean reward: 2.6018518518518516, Mean Entropy: 0.6906017065048218, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 180,  Mean reward: 1.2647058823529411, Mean Entropy: 0.6744381785392761, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 181,  Mean reward: 3.316666666666667, Mean Entropy: 0.48641306161880493, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 182,  Mean reward: 1.471698113207547, Mean Entropy: 0.711074948310852, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 183,  Mean reward: 0.23214285714285715, Mean Entropy: 0.7314825057983398, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 184,  Mean reward: -0.6382978723404256, Mean Entropy: 0.8244101405143738, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 185,  Mean reward: 2.4224137931034484, Mean Entropy: 0.6425953507423401, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 186,  Mean reward: 1.8508771929824561, Mean Entropy: 0.665476381778717, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 187,  Mean reward: 3.1296296296296298, Mean Entropy: 0.760795533657074, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 188,  Mean reward: 2.288135593220339, Mean Entropy: 0.6110211610794067, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 189,  Mean reward: 2.941666666666667, Mean Entropy: 0.7407600283622742, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 190,  Mean reward: 1.2169811320754718, Mean Entropy: 0.7542499303817749, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 191,  Mean reward: -0.7, Mean Entropy: 0.7023123502731323, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 2.13s
Iteration: 192,  Mean reward: 2.4237288135593222, Mean Entropy: 0.5815058946609497, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 193,  Mean reward: 2.864406779661017, Mean Entropy: 0.6333643794059753, complete_episode_count: 59.0, Gather time: 0.58s, Train time: 1.91s
Iteration: 194,  Mean reward: 1.4433962264150944, Mean Entropy: 0.7605170011520386, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.90s
Iteration: 195,  Mean reward: 2.75, Mean Entropy: 0.5598674416542053, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.87s
Iteration: 196,  Mean reward: 3.641509433962264, Mean Entropy: 0.5903564691543579, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 197,  Mean reward: -0.02830188679245283, Mean Entropy: 0.7320913076400757, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.87s
Iteration: 198,  Mean reward: 2.1020408163265305, Mean Entropy: 0.7269634008407593, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 199,  Mean reward: 1.8135593220338984, Mean Entropy: 0.6426910161972046, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 200,  Mean reward: 1.787037037037037, Mean Entropy: 0.7557291984558105, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 3.0833333333333335, Mean Entropy: 0.6661515235900879, complete_episode_count: 54.0, Gather time: 0.70s, Train time: 1.91s
Iteration: 202,  Mean reward: 1.9655172413793103, Mean Entropy: 0.567660927772522, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 203,  Mean reward: 1.736842105263158, Mean Entropy: 0.7758886218070984, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 204,  Mean reward: 3.0258620689655173, Mean Entropy: 0.6999868154525757, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 205,  Mean reward: 1.8653846153846154, Mean Entropy: 0.7138587832450867, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 206,  Mean reward: 1.8333333333333333, Mean Entropy: 0.6991850137710571, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 207,  Mean reward: 0.5333333333333333, Mean Entropy: 0.6116425395011902, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 208,  Mean reward: 0.8508771929824561, Mean Entropy: 0.6303076148033142, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.94s
Iteration: 209,  Mean reward: 1.7272727272727273, Mean Entropy: 0.7318669557571411, complete_episode_count: 55.0, Gather time: 0.57s, Train time: 1.90s
Iteration: 210,  Mean reward: 1.4607843137254901, Mean Entropy: 0.712740421295166, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.86s
Iteration: 211,  Mean reward: 0.47959183673469385, Mean Entropy: 0.8689121603965759, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 212,  Mean reward: 2.3727272727272726, Mean Entropy: 0.6710283756256104, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 213,  Mean reward: 1.4803921568627452, Mean Entropy: 0.6450210809707642, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 214,  Mean reward: 1.3363636363636364, Mean Entropy: 0.6644672155380249, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 215,  Mean reward: -0.3888888888888889, Mean Entropy: 0.7754786014556885, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 216,  Mean reward: 3.381818181818182, Mean Entropy: 0.6536773443222046, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 217,  Mean reward: 0.9274193548387096, Mean Entropy: 0.6116873025894165, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 218,  Mean reward: 1.3333333333333333, Mean Entropy: 0.703655481338501, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 219,  Mean reward: 0.9285714285714286, Mean Entropy: 0.752984881401062, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 220,  Mean reward: 0.2916666666666667, Mean Entropy: 0.7396690249443054, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 221,  Mean reward: 3.456140350877193, Mean Entropy: 0.622637152671814, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 222,  Mean reward: -0.12727272727272726, Mean Entropy: 0.6705271601676941, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 223,  Mean reward: 1.9607843137254901, Mean Entropy: 0.7769019603729248, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 224,  Mean reward: 2.425925925925926, Mean Entropy: 0.6569027304649353, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 2.11s
Iteration: 225,  Mean reward: 2.353448275862069, Mean Entropy: 0.5410598516464233, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 226,  Mean reward: 1.7454545454545454, Mean Entropy: 0.776975154876709, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 227,  Mean reward: 2.906779661016949, Mean Entropy: 0.6403862237930298, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 228,  Mean reward: 3.23728813559322, Mean Entropy: 0.7538062930107117, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 229,  Mean reward: 0.08490566037735849, Mean Entropy: 0.6897866129875183, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 230,  Mean reward: 0.40625, Mean Entropy: 0.7671246528625488, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.94s
Iteration: 231,  Mean reward: 3.4285714285714284, Mean Entropy: 0.7253999710083008, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 232,  Mean reward: 1.6851851851851851, Mean Entropy: 0.6673969030380249, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 233,  Mean reward: 2.219298245614035, Mean Entropy: 0.5459742546081543, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 234,  Mean reward: 0.6041666666666666, Mean Entropy: 0.7596022486686707, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 235,  Mean reward: 2.2410714285714284, Mean Entropy: 0.7532384395599365, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 236,  Mean reward: 2.6792452830188678, Mean Entropy: 0.6883050799369812, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 237,  Mean reward: 1.712962962962963, Mean Entropy: 0.680915355682373, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.91s
Iteration: 238,  Mean reward: 2.4272727272727272, Mean Entropy: 0.6733784675598145, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 239,  Mean reward: 0.7545454545454545, Mean Entropy: 0.644286572933197, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 240,  Mean reward: 2.843137254901961, Mean Entropy: 0.7377711534500122, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 241,  Mean reward: 1.7211538461538463, Mean Entropy: 0.7240843772888184, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.89s
Iteration: 242,  Mean reward: 1.2767857142857142, Mean Entropy: 0.7098021507263184, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 243,  Mean reward: 0.2826086956521739, Mean Entropy: 0.881727397441864, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 244,  Mean reward: 3.2, Mean Entropy: 0.5082657337188721, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 245,  Mean reward: 3.109090909090909, Mean Entropy: 0.6818756461143494, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 246,  Mean reward: 0.7053571428571429, Mean Entropy: 0.7240473628044128, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 247,  Mean reward: 2.0576923076923075, Mean Entropy: 0.6588054895401001, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 248,  Mean reward: 0.8888888888888888, Mean Entropy: 0.7666721343994141, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 249,  Mean reward: 1.3796296296296295, Mean Entropy: 0.6445607542991638, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 250,  Mean reward: 2.6379310344827585, Mean Entropy: 0.6592490673065186, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 251,  Mean reward: 3.210526315789474, Mean Entropy: 0.6823973655700684, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 252,  Mean reward: 0.32075471698113206, Mean Entropy: 0.7459696531295776, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 253,  Mean reward: 0.4166666666666667, Mean Entropy: 0.6520647406578064, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.92s
Iteration: 254,  Mean reward: 0.45614035087719296, Mean Entropy: 0.5656414031982422, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 255,  Mean reward: -0.851063829787234, Mean Entropy: 0.7520487308502197, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 256,  Mean reward: 1.0462962962962963, Mean Entropy: 0.7818788290023804, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 2.12s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 257,  Mean reward: 5.05, Mean Entropy: 0.5707178115844727, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.89s
Iteration: 258,  Mean reward: -5.468253968253968, Mean Entropy: 0.7126700282096863, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 259,  Mean reward: 2.0892857142857144, Mean Entropy: 0.6531595587730408, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 260,  Mean reward: 1.4423076923076923, Mean Entropy: 0.7953347563743591, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 261,  Mean reward: 2.2672413793103448, Mean Entropy: 0.6937474012374878, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 262,  Mean reward: 0.5217391304347826, Mean Entropy: 0.6941165328025818, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 263,  Mean reward: 0.9074074074074074, Mean Entropy: 0.6724488139152527, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 264,  Mean reward: 1.3214285714285714, Mean Entropy: 0.64280766248703, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 265,  Mean reward: 2.3333333333333335, Mean Entropy: 0.794487476348877, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 266,  Mean reward: 2.5727272727272728, Mean Entropy: 0.6001890897750854, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 267,  Mean reward: 1.1153846153846154, Mean Entropy: 0.6863133311271667, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 268,  Mean reward: 2.892857142857143, Mean Entropy: 0.6716215014457703, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 269,  Mean reward: 2.1037735849056602, Mean Entropy: 0.5199599862098694, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 270,  Mean reward: 3.793103448275862, Mean Entropy: 0.7367198467254639, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 271,  Mean reward: 2.618181818181818, Mean Entropy: 0.7371479868888855, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 272,  Mean reward: 2.77, Mean Entropy: 0.6803493499755859, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.93s
Iteration: 273,  Mean reward: 1.1, Mean Entropy: 0.6730363368988037, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.96s
Iteration: 274,  Mean reward: 1.8333333333333333, Mean Entropy: 0.7008622884750366, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 275,  Mean reward: 1.875, Mean Entropy: 0.6485741138458252, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 276,  Mean reward: 0.8333333333333334, Mean Entropy: 0.8309916257858276, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 277,  Mean reward: 2.62, Mean Entropy: 0.7225582003593445, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 278,  Mean reward: -3.2450980392156863, Mean Entropy: 0.6799585223197937, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 279,  Mean reward: 3.6320754716981134, Mean Entropy: 0.6240559816360474, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 280,  Mean reward: 1.537037037037037, Mean Entropy: 0.6657858490943909, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 281,  Mean reward: 2.3545454545454545, Mean Entropy: 0.6892344355583191, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 282,  Mean reward: 3.138888888888889, Mean Entropy: 0.6909944415092468, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 283,  Mean reward: -1.4649122807017543, Mean Entropy: 0.6958810091018677, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 284,  Mean reward: 1.9727272727272727, Mean Entropy: 0.7389025688171387, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 285,  Mean reward: 2.11, Mean Entropy: 0.7328615188598633, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 286,  Mean reward: 2.330188679245283, Mean Entropy: 0.6386053562164307, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 287,  Mean reward: 0.8775510204081632, Mean Entropy: 0.7033235430717468, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 288,  Mean reward: 3.5384615384615383, Mean Entropy: 0.7341266870498657, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 289,  Mean reward: -1.6, Mean Entropy: 0.8358706831932068, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 2.13s
Iteration: 290,  Mean reward: -0.8723404255319149, Mean Entropy: 0.7710460424423218, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 291,  Mean reward: -3.141025641025641, Mean Entropy: 0.8680934906005859, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 292,  Mean reward: -2.0851063829787235, Mean Entropy: 0.8264540433883667, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 293,  Mean reward: -1.2209302325581395, Mean Entropy: 0.6944179534912109, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 294,  Mean reward: 2.481818181818182, Mean Entropy: 0.6262384653091431, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 295,  Mean reward: -1.3823529411764706, Mean Entropy: 0.7573180198669434, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 296,  Mean reward: 2.107843137254902, Mean Entropy: 0.6957993507385254, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 297,  Mean reward: 4.032258064516129, Mean Entropy: 0.45357558131217957, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 298,  Mean reward: 1.6862745098039216, Mean Entropy: 0.7041715383529663, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 299,  Mean reward: 3.6842105263157894, Mean Entropy: 0.5566123127937317, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 300,  Mean reward: 3.952830188679245, Mean Entropy: 0.6792891025543213, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 1.4270833333333333, Mean Entropy: 0.7048096656799316, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 302,  Mean reward: 1.8942307692307692, Mean Entropy: 0.700131893157959, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 303,  Mean reward: 4.857142857142857, Mean Entropy: 0.608829140663147, complete_episode_count: 56.0, Gather time: 0.58s, Train time: 1.89s
Iteration: 304,  Mean reward: 4.043859649122807, Mean Entropy: 0.7224205732345581, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.94s
Iteration: 305,  Mean reward: 3.4473684210526314, Mean Entropy: 0.5719760656356812, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 306,  Mean reward: 1.8363636363636364, Mean Entropy: 0.5892430543899536, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.88s
Iteration: 307,  Mean reward: 2.6704545454545454, Mean Entropy: 0.6837024092674255, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 308,  Mean reward: 3.6792452830188678, Mean Entropy: 0.6571792960166931, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 309,  Mean reward: 2.4423076923076925, Mean Entropy: 0.6341775059700012, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 310,  Mean reward: 3.1979166666666665, Mean Entropy: 0.7391201853752136, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 311,  Mean reward: 4.632075471698113, Mean Entropy: 0.5687777996063232, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 312,  Mean reward: -2.6293103448275863, Mean Entropy: 0.6369551420211792, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 313,  Mean reward: 3.2818181818181817, Mean Entropy: 0.6346660852432251, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 314,  Mean reward: 3.48, Mean Entropy: 0.6763772368431091, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 315,  Mean reward: 4.352941176470588, Mean Entropy: 0.6510223150253296, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 316,  Mean reward: 3.518867924528302, Mean Entropy: 0.5998703241348267, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 317,  Mean reward: 1.6057692307692308, Mean Entropy: 0.7814853191375732, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 318,  Mean reward: 0.8363636363636363, Mean Entropy: 0.733156681060791, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 319,  Mean reward: 1.4636363636363636, Mean Entropy: 0.6603673696517944, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 320,  Mean reward: 0.9537037037037037, Mean Entropy: 0.7300680875778198, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 321,  Mean reward: 1.9375, Mean Entropy: 0.6433055400848389, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 322,  Mean reward: 3.1339285714285716, Mean Entropy: 0.6231841444969177, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 2.12s
Iteration: 323,  Mean reward: 2.3771929824561404, Mean Entropy: 0.6449872851371765, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 324,  Mean reward: 1.3333333333333333, Mean Entropy: 0.7003555297851562, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 325,  Mean reward: 2.8035714285714284, Mean Entropy: 0.5495243072509766, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 326,  Mean reward: 0.0392156862745098, Mean Entropy: 0.7296109199523926, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 327,  Mean reward: 2.9423076923076925, Mean Entropy: 0.6684295535087585, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 328,  Mean reward: 1.392156862745098, Mean Entropy: 0.5675939321517944, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 329,  Mean reward: 2.3962264150943398, Mean Entropy: 0.6688627004623413, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 330,  Mean reward: 3.704081632653061, Mean Entropy: 0.5068522691726685, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 331,  Mean reward: -4.327868852459017, Mean Entropy: 0.5192765593528748, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 332,  Mean reward: 3.462962962962963, Mean Entropy: 0.5528478026390076, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 333,  Mean reward: 4.807017543859649, Mean Entropy: 0.561694324016571, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 334,  Mean reward: 3.480769230769231, Mean Entropy: 0.6486048698425293, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.89s
Iteration: 335,  Mean reward: 4.9, Mean Entropy: 0.43655332922935486, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.90s
Iteration: 336,  Mean reward: -1.5, Mean Entropy: 0.491111695766449, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 337,  Mean reward: 3.6372549019607843, Mean Entropy: 0.6571240425109863, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.94s
Iteration: 338,  Mean reward: 4.036363636363636, Mean Entropy: 0.597119927406311, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 339,  Mean reward: 1.5520833333333333, Mean Entropy: 0.6672486662864685, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 340,  Mean reward: 3.627450980392157, Mean Entropy: 0.6460269689559937, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.91s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 341,  Mean reward: 5.859649122807017, Mean Entropy: 0.5074102878570557, complete_episode_count: 57.0, Gather time: 0.69s, Train time: 1.89s
Iteration: 342,  Mean reward: -4.340277777777778, Mean Entropy: 0.4112873077392578, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 343,  Mean reward: -5.911764705882353, Mean Entropy: 0.7901784777641296, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 344,  Mean reward: 3.0686274509803924, Mean Entropy: 0.719660758972168, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 345,  Mean reward: 2.9411764705882355, Mean Entropy: 0.6736316680908203, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 346,  Mean reward: 4.2894736842105265, Mean Entropy: 0.6402283310890198, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.94s
Iteration: 347,  Mean reward: 3.010204081632653, Mean Entropy: 0.6205996870994568, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 348,  Mean reward: 2.4642857142857144, Mean Entropy: 0.4451793134212494, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 349,  Mean reward: 1.8571428571428572, Mean Entropy: 0.6758410930633545, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 350,  Mean reward: 3.1176470588235294, Mean Entropy: 0.523595929145813, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 351,  Mean reward: 4.446428571428571, Mean Entropy: 0.46202796697616577, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 352,  Mean reward: 3.549019607843137, Mean Entropy: 0.6318986415863037, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 353,  Mean reward: 3.2083333333333335, Mean Entropy: 0.571860134601593, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 354,  Mean reward: -2.9285714285714284, Mean Entropy: 0.701854944229126, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 355,  Mean reward: 3.7982456140350878, Mean Entropy: 0.6751320362091064, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 2.09s
Iteration: 356,  Mean reward: 3.6203703703703702, Mean Entropy: 0.6478142142295837, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 357,  Mean reward: 5.0754716981132075, Mean Entropy: 0.40868622064590454, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 358,  Mean reward: -0.47619047619047616, Mean Entropy: 0.4745761752128601, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 359,  Mean reward: 3.9411764705882355, Mean Entropy: 0.5152058601379395, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 360,  Mean reward: -1.9464285714285714, Mean Entropy: 0.734683632850647, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 361,  Mean reward: 2.7941176470588234, Mean Entropy: 0.6166476011276245, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 362,  Mean reward: 4.028301886792453, Mean Entropy: 0.5247578024864197, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 363,  Mean reward: -1.5648148148148149, Mean Entropy: 0.5851951837539673, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 364,  Mean reward: 4.718181818181818, Mean Entropy: 0.6074111461639404, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 365,  Mean reward: 5.553571428571429, Mean Entropy: 0.5579022765159607, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 366,  Mean reward: -3.9482758620689653, Mean Entropy: 0.5480741858482361, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 367,  Mean reward: 5.287037037037037, Mean Entropy: 0.5782536864280701, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 368,  Mean reward: 5.220338983050848, Mean Entropy: 0.5084958076477051, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 369,  Mean reward: -7.391666666666667, Mean Entropy: 0.8010268211364746, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 370,  Mean reward: 0.4888888888888889, Mean Entropy: 0.6062184572219849, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 371,  Mean reward: 3.86734693877551, Mean Entropy: 0.6065373420715332, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 372,  Mean reward: 0.5784313725490197, Mean Entropy: 0.6293538808822632, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 373,  Mean reward: 4.36734693877551, Mean Entropy: 0.6500258445739746, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 374,  Mean reward: 4.44, Mean Entropy: 0.38359570503234863, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 2.04s
Iteration: 375,  Mean reward: -5.172131147540983, Mean Entropy: 0.4725041091442108, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 376,  Mean reward: 3.71, Mean Entropy: 0.6697829961776733, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 377,  Mean reward: 1.2169811320754718, Mean Entropy: 0.6149053573608398, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 378,  Mean reward: 2.4814814814814814, Mean Entropy: 0.6045984029769897, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 379,  Mean reward: 4.116071428571429, Mean Entropy: 0.5970259308815002, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 380,  Mean reward: 3.82, Mean Entropy: 0.6366487741470337, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 381,  Mean reward: -1.5833333333333333, Mean Entropy: 0.5576139688491821, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 382,  Mean reward: 3.150943396226415, Mean Entropy: 0.5475435256958008, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 383,  Mean reward: 4.519607843137255, Mean Entropy: 0.634566068649292, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 384,  Mean reward: 1.3863636363636365, Mean Entropy: 0.6469422578811646, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 385,  Mean reward: 2.75, Mean Entropy: 0.6422466039657593, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 386,  Mean reward: 5.657407407407407, Mean Entropy: 0.5803122520446777, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 2.03s
Iteration: 387,  Mean reward: -5.114583333333333, Mean Entropy: 0.581851601600647, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 388,  Mean reward: 4.4818181818181815, Mean Entropy: 0.5235036611557007, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 2.10s
Iteration: 389,  Mean reward: 5.0, Mean Entropy: 0.5027300715446472, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 390,  Mean reward: -3.217741935483871, Mean Entropy: 0.5733790397644043, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 391,  Mean reward: -6.3090909090909095, Mean Entropy: 0.5761122703552246, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 392,  Mean reward: 3.8981481481481484, Mean Entropy: 0.5313313007354736, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 393,  Mean reward: 2.8854166666666665, Mean Entropy: 0.6107149124145508, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 394,  Mean reward: 4.5576923076923075, Mean Entropy: 0.6176089644432068, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 395,  Mean reward: 4.7407407407407405, Mean Entropy: 0.6621100306510925, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 396,  Mean reward: 4.425925925925926, Mean Entropy: 0.5528962016105652, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 397,  Mean reward: 3.849056603773585, Mean Entropy: 0.5666552782058716, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 398,  Mean reward: 3.9, Mean Entropy: 0.6446413397789001, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 399,  Mean reward: -1.2583333333333333, Mean Entropy: 0.5549752116203308, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 400,  Mean reward: 1.5943396226415094, Mean Entropy: 0.5614649653434753, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.90s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 2.59375, Mean Entropy: 0.6096614599227905, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 402,  Mean reward: 4.394230769230769, Mean Entropy: 0.47240564227104187, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 403,  Mean reward: 4.239583333333333, Mean Entropy: 0.6183969974517822, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 404,  Mean reward: 2.0090909090909093, Mean Entropy: 0.6480060815811157, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 405,  Mean reward: 5.283333333333333, Mean Entropy: 0.4343344569206238, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 406,  Mean reward: 4.830357142857143, Mean Entropy: 0.5214335918426514, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 407,  Mean reward: 2.75, Mean Entropy: 0.5434626340866089, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 408,  Mean reward: 3.4215686274509802, Mean Entropy: 0.5913108587265015, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.95s
Iteration: 409,  Mean reward: 4.910714285714286, Mean Entropy: 0.5306086540222168, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.94s
Iteration: 410,  Mean reward: 3.489795918367347, Mean Entropy: 0.6318527460098267, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 411,  Mean reward: 5.398305084745763, Mean Entropy: 0.4548289179801941, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.94s
Iteration: 412,  Mean reward: 3.0408163265306123, Mean Entropy: 0.5770196318626404, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 413,  Mean reward: 5.30188679245283, Mean Entropy: 0.5111759901046753, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 414,  Mean reward: 3.4245283018867925, Mean Entropy: 0.5566272139549255, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 415,  Mean reward: 3.160377358490566, Mean Entropy: 0.5282002687454224, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 416,  Mean reward: 4.148148148148148, Mean Entropy: 0.5400571227073669, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 417,  Mean reward: 3.5729166666666665, Mean Entropy: 0.5855401158332825, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 418,  Mean reward: 4.201923076923077, Mean Entropy: 0.4547322690486908, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 419,  Mean reward: 3.4791666666666665, Mean Entropy: 0.589621901512146, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 420,  Mean reward: 4.5673076923076925, Mean Entropy: 0.5567196011543274, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 421,  Mean reward: 5.160714285714286, Mean Entropy: 0.45242971181869507, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 2.12s
Iteration: 422,  Mean reward: 2.2, Mean Entropy: 0.6246203780174255, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.91s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 423,  Mean reward: 5.901785714285714, Mean Entropy: 0.46906429529190063, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.90s
Iteration: 424,  Mean reward: -2.95, Mean Entropy: 0.6646419763565063, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 425,  Mean reward: -2.686046511627907, Mean Entropy: 0.9994254112243652, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 426,  Mean reward: -4.560975609756097, Mean Entropy: 0.9879298210144043, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 427,  Mean reward: -6.163043478260869, Mean Entropy: 0.9735293388366699, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 428,  Mean reward: -6.488372093023256, Mean Entropy: 0.909132182598114, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 429,  Mean reward: -2.802325581395349, Mean Entropy: 0.8640463352203369, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 430,  Mean reward: -4.391891891891892, Mean Entropy: 0.9057502746582031, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 431,  Mean reward: -6.2105263157894735, Mean Entropy: 0.8954653739929199, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 432,  Mean reward: -6.093023255813954, Mean Entropy: 0.9096341133117676, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 433,  Mean reward: -3.8125, Mean Entropy: 0.9634599685668945, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 434,  Mean reward: -5.963414634146342, Mean Entropy: 0.9280595779418945, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 435,  Mean reward: -4.681818181818182, Mean Entropy: 0.9427331686019897, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 436,  Mean reward: -6.551282051282051, Mean Entropy: 0.9992339611053467, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.93s
Iteration: 437,  Mean reward: -5.3108108108108105, Mean Entropy: 0.8990626335144043, complete_episode_count: 37.0, Gather time: 0.56s, Train time: 1.93s
Iteration: 438,  Mean reward: -6.059523809523809, Mean Entropy: 0.9781050682067871, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 439,  Mean reward: -5.414634146341464, Mean Entropy: 0.9852317571640015, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 440,  Mean reward: -3.75, Mean Entropy: 0.9989919066429138, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 441,  Mean reward: -6.538461538461538, Mean Entropy: 0.9689563512802124, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 442,  Mean reward: -5.3522727272727275, Mean Entropy: 0.9932458400726318, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 443,  Mean reward: -5.829545454545454, Mean Entropy: 0.9585522413253784, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 444,  Mean reward: -2.9047619047619047, Mean Entropy: 0.9578146934509277, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 445,  Mean reward: -4.010869565217392, Mean Entropy: 0.9504165053367615, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 446,  Mean reward: -3.975, Mean Entropy: 0.9214924573898315, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 447,  Mean reward: -4.677777777777778, Mean Entropy: 1.02237069606781, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 448,  Mean reward: -5.3875, Mean Entropy: 0.9403037428855896, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 449,  Mean reward: -6.886363636363637, Mean Entropy: 0.9201540946960449, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 450,  Mean reward: -3.2, Mean Entropy: 0.926921010017395, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 451,  Mean reward: -3.088888888888889, Mean Entropy: 0.972488284111023, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 452,  Mean reward: -5.475609756097561, Mean Entropy: 0.8567163348197937, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 453,  Mean reward: -6.115384615384615, Mean Entropy: 0.9986085891723633, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 454,  Mean reward: -3.1931818181818183, Mean Entropy: 0.9430424571037292, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 455,  Mean reward: -3.65, Mean Entropy: 0.911407470703125, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 456,  Mean reward: -3.3369565217391304, Mean Entropy: 0.9185574054718018, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 2.15s
Iteration: 457,  Mean reward: -5.7073170731707314, Mean Entropy: 0.8746871948242188, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 458,  Mean reward: -4.184210526315789, Mean Entropy: 0.9450744390487671, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 459,  Mean reward: -5.365853658536586, Mean Entropy: 0.8914437294006348, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 460,  Mean reward: -1.7209302325581395, Mean Entropy: 0.9356383681297302, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 2.04s
Iteration: 461,  Mean reward: -5.646341463414634, Mean Entropy: 0.9773704409599304, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 462,  Mean reward: -4.769230769230769, Mean Entropy: 0.946455180644989, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 463,  Mean reward: -5.440476190476191, Mean Entropy: 0.9574045538902283, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 464,  Mean reward: -3.108695652173913, Mean Entropy: 0.9289736151695251, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 465,  Mean reward: -4.25, Mean Entropy: 0.9494460821151733, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 466,  Mean reward: -3.6444444444444444, Mean Entropy: 0.9548282623291016, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 467,  Mean reward: -2.1341463414634148, Mean Entropy: 0.9591643810272217, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 468,  Mean reward: -4.617021276595745, Mean Entropy: 0.9788455367088318, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 469,  Mean reward: -5.82051282051282, Mean Entropy: 0.9742986559867859, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 470,  Mean reward: -5.043478260869565, Mean Entropy: 0.9845701456069946, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 471,  Mean reward: -5.402439024390244, Mean Entropy: 1.0107526779174805, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 472,  Mean reward: -6.065217391304348, Mean Entropy: 0.9248502254486084, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 473,  Mean reward: -4.791666666666667, Mean Entropy: 0.9870026111602783, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 474,  Mean reward: -3.15, Mean Entropy: 0.9305540919303894, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 475,  Mean reward: -1.4444444444444444, Mean Entropy: 0.9519332647323608, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 476,  Mean reward: -2.597826086956522, Mean Entropy: 0.8520164489746094, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 477,  Mean reward: -2.4782608695652173, Mean Entropy: 0.9327327013015747, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 478,  Mean reward: -5.226190476190476, Mean Entropy: 0.8389584422111511, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 479,  Mean reward: -2.511111111111111, Mean Entropy: 0.8755554556846619, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 480,  Mean reward: -3.0510204081632653, Mean Entropy: 0.951683521270752, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 481,  Mean reward: -2.6666666666666665, Mean Entropy: 0.823370635509491, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 482,  Mean reward: -1.815217391304348, Mean Entropy: 0.8834463953971863, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 483,  Mean reward: -4.24, Mean Entropy: 0.8312012553215027, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 484,  Mean reward: -3.358695652173913, Mean Entropy: 0.8848470449447632, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 485,  Mean reward: 0.14772727272727273, Mean Entropy: 0.8045913577079773, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 486,  Mean reward: -2.1704545454545454, Mean Entropy: 0.8498508334159851, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 487,  Mean reward: 0.30952380952380953, Mean Entropy: 0.8265376091003418, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 488,  Mean reward: -2.0568181818181817, Mean Entropy: 0.7694589495658875, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 489,  Mean reward: 1.56, Mean Entropy: 0.816295325756073, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 490,  Mean reward: -5.365853658536586, Mean Entropy: 0.6789252161979675, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 2.10s
Iteration: 491,  Mean reward: 1.9361702127659575, Mean Entropy: 0.8404896259307861, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 492,  Mean reward: 0.8666666666666667, Mean Entropy: 0.7910170555114746, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 493,  Mean reward: 0.08695652173913043, Mean Entropy: 0.7966709136962891, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 494,  Mean reward: 2.1770833333333335, Mean Entropy: 0.7258565425872803, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 495,  Mean reward: 1.7244897959183674, Mean Entropy: 0.7005316615104675, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 496,  Mean reward: 0.6041666666666666, Mean Entropy: 0.8797301054000854, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 497,  Mean reward: -1.8863636363636365, Mean Entropy: 0.5085400342941284, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.99s
Iteration: 498,  Mean reward: 4.138888888888889, Mean Entropy: 0.46946820616722107, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 499,  Mean reward: 2.826530612244898, Mean Entropy: 0.7127355337142944, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 500,  Mean reward: 4.336363636363636, Mean Entropy: 0.6450040936470032, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 3.4716981132075473, Mean Entropy: 0.6617130041122437, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 502,  Mean reward: 4.137931034482759, Mean Entropy: 0.5604066848754883, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 503,  Mean reward: 2.656862745098039, Mean Entropy: 0.6768522262573242, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 504,  Mean reward: 2.473684210526316, Mean Entropy: 0.7371562719345093, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 505,  Mean reward: 0.7596153846153846, Mean Entropy: 0.752600908279419, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 506,  Mean reward: 2.6792452830188678, Mean Entropy: 0.6904643774032593, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 507,  Mean reward: 0.6964285714285714, Mean Entropy: 0.6923765540122986, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 508,  Mean reward: 1.8703703703703705, Mean Entropy: 0.6676288843154907, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 509,  Mean reward: 1.6153846153846154, Mean Entropy: 0.7581393122673035, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 2.03s
Iteration: 510,  Mean reward: 3.0084745762711864, Mean Entropy: 0.674896240234375, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 511,  Mean reward: 1.2843137254901962, Mean Entropy: 0.5072156190872192, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 512,  Mean reward: 4.0, Mean Entropy: 0.5105882883071899, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 513,  Mean reward: 5.1440677966101696, Mean Entropy: 0.48519420623779297, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 514,  Mean reward: 4.0508474576271185, Mean Entropy: 0.5823068618774414, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 515,  Mean reward: 3.32, Mean Entropy: 0.6363122463226318, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 516,  Mean reward: 4.543103448275862, Mean Entropy: 0.5788258910179138, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 517,  Mean reward: 1.3, Mean Entropy: 0.6425631642341614, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 518,  Mean reward: 3.327272727272727, Mean Entropy: 0.589920699596405, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 519,  Mean reward: 2.7058823529411766, Mean Entropy: 0.6128122806549072, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 520,  Mean reward: 2.9468085106382977, Mean Entropy: 0.626527726650238, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 521,  Mean reward: 2.9591836734693877, Mean Entropy: 0.6523551344871521, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 522,  Mean reward: 0.8061224489795918, Mean Entropy: 0.7091237902641296, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 523,  Mean reward: 0.8207547169811321, Mean Entropy: 0.5744845271110535, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 2.09s
Iteration: 524,  Mean reward: 3.6, Mean Entropy: 0.674292266368866, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 525,  Mean reward: 2.638888888888889, Mean Entropy: 0.7220454216003418, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 526,  Mean reward: 2.8703703703703702, Mean Entropy: 0.5999124050140381, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 527,  Mean reward: 3.357142857142857, Mean Entropy: 0.4307057857513428, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 528,  Mean reward: 3.451923076923077, Mean Entropy: 0.5014467239379883, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 529,  Mean reward: 1.98, Mean Entropy: 0.7785441875457764, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.94s
Iteration: 530,  Mean reward: 1.3333333333333333, Mean Entropy: 0.7118041515350342, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 531,  Mean reward: 1.6759259259259258, Mean Entropy: 0.700219988822937, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 532,  Mean reward: 3.7, Mean Entropy: 0.5779051184654236, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 533,  Mean reward: 3.9722222222222223, Mean Entropy: 0.5125818252563477, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 534,  Mean reward: 2.36734693877551, Mean Entropy: 0.6532254815101624, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 535,  Mean reward: 2.7244897959183674, Mean Entropy: 0.5743606090545654, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 536,  Mean reward: 2.0, Mean Entropy: 0.6100592613220215, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 537,  Mean reward: 4.701612903225806, Mean Entropy: 0.556864857673645, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 538,  Mean reward: 5.214285714285714, Mean Entropy: 0.5529183745384216, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 539,  Mean reward: -0.1171875, Mean Entropy: 0.5387899875640869, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 540,  Mean reward: 4.214285714285714, Mean Entropy: 0.4578193426132202, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 541,  Mean reward: 2.0, Mean Entropy: 0.7514948844909668, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 542,  Mean reward: 3.8981481481481484, Mean Entropy: 0.6890579462051392, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 543,  Mean reward: -1.8425925925925926, Mean Entropy: 0.6434621810913086, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 544,  Mean reward: 2.402173913043478, Mean Entropy: 0.782336413860321, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 545,  Mean reward: 2.560344827586207, Mean Entropy: 0.5864882469177246, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 546,  Mean reward: 2.336206896551724, Mean Entropy: 0.5660479068756104, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 547,  Mean reward: 2.235294117647059, Mean Entropy: 0.657956600189209, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 548,  Mean reward: 3.556603773584906, Mean Entropy: 0.6580722332000732, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 549,  Mean reward: 2.390909090909091, Mean Entropy: 0.6943873763084412, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 550,  Mean reward: 3.080357142857143, Mean Entropy: 0.6886295080184937, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 551,  Mean reward: 2.6293103448275863, Mean Entropy: 0.6127113699913025, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 552,  Mean reward: 2.6944444444444446, Mean Entropy: 0.5389654636383057, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 553,  Mean reward: 2.1862745098039214, Mean Entropy: 0.569882869720459, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 554,  Mean reward: 3.9615384615384617, Mean Entropy: 0.6089752912521362, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 555,  Mean reward: 4.983050847457627, Mean Entropy: 0.4723967909812927, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 556,  Mean reward: 3.240740740740741, Mean Entropy: 0.605888307094574, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 2.11s
Iteration: 557,  Mean reward: 4.6875, Mean Entropy: 0.6112259030342102, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 558,  Mean reward: 4.226415094339623, Mean Entropy: 0.5425405502319336, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 559,  Mean reward: 3.7830188679245285, Mean Entropy: 0.5269742012023926, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 560,  Mean reward: 4.081818181818182, Mean Entropy: 0.582621693611145, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 561,  Mean reward: 3.8137254901960786, Mean Entropy: 0.5893463492393494, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 562,  Mean reward: 1.096774193548387, Mean Entropy: 0.489998996257782, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 563,  Mean reward: 4.1, Mean Entropy: 0.5823265910148621, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 564,  Mean reward: 3.951923076923077, Mean Entropy: 0.69176185131073, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 565,  Mean reward: 3.04, Mean Entropy: 0.6136806011199951, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 566,  Mean reward: 4.181818181818182, Mean Entropy: 0.5896407961845398, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 567,  Mean reward: 3.423076923076923, Mean Entropy: 0.4434826970100403, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 568,  Mean reward: 2.9545454545454546, Mean Entropy: 0.5802819728851318, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 569,  Mean reward: 3.29, Mean Entropy: 0.6631557941436768, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 570,  Mean reward: 1.2777777777777777, Mean Entropy: 0.6474262475967407, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 571,  Mean reward: 2.8421052631578947, Mean Entropy: 0.7343029379844666, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 572,  Mean reward: 3.330508474576271, Mean Entropy: 0.5633869171142578, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 573,  Mean reward: 2.861111111111111, Mean Entropy: 0.7095350027084351, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 574,  Mean reward: 2.8823529411764706, Mean Entropy: 0.5639166235923767, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 575,  Mean reward: 4.196078431372549, Mean Entropy: 0.5999480485916138, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 576,  Mean reward: -0.864406779661017, Mean Entropy: 0.6150133609771729, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 577,  Mean reward: 3.1770833333333335, Mean Entropy: 0.6505417823791504, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 578,  Mean reward: 2.443396226415094, Mean Entropy: 0.708106279373169, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 579,  Mean reward: 1.7452830188679245, Mean Entropy: 0.5860085487365723, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 580,  Mean reward: 4.313725490196078, Mean Entropy: 0.5048596858978271, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 581,  Mean reward: -2.0, Mean Entropy: 0.6843129396438599, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 582,  Mean reward: 4.508928571428571, Mean Entropy: 0.5780200958251953, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 583,  Mean reward: 4.324074074074074, Mean Entropy: 0.5775076150894165, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 584,  Mean reward: 3.150943396226415, Mean Entropy: 0.59961998462677, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 585,  Mean reward: 3.1666666666666665, Mean Entropy: 0.6689680814743042, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 586,  Mean reward: 3.962962962962963, Mean Entropy: 0.4845373332500458, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 587,  Mean reward: 3.2211538461538463, Mean Entropy: 0.5384836196899414, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 588,  Mean reward: 3.5677966101694913, Mean Entropy: 0.5040996074676514, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 589,  Mean reward: 3.22, Mean Entropy: 0.5586264729499817, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 2.11s
Iteration: 590,  Mean reward: 3.0760869565217392, Mean Entropy: 0.5287428498268127, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 591,  Mean reward: 4.037037037037037, Mean Entropy: 0.5695050954818726, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 592,  Mean reward: 3.074074074074074, Mean Entropy: 0.678759753704071, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 593,  Mean reward: 3.215686274509804, Mean Entropy: 0.490039199590683, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 594,  Mean reward: 2.7254901960784315, Mean Entropy: 0.6211534738540649, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 595,  Mean reward: 2.044642857142857, Mean Entropy: 0.6825380325317383, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 596,  Mean reward: 2.882978723404255, Mean Entropy: 0.6174862384796143, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 597,  Mean reward: 1.1470588235294117, Mean Entropy: 0.6542007327079773, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 598,  Mean reward: 4.75, Mean Entropy: 0.6029040813446045, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 599,  Mean reward: 2.564814814814815, Mean Entropy: 0.4856112599372864, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 600,  Mean reward: 5.138888888888889, Mean Entropy: 0.5455813407897949, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 3.642857142857143, Mean Entropy: 0.5876764059066772, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 602,  Mean reward: 4.867924528301887, Mean Entropy: 0.6469483375549316, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 603,  Mean reward: -1.0818181818181818, Mean Entropy: 0.7572506666183472, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 604,  Mean reward: 3.911764705882353, Mean Entropy: 0.7263859510421753, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 605,  Mean reward: -2.830508474576271, Mean Entropy: 0.6916718482971191, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 606,  Mean reward: 0.3953488372093023, Mean Entropy: 0.6062095165252686, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 607,  Mean reward: 3.963636363636364, Mean Entropy: 0.46684321761131287, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 608,  Mean reward: 4.137254901960785, Mean Entropy: 0.3159067630767822, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 609,  Mean reward: 3.2549019607843137, Mean Entropy: 0.48810338973999023, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 610,  Mean reward: 0.24528301886792453, Mean Entropy: 0.8343088030815125, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 611,  Mean reward: 0.9905660377358491, Mean Entropy: 0.6811873912811279, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 612,  Mean reward: 1.0185185185185186, Mean Entropy: 0.7428316473960876, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 613,  Mean reward: 4.586538461538462, Mean Entropy: 0.475881427526474, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 614,  Mean reward: 1.4919354838709677, Mean Entropy: 0.5299984216690063, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 615,  Mean reward: 2.94, Mean Entropy: 0.6237546801567078, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 616,  Mean reward: 0.8452380952380952, Mean Entropy: 0.7797145843505859, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 617,  Mean reward: 1.5816326530612246, Mean Entropy: 0.8244441747665405, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.85s
Iteration: 618,  Mean reward: -1.3278688524590163, Mean Entropy: 0.6356481909751892, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 619,  Mean reward: 3.688679245283019, Mean Entropy: 0.5201008319854736, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 620,  Mean reward: 1.971698113207547, Mean Entropy: 0.6899356842041016, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 621,  Mean reward: -1.3870967741935485, Mean Entropy: 0.5907255411148071, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 622,  Mean reward: 1.9038461538461537, Mean Entropy: 0.6289665102958679, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 2.08s
Iteration: 623,  Mean reward: 3.605769230769231, Mean Entropy: 0.6743113994598389, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 624,  Mean reward: 0.5625, Mean Entropy: 0.737728476524353, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 625,  Mean reward: 4.952830188679245, Mean Entropy: 0.6458621025085449, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 626,  Mean reward: 2.795918367346939, Mean Entropy: 0.5219888687133789, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 627,  Mean reward: 4.666666666666667, Mean Entropy: 0.5168012380599976, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 628,  Mean reward: 3.7264150943396226, Mean Entropy: 0.6679903864860535, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 629,  Mean reward: 2.0686274509803924, Mean Entropy: 0.7021891474723816, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 630,  Mean reward: 4.580357142857143, Mean Entropy: 0.5259672403335571, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 631,  Mean reward: 2.2063492063492065, Mean Entropy: 0.3916585147380829, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.96s
Iteration: 632,  Mean reward: 0.3968253968253968, Mean Entropy: 0.4738863408565521, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 633,  Mean reward: 2.542372881355932, Mean Entropy: 0.34682345390319824, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 634,  Mean reward: -1.2936507936507937, Mean Entropy: 0.5341936945915222, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 635,  Mean reward: -0.6637931034482759, Mean Entropy: 0.4941185414791107, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 636,  Mean reward: -2.2542372881355934, Mean Entropy: 0.5756024122238159, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 637,  Mean reward: 0.12931034482758622, Mean Entropy: 0.6571385860443115, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 638,  Mean reward: -1.6727272727272726, Mean Entropy: 0.6027224659919739, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 639,  Mean reward: -1.5818181818181818, Mean Entropy: 0.6454805135726929, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 640,  Mean reward: -1.7924528301886793, Mean Entropy: 0.5038747787475586, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 641,  Mean reward: 3.841666666666667, Mean Entropy: 0.44194191694259644, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 642,  Mean reward: -0.9056603773584906, Mean Entropy: 0.6390531063079834, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 643,  Mean reward: 0.8444444444444444, Mean Entropy: 0.47427913546562195, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 644,  Mean reward: 1.51, Mean Entropy: 0.5417575836181641, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 645,  Mean reward: 5.358333333333333, Mean Entropy: 0.38623103499412537, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 646,  Mean reward: 2.323529411764706, Mean Entropy: 0.5558587312698364, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 647,  Mean reward: 3.8962264150943398, Mean Entropy: 0.3560793399810791, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 648,  Mean reward: 1.0333333333333334, Mean Entropy: 0.6674014925956726, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 649,  Mean reward: 2.5508474576271185, Mean Entropy: 0.6662497520446777, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 650,  Mean reward: 2.3425925925925926, Mean Entropy: 0.49237364530563354, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 651,  Mean reward: 2.836734693877551, Mean Entropy: 0.5951383113861084, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 652,  Mean reward: 3.9375, Mean Entropy: 0.5424767732620239, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 653,  Mean reward: 3.61, Mean Entropy: 0.6349369883537292, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 654,  Mean reward: 3.081818181818182, Mean Entropy: 0.6494460105895996, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 655,  Mean reward: 0.8113207547169812, Mean Entropy: 0.6840404272079468, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 2.11s
Iteration: 656,  Mean reward: 3.306122448979592, Mean Entropy: 0.21896593272686005, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 657,  Mean reward: 3.160377358490566, Mean Entropy: 0.19493386149406433, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 658,  Mean reward: 1.3645833333333333, Mean Entropy: 0.39525800943374634, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 659,  Mean reward: 0.7674418604651163, Mean Entropy: 0.43570610880851746, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 660,  Mean reward: 1.6956521739130435, Mean Entropy: 0.5400063991546631, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 661,  Mean reward: 5.427272727272728, Mean Entropy: 0.2623837888240814, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 662,  Mean reward: 2.647727272727273, Mean Entropy: 0.2896566092967987, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 663,  Mean reward: 4.9423076923076925, Mean Entropy: 0.2715722918510437, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 664,  Mean reward: 5.587719298245614, Mean Entropy: 0.17866313457489014, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 665,  Mean reward: 5.365384615384615, Mean Entropy: 0.4884148836135864, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 666,  Mean reward: 6.370689655172414, Mean Entropy: 0.16048173606395721, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.87s
Iteration: 667,  Mean reward: 4.5588235294117645, Mean Entropy: 0.3749887943267822, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 668,  Mean reward: 3.950980392156863, Mean Entropy: 0.4221510887145996, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 669,  Mean reward: 4.715686274509804, Mean Entropy: 0.471027135848999, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 670,  Mean reward: 4.166666666666667, Mean Entropy: 0.43244218826293945, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 671,  Mean reward: 5.324561403508772, Mean Entropy: 0.3295082449913025, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 672,  Mean reward: 6.267241379310345, Mean Entropy: 0.33007335662841797, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 673,  Mean reward: 4.676470588235294, Mean Entropy: 0.43212032318115234, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 674,  Mean reward: 4.886792452830188, Mean Entropy: 0.47141027450561523, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 675,  Mean reward: 5.827586206896552, Mean Entropy: 0.3152786195278168, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 676,  Mean reward: 6.026315789473684, Mean Entropy: 0.3602181077003479, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 677,  Mean reward: 5.723214285714286, Mean Entropy: 0.29985174536705017, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 678,  Mean reward: 5.872881355932203, Mean Entropy: 0.3487341105937958, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 679,  Mean reward: 5.931034482758621, Mean Entropy: 0.2598930895328522, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 680,  Mean reward: 6.333333333333333, Mean Entropy: 0.33408674597740173, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 681,  Mean reward: 6.388888888888889, Mean Entropy: 0.22270652651786804, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 682,  Mean reward: 4.111111111111111, Mean Entropy: 0.5447351932525635, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 683,  Mean reward: 3.0961538461538463, Mean Entropy: 0.3990716338157654, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 684,  Mean reward: 4.1, Mean Entropy: 0.5848315954208374, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 685,  Mean reward: 5.583333333333333, Mean Entropy: 0.3461150527000427, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.86s
Iteration: 686,  Mean reward: 6.025, Mean Entropy: 0.4587733745574951, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 687,  Mean reward: 5.610169491525424, Mean Entropy: 0.44613903760910034, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 688,  Mean reward: 5.527272727272727, Mean Entropy: 0.5024274587631226, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 2.09s
Iteration: 689,  Mean reward: 5.194444444444445, Mean Entropy: 0.44840821623802185, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 690,  Mean reward: 5.945454545454545, Mean Entropy: 0.29503244161605835, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 691,  Mean reward: 3.215686274509804, Mean Entropy: 0.633894145488739, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 692,  Mean reward: 4.428571428571429, Mean Entropy: 0.6387977600097656, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 693,  Mean reward: 2.1847826086956523, Mean Entropy: 0.6174432635307312, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 694,  Mean reward: 4.19811320754717, Mean Entropy: 0.5262521505355835, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 695,  Mean reward: 3.8627450980392157, Mean Entropy: 0.5333026647567749, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 696,  Mean reward: 6.065573770491803, Mean Entropy: 0.3703783452510834, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 697,  Mean reward: 5.184210526315789, Mean Entropy: 0.42420047521591187, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 698,  Mean reward: 4.185185185185185, Mean Entropy: 0.6582497358322144, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 699,  Mean reward: 3.99, Mean Entropy: 0.5700942277908325, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 700,  Mean reward: 3.2211538461538463, Mean Entropy: 0.6195225715637207, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.91s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: 5.0625, Mean Entropy: 0.36410531401634216, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 702,  Mean reward: 5.146551724137931, Mean Entropy: 0.48784399032592773, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 703,  Mean reward: 3.3859649122807016, Mean Entropy: 0.6020801067352295, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 704,  Mean reward: 3.4454545454545453, Mean Entropy: 0.5557045936584473, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 705,  Mean reward: 3.4098360655737703, Mean Entropy: 0.6371753811836243, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 706,  Mean reward: 0.71875, Mean Entropy: 0.646568775177002, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 707,  Mean reward: 1.7169811320754718, Mean Entropy: 0.5968925952911377, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 708,  Mean reward: 1.6938775510204083, Mean Entropy: 0.5014845728874207, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 709,  Mean reward: 3.169811320754717, Mean Entropy: 0.4193994402885437, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 710,  Mean reward: 1.6627906976744187, Mean Entropy: 0.502493143081665, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 711,  Mean reward: 2.4081632653061225, Mean Entropy: 0.5380902886390686, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 2.22s
Iteration: 712,  Mean reward: 4.732142857142857, Mean Entropy: 0.4658299684524536, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.91s
Iteration: 713,  Mean reward: 4.776785714285714, Mean Entropy: 0.45165354013442993, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 714,  Mean reward: 5.491666666666666, Mean Entropy: 0.3704906404018402, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 715,  Mean reward: 4.572727272727272, Mean Entropy: 0.46425169706344604, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 716,  Mean reward: 4.9363636363636365, Mean Entropy: 0.3829039931297302, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 717,  Mean reward: 5.291666666666667, Mean Entropy: 0.476815402507782, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 718,  Mean reward: 5.178571428571429, Mean Entropy: 0.5702899694442749, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 719,  Mean reward: 3.8846153846153846, Mean Entropy: 0.6806408762931824, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 720,  Mean reward: 3.0510204081632653, Mean Entropy: 0.6716215014457703, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 2.09s
Iteration: 721,  Mean reward: 4.386792452830188, Mean Entropy: 0.5557226538658142, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 722,  Mean reward: 5.716666666666667, Mean Entropy: 0.4618825912475586, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 723,  Mean reward: 4.754385964912281, Mean Entropy: 0.5416437387466431, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 724,  Mean reward: 5.935483870967742, Mean Entropy: 0.49477142095565796, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 725,  Mean reward: 4.833333333333333, Mean Entropy: 0.6626720428466797, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 726,  Mean reward: 3.596774193548387, Mean Entropy: 0.5893552899360657, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 727,  Mean reward: 4.4375, Mean Entropy: 0.6706058382987976, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 728,  Mean reward: 1.5833333333333333, Mean Entropy: 0.6888418197631836, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 729,  Mean reward: 5.232758620689655, Mean Entropy: 0.6554744243621826, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 730,  Mean reward: 3.896551724137931, Mean Entropy: 0.5202140808105469, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 731,  Mean reward: 4.153846153846154, Mean Entropy: 0.5150884985923767, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 732,  Mean reward: 4.933333333333334, Mean Entropy: 0.42731809616088867, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 733,  Mean reward: 4.611111111111111, Mean Entropy: 0.5547257661819458, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 734,  Mean reward: 4.928571428571429, Mean Entropy: 0.4495014548301697, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 735,  Mean reward: 5.675, Mean Entropy: 0.5132862329483032, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 736,  Mean reward: 4.245762711864407, Mean Entropy: 0.6227335929870605, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 737,  Mean reward: 5.324561403508772, Mean Entropy: 0.6296150088310242, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 738,  Mean reward: 2.694915254237288, Mean Entropy: 0.5854505896568298, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 739,  Mean reward: 4.636363636363637, Mean Entropy: 0.48263248801231384, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 740,  Mean reward: 5.933333333333334, Mean Entropy: 0.4300107955932617, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 741,  Mean reward: 1.8804347826086956, Mean Entropy: 0.5611559748649597, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 742,  Mean reward: 3.696078431372549, Mean Entropy: 0.6132103800773621, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 743,  Mean reward: 1.3951612903225807, Mean Entropy: 0.6241592168807983, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 744,  Mean reward: 3.549019607843137, Mean Entropy: 0.5920235514640808, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 745,  Mean reward: 4.412280701754386, Mean Entropy: 0.5380606055259705, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 746,  Mean reward: 3.9711538461538463, Mean Entropy: 0.4191565215587616, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 747,  Mean reward: 4.955357142857143, Mean Entropy: 0.4314644932746887, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 748,  Mean reward: 5.541666666666667, Mean Entropy: 0.4638908803462982, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 749,  Mean reward: 5.725, Mean Entropy: 0.40434330701828003, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 750,  Mean reward: 4.324561403508772, Mean Entropy: 0.5020021796226501, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 751,  Mean reward: 4.709090909090909, Mean Entropy: 0.4608474671840668, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 752,  Mean reward: 4.064814814814815, Mean Entropy: 0.4712149500846863, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 753,  Mean reward: 2.6, Mean Entropy: 0.5830356478691101, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 2.12s
Iteration: 754,  Mean reward: 3.5727272727272728, Mean Entropy: 0.6171768307685852, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 755,  Mean reward: 3.7982456140350878, Mean Entropy: 0.648128092288971, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 756,  Mean reward: 4.683333333333334, Mean Entropy: 0.49141716957092285, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.92s
Iteration: 757,  Mean reward: 1.9516129032258065, Mean Entropy: 0.5925111174583435, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 758,  Mean reward: 2.1320754716981134, Mean Entropy: 0.6866823434829712, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 759,  Mean reward: 3.1293103448275863, Mean Entropy: 0.5317294597625732, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 760,  Mean reward: 4.61864406779661, Mean Entropy: 0.4349192976951599, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 761,  Mean reward: 3.7083333333333335, Mean Entropy: 0.6333154439926147, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 762,  Mean reward: 4.379032258064516, Mean Entropy: 0.5787727236747742, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 763,  Mean reward: 1.6416666666666666, Mean Entropy: 0.5167941451072693, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 764,  Mean reward: -0.873015873015873, Mean Entropy: 0.5538257956504822, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 765,  Mean reward: 4.043103448275862, Mean Entropy: 0.5990722179412842, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 766,  Mean reward: 3.547169811320755, Mean Entropy: 0.6105927228927612, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 767,  Mean reward: 2.625, Mean Entropy: 0.6261460781097412, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.93s
Iteration: 768,  Mean reward: 1.4516129032258065, Mean Entropy: 0.4927980303764343, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 769,  Mean reward: 4.456140350877193, Mean Entropy: 0.6615194082260132, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 770,  Mean reward: 2.1160714285714284, Mean Entropy: 0.6403381824493408, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 771,  Mean reward: 5.057377049180328, Mean Entropy: 0.4962449073791504, complete_episode_count: 61.0, Gather time: 0.61s, Train time: 2.03s
Iteration: 772,  Mean reward: 1.9351851851851851, Mean Entropy: 0.6328449249267578, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 773,  Mean reward: 3.5555555555555554, Mean Entropy: 0.6071997880935669, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 774,  Mean reward: 4.467213114754099, Mean Entropy: 0.5802403688430786, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 775,  Mean reward: 3.7241379310344827, Mean Entropy: 0.7727861404418945, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 776,  Mean reward: 3.3846153846153846, Mean Entropy: 0.6553489565849304, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 777,  Mean reward: 4.798245614035087, Mean Entropy: 0.5635677576065063, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 778,  Mean reward: 4.544642857142857, Mean Entropy: 0.5610150694847107, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 779,  Mean reward: 3.4215686274509802, Mean Entropy: 0.554029107093811, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 780,  Mean reward: 3.7280701754385963, Mean Entropy: 0.5954263210296631, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 781,  Mean reward: 2.9642857142857144, Mean Entropy: 0.6453479528427124, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 782,  Mean reward: 2.2755102040816326, Mean Entropy: 0.5303151607513428, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 783,  Mean reward: 5.401639344262295, Mean Entropy: 0.47153013944625854, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 784,  Mean reward: 4.432203389830509, Mean Entropy: 0.5536168813705444, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 785,  Mean reward: 2.1864406779661016, Mean Entropy: 0.6198430061340332, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 786,  Mean reward: 5.060344827586207, Mean Entropy: 0.4646555781364441, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 2.15s
Iteration: 787,  Mean reward: 3.7734375, Mean Entropy: 0.39651918411254883, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 788,  Mean reward: 4.028301886792453, Mean Entropy: 0.5288599133491516, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.92s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 789,  Mean reward: 6.428571428571429, Mean Entropy: 0.39315804839134216, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 790,  Mean reward: 4.907407407407407, Mean Entropy: 0.5758281350135803, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 791,  Mean reward: 4.875, Mean Entropy: 0.45875632762908936, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 792,  Mean reward: 5.313559322033898, Mean Entropy: 0.5047611594200134, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 793,  Mean reward: 3.40625, Mean Entropy: 0.5158500671386719, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 794,  Mean reward: 3.673076923076923, Mean Entropy: 0.6790892481803894, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 795,  Mean reward: 3.064814814814815, Mean Entropy: 0.6523277759552002, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 796,  Mean reward: 3.419642857142857, Mean Entropy: 0.5896133184432983, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 797,  Mean reward: 4.2592592592592595, Mean Entropy: 0.6274616122245789, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 798,  Mean reward: 2.439655172413793, Mean Entropy: 0.5847569704055786, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 799,  Mean reward: 5.60655737704918, Mean Entropy: 0.5521966814994812, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 800,  Mean reward: 5.830645161290323, Mean Entropy: 0.5089234113693237, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.95s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: 3.9310344827586206, Mean Entropy: 0.5371460914611816, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 802,  Mean reward: 5.073770491803279, Mean Entropy: 0.5065780878067017, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 803,  Mean reward: 3.8017241379310347, Mean Entropy: 0.6334887742996216, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 804,  Mean reward: 4.409090909090909, Mean Entropy: 0.5781770944595337, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 805,  Mean reward: 4.232758620689655, Mean Entropy: 0.6454182863235474, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 806,  Mean reward: 4.754098360655738, Mean Entropy: 0.4675526022911072, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 807,  Mean reward: 4.175438596491228, Mean Entropy: 0.4972829520702362, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 808,  Mean reward: 3.6923076923076925, Mean Entropy: 0.590953528881073, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 809,  Mean reward: 3.8981481481481484, Mean Entropy: 0.5936575531959534, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 810,  Mean reward: 4.718181818181818, Mean Entropy: 0.5303012132644653, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 811,  Mean reward: 5.4, Mean Entropy: 0.5269057154655457, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 812,  Mean reward: 3.8706896551724137, Mean Entropy: 0.6528390645980835, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 813,  Mean reward: 3.3257575757575757, Mean Entropy: 0.49179014563560486, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 814,  Mean reward: 4.009803921568627, Mean Entropy: 0.6761616468429565, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 815,  Mean reward: 5.76984126984127, Mean Entropy: 0.524096667766571, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 816,  Mean reward: 5.492063492063492, Mean Entropy: 0.4362911880016327, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 817,  Mean reward: 5.330508474576271, Mean Entropy: 0.436436265707016, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 818,  Mean reward: 4.677966101694915, Mean Entropy: 0.4873862862586975, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 819,  Mean reward: 5.9491525423728815, Mean Entropy: 0.4767886996269226, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 820,  Mean reward: 3.9017857142857144, Mean Entropy: 0.49007630348205566, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 821,  Mean reward: 5.298387096774194, Mean Entropy: 0.32965797185897827, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.16s
Iteration: 822,  Mean reward: 0.2627118644067797, Mean Entropy: 0.5440778732299805, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 823,  Mean reward: 4.296296296296297, Mean Entropy: 0.5130270719528198, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 2.02s
Iteration: 824,  Mean reward: 3.25, Mean Entropy: 0.5632547736167908, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 825,  Mean reward: 4.745901639344262, Mean Entropy: 0.4958752393722534, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 826,  Mean reward: 3.5859375, Mean Entropy: 0.41959208250045776, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 827,  Mean reward: 5.120689655172414, Mean Entropy: 0.39550429582595825, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 828,  Mean reward: 3.9215686274509802, Mean Entropy: 0.5060243606567383, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 829,  Mean reward: 5.959677419354839, Mean Entropy: 0.4252171218395233, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 830,  Mean reward: 4.473214285714286, Mean Entropy: 0.5246152877807617, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 831,  Mean reward: 5.669642857142857, Mean Entropy: 0.44536107778549194, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 832,  Mean reward: 3.8392857142857144, Mean Entropy: 0.5821602940559387, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 833,  Mean reward: 1.8189655172413792, Mean Entropy: 0.712603747844696, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 834,  Mean reward: 4.30188679245283, Mean Entropy: 0.5692833662033081, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 835,  Mean reward: 3.7777777777777777, Mean Entropy: 0.5436612963676453, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 836,  Mean reward: 4.290909090909091, Mean Entropy: 0.5167736411094666, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 837,  Mean reward: 4.482142857142857, Mean Entropy: 0.5464979410171509, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 838,  Mean reward: 4.939655172413793, Mean Entropy: 0.5295708179473877, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 839,  Mean reward: 4.179245283018868, Mean Entropy: 0.571704626083374, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 840,  Mean reward: 5.298245614035087, Mean Entropy: 0.5524052977561951, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 841,  Mean reward: 5.322033898305085, Mean Entropy: 0.4735404849052429, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 842,  Mean reward: 5.305555555555555, Mean Entropy: 0.5102121233940125, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 843,  Mean reward: 5.901639344262295, Mean Entropy: 0.4319155216217041, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 844,  Mean reward: 4.75, Mean Entropy: 0.5582568049430847, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 845,  Mean reward: 4.964912280701754, Mean Entropy: 0.4665265679359436, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 846,  Mean reward: 5.275862068965517, Mean Entropy: 0.5172665119171143, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 847,  Mean reward: 4.410714285714286, Mean Entropy: 0.473369836807251, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 848,  Mean reward: 5.325, Mean Entropy: 0.5717973113059998, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 849,  Mean reward: 5.220338983050848, Mean Entropy: 0.5202285647392273, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 850,  Mean reward: 5.483870967741935, Mean Entropy: 0.44770440459251404, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 851,  Mean reward: 4.807017543859649, Mean Entropy: 0.5421800017356873, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 852,  Mean reward: 5.341666666666667, Mean Entropy: 0.4292113184928894, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.91s
Iteration: 853,  Mean reward: 5.719298245614035, Mean Entropy: 0.6346808671951294, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 854,  Mean reward: 5.635593220338983, Mean Entropy: 0.5216971039772034, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 2.13s
Iteration: 855,  Mean reward: 4.680327868852459, Mean Entropy: 0.4904860258102417, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 856,  Mean reward: 4.907407407407407, Mean Entropy: 0.5503661036491394, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 857,  Mean reward: 6.291666666666667, Mean Entropy: 0.5915921926498413, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 858,  Mean reward: 3.561224489795918, Mean Entropy: 0.6117848753929138, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 859,  Mean reward: 4.536363636363636, Mean Entropy: 0.5512650012969971, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 860,  Mean reward: 5.446428571428571, Mean Entropy: 0.4854990243911743, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 861,  Mean reward: 3.1545454545454548, Mean Entropy: 0.5486932992935181, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 862,  Mean reward: 4.135593220338983, Mean Entropy: 0.4799092710018158, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 863,  Mean reward: 4.540983606557377, Mean Entropy: 0.5891724824905396, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.89s
Iteration: 864,  Mean reward: 4.026315789473684, Mean Entropy: 0.47052299976348877, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 865,  Mean reward: 5.0, Mean Entropy: 0.4297495484352112, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 866,  Mean reward: 4.875, Mean Entropy: 0.6004012823104858, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 867,  Mean reward: 5.467741935483871, Mean Entropy: 0.5880817174911499, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 868,  Mean reward: 5.0390625, Mean Entropy: 0.48585325479507446, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 869,  Mean reward: 4.37962962962963, Mean Entropy: 0.5271680355072021, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 870,  Mean reward: 4.857142857142857, Mean Entropy: 0.40258368849754333, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 871,  Mean reward: 5.0, Mean Entropy: 0.46956437826156616, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 872,  Mean reward: 5.808333333333334, Mean Entropy: 0.3455681800842285, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 873,  Mean reward: 5.991935483870968, Mean Entropy: 0.3833974301815033, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 874,  Mean reward: 5.175438596491228, Mean Entropy: 0.4674670696258545, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 875,  Mean reward: 3.5849056603773586, Mean Entropy: 0.468127578496933, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 876,  Mean reward: 4.912280701754386, Mean Entropy: 0.5644190907478333, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 877,  Mean reward: 4.69811320754717, Mean Entropy: 0.5260999202728271, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 878,  Mean reward: 5.409090909090909, Mean Entropy: 0.4852640628814697, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 879,  Mean reward: 5.722222222222222, Mean Entropy: 0.36042627692222595, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 880,  Mean reward: 4.632075471698113, Mean Entropy: 0.6163545250892639, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 881,  Mean reward: 5.87719298245614, Mean Entropy: 0.3925856947898865, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 882,  Mean reward: 5.4298245614035086, Mean Entropy: 0.6345198750495911, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 883,  Mean reward: 5.233333333333333, Mean Entropy: 0.4622657299041748, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 884,  Mean reward: 6.208333333333333, Mean Entropy: 0.5415250062942505, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 885,  Mean reward: 3.169811320754717, Mean Entropy: 0.6821999549865723, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 886,  Mean reward: -1.48, Mean Entropy: 0.9331666827201843, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 887,  Mean reward: -7.267441860465116, Mean Entropy: 0.9260910749435425, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 888,  Mean reward: -3.9615384615384617, Mean Entropy: 0.873552680015564, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 2.10s
Iteration: 889,  Mean reward: -6.554054054054054, Mean Entropy: 0.8638283014297485, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 890,  Mean reward: -3.05, Mean Entropy: 0.8507469892501831, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 891,  Mean reward: -2.45, Mean Entropy: 0.8988837599754333, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 892,  Mean reward: -1.6081081081081081, Mean Entropy: 0.8391634225845337, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 893,  Mean reward: -4.328947368421052, Mean Entropy: 0.836976945400238, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.88s
Iteration: 894,  Mean reward: -3.441860465116279, Mean Entropy: 0.8654946088790894, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 895,  Mean reward: -4.8023255813953485, Mean Entropy: 0.8711058497428894, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 896,  Mean reward: -3.6025641025641026, Mean Entropy: 0.9047183990478516, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 897,  Mean reward: -3.548780487804878, Mean Entropy: 0.9456014037132263, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 898,  Mean reward: -4.056818181818182, Mean Entropy: 0.9102708697319031, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 899,  Mean reward: -2.8, Mean Entropy: 0.8701850771903992, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 900,  Mean reward: -4.15, Mean Entropy: 0.9189261198043823, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.87s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -2.522727272727273, Mean Entropy: 0.9304805994033813, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 902,  Mean reward: -2.4875, Mean Entropy: 0.8585779070854187, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 903,  Mean reward: -6.333333333333333, Mean Entropy: 0.9722031950950623, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 904,  Mean reward: -1.9186046511627908, Mean Entropy: 0.9280110597610474, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 905,  Mean reward: -1.5714285714285714, Mean Entropy: 0.9212584495544434, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 906,  Mean reward: -5.5375, Mean Entropy: 0.9382354617118835, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 907,  Mean reward: -4.5875, Mean Entropy: 0.9651626944541931, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 908,  Mean reward: -5.556818181818182, Mean Entropy: 0.9377202987670898, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 909,  Mean reward: -4.928571428571429, Mean Entropy: 0.9378644824028015, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 910,  Mean reward: -4.755813953488372, Mean Entropy: 0.9740442037582397, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 911,  Mean reward: -2.625, Mean Entropy: 0.9305066466331482, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 912,  Mean reward: -4.695121951219512, Mean Entropy: 0.940723180770874, complete_episode_count: 41.0, Gather time: 0.68s, Train time: 1.92s
Iteration: 913,  Mean reward: -0.2073170731707317, Mean Entropy: 0.9340879321098328, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 914,  Mean reward: -5.425, Mean Entropy: 0.9543577432632446, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 915,  Mean reward: -6.151162790697675, Mean Entropy: 0.9487730860710144, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 916,  Mean reward: -5.075, Mean Entropy: 0.938823401927948, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 917,  Mean reward: -3.395348837209302, Mean Entropy: 0.95311439037323, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 918,  Mean reward: -1.4473684210526316, Mean Entropy: 0.9368641376495361, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 919,  Mean reward: -4.134146341463414, Mean Entropy: 0.932287871837616, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 920,  Mean reward: -4.46875, Mean Entropy: 0.908202052116394, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 921,  Mean reward: -5.267441860465116, Mean Entropy: 0.8674353361129761, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 922,  Mean reward: -5.440476190476191, Mean Entropy: 0.9287604093551636, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 2.11s
Iteration: 923,  Mean reward: -3.988888888888889, Mean Entropy: 0.8153252601623535, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 924,  Mean reward: -2.23, Mean Entropy: 0.8044323325157166, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 2.02s
Iteration: 925,  Mean reward: -1.1585365853658536, Mean Entropy: 0.6709191799163818, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 926,  Mean reward: 0.696078431372549, Mean Entropy: 0.8028117418289185, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 927,  Mean reward: -0.9, Mean Entropy: 0.8646469116210938, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 928,  Mean reward: -4.33, Mean Entropy: 0.9032365679740906, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 929,  Mean reward: -3.519607843137255, Mean Entropy: 1.0093473196029663, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.92s
Iteration: 930,  Mean reward: -2.130434782608696, Mean Entropy: 0.9135918021202087, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 931,  Mean reward: -1.4021739130434783, Mean Entropy: 0.909993052482605, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 932,  Mean reward: -0.7282608695652174, Mean Entropy: 0.9135822653770447, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 933,  Mean reward: -0.15555555555555556, Mean Entropy: 0.772523820400238, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 934,  Mean reward: 2.9272727272727272, Mean Entropy: 0.6284754276275635, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 935,  Mean reward: 2.4, Mean Entropy: 0.8167707324028015, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 936,  Mean reward: 1.411764705882353, Mean Entropy: 0.8311929106712341, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 2.02s
Iteration: 937,  Mean reward: 0.6333333333333333, Mean Entropy: 0.7807032465934753, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 938,  Mean reward: 4.775862068965517, Mean Entropy: 0.784403920173645, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.92s
Iteration: 939,  Mean reward: 0.8775510204081632, Mean Entropy: 0.6641242504119873, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 940,  Mean reward: -2.292452830188679, Mean Entropy: 0.7142068147659302, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 941,  Mean reward: -0.68, Mean Entropy: 0.706286609172821, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 942,  Mean reward: 3.888888888888889, Mean Entropy: 0.5689293742179871, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 943,  Mean reward: 3.5, Mean Entropy: 0.7901267409324646, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 944,  Mean reward: 1.8703703703703705, Mean Entropy: 0.7266116142272949, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 945,  Mean reward: 2.8125, Mean Entropy: 0.7755064964294434, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 946,  Mean reward: 2.05, Mean Entropy: 0.8205785751342773, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 947,  Mean reward: 2.7719298245614037, Mean Entropy: 0.7330178022384644, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 948,  Mean reward: 1.425531914893617, Mean Entropy: 0.7588261365890503, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 2.00s
Iteration: 949,  Mean reward: 0.9224137931034483, Mean Entropy: 0.8315936923027039, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 950,  Mean reward: 1.8877551020408163, Mean Entropy: 0.7921315431594849, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 951,  Mean reward: 1.3611111111111112, Mean Entropy: 0.6436283588409424, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 952,  Mean reward: 2.913793103448276, Mean Entropy: 0.6670875549316406, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 953,  Mean reward: 0.3695652173913043, Mean Entropy: 0.7886986136436462, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 954,  Mean reward: 2.227272727272727, Mean Entropy: 0.7718366384506226, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 955,  Mean reward: 1.29, Mean Entropy: 0.7477449178695679, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 2.06s
Iteration: 956,  Mean reward: 2.9035087719298245, Mean Entropy: 0.7645823359489441, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 957,  Mean reward: 1.35, Mean Entropy: 0.5851256847381592, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 958,  Mean reward: 1.0192307692307692, Mean Entropy: 0.7661625146865845, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 959,  Mean reward: 4.008333333333334, Mean Entropy: 0.5293344259262085, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 960,  Mean reward: 2.8297872340425534, Mean Entropy: 0.565434455871582, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 961,  Mean reward: 2.675925925925926, Mean Entropy: 0.6942824125289917, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 962,  Mean reward: 3.4150943396226414, Mean Entropy: 0.5439387559890747, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 963,  Mean reward: 2.7596153846153846, Mean Entropy: 0.47427988052368164, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 964,  Mean reward: 6.076923076923077, Mean Entropy: 0.48808640241622925, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 965,  Mean reward: 3.8189655172413794, Mean Entropy: 0.507946252822876, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 966,  Mean reward: 2.7708333333333335, Mean Entropy: 0.3433918356895447, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 967,  Mean reward: 5.788135593220339, Mean Entropy: 0.4623052477836609, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 968,  Mean reward: 2.4705882352941178, Mean Entropy: 0.7470636367797852, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 969,  Mean reward: 3.1818181818181817, Mean Entropy: 0.5472923517227173, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 970,  Mean reward: 3.980392156862745, Mean Entropy: 0.5933253169059753, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 971,  Mean reward: 2.7844827586206895, Mean Entropy: 0.5054492950439453, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 972,  Mean reward: 4.589285714285714, Mean Entropy: 0.3909721374511719, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 973,  Mean reward: 5.016666666666667, Mean Entropy: 0.4283290505409241, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 974,  Mean reward: 4.660714285714286, Mean Entropy: 0.44649696350097656, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 975,  Mean reward: 4.267857142857143, Mean Entropy: 0.539315938949585, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 976,  Mean reward: 4.796296296296297, Mean Entropy: 0.5604667067527771, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 977,  Mean reward: 4.919642857142857, Mean Entropy: 0.5354635715484619, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 978,  Mean reward: 5.375, Mean Entropy: 0.40586552023887634, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 979,  Mean reward: 4.654545454545454, Mean Entropy: 0.5388929843902588, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 980,  Mean reward: 2.9649122807017543, Mean Entropy: 0.49955040216445923, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 981,  Mean reward: 3.740740740740741, Mean Entropy: 0.45643290877342224, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 982,  Mean reward: 4.88135593220339, Mean Entropy: 0.4667914807796478, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 983,  Mean reward: 5.52542372881356, Mean Entropy: 0.424905002117157, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 984,  Mean reward: 4.918032786885246, Mean Entropy: 0.39272844791412354, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 985,  Mean reward: 4.607142857142857, Mean Entropy: 0.4665645956993103, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 986,  Mean reward: 5.626984126984127, Mean Entropy: 0.4537193179130554, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 987,  Mean reward: 4.4818181818181815, Mean Entropy: 0.5331647396087646, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 988,  Mean reward: 6.185483870967742, Mean Entropy: 0.46656984090805054, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 989,  Mean reward: 3.4655172413793105, Mean Entropy: 0.3431270122528076, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 2.09s
Iteration: 990,  Mean reward: 5.30327868852459, Mean Entropy: 0.2958163619041443, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 991,  Mean reward: 4.881818181818182, Mean Entropy: 0.38943466544151306, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 992,  Mean reward: 4.431372549019608, Mean Entropy: 0.4139025807380676, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 993,  Mean reward: 5.114035087719298, Mean Entropy: 0.47169095277786255, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 994,  Mean reward: 4.418181818181818, Mean Entropy: 0.48731979727745056, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 995,  Mean reward: 3.5, Mean Entropy: 0.43995600938796997, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 996,  Mean reward: 3.21, Mean Entropy: 0.5117980241775513, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 997,  Mean reward: 5.533333333333333, Mean Entropy: 0.3536110818386078, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 998,  Mean reward: 5.155172413793103, Mean Entropy: 0.4413720965385437, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 999,  Mean reward: 4.620689655172414, Mean Entropy: 0.4541681706905365, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 1000,  Mean reward: 4.640350877192983, Mean Entropy: 0.3109138607978821, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.86s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: 5.17741935483871, Mean Entropy: 0.3713153302669525, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 1002,  Mean reward: 5.341666666666667, Mean Entropy: 0.5274210572242737, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 1003,  Mean reward: 4.0588235294117645, Mean Entropy: 0.616757333278656, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 1004,  Mean reward: 4.741379310344827, Mean Entropy: 0.48733898997306824, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 1005,  Mean reward: 5.5, Mean Entropy: 0.36561059951782227, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.96s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1006,  Mean reward: 6.557971014492754, Mean Entropy: 0.28438302874565125, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.95s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1007,  Mean reward: 6.70945945945946, Mean Entropy: 0.242723286151886, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.94s
Iteration: 1008,  Mean reward: -0.7236842105263158, Mean Entropy: 0.18603476881980896, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1009,  Mean reward: 6.046153846153846, Mean Entropy: 0.36175912618637085, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1010,  Mean reward: 5.685483870967742, Mean Entropy: 0.374552845954895, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.96s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1011,  Mean reward: 7.240259740259741, Mean Entropy: 0.10507544130086899, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.95s
Iteration: 1012,  Mean reward: 5.479166666666667, Mean Entropy: 0.1618107557296753, complete_episode_count: 72.0, Gather time: 0.57s, Train time: 0.97s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1013,  Mean reward: 7.981012658227848, Mean Entropy: 0.232264444231987, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.95s
Iteration: 1014,  Mean reward: -1.4294871794871795, Mean Entropy: 0.20681880414485931, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1015,  Mean reward: -1.619718309859155, Mean Entropy: 0.14238481223583221, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1016,  Mean reward: -1.12987012987013, Mean Entropy: 0.3815133273601532, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1017,  Mean reward: 2.6403508771929824, Mean Entropy: 0.42640870809555054, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.93s
Iteration: 1018,  Mean reward: 4.955357142857143, Mean Entropy: 0.26207995414733887, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 1019,  Mean reward: 5.2155172413793105, Mean Entropy: 0.250113844871521, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 1020,  Mean reward: 5.0964912280701755, Mean Entropy: 0.23553399741649628, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 1021,  Mean reward: 4.7631578947368425, Mean Entropy: 0.31494277715682983, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 1022,  Mean reward: 3.6574074074074074, Mean Entropy: 0.27247774600982666, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.91s
Iteration: 1023,  Mean reward: 4.2, Mean Entropy: 0.39026185870170593, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 2.10s
Iteration: 1024,  Mean reward: 5.38135593220339, Mean Entropy: 0.3857559859752655, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.90s
Iteration: 1025,  Mean reward: 3.5588235294117645, Mean Entropy: 0.23423951864242554, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.91s
Iteration: 1026,  Mean reward: 5.275, Mean Entropy: 0.20833075046539307, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 1027,  Mean reward: 3.769230769230769, Mean Entropy: 0.29141896963119507, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 1028,  Mean reward: 4.719298245614035, Mean Entropy: 0.3956999182701111, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 1029,  Mean reward: 5.919642857142857, Mean Entropy: 0.38697952032089233, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 1030,  Mean reward: 5.264705882352941, Mean Entropy: 0.562909722328186, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 1031,  Mean reward: 5.816666666666666, Mean Entropy: 0.381203830242157, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 1032,  Mean reward: 4.447368421052632, Mean Entropy: 0.39803165197372437, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 1033,  Mean reward: 5.203389830508475, Mean Entropy: 0.374434232711792, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 1034,  Mean reward: 6.813559322033898, Mean Entropy: 0.37321150302886963, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.88s
Iteration: 1035,  Mean reward: 5.245614035087719, Mean Entropy: 0.4849473834037781, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 1036,  Mean reward: 5.508928571428571, Mean Entropy: 0.40951013565063477, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.89s
Iteration: 1037,  Mean reward: 5.0390625, Mean Entropy: 0.3770933747291565, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1038,  Mean reward: 5.0636363636363635, Mean Entropy: 0.48982685804367065, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 1039,  Mean reward: 5.61864406779661, Mean Entropy: 0.49758994579315186, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.90s
Iteration: 1040,  Mean reward: 5.155172413793103, Mean Entropy: 0.2652475833892822, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.88s
Iteration: 1041,  Mean reward: 2.216666666666667, Mean Entropy: 0.08146792650222778, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 1042,  Mean reward: -2.2733333333333334, Mean Entropy: 0.09156098961830139, complete_episode_count: 75.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1043,  Mean reward: -1.3289473684210527, Mean Entropy: 0.12638765573501587, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1044,  Mean reward: -4.364864864864865, Mean Entropy: 0.15756863355636597, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1045,  Mean reward: -1.8918918918918919, Mean Entropy: 0.08828046172857285, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1046,  Mean reward: -0.26282051282051283, Mean Entropy: 0.028829053044319153, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1047,  Mean reward: -3.6645569620253164, Mean Entropy: 0.009806139394640923, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.96s
Iteration: 1048,  Mean reward: -3.25, Mean Entropy: 0.00413942476734519, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1049,  Mean reward: -1.8924050632911393, Mean Entropy: 0.004114352632313967, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.94s
Iteration: 1050,  Mean reward: 1.0, Mean Entropy: 0.002078560646623373, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.99s
Iteration: 1051,  Mean reward: -1.25, Mean Entropy: 0.0023982159327715635, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1052,  Mean reward: -3.0, Mean Entropy: 0.0032517509534955025, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1053,  Mean reward: -2.5, Mean Entropy: 0.0030867604073137045, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1054,  Mean reward: -1.5, Mean Entropy: 0.00814059842377901, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1055,  Mean reward: -2.25, Mean Entropy: 0.006825198885053396, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1056,  Mean reward: -3.75, Mean Entropy: 0.007524789776653051, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1057,  Mean reward: -1.639240506329114, Mean Entropy: 0.012390120886266232, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1058,  Mean reward: -1.75, Mean Entropy: 0.046012312173843384, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1059,  Mean reward: -3.0641025641025643, Mean Entropy: 0.009851268492639065, complete_episode_count: 78.0, Gather time: 0.57s, Train time: 1.16s
Iteration: 1060,  Mean reward: -3.25, Mean Entropy: 0.004175487905740738, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1061,  Mean reward: -1.0, Mean Entropy: 0.0021389853209257126, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1062,  Mean reward: -0.25, Mean Entropy: 0.0012383684515953064, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1063,  Mean reward: -0.5, Mean Entropy: 0.001457761973142624, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1064,  Mean reward: -2.25, Mean Entropy: 0.0011762510985136032, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1065,  Mean reward: -3.5, Mean Entropy: 0.0008974497905001044, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1066,  Mean reward: -1.0, Mean Entropy: 0.000722284778021276, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1067,  Mean reward: -3.25, Mean Entropy: 0.0006111934198997915, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1068,  Mean reward: -3.25, Mean Entropy: 0.0005725841620005667, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1069,  Mean reward: -0.75, Mean Entropy: 0.000631137692835182, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1070,  Mean reward: -2.75, Mean Entropy: 0.0005645953351631761, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1071,  Mean reward: -1.0, Mean Entropy: 0.0006542669143527746, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1072,  Mean reward: -2.75, Mean Entropy: 0.000604691740591079, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1073,  Mean reward: -1.75, Mean Entropy: 0.0006541945040225983, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1074,  Mean reward: -2.25, Mean Entropy: 0.0006621598149649799, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1075,  Mean reward: -3.25, Mean Entropy: 0.0006628576084040105, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1076,  Mean reward: -3.25, Mean Entropy: 0.0007127315038815141, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1077,  Mean reward: -1.75, Mean Entropy: 0.0007698037079535425, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1078,  Mean reward: -2.25, Mean Entropy: 0.0007346731144934893, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1079,  Mean reward: -0.5, Mean Entropy: 0.0008360647480003536, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 1.06s
Iteration: 1080,  Mean reward: -1.5, Mean Entropy: 0.0007473997538909316, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1081,  Mean reward: -5.0, Mean Entropy: 0.0008237168076448143, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1082,  Mean reward: -1.75, Mean Entropy: 0.0009425714961253107, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1083,  Mean reward: -2.0, Mean Entropy: 0.0008877182262949646, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1084,  Mean reward: -0.25, Mean Entropy: 0.0010593155166134238, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1085,  Mean reward: -1.639240506329114, Mean Entropy: 0.00035864306846633554, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1086,  Mean reward: -3.25, Mean Entropy: 0.0003463597968220711, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1087,  Mean reward: -1.0, Mean Entropy: 0.00025682145496830344, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1088,  Mean reward: -3.0, Mean Entropy: 0.00030610308749601245, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1089,  Mean reward: -2.5, Mean Entropy: 0.0002874530619010329, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1090,  Mean reward: -2.25, Mean Entropy: 0.00033541000448167324, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1091,  Mean reward: -4.0, Mean Entropy: 0.00035638685221783817, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1092,  Mean reward: -2.5, Mean Entropy: 0.000301280349958688, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1093,  Mean reward: -2.0, Mean Entropy: 0.00032184919109568, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1094,  Mean reward: -3.5, Mean Entropy: 0.00039906182792037725, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1095,  Mean reward: -4.25, Mean Entropy: 0.00035510119050741196, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1096,  Mean reward: -2.75, Mean Entropy: 0.00035239977296441793, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1097,  Mean reward: -5.5, Mean Entropy: 0.0004124203114770353, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1098,  Mean reward: -1.5, Mean Entropy: 0.00035126187140122056, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1099,  Mean reward: -2.0, Mean Entropy: 0.0003695701598189771, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 1.17s
Iteration: 1100,  Mean reward: -1.25, Mean Entropy: 0.00037214672192931175, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: -3.25, Mean Entropy: 0.00035715324338525534, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1102,  Mean reward: -2.0, Mean Entropy: 0.0003984555951319635, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1103,  Mean reward: 0.75, Mean Entropy: 0.00036147268838249147, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1104,  Mean reward: -2.75, Mean Entropy: 0.0004309508949518204, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1105,  Mean reward: -2.25, Mean Entropy: 0.0004042316577397287, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1106,  Mean reward: -2.25, Mean Entropy: 0.00043244188418611884, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1107,  Mean reward: -0.5, Mean Entropy: 0.0004161364340689033, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1108,  Mean reward: -1.0, Mean Entropy: 0.00047097200877033174, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1109,  Mean reward: -2.5, Mean Entropy: 0.00045294431038200855, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1110,  Mean reward: -2.5, Mean Entropy: 0.0004890227573923767, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1111,  Mean reward: -1.75, Mean Entropy: 0.0004682143626268953, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.98s
Iteration: 1112,  Mean reward: -2.25, Mean Entropy: 0.0005243425257503986, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1113,  Mean reward: -3.0, Mean Entropy: 0.0004926992114633322, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1114,  Mean reward: -2.75, Mean Entropy: 0.0005459159146994352, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1115,  Mean reward: -2.25, Mean Entropy: 0.0005186791531741619, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1116,  Mean reward: -2.25, Mean Entropy: 0.000572256394661963, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1117,  Mean reward: -3.0, Mean Entropy: 0.0005263527855277061, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1118,  Mean reward: -0.75, Mean Entropy: 0.0005842112004756927, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.10s
Iteration: 1119,  Mean reward: -3.0, Mean Entropy: 0.0005441881949082017, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1120,  Mean reward: -2.75, Mean Entropy: 0.0005888573359698057, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1121,  Mean reward: -1.5, Mean Entropy: 0.0005814334144815803, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1122,  Mean reward: -2.0, Mean Entropy: 0.0006542093469761312, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1123,  Mean reward: -2.0, Mean Entropy: 0.0006177893374115229, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1124,  Mean reward: -1.75, Mean Entropy: 0.0006942641921341419, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1125,  Mean reward: -2.75, Mean Entropy: 0.0006612638244405389, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1126,  Mean reward: -3.0, Mean Entropy: 0.0007084766402840614, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1127,  Mean reward: -3.75, Mean Entropy: 0.0006814728258177638, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1128,  Mean reward: -1.25, Mean Entropy: 0.0008261280017904937, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1129,  Mean reward: -2.5, Mean Entropy: 0.0007372323889285326, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1130,  Mean reward: -1.0, Mean Entropy: 0.0008100964478217065, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1131,  Mean reward: -1.5, Mean Entropy: 0.0007657265523448586, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1132,  Mean reward: -0.25, Mean Entropy: 0.000882589491084218, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1133,  Mean reward: -1.75, Mean Entropy: 0.0007473188452422619, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1134,  Mean reward: -2.75, Mean Entropy: 0.0008175730472430587, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1135,  Mean reward: -1.5, Mean Entropy: 0.0007535129552707076, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1136,  Mean reward: -1.0, Mean Entropy: 0.0008579124696552753, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1137,  Mean reward: -0.75, Mean Entropy: 0.0007287228363566101, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1138,  Mean reward: -3.0, Mean Entropy: 0.0007518756901845336, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1139,  Mean reward: -4.25, Mean Entropy: 0.0006770191248506308, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 1.16s
Iteration: 1140,  Mean reward: -0.75, Mean Entropy: 0.0007940809009596705, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1141,  Mean reward: -0.5, Mean Entropy: 0.0006928177317604423, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1142,  Mean reward: -2.25, Mean Entropy: 0.0007400405593216419, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1143,  Mean reward: -3.25, Mean Entropy: 0.0006789637263864279, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1144,  Mean reward: -1.75, Mean Entropy: 0.0007851134287193418, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1145,  Mean reward: -4.25, Mean Entropy: 0.0007087310659699142, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1146,  Mean reward: -3.25, Mean Entropy: 0.0008255100110545754, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1147,  Mean reward: -2.5, Mean Entropy: 0.000823909998871386, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1148,  Mean reward: -2.75, Mean Entropy: 0.000927648157812655, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1149,  Mean reward: -1.75, Mean Entropy: 0.0008514561923220754, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1150,  Mean reward: -2.0, Mean Entropy: 0.0009494288824498653, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 1151,  Mean reward: -3.5, Mean Entropy: 0.0008445298299193382, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1152,  Mean reward: -2.5, Mean Entropy: 0.0010797717841342092, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1153,  Mean reward: -4.0, Mean Entropy: 0.001005716621875763, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1154,  Mean reward: -1.75, Mean Entropy: 0.0011964495060965419, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1155,  Mean reward: -3.25, Mean Entropy: 0.0010462854988873005, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1156,  Mean reward: -1.0, Mean Entropy: 0.0014275666326284409, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1157,  Mean reward: -1.0, Mean Entropy: 0.001340092858299613, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.06s
Iteration: 1158,  Mean reward: -4.25, Mean Entropy: 0.0012644128873944283, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1159,  Mean reward: -2.75, Mean Entropy: 0.0013441245537251234, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1160,  Mean reward: -1.5, Mean Entropy: 0.0016815881244838238, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1161,  Mean reward: -2.0, Mean Entropy: 0.0011781867360696197, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1162,  Mean reward: -2.25, Mean Entropy: 0.0011807642877101898, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1163,  Mean reward: -1.75, Mean Entropy: 0.0011612551752477884, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1164,  Mean reward: -1.0, Mean Entropy: 0.0014974083751440048, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1165,  Mean reward: 1.5, Mean Entropy: 0.0013499101623892784, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1166,  Mean reward: -3.0, Mean Entropy: 0.0013016000157222152, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1167,  Mean reward: -1.25, Mean Entropy: 0.0012721656821668148, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1168,  Mean reward: -3.0, Mean Entropy: 0.001039192546159029, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1169,  Mean reward: -1.75, Mean Entropy: 0.000965096871368587, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1170,  Mean reward: -3.0, Mean Entropy: 0.0010079150088131428, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1171,  Mean reward: -4.0, Mean Entropy: 0.0007838515448383987, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1172,  Mean reward: -2.75, Mean Entropy: 0.0008309530094265938, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1173,  Mean reward: -2.0, Mean Entropy: 0.0008560306159779429, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1174,  Mean reward: -3.5, Mean Entropy: 0.0009769763564690948, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1175,  Mean reward: -2.75, Mean Entropy: 0.0008720203768461943, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1176,  Mean reward: -4.5, Mean Entropy: 0.0009779301472008228, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1177,  Mean reward: -2.5, Mean Entropy: 0.0008682459592819214, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1178,  Mean reward: -0.5, Mean Entropy: 0.0008513130014762282, complete_episode_count: 80.0, Gather time: 0.70s, Train time: 0.94s
Iteration: 1179,  Mean reward: -2.75, Mean Entropy: 0.000758282549213618, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.16s
Iteration: 1180,  Mean reward: -0.75, Mean Entropy: 0.0008722009370103478, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1181,  Mean reward: -1.5, Mean Entropy: 0.0007951617590151727, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1182,  Mean reward: -3.25, Mean Entropy: 0.000886957161128521, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1183,  Mean reward: -2.5, Mean Entropy: 0.0008123114239424467, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1184,  Mean reward: -0.75, Mean Entropy: 0.0009287832072004676, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1185,  Mean reward: -3.0, Mean Entropy: 0.0008202907629311085, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1186,  Mean reward: -1.75, Mean Entropy: 0.0010608709417283535, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1187,  Mean reward: -2.5, Mean Entropy: 0.0008613417157903314, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1188,  Mean reward: -3.5, Mean Entropy: 0.0009524160996079445, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1189,  Mean reward: -1.25, Mean Entropy: 0.0011275658616796136, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1190,  Mean reward: -2.0, Mean Entropy: 0.0012030357029289007, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.99s
Iteration: 1191,  Mean reward: -2.25, Mean Entropy: 0.0008950058254413307, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1192,  Mean reward: -2.25, Mean Entropy: 0.0009104626951739192, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1193,  Mean reward: -2.5, Mean Entropy: 0.0007611237815581262, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 1194,  Mean reward: -2.25, Mean Entropy: 0.0008371305884793401, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1195,  Mean reward: -5.0, Mean Entropy: 0.0007415168802253902, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1196,  Mean reward: -1.5, Mean Entropy: 0.0009194263257086277, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.03s
Iteration: 1197,  Mean reward: -0.25, Mean Entropy: 0.0009545987704768777, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1198,  Mean reward: -2.5, Mean Entropy: 0.001067121047526598, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1199,  Mean reward: -2.5, Mean Entropy: 0.000937071512453258, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1200,  Mean reward: -3.0, Mean Entropy: 0.000984534271992743, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: -1.5, Mean Entropy: 0.0008996228571049869, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1202,  Mean reward: -2.5, Mean Entropy: 0.0008423905819654465, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1203,  Mean reward: -0.5, Mean Entropy: 0.000757187488488853, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1204,  Mean reward: -1.0, Mean Entropy: 0.0008044871501624584, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1205,  Mean reward: -0.5, Mean Entropy: 0.0007240689592435956, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1206,  Mean reward: -1.25, Mean Entropy: 0.000817604479379952, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1207,  Mean reward: -3.5, Mean Entropy: 0.0007405108772218227, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1208,  Mean reward: -2.25, Mean Entropy: 0.0008302258211188018, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1209,  Mean reward: -1.0, Mean Entropy: 0.0008772920118644834, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1210,  Mean reward: -1.0, Mean Entropy: 0.0010817736620083451, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1211,  Mean reward: -4.25, Mean Entropy: 0.0009719408117234707, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1212,  Mean reward: -1.5, Mean Entropy: 0.0011373795568943024, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1213,  Mean reward: -2.5, Mean Entropy: 0.0010049805277958512, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1214,  Mean reward: -2.25, Mean Entropy: 0.0014530867338180542, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1215,  Mean reward: -0.25, Mean Entropy: 0.0020772514399141073, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1216,  Mean reward: -2.5, Mean Entropy: 0.002664756029844284, complete_episode_count: 80.0, Gather time: 0.70s, Train time: 0.94s
Iteration: 1217,  Mean reward: -2.75, Mean Entropy: 0.0014545442536473274, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1218,  Mean reward: -2.75, Mean Entropy: 0.0013338970020413399, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1219,  Mean reward: -2.75, Mean Entropy: 0.0011818601051345468, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.16s
Iteration: 1220,  Mean reward: -3.75, Mean Entropy: 0.0023058524820953608, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1221,  Mean reward: 0.25, Mean Entropy: 0.0015977062284946442, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1222,  Mean reward: -2.75, Mean Entropy: 0.0006721587269566953, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1223,  Mean reward: -3.0, Mean Entropy: 0.0005690061370842159, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1224,  Mean reward: -2.0, Mean Entropy: 0.0004634761717170477, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1225,  Mean reward: -2.0, Mean Entropy: 0.0005322984652593732, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1226,  Mean reward: -2.5, Mean Entropy: 0.0005770318093709648, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.96s
Iteration: 1227,  Mean reward: -0.5, Mean Entropy: 0.0005457957158796489, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1228,  Mean reward: -1.25, Mean Entropy: 0.0004978561191819608, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1229,  Mean reward: -3.0, Mean Entropy: 0.0006919367588125169, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 1230,  Mean reward: 0.0, Mean Entropy: 0.0006354567594826221, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1231,  Mean reward: -2.75, Mean Entropy: 0.0007384443888440728, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1232,  Mean reward: -0.5, Mean Entropy: 0.0009280610829591751, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1233,  Mean reward: -1.5, Mean Entropy: 0.0011910574976354837, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1234,  Mean reward: -1.5, Mean Entropy: 0.0015142257325351238, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1235,  Mean reward: -3.25, Mean Entropy: 0.0018135558348149061, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 1.11s
Iteration: 1236,  Mean reward: -2.75, Mean Entropy: 0.0016467513050884008, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1237,  Mean reward: -1.25, Mean Entropy: 0.0016263540601357818, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1238,  Mean reward: -1.75, Mean Entropy: 0.0017407909035682678, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1239,  Mean reward: -0.75, Mean Entropy: 0.001279313350096345, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1240,  Mean reward: -2.5, Mean Entropy: 0.000894151977263391, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1241,  Mean reward: -4.0, Mean Entropy: 0.0009810057235881686, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1242,  Mean reward: -4.75, Mean Entropy: 0.0008578215492889285, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1243,  Mean reward: -2.0, Mean Entropy: 0.0008004257688298821, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1244,  Mean reward: -0.75, Mean Entropy: 0.0009986881632357836, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1245,  Mean reward: -2.25, Mean Entropy: 0.0008829402504488826, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1246,  Mean reward: -0.5, Mean Entropy: 0.0007278604316525161, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1247,  Mean reward: -2.25, Mean Entropy: 0.0008450098102912307, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1248,  Mean reward: -3.5, Mean Entropy: 0.0007901053759269416, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1249,  Mean reward: -0.75, Mean Entropy: 0.000691831752192229, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1250,  Mean reward: -2.0, Mean Entropy: 0.0008406335255131125, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1251,  Mean reward: -2.5, Mean Entropy: 0.0007747760391794145, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1252,  Mean reward: -1.0, Mean Entropy: 0.0006547733210027218, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1253,  Mean reward: -1.75, Mean Entropy: 0.0007374052656814456, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1254,  Mean reward: -0.5, Mean Entropy: 0.000752574298530817, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1255,  Mean reward: -2.75, Mean Entropy: 0.0006655617617070675, complete_episode_count: 80.0, Gather time: 0.66s, Train time: 0.96s
Iteration: 1256,  Mean reward: -1.25, Mean Entropy: 0.0008388154092244804, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1257,  Mean reward: -1.0, Mean Entropy: 0.0007245838642120361, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1258,  Mean reward: -2.5, Mean Entropy: 0.0006319691310636699, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1259,  Mean reward: -0.5, Mean Entropy: 0.0007875902811065316, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.19s
Iteration: 1260,  Mean reward: -1.5, Mean Entropy: 0.0007160198292694986, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1261,  Mean reward: -1.75, Mean Entropy: 0.0005759068299084902, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1262,  Mean reward: -1.25, Mean Entropy: 0.000720973766874522, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.97s
Iteration: 1263,  Mean reward: -0.75, Mean Entropy: 0.0007413550047203898, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1264,  Mean reward: -2.75, Mean Entropy: 0.0006264702533371747, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.96s
Iteration: 1265,  Mean reward: -1.25, Mean Entropy: 0.0007753562531434, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1266,  Mean reward: -3.75, Mean Entropy: 0.0007389627280645072, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1267,  Mean reward: -1.0, Mean Entropy: 0.0006096379365772009, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1268,  Mean reward: -3.5, Mean Entropy: 0.0007256150711327791, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1269,  Mean reward: -1.0, Mean Entropy: 0.0007936264737509191, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1270,  Mean reward: -4.5, Mean Entropy: 0.0007034636801108718, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1271,  Mean reward: -2.5, Mean Entropy: 0.0009097952279262245, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1272,  Mean reward: -2.25, Mean Entropy: 0.0008195751579478383, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1273,  Mean reward: -1.25, Mean Entropy: 0.0007025777595117688, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1274,  Mean reward: -2.25, Mean Entropy: 0.0006488943472504616, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1275,  Mean reward: -2.75, Mean Entropy: 0.0005844763945788145, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1276,  Mean reward: -1.75, Mean Entropy: 0.0004418531316332519, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1277,  Mean reward: -3.0, Mean Entropy: 0.0004587833536788821, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1278,  Mean reward: -3.0, Mean Entropy: 0.000421479984652251, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1279,  Mean reward: -1.5, Mean Entropy: 0.0004061940999235958, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1280,  Mean reward: -2.0, Mean Entropy: 0.00040590623393654823, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1281,  Mean reward: -4.0, Mean Entropy: 0.0005142503068782389, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1282,  Mean reward: -0.75, Mean Entropy: 0.00039027229649946094, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1283,  Mean reward: -3.0, Mean Entropy: 0.0005258426535874605, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1284,  Mean reward: -0.75, Mean Entropy: 0.0004971277667209506, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1285,  Mean reward: -1.0, Mean Entropy: 0.0004468172264751047, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1286,  Mean reward: -2.5, Mean Entropy: 0.0005259402096271515, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1287,  Mean reward: -1.5, Mean Entropy: 0.0005381885566748679, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 1288,  Mean reward: -1.0, Mean Entropy: 0.0004465152742341161, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 1289,  Mean reward: -2.75, Mean Entropy: 0.0005415782216005027, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1290,  Mean reward: -3.0, Mean Entropy: 0.0006076309364289045, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1291,  Mean reward: -2.0, Mean Entropy: 0.0004970749141648412, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1292,  Mean reward: -2.25, Mean Entropy: 0.0006205934914760292, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1293,  Mean reward: -2.25, Mean Entropy: 0.0006217873306013644, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1294,  Mean reward: -1.25, Mean Entropy: 0.0005389461293816566, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1295,  Mean reward: -1.25, Mean Entropy: 0.000637242803350091, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1296,  Mean reward: -2.75, Mean Entropy: 0.0006601381464861333, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1297,  Mean reward: -1.75, Mean Entropy: 0.0005287007661536336, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1298,  Mean reward: -1.75, Mean Entropy: 0.0006794200162403286, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1299,  Mean reward: -2.0, Mean Entropy: 0.0006749773165211082, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.15s
Iteration: 1300,  Mean reward: -2.0, Mean Entropy: 0.0005391343147493899, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
rec seq len 2
actor lr 0.0005
Iteration: 1301,  Mean reward: -1.5, Mean Entropy: 0.0006536608561873436, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1302,  Mean reward: -2.5, Mean Entropy: 0.0006731941248290241, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1303,  Mean reward: -1.5, Mean Entropy: 0.0005211815005168319, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1304,  Mean reward: -1.75, Mean Entropy: 0.0006882208399474621, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1305,  Mean reward: -2.25, Mean Entropy: 0.0006723293918184936, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1306,  Mean reward: -3.0, Mean Entropy: 0.0005442664842121303, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1307,  Mean reward: 0.0, Mean Entropy: 0.0006653978489339352, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 1308,  Mean reward: -1.25, Mean Entropy: 0.0006394623778760433, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1309,  Mean reward: -1.25, Mean Entropy: 0.0005146076437085867, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1310,  Mean reward: 1.0, Mean Entropy: 0.000596950703766197, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1311,  Mean reward: -2.0, Mean Entropy: 0.0006279045483097434, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1312,  Mean reward: -1.75, Mean Entropy: 0.00048203099868260324, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1313,  Mean reward: -3.75, Mean Entropy: 0.000620263977907598, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1314,  Mean reward: -3.25, Mean Entropy: 0.0005943628493696451, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1315,  Mean reward: -1.0, Mean Entropy: 0.0004831112455576658, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1316,  Mean reward: -1.0, Mean Entropy: 0.0006020356668159366, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1317,  Mean reward: -1.75, Mean Entropy: 0.0005869977176189423, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1318,  Mean reward: -3.5, Mean Entropy: 0.0004937985213473439, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1319,  Mean reward: -1.25, Mean Entropy: 0.0006079892627894878, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1320,  Mean reward: -2.5, Mean Entropy: 0.0005536925746127963, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1321,  Mean reward: -2.5, Mean Entropy: 0.0004931548028253019, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1322,  Mean reward: -1.639240506329114, Mean Entropy: 0.0006504574557766318, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1323,  Mean reward: -2.5, Mean Entropy: 0.00021570918033830822, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1324,  Mean reward: -2.25, Mean Entropy: 0.0001915195898618549, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1325,  Mean reward: -2.0, Mean Entropy: 0.000250593846431002, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1326,  Mean reward: -4.75, Mean Entropy: 0.0002903824206441641, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1327,  Mean reward: -3.0, Mean Entropy: 0.0003899098373949528, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1328,  Mean reward: -3.25, Mean Entropy: 0.00035045560798607767, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1329,  Mean reward: -1.75, Mean Entropy: 0.00044317645370028913, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1330,  Mean reward: -4.25, Mean Entropy: 0.0005738926702179015, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1331,  Mean reward: -2.25, Mean Entropy: 0.001388070173561573, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1332,  Mean reward: -1.5, Mean Entropy: 0.0014765638625249267, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1333,  Mean reward: -5.5, Mean Entropy: 0.0015054179821163416, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1334,  Mean reward: -2.0, Mean Entropy: 0.0033397432416677475, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.95s
Iteration: 1335,  Mean reward: -1.25, Mean Entropy: 0.003438867162913084, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1336,  Mean reward: -0.5, Mean Entropy: 0.04994640871882439, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1337,  Mean reward: -1.3896103896103895, Mean Entropy: 0.029007313773036003, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1338,  Mean reward: -1.3860759493670887, Mean Entropy: 0.0012108415830880404, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1339,  Mean reward: -3.5, Mean Entropy: 0.00022732693469151855, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1340,  Mean reward: -2.75, Mean Entropy: 0.00011894786439370364, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1341,  Mean reward: -2.0, Mean Entropy: 0.0001641830604057759, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1342,  Mean reward: -0.75, Mean Entropy: 0.00020005504484288394, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.16s
Iteration: 1343,  Mean reward: -2.75, Mean Entropy: 0.00018073906539939344, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 1344,  Mean reward: -0.75, Mean Entropy: 0.0002282306959386915, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 1345,  Mean reward: -2.25, Mean Entropy: 0.0002378778299316764, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1346,  Mean reward: -2.5, Mean Entropy: 0.00019582609820645303, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1347,  Mean reward: -1.25, Mean Entropy: 0.00021911392104811966, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1348,  Mean reward: 0.0, Mean Entropy: 0.00022916283342055976, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1349,  Mean reward: 0.25, Mean Entropy: 0.00020182362641207874, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1350,  Mean reward: -2.25, Mean Entropy: 0.00024329370353370905, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1351,  Mean reward: 0.75, Mean Entropy: 0.00023958375095389783, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1352,  Mean reward: -0.25, Mean Entropy: 0.000206075725145638, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1353,  Mean reward: -1.25, Mean Entropy: 0.0002583178284112364, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1354,  Mean reward: -0.5, Mean Entropy: 0.00026084401179105043, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1355,  Mean reward: -2.75, Mean Entropy: 0.00022465165238827467, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1356,  Mean reward: -3.25, Mean Entropy: 0.0002774848835542798, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1357,  Mean reward: -2.25, Mean Entropy: 0.0002834719489328563, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1358,  Mean reward: -2.5, Mean Entropy: 0.0002260434557683766, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1359,  Mean reward: -0.25, Mean Entropy: 0.0002759980270639062, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1360,  Mean reward: -3.0, Mean Entropy: 0.0003111859259661287, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1361,  Mean reward: -3.5, Mean Entropy: 0.00025862010079436004, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1362,  Mean reward: -0.75, Mean Entropy: 0.0002887706214096397, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1363,  Mean reward: -0.5, Mean Entropy: 0.00035600943374447525, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1364,  Mean reward: -2.75, Mean Entropy: 0.00033049145713448524, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1365,  Mean reward: -0.25, Mean Entropy: 0.0002802824601531029, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1366,  Mean reward: -1.5, Mean Entropy: 0.0003039457369595766, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1367,  Mean reward: 1.0, Mean Entropy: 0.00043363842996768653, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1368,  Mean reward: -0.75, Mean Entropy: 0.0005051352782174945, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1369,  Mean reward: -2.75, Mean Entropy: 0.0003659114008769393, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1370,  Mean reward: -1.75, Mean Entropy: 0.00043191350414417684, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.95s
Iteration: 1371,  Mean reward: -0.5, Mean Entropy: 0.000419351679738611, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1372,  Mean reward: -3.25, Mean Entropy: 0.00037464097840711474, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1373,  Mean reward: -3.25, Mean Entropy: 0.00029353887657634914, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.99s
Iteration: 1374,  Mean reward: -2.25, Mean Entropy: 0.0003401752037461847, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1375,  Mean reward: -1.75, Mean Entropy: 0.0004161035758443177, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1376,  Mean reward: -2.75, Mean Entropy: 0.0003952584811486304, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1377,  Mean reward: -1.5, Mean Entropy: 0.0003292364999651909, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1378,  Mean reward: -1.75, Mean Entropy: 0.0003567540261428803, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1379,  Mean reward: -0.75, Mean Entropy: 0.000358152377884835, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1380,  Mean reward: -1.75, Mean Entropy: 0.0002963861043099314, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1381,  Mean reward: -3.0, Mean Entropy: 0.0003157525206916034, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1382,  Mean reward: -0.5, Mean Entropy: 0.0002975827956106514, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1383,  Mean reward: -1.75, Mean Entropy: 0.00033822376281023026, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 1384,  Mean reward: -2.0, Mean Entropy: 0.0002807946875691414, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1385,  Mean reward: -1.75, Mean Entropy: 0.00031184457475319505, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.11s
Iteration: 1386,  Mean reward: -1.5, Mean Entropy: 0.00029298593290150166, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1387,  Mean reward: -3.25, Mean Entropy: 0.0003349161415826529, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 1388,  Mean reward: -2.25, Mean Entropy: 0.000308479240629822, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1389,  Mean reward: -0.75, Mean Entropy: 0.0003599229676183313, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1390,  Mean reward: -1.5, Mean Entropy: 0.00030579950544051826, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1391,  Mean reward: -3.25, Mean Entropy: 0.00033684258232824504, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1392,  Mean reward: -2.5, Mean Entropy: 0.0002953395014628768, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 1393,  Mean reward: -2.75, Mean Entropy: 0.00032676238333806396, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1394,  Mean reward: -3.0, Mean Entropy: 0.0002789630671031773, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.00s
Iteration: 1395,  Mean reward: -2.0, Mean Entropy: 0.00030124650220386684, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1396,  Mean reward: -3.5, Mean Entropy: 0.0002681957557797432, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1397,  Mean reward: 0.0, Mean Entropy: 0.00030915456591174006, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1398,  Mean reward: -2.5, Mean Entropy: 0.0002738884650170803, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1399,  Mean reward: -2.25, Mean Entropy: 0.0003213291638530791, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1400,  Mean reward: -2.25, Mean Entropy: 0.00029682929744012654, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
rec seq len 2
actor lr 0.0005
Iteration: 1401,  Mean reward: -1.0, Mean Entropy: 0.00033559949952177703, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 1402,  Mean reward: -3.0, Mean Entropy: 0.0002903294225689024, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1403,  Mean reward: -2.75, Mean Entropy: 0.0003258095239289105, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1404,  Mean reward: -3.25, Mean Entropy: 0.00030841765692457557, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1405,  Mean reward: -0.5, Mean Entropy: 0.00039913476211950183, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 1406,  Mean reward: -3.25, Mean Entropy: 0.0003720491658896208, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1407,  Mean reward: -2.5, Mean Entropy: 0.00045563007006421685, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1408,  Mean reward: -2.0, Mean Entropy: 0.0005260233301669359, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1409,  Mean reward: -2.75, Mean Entropy: 0.0006727999425493181, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1410,  Mean reward: -3.25, Mean Entropy: 0.0005571051733568311, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1411,  Mean reward: -3.5, Mean Entropy: 0.0006248106947168708, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1412,  Mean reward: -1.5, Mean Entropy: 0.000640081474557519, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.98s
Iteration: 1413,  Mean reward: -2.25, Mean Entropy: 0.0007054493762552738, complete_episode_count: 80.0, Gather time: 0.65s, Train time: 0.96s
Iteration: 1414,  Mean reward: -0.75, Mean Entropy: 0.0005601858720183372, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.96s
Iteration: 1415,  Mean reward: -2.0, Mean Entropy: 0.0006208221893757582, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1416,  Mean reward: -2.5, Mean Entropy: 0.000427587132435292, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1417,  Mean reward: -0.75, Mean Entropy: 0.00048154755495488644, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1418,  Mean reward: -1.75, Mean Entropy: 0.00039027276216074824, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1419,  Mean reward: -3.0, Mean Entropy: 0.0004259039997123182, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1420,  Mean reward: -2.25, Mean Entropy: 0.0004091713926754892, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1421,  Mean reward: -2.0, Mean Entropy: 0.0005808976711705327, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1422,  Mean reward: -2.0, Mean Entropy: 0.0004916546167805791, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1423,  Mean reward: -2.75, Mean Entropy: 0.0005499409744516015, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 1424,  Mean reward: 0.0, Mean Entropy: 0.0005178459687158465, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1425,  Mean reward: -1.5, Mean Entropy: 0.0005158426938578486, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1426,  Mean reward: -5.0, Mean Entropy: 0.0004014357109554112, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1427,  Mean reward: 0.0, Mean Entropy: 0.0006329526077024639, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1428,  Mean reward: -1.5, Mean Entropy: 0.0007204498979263008, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1429,  Mean reward: -1.75, Mean Entropy: 0.0010941531509160995, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1430,  Mean reward: -1.25, Mean Entropy: 0.00321235042065382, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1431,  Mean reward: -1.25, Mean Entropy: 0.0027277811896055937, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1432,  Mean reward: 0.75, Mean Entropy: 0.002478665439411998, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.04s
Iteration: 1433,  Mean reward: -2.75, Mean Entropy: 0.0017120088450610638, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1434,  Mean reward: -2.0, Mean Entropy: 0.0008868805598467588, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 1435,  Mean reward: -2.5, Mean Entropy: 0.0004819168243557215, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1436,  Mean reward: -1.0, Mean Entropy: 0.00043184321839362383, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1437,  Mean reward: -3.0, Mean Entropy: 0.0003242880920879543, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1438,  Mean reward: -3.5, Mean Entropy: 0.00036699240445159376, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 1439,  Mean reward: -1.25, Mean Entropy: 0.0004483564698603004, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1440,  Mean reward: -0.75, Mean Entropy: 0.0005067536258138716, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1441,  Mean reward: -2.75, Mean Entropy: 0.0004490969586186111, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 1442,  Mean reward: -2.75, Mean Entropy: 0.0005755149759352207, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 1443,  Mean reward: -1.5, Mean Entropy: 0.0006064113695174456, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1444,  Mean reward: -2.25, Mean Entropy: 0.0007089608116075397, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 1445,  Mean reward: -1.5, Mean Entropy: 0.0005956000532023609, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.96s
Iteration: 1446,  Mean reward: -1.25, Mean Entropy: 0.0007663402939215302, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 1447,  Mean reward: -0.75, Mean Entropy: 0.000606885994784534, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 1448,  Mean reward: -3.0, Mean Entropy: 0.0005883132107555866, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1449,  Mean reward: -3.0, Mean Entropy: 0.0004467239778023213, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.96s
Iteration: 1450,  Mean reward: -1.5, Mean Entropy: 0.00044785928912460804, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.94s
Iteration: 1451,  Mean reward: -2.25, Mean Entropy: 0.0004109072033315897, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 1452,  Mean reward: -1.75, Mean Entropy: 0.0005654857959598303, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1453,  Mean reward: -1.25, Mean Entropy: 0.0005351559957489371, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1454,  Mean reward: -1.5, Mean Entropy: 0.0006370351184159517, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1455,  Mean reward: -1.75, Mean Entropy: 0.0006234740721993148, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1456,  Mean reward: -3.25, Mean Entropy: 0.000571424316149205, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1457,  Mean reward: -0.75, Mean Entropy: 0.0005504095461219549, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1458,  Mean reward: -2.5, Mean Entropy: 0.0005726119852624834, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1459,  Mean reward: 0.0, Mean Entropy: 0.0006395347299985588, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1460,  Mean reward: -1.75, Mean Entropy: 0.0012342266272753477, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 1461,  Mean reward: -2.25, Mean Entropy: 0.0016455957666039467, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1462,  Mean reward: -3.0, Mean Entropy: 0.0020288322120904922, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.96s
Iteration: 1463,  Mean reward: -3.5, Mean Entropy: 0.005989432334899902, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1464,  Mean reward: -3.0, Mean Entropy: 0.0030783030670136213, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1465,  Mean reward: 0.3860759493670886, Mean Entropy: 5.4443866247311234e-05, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1466,  Mean reward: -0.25, Mean Entropy: 6.691231101285666e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1467,  Mean reward: -0.25, Mean Entropy: 6.67039566906169e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 1468,  Mean reward: -1.75, Mean Entropy: 7.166687282733619e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 1469,  Mean reward: -0.75, Mean Entropy: 6.0565911553567275e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 1470,  Mean reward: -1.0, Mean Entropy: 6.448350904975086e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1471,  Mean reward: -4.25, Mean Entropy: 9.173293801723048e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1472,  Mean reward: -2.5, Mean Entropy: 7.38402595743537e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1473,  Mean reward: -2.75, Mean Entropy: 9.123674681177363e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 1474,  Mean reward: -2.5, Mean Entropy: 7.083168748067692e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1475,  Mean reward: -2.5, Mean Entropy: 7.301151345018297e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1476,  Mean reward: -2.0, Mean Entropy: 7.711689977440983e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1477,  Mean reward: -0.75, Mean Entropy: 6.626489630434662e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 1478,  Mean reward: -2.5, Mean Entropy: 7.137685315683484e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1479,  Mean reward: -3.0, Mean Entropy: 8.563610026612878e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1480,  Mean reward: -3.25, Mean Entropy: 9.322517144028097e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 1481,  Mean reward: -2.25, Mean Entropy: 9.26467910176143e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1482,  Mean reward: -1.5, Mean Entropy: 6.527379446197301e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1483,  Mean reward: 0.5, Mean Entropy: 4.488530976232141e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1484,  Mean reward: -3.0, Mean Entropy: 7.57248344598338e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1485,  Mean reward: -0.75, Mean Entropy: 5.433477781480178e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 1486,  Mean reward: -1.25, Mean Entropy: 6.907050556037575e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1487,  Mean reward: -1.5, Mean Entropy: 6.96059869369492e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1488,  Mean reward: -0.75, Mean Entropy: 5.8766891015693545e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1489,  Mean reward: -3.5, Mean Entropy: 0.00010176777141168714, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1490,  Mean reward: -1.0, Mean Entropy: 7.351093518082052e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1491,  Mean reward: -1.25, Mean Entropy: 6.001435031066649e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1492,  Mean reward: -1.25, Mean Entropy: 6.440624565584585e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1493,  Mean reward: -1.0, Mean Entropy: 6.994905561441556e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 1.12s
Iteration: 1494,  Mean reward: -0.25, Mean Entropy: 6.490966188721359e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1495,  Mean reward: -3.25, Mean Entropy: 8.462855475954711e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1496,  Mean reward: -1.25, Mean Entropy: 6.352761556627229e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.97s
Iteration: 1497,  Mean reward: -2.75, Mean Entropy: 7.337837450904772e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.96s
Iteration: 1498,  Mean reward: -1.5, Mean Entropy: 6.618615589104593e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1499,  Mean reward: -0.75, Mean Entropy: 6.978183228056878e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1500,  Mean reward: -3.0, Mean Entropy: 7.225993613246828e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
rec seq len 2
actor lr 0.0005
Iteration: 1501,  Mean reward: -2.0, Mean Entropy: 6.162592035252601e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 1502,  Mean reward: -2.75, Mean Entropy: 8.023924601729959e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1503,  Mean reward: -0.25, Mean Entropy: 6.17968471487984e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 1504,  Mean reward: -0.25, Mean Entropy: 6.243961252039298e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1505,  Mean reward: -1.25, Mean Entropy: 7.423624629154801e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 1506,  Mean reward: -2.5, Mean Entropy: 8.11786885606125e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1507,  Mean reward: -1.0, Mean Entropy: 7.144579285522923e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1508,  Mean reward: -3.0, Mean Entropy: 9.515888814348727e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 1509,  Mean reward: -3.25, Mean Entropy: 9.359613613924012e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Iteration: 1510,  Mean reward: -3.0, Mean Entropy: 9.230147406924516e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1511,  Mean reward: -3.5, Mean Entropy: 9.419387060916051e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 1512,  Mean reward: -2.25, Mean Entropy: 8.834324398776516e-05, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.95s
Iteration: 1513,  Mean reward: -3.75, Mean Entropy: 0.00010170412133447826, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.97s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -3.352272727272727, Mean Entropy: 0.9386367797851562, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.86s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.440476190476191, Mean Entropy: 0.9602975845336914, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 2,  Mean reward: -4.230769230769231, Mean Entropy: 0.9025354385375977, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 3,  Mean reward: -5.6, Mean Entropy: 0.8375527262687683, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.90s
Iteration: 4,  Mean reward: -6.0, Mean Entropy: 0.9386324882507324, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 5,  Mean reward: -4.802631578947368, Mean Entropy: 0.9530765414237976, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 6,  Mean reward: -4.9523809523809526, Mean Entropy: 0.9458569884300232, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 7,  Mean reward: -2.8292682926829267, Mean Entropy: 0.9602975845336914, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 8,  Mean reward: -3.573170731707317, Mean Entropy: 0.8736540675163269, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 9,  Mean reward: -5.177777777777778, Mean Entropy: 0.9458565711975098, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 10,  Mean reward: -4.329545454545454, Mean Entropy: 0.9241958856582642, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 11,  Mean reward: -5.273809523809524, Mean Entropy: 0.9891786575317383, complete_episode_count: 42.0, Gather time: 0.66s, Train time: 1.88s
Iteration: 12,  Mean reward: -3.0238095238095237, Mean Entropy: 0.9097554087638855, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.88s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 13,  Mean reward: -0.5952380952380952, Mean Entropy: 0.9241880774497986, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 14,  Mean reward: -3.953488372093023, Mean Entropy: 0.9241682887077332, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 15,  Mean reward: -5.121951219512195, Mean Entropy: 0.9602717161178589, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 16,  Mean reward: -5.535714285714286, Mean Entropy: 0.9241721630096436, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 17,  Mean reward: -3.8, Mean Entropy: 0.9457989931106567, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.88s
Iteration: 18,  Mean reward: -5.211111111111111, Mean Entropy: 0.9674214124679565, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 19,  Mean reward: -1.8414634146341464, Mean Entropy: 0.9817575216293335, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 20,  Mean reward: -3.0875, Mean Entropy: 0.9312208890914917, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 21,  Mean reward: -3.4468085106382977, Mean Entropy: 0.9888020753860474, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 22,  Mean reward: -5.238095238095238, Mean Entropy: 0.9093803763389587, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 2.06s
Iteration: 23,  Mean reward: -6.5, Mean Entropy: 0.9885836839675903, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 24,  Mean reward: -2.453488372093023, Mean Entropy: 0.9309203624725342, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 25,  Mean reward: -6.583333333333333, Mean Entropy: 0.96688312292099, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 26,  Mean reward: -6.454545454545454, Mean Entropy: 1.0171515941619873, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 27,  Mean reward: -4.7023809523809526, Mean Entropy: 1.002955436706543, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 28,  Mean reward: -5.345238095238095, Mean Entropy: 0.9163559675216675, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 29,  Mean reward: -4.344444444444444, Mean Entropy: 0.9452520608901978, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 30,  Mean reward: -3.813953488372093, Mean Entropy: 1.002802848815918, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 31,  Mean reward: -6.195121951219512, Mean Entropy: 0.9161204099655151, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 32,  Mean reward: -3.7976190476190474, Mean Entropy: 0.8946143388748169, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 33,  Mean reward: -2.7333333333333334, Mean Entropy: 0.8873245716094971, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 34,  Mean reward: -3.011904761904762, Mean Entropy: 0.9664977788925171, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 35,  Mean reward: -2.775, Mean Entropy: 0.9082739353179932, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 36,  Mean reward: -6.0, Mean Entropy: 0.9728372097015381, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.82s
Iteration: 37,  Mean reward: -3.7045454545454546, Mean Entropy: 1.0088739395141602, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 38,  Mean reward: -4.877777777777778, Mean Entropy: 0.9500352740287781, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 39,  Mean reward: -3.13953488372093, Mean Entropy: 0.8998488783836365, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 40,  Mean reward: -1.9659090909090908, Mean Entropy: 0.958875834941864, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 41,  Mean reward: -2.2560975609756095, Mean Entropy: 0.9448821544647217, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 42,  Mean reward: -5.2444444444444445, Mean Entropy: 0.9851905703544617, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 43,  Mean reward: -3.909090909090909, Mean Entropy: 0.9521676301956177, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 44,  Mean reward: -4.858695652173913, Mean Entropy: 0.8971352577209473, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 45,  Mean reward: -3.966666666666667, Mean Entropy: 0.892586886882782, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 46,  Mean reward: -2.9166666666666665, Mean Entropy: 0.8995614051818848, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 47,  Mean reward: -4.7976190476190474, Mean Entropy: 0.8407441973686218, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 48,  Mean reward: -3.8295454545454546, Mean Entropy: 0.9208365678787231, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 49,  Mean reward: -5.138297872340425, Mean Entropy: 0.9627010822296143, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 50,  Mean reward: -6.238095238095238, Mean Entropy: 0.9347135424613953, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 51,  Mean reward: -3.4204545454545454, Mean Entropy: 0.967627227306366, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 52,  Mean reward: -6.416666666666667, Mean Entropy: 0.8718128204345703, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 53,  Mean reward: -4.034883720930233, Mean Entropy: 0.9370399713516235, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 54,  Mean reward: -4.988888888888889, Mean Entropy: 0.9440212249755859, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 55,  Mean reward: -3.813953488372093, Mean Entropy: 0.8032757043838501, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 56,  Mean reward: -4.2727272727272725, Mean Entropy: 0.8616070747375488, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 2.03s
Iteration: 57,  Mean reward: -1.26, Mean Entropy: 0.9315211772918701, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 58,  Mean reward: -3.9555555555555557, Mean Entropy: 0.8725570440292358, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 59,  Mean reward: -2.65625, Mean Entropy: 0.9337815642356873, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 60,  Mean reward: -1.16, Mean Entropy: 0.9516283869743347, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 61,  Mean reward: -3.0638297872340425, Mean Entropy: 0.8819214701652527, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 62,  Mean reward: -4.184782608695652, Mean Entropy: 0.9114334583282471, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 63,  Mean reward: -3.5681818181818183, Mean Entropy: 0.9112721681594849, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 64,  Mean reward: -2.423076923076923, Mean Entropy: 0.8902291059494019, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 65,  Mean reward: -3.0288461538461537, Mean Entropy: 0.795573890209198, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 66,  Mean reward: -3.9468085106382977, Mean Entropy: 0.7187584638595581, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 67,  Mean reward: -1.7830188679245282, Mean Entropy: 0.7723020911216736, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 68,  Mean reward: -3.336734693877551, Mean Entropy: 0.8693784475326538, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 69,  Mean reward: -2.625, Mean Entropy: 0.7834852933883667, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 70,  Mean reward: -3.018181818181818, Mean Entropy: 0.730352520942688, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 71,  Mean reward: -2.8125, Mean Entropy: 0.7690016627311707, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 72,  Mean reward: -5.364583333333333, Mean Entropy: 0.6972355842590332, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 73,  Mean reward: -2.8389830508474576, Mean Entropy: 0.6942374110221863, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 74,  Mean reward: -4.175925925925926, Mean Entropy: 0.6419665813446045, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 75,  Mean reward: -2.711864406779661, Mean Entropy: 0.6918665766716003, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 76,  Mean reward: -1.5887096774193548, Mean Entropy: 0.7317811250686646, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 77,  Mean reward: -3.169491525423729, Mean Entropy: 0.6551918387413025, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 78,  Mean reward: -3.135593220338983, Mean Entropy: 0.49013751745224, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 79,  Mean reward: -5.611940298507463, Mean Entropy: 0.34824737906455994, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 80,  Mean reward: -3.242857142857143, Mean Entropy: 0.26700854301452637, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 81,  Mean reward: -1.5133333333333334, Mean Entropy: 0.4054437577724457, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 82,  Mean reward: -1.9357142857142857, Mean Entropy: 0.7511054277420044, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 83,  Mean reward: -2.4711538461538463, Mean Entropy: 0.9702467918395996, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 84,  Mean reward: -5.22093023255814, Mean Entropy: 0.924843430519104, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 85,  Mean reward: -1.9880952380952381, Mean Entropy: 0.9433943629264832, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 86,  Mean reward: -0.8902439024390244, Mean Entropy: 0.9436046481132507, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 87,  Mean reward: -3.8625, Mean Entropy: 0.6924189329147339, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 88,  Mean reward: -9.23469387755102, Mean Entropy: 0.8356657028198242, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 89,  Mean reward: -7.204545454545454, Mean Entropy: 0.8232576251029968, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 90,  Mean reward: -4.5181818181818185, Mean Entropy: 0.8523762226104736, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 2.04s
Iteration: 91,  Mean reward: -7.037037037037037, Mean Entropy: 0.6129467487335205, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.88s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 92,  Mean reward: -0.3515625, Mean Entropy: 0.6659697890281677, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.96s
Iteration: 93,  Mean reward: -3.490566037735849, Mean Entropy: 0.856025218963623, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 94,  Mean reward: -6.622222222222222, Mean Entropy: 0.7912662029266357, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 95,  Mean reward: -4.732758620689655, Mean Entropy: 0.8006765842437744, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 96,  Mean reward: -5.184210526315789, Mean Entropy: 0.8579378128051758, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 97,  Mean reward: -5.12962962962963, Mean Entropy: 0.8174301981925964, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 98,  Mean reward: -3.306122448979592, Mean Entropy: 0.8088511824607849, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 99,  Mean reward: -1.320754716981132, Mean Entropy: 0.6579077243804932, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 100,  Mean reward: 3.7291666666666665, Mean Entropy: 0.5712587237358093, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.89s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 3.1145833333333335, Mean Entropy: 0.6675085425376892, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 102,  Mean reward: 3.89, Mean Entropy: 0.6308705806732178, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.84s
Iteration: 103,  Mean reward: 3.193877551020408, Mean Entropy: 0.7038663625717163, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 104,  Mean reward: 2.433333333333333, Mean Entropy: 0.6938106417655945, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.87s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 105,  Mean reward: 4.355769230769231, Mean Entropy: 0.5881263017654419, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.86s
Iteration: 106,  Mean reward: 4.283018867924528, Mean Entropy: 0.6486412286758423, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 107,  Mean reward: 3.32, Mean Entropy: 0.6170977354049683, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 108,  Mean reward: 4.398148148148148, Mean Entropy: 0.6015617847442627, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.84s
Iteration: 109,  Mean reward: 4.028846153846154, Mean Entropy: 0.7030550837516785, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 110,  Mean reward: 2.607142857142857, Mean Entropy: 0.7744519114494324, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 111,  Mean reward: 4.13265306122449, Mean Entropy: 0.6576811671257019, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 112,  Mean reward: 4.657407407407407, Mean Entropy: 0.605327844619751, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.85s
Iteration: 113,  Mean reward: 4.117647058823529, Mean Entropy: 0.6713849306106567, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 114,  Mean reward: 2.627659574468085, Mean Entropy: 0.6777197122573853, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 115,  Mean reward: 3.7395833333333335, Mean Entropy: 0.6936801671981812, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.88s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 116,  Mean reward: 5.2727272727272725, Mean Entropy: 0.5930647850036621, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.86s
Iteration: 117,  Mean reward: 0.7916666666666666, Mean Entropy: 0.7253429889678955, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 118,  Mean reward: 4.3173076923076925, Mean Entropy: 0.6144170761108398, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 119,  Mean reward: 3.693877551020408, Mean Entropy: 0.6491994857788086, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 120,  Mean reward: 4.392156862745098, Mean Entropy: 0.7161781191825867, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 121,  Mean reward: 3.090909090909091, Mean Entropy: 0.7276965379714966, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 122,  Mean reward: 4.081632653061225, Mean Entropy: 0.6229009032249451, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 123,  Mean reward: 1.221311475409836, Mean Entropy: 0.589673638343811, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 2.01s
Iteration: 124,  Mean reward: -0.62, Mean Entropy: 0.7366292476654053, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 125,  Mean reward: -0.803921568627451, Mean Entropy: 0.6785711050033569, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 2.01s
Iteration: 126,  Mean reward: 0.6274509803921569, Mean Entropy: 0.7486788630485535, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 127,  Mean reward: 3.1666666666666665, Mean Entropy: 0.8452789187431335, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 128,  Mean reward: 1.4509803921568627, Mean Entropy: 0.7101806402206421, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 129,  Mean reward: 1.4519230769230769, Mean Entropy: 0.6779993772506714, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 130,  Mean reward: 1.7, Mean Entropy: 0.7168745994567871, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 131,  Mean reward: 3.033333333333333, Mean Entropy: 0.6016566753387451, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 132,  Mean reward: 3.660377358490566, Mean Entropy: 0.69720858335495, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 133,  Mean reward: -0.75, Mean Entropy: 0.6388732194900513, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.88s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 134,  Mean reward: 5.413793103448276, Mean Entropy: 0.5485444068908691, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.88s
Iteration: 135,  Mean reward: 3.4347826086956523, Mean Entropy: 0.7307013273239136, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 136,  Mean reward: 4.215686274509804, Mean Entropy: 0.7481212019920349, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 137,  Mean reward: 1.0909090909090908, Mean Entropy: 0.6833707094192505, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 138,  Mean reward: 4.0754716981132075, Mean Entropy: 0.5439410209655762, complete_episode_count: 53.0, Gather time: 0.67s, Train time: 1.87s
Iteration: 139,  Mean reward: 3.29, Mean Entropy: 0.5483718514442444, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.90s
Iteration: 140,  Mean reward: 3.0918367346938775, Mean Entropy: 0.6649650931358337, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 141,  Mean reward: 4.42, Mean Entropy: 0.7435171008110046, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 142,  Mean reward: 2.7666666666666666, Mean Entropy: 0.6543120741844177, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 143,  Mean reward: 4.375, Mean Entropy: 0.5924060940742493, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 144,  Mean reward: 3.784313725490196, Mean Entropy: 0.6919931173324585, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.87s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 145,  Mean reward: 5.991071428571429, Mean Entropy: 0.6001397371292114, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.87s
Iteration: 146,  Mean reward: 5.427272727272728, Mean Entropy: 0.5803778171539307, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 147,  Mean reward: 3.85, Mean Entropy: 0.6487082242965698, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 148,  Mean reward: 3.382978723404255, Mean Entropy: 0.7882890701293945, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 149,  Mean reward: 5.291666666666667, Mean Entropy: 0.5600292682647705, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.87s
Iteration: 150,  Mean reward: 3.683673469387755, Mean Entropy: 0.629767894744873, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 151,  Mean reward: 3.75, Mean Entropy: 0.7161673307418823, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 152,  Mean reward: 3.88, Mean Entropy: 0.7445060610771179, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 153,  Mean reward: 2.561224489795918, Mean Entropy: 0.7025549411773682, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 154,  Mean reward: 2.67, Mean Entropy: 0.6037721633911133, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 155,  Mean reward: -1.2954545454545454, Mean Entropy: 0.6867637634277344, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.88s
Iteration: 156,  Mean reward: 0.7551020408163265, Mean Entropy: 0.6732503771781921, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 157,  Mean reward: 2.3085106382978724, Mean Entropy: 0.7295207381248474, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 2.04s
Iteration: 158,  Mean reward: 2.9313725490196076, Mean Entropy: 0.6768752932548523, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 159,  Mean reward: 2.088235294117647, Mean Entropy: 0.5527874827384949, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 160,  Mean reward: 3.75, Mean Entropy: 0.38499245047569275, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 161,  Mean reward: -0.0546875, Mean Entropy: 0.5306148529052734, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 162,  Mean reward: 0.2786885245901639, Mean Entropy: 0.6480545997619629, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 163,  Mean reward: 2.048076923076923, Mean Entropy: 0.6158996820449829, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 164,  Mean reward: 1.5943396226415094, Mean Entropy: 0.6550430655479431, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.89s
Iteration: 165,  Mean reward: 3.173076923076923, Mean Entropy: 0.8005434274673462, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 166,  Mean reward: 3.912280701754386, Mean Entropy: 0.7236446738243103, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.88s
Iteration: 167,  Mean reward: 3.455357142857143, Mean Entropy: 0.687231183052063, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 168,  Mean reward: 2.2181818181818183, Mean Entropy: 0.7469595670700073, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.86s
Iteration: 169,  Mean reward: 1.7410714285714286, Mean Entropy: 0.7061638832092285, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 170,  Mean reward: 3.8207547169811322, Mean Entropy: 0.6486293077468872, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.85s
Iteration: 171,  Mean reward: 2.2641509433962264, Mean Entropy: 0.7397872805595398, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.82s
Iteration: 172,  Mean reward: 1.3775510204081634, Mean Entropy: 0.6212182641029358, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 173,  Mean reward: 2.6530612244897958, Mean Entropy: 0.7342143058776855, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 174,  Mean reward: 3.3846153846153846, Mean Entropy: 0.6622956991195679, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 175,  Mean reward: 1.053191489361702, Mean Entropy: 0.6938029527664185, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 176,  Mean reward: 2.6339285714285716, Mean Entropy: 0.5843638181686401, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 177,  Mean reward: 3.1636363636363636, Mean Entropy: 0.6563639044761658, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 178,  Mean reward: 4.267241379310345, Mean Entropy: 0.7250466346740723, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 179,  Mean reward: 2.5588235294117645, Mean Entropy: 0.7027897834777832, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 180,  Mean reward: 3.2346938775510203, Mean Entropy: 0.6682256460189819, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 181,  Mean reward: 1.9479166666666667, Mean Entropy: 0.6654736995697021, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 182,  Mean reward: 4.548387096774194, Mean Entropy: 0.5114067792892456, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 183,  Mean reward: 3.3653846153846154, Mean Entropy: 0.6029183268547058, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 184,  Mean reward: 6.185483870967742, Mean Entropy: 0.4049907624721527, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.91s
Iteration: 185,  Mean reward: -1.9453125, Mean Entropy: 0.5219464302062988, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.88s
Iteration: 186,  Mean reward: 3.925925925925926, Mean Entropy: 0.3989766538143158, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 187,  Mean reward: -4.05, Mean Entropy: 0.7144486904144287, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 188,  Mean reward: 5.283333333333333, Mean Entropy: 0.5489605069160461, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 189,  Mean reward: 5.370689655172414, Mean Entropy: 0.5192160606384277, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 190,  Mean reward: 4.37962962962963, Mean Entropy: 0.5014833807945251, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 191,  Mean reward: 4.819672131147541, Mean Entropy: 0.5948810577392578, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.94s
Iteration: 192,  Mean reward: 4.016949152542373, Mean Entropy: 0.578559160232544, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 193,  Mean reward: 5.087719298245614, Mean Entropy: 0.4963817000389099, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 194,  Mean reward: 4.7844827586206895, Mean Entropy: 0.5222440361976624, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 195,  Mean reward: 3.544776119402985, Mean Entropy: 0.3747987151145935, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 196,  Mean reward: 5.048076923076923, Mean Entropy: 0.5243784785270691, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 197,  Mean reward: 1.7230769230769232, Mean Entropy: 0.5208746194839478, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 198,  Mean reward: 3.9814814814814814, Mean Entropy: 0.4483315348625183, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 199,  Mean reward: 4.173076923076923, Mean Entropy: 0.5794858336448669, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 200,  Mean reward: 5.52542372881356, Mean Entropy: 0.3940599858760834, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.77s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 5.80327868852459, Mean Entropy: 0.4409174621105194, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 202,  Mean reward: 5.52542372881356, Mean Entropy: 0.4667089879512787, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 203,  Mean reward: 4.827586206896552, Mean Entropy: 0.5747581720352173, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 204,  Mean reward: 4.675925925925926, Mean Entropy: 0.5451571345329285, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 205,  Mean reward: 5.4576271186440675, Mean Entropy: 0.42810338735580444, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 206,  Mean reward: 2.015151515151515, Mean Entropy: 0.34312647581100464, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.88s
Iteration: 207,  Mean reward: 5.267857142857143, Mean Entropy: 0.41658854484558105, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 208,  Mean reward: 6.769230769230769, Mean Entropy: 0.3245554566383362, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.90s
Iteration: 209,  Mean reward: 5.408333333333333, Mean Entropy: 0.40138980746269226, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 210,  Mean reward: 5.093220338983051, Mean Entropy: 0.3463461399078369, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 211,  Mean reward: 5.172413793103448, Mean Entropy: 0.5092014074325562, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 212,  Mean reward: 4.223214285714286, Mean Entropy: 0.4480289816856384, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 213,  Mean reward: 5.353448275862069, Mean Entropy: 0.40884503722190857, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 214,  Mean reward: 5.008928571428571, Mean Entropy: 0.40085506439208984, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 215,  Mean reward: 5.219298245614035, Mean Entropy: 0.2511664032936096, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 216,  Mean reward: 4.6454545454545455, Mean Entropy: 0.40331798791885376, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 217,  Mean reward: 5.0, Mean Entropy: 0.40439534187316895, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 218,  Mean reward: 5.766666666666667, Mean Entropy: 0.4420875012874603, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 219,  Mean reward: 5.154545454545454, Mean Entropy: 0.5272891521453857, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 220,  Mean reward: 5.379310344827586, Mean Entropy: 0.4859180152416229, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 221,  Mean reward: 2.8157894736842106, Mean Entropy: 0.6705073118209839, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 222,  Mean reward: 4.359649122807017, Mean Entropy: 0.558648943901062, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 223,  Mean reward: 5.940677966101695, Mean Entropy: 0.49159860610961914, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 224,  Mean reward: 2.99, Mean Entropy: 0.6055011749267578, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.94s
Iteration: 225,  Mean reward: 4.349056603773585, Mean Entropy: 0.586533784866333, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.81s
Iteration: 226,  Mean reward: 4.362745098039215, Mean Entropy: 0.5351149439811707, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 227,  Mean reward: 5.767241379310345, Mean Entropy: 0.4849291741847992, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 228,  Mean reward: 4.990909090909091, Mean Entropy: 0.4967597424983978, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 229,  Mean reward: 5.7844827586206895, Mean Entropy: 0.5230708122253418, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 230,  Mean reward: 5.7631578947368425, Mean Entropy: 0.43956512212753296, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 231,  Mean reward: 6.23015873015873, Mean Entropy: 0.3852806091308594, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 232,  Mean reward: 5.090909090909091, Mean Entropy: 0.49554532766342163, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.80s
Iteration: 233,  Mean reward: 5.9, Mean Entropy: 0.43835192918777466, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 234,  Mean reward: 5.043859649122807, Mean Entropy: 0.603915810585022, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 235,  Mean reward: 5.383333333333334, Mean Entropy: 0.4978712499141693, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 236,  Mean reward: 5.416666666666667, Mean Entropy: 0.45099109411239624, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 237,  Mean reward: 5.431034482758621, Mean Entropy: 0.42350637912750244, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 238,  Mean reward: 4.633928571428571, Mean Entropy: 0.4610385596752167, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 239,  Mean reward: 4.685185185185185, Mean Entropy: 0.607368528842926, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 240,  Mean reward: 5.5, Mean Entropy: 0.413829505443573, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 241,  Mean reward: 5.677966101694915, Mean Entropy: 0.42369741201400757, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 242,  Mean reward: 5.603448275862069, Mean Entropy: 0.4438285529613495, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.80s
Iteration: 243,  Mean reward: 4.886792452830188, Mean Entropy: 0.49284276366233826, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 244,  Mean reward: 5.796610169491525, Mean Entropy: 0.46327337622642517, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 245,  Mean reward: 6.11864406779661, Mean Entropy: 0.49940556287765503, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 246,  Mean reward: 4.027777777777778, Mean Entropy: 0.6607755422592163, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 247,  Mean reward: 3.2314814814814814, Mean Entropy: 0.6183536052703857, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 248,  Mean reward: 3.4270833333333335, Mean Entropy: 0.7060503959655762, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 249,  Mean reward: 1.641304347826087, Mean Entropy: 0.7007567882537842, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 250,  Mean reward: 5.2, Mean Entropy: 0.37864112854003906, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 251,  Mean reward: 5.577586206896552, Mean Entropy: 0.4168148636817932, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 252,  Mean reward: 5.991525423728813, Mean Entropy: 0.5103082060813904, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 253,  Mean reward: 6.142857142857143, Mean Entropy: 0.34574609994888306, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 254,  Mean reward: 5.087719298245614, Mean Entropy: 0.4151517152786255, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 255,  Mean reward: 5.111111111111111, Mean Entropy: 0.4248277544975281, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 256,  Mean reward: 5.5, Mean Entropy: 0.5012660026550293, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.95s
Iteration: 257,  Mean reward: 4.394736842105263, Mean Entropy: 0.4613015055656433, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.76s
Iteration: 258,  Mean reward: 6.1484375, Mean Entropy: 0.4052980840206146, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 259,  Mean reward: 4.566037735849057, Mean Entropy: 0.538052499294281, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 260,  Mean reward: 6.407692307692308, Mean Entropy: 0.3369930684566498, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 261,  Mean reward: 5.728070175438597, Mean Entropy: 0.4572436213493347, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 262,  Mean reward: 4.866071428571429, Mean Entropy: 0.4998924136161804, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 263,  Mean reward: 5.231481481481482, Mean Entropy: 0.5008631944656372, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 264,  Mean reward: 4.991379310344827, Mean Entropy: 0.3930566906929016, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 265,  Mean reward: 5.672413793103448, Mean Entropy: 0.3784346580505371, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 266,  Mean reward: 6.4140625, Mean Entropy: 0.45481953024864197, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 267,  Mean reward: 5.370689655172414, Mean Entropy: 0.4887574315071106, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 268,  Mean reward: 4.392156862745098, Mean Entropy: 0.5743421912193298, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 269,  Mean reward: 5.778688524590164, Mean Entropy: 0.44294607639312744, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 270,  Mean reward: 5.083333333333333, Mean Entropy: 0.4081052243709564, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 271,  Mean reward: 4.254545454545455, Mean Entropy: 0.45042070746421814, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.74s
Iteration: 272,  Mean reward: 5.627118644067797, Mean Entropy: 0.4927307963371277, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 273,  Mean reward: 5.161016949152542, Mean Entropy: 0.382685124874115, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 274,  Mean reward: 5.2894736842105265, Mean Entropy: 0.5316749811172485, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 275,  Mean reward: 4.580357142857143, Mean Entropy: 0.43859514594078064, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.80s
Iteration: 276,  Mean reward: 5.719298245614035, Mean Entropy: 0.4927738606929779, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.80s
Iteration: 277,  Mean reward: 5.392857142857143, Mean Entropy: 0.45576411485671997, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 278,  Mean reward: 5.4576271186440675, Mean Entropy: 0.4547145962715149, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.75s
Iteration: 279,  Mean reward: 4.508928571428571, Mean Entropy: 0.4927160441875458, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 280,  Mean reward: 4.754716981132075, Mean Entropy: 0.5408139824867249, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 281,  Mean reward: 5.363636363636363, Mean Entropy: 0.48875460028648376, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 282,  Mean reward: 5.39344262295082, Mean Entropy: 0.3760612905025482, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 283,  Mean reward: 5.093220338983051, Mean Entropy: 0.4315037131309509, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 284,  Mean reward: 4.177083333333333, Mean Entropy: 0.5239052176475525, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 285,  Mean reward: 5.703389830508475, Mean Entropy: 0.5490778684616089, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.80s
Iteration: 286,  Mean reward: 6.063492063492063, Mean Entropy: 0.41148465871810913, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 287,  Mean reward: 5.775, Mean Entropy: 0.46384239196777344, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 288,  Mean reward: 5.934426229508197, Mean Entropy: 0.554027259349823, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.74s
Iteration: 289,  Mean reward: 3.2755102040816326, Mean Entropy: 0.651009738445282, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 290,  Mean reward: 4.435185185185185, Mean Entropy: 0.6011031866073608, complete_episode_count: 54.0, Gather time: 0.69s, Train time: 1.78s
Iteration: 291,  Mean reward: 4.861111111111111, Mean Entropy: 0.5952454805374146, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 292,  Mean reward: 4.781818181818182, Mean Entropy: 0.44502806663513184, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 293,  Mean reward: 4.972727272727273, Mean Entropy: 0.4666600227355957, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 294,  Mean reward: 5.915254237288136, Mean Entropy: 0.5310984253883362, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 295,  Mean reward: 5.741379310344827, Mean Entropy: 0.37525326013565063, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.80s
Iteration: 296,  Mean reward: 4.861111111111111, Mean Entropy: 0.4377982020378113, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 297,  Mean reward: 5.342592592592593, Mean Entropy: 0.40208494663238525, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 298,  Mean reward: 5.625, Mean Entropy: 0.4530399441719055, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.73s
Iteration: 299,  Mean reward: 4.518867924528302, Mean Entropy: 0.5750113725662231, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 300,  Mean reward: 5.348484848484849, Mean Entropy: 0.4226151406764984, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.91s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 5.491525423728813, Mean Entropy: 0.41580232977867126, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 302,  Mean reward: 5.745762711864407, Mean Entropy: 0.44860750436782837, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.81s
Iteration: 303,  Mean reward: 6.112903225806452, Mean Entropy: 0.4338362216949463, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 304,  Mean reward: 6.262711864406779, Mean Entropy: 0.5819410085678101, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 305,  Mean reward: 4.3979591836734695, Mean Entropy: 0.6961109638214111, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 306,  Mean reward: 2.925531914893617, Mean Entropy: 0.848331868648529, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 307,  Mean reward: 2.3222222222222224, Mean Entropy: 0.7813335657119751, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.74s
Iteration: 308,  Mean reward: 3.489795918367347, Mean Entropy: 0.49182063341140747, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 309,  Mean reward: 5.982758620689655, Mean Entropy: 0.5609685778617859, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 310,  Mean reward: 5.087719298245614, Mean Entropy: 0.5799357295036316, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 311,  Mean reward: 7.1716417910447765, Mean Entropy: 0.30060455203056335, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.88s
Iteration: 312,  Mean reward: 6.1525423728813555, Mean Entropy: 0.43800628185272217, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 313,  Mean reward: 4.443396226415095, Mean Entropy: 0.4743295907974243, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 314,  Mean reward: 5.482758620689655, Mean Entropy: 0.5442899465560913, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 315,  Mean reward: 4.872727272727273, Mean Entropy: 0.5509744882583618, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 316,  Mean reward: 3.326530612244898, Mean Entropy: 0.5965120792388916, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 317,  Mean reward: 5.052631578947368, Mean Entropy: 0.500805139541626, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 318,  Mean reward: 6.317460317460317, Mean Entropy: 0.3741522431373596, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 319,  Mean reward: 5.422413793103448, Mean Entropy: 0.3904920816421509, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 320,  Mean reward: 5.2155172413793105, Mean Entropy: 0.48594340682029724, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 321,  Mean reward: 5.807017543859649, Mean Entropy: 0.5676484107971191, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 322,  Mean reward: 5.4375, Mean Entropy: 0.5977188348770142, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 323,  Mean reward: 5.508771929824562, Mean Entropy: 0.6032825708389282, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.96s
Iteration: 324,  Mean reward: 6.203389830508475, Mean Entropy: 0.5129510760307312, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.79s
Iteration: 325,  Mean reward: 4.083333333333333, Mean Entropy: 0.5579542517662048, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 326,  Mean reward: 6.094827586206897, Mean Entropy: 0.4861289858818054, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.78s
Iteration: 327,  Mean reward: 5.228070175438597, Mean Entropy: 0.5937507748603821, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 328,  Mean reward: 4.063492063492063, Mean Entropy: 0.39594727754592896, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 329,  Mean reward: 4.336065573770492, Mean Entropy: 0.40260419249534607, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 330,  Mean reward: 5.849056603773585, Mean Entropy: 0.5965262651443481, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 331,  Mean reward: 4.754901960784314, Mean Entropy: 0.5767186284065247, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 332,  Mean reward: 4.9, Mean Entropy: 0.5337202548980713, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 333,  Mean reward: 4.547169811320755, Mean Entropy: 0.5655645132064819, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.74s
Iteration: 334,  Mean reward: 5.5, Mean Entropy: 0.4631670415401459, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 335,  Mean reward: 4.7, Mean Entropy: 0.4630776047706604, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 336,  Mean reward: 6.232758620689655, Mean Entropy: 0.3806106746196747, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.81s
Iteration: 337,  Mean reward: 5.536363636363636, Mean Entropy: 0.67681884765625, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 338,  Mean reward: 4.333333333333333, Mean Entropy: 0.6926392316818237, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 339,  Mean reward: 5.336206896551724, Mean Entropy: 0.5035256743431091, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 340,  Mean reward: 3.520408163265306, Mean Entropy: 0.5396151542663574, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 341,  Mean reward: 4.935185185185185, Mean Entropy: 0.5288153886795044, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 342,  Mean reward: 4.1923076923076925, Mean Entropy: 0.5367645621299744, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 343,  Mean reward: 3.9607843137254903, Mean Entropy: 0.5307537913322449, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 344,  Mean reward: 5.446428571428571, Mean Entropy: 0.47878241539001465, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 345,  Mean reward: 3.142857142857143, Mean Entropy: 0.6309672594070435, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 346,  Mean reward: 3.710526315789474, Mean Entropy: 0.5414214134216309, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 347,  Mean reward: 5.086538461538462, Mean Entropy: 0.5777580738067627, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 348,  Mean reward: 4.105769230769231, Mean Entropy: 0.6126952171325684, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 349,  Mean reward: 4.34, Mean Entropy: 0.6918624639511108, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 350,  Mean reward: 4.754901960784314, Mean Entropy: 0.6419945955276489, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 351,  Mean reward: 5.508196721311475, Mean Entropy: 0.5189916491508484, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 352,  Mean reward: 5.214285714285714, Mean Entropy: 0.4823550581932068, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 353,  Mean reward: 6.032258064516129, Mean Entropy: 0.346480131149292, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 354,  Mean reward: 5.410714285714286, Mean Entropy: 0.655707597732544, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 355,  Mean reward: 2.8404255319148937, Mean Entropy: 0.6515305638313293, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 356,  Mean reward: 4.62, Mean Entropy: 0.594648003578186, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.91s
Iteration: 357,  Mean reward: 5.018518518518518, Mean Entropy: 0.5328551530838013, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.77s
Iteration: 358,  Mean reward: 3.1770833333333335, Mean Entropy: 0.5720906257629395, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.74s
Iteration: 359,  Mean reward: 6.185483870967742, Mean Entropy: 0.5047177672386169, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 360,  Mean reward: 4.689655172413793, Mean Entropy: 0.5233373045921326, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 361,  Mean reward: 4.020833333333333, Mean Entropy: 0.6058440208435059, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 362,  Mean reward: 3.6020408163265305, Mean Entropy: 0.7068357467651367, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 363,  Mean reward: 5.87719298245614, Mean Entropy: 0.5232284665107727, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 364,  Mean reward: 6.180327868852459, Mean Entropy: 0.5026479959487915, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 365,  Mean reward: 6.611940298507463, Mean Entropy: 0.3705941438674927, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 366,  Mean reward: 5.5, Mean Entropy: 0.5454627871513367, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 367,  Mean reward: 5.716981132075472, Mean Entropy: 0.5721946358680725, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 368,  Mean reward: 6.290322580645161, Mean Entropy: 0.47209954261779785, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 369,  Mean reward: 5.009090909090909, Mean Entropy: 0.5705333948135376, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 370,  Mean reward: 5.375, Mean Entropy: 0.5563468337059021, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 371,  Mean reward: 5.967213114754099, Mean Entropy: 0.4830467700958252, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.82s
Iteration: 372,  Mean reward: 5.594827586206897, Mean Entropy: 0.5371519327163696, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 373,  Mean reward: 3.3469387755102042, Mean Entropy: 0.6671363711357117, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 374,  Mean reward: 4.221153846153846, Mean Entropy: 0.5836591124534607, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 375,  Mean reward: 5.719298245614035, Mean Entropy: 0.5161437392234802, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 376,  Mean reward: 4.240384615384615, Mean Entropy: 0.5921788215637207, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 377,  Mean reward: 5.412280701754386, Mean Entropy: 0.5851380228996277, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 378,  Mean reward: 5.807017543859649, Mean Entropy: 0.5300813317298889, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 379,  Mean reward: 4.203703703703703, Mean Entropy: 0.6260740160942078, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 380,  Mean reward: 4.9818181818181815, Mean Entropy: 0.5534799098968506, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 381,  Mean reward: 5.403225806451613, Mean Entropy: 0.5594767332077026, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 382,  Mean reward: 3.9035087719298245, Mean Entropy: 0.6077183485031128, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 383,  Mean reward: 5.686440677966102, Mean Entropy: 0.4804767966270447, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 384,  Mean reward: 4.407407407407407, Mean Entropy: 0.568534255027771, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 385,  Mean reward: 4.971698113207547, Mean Entropy: 0.626414954662323, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 386,  Mean reward: 5.3, Mean Entropy: 0.572798490524292, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 387,  Mean reward: 5.6, Mean Entropy: 0.4735282063484192, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 388,  Mean reward: 5.105263157894737, Mean Entropy: 0.5172604918479919, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 389,  Mean reward: 5.692982456140351, Mean Entropy: 0.5341771841049194, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.97s
Iteration: 390,  Mean reward: 5.311320754716981, Mean Entropy: 0.4613322615623474, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 391,  Mean reward: 4.618181818181818, Mean Entropy: 0.6041114330291748, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 392,  Mean reward: 5.415254237288136, Mean Entropy: 0.5181922912597656, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 393,  Mean reward: 5.072727272727272, Mean Entropy: 0.5743062496185303, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 394,  Mean reward: 4.764150943396227, Mean Entropy: 0.5829627513885498, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 395,  Mean reward: 5.5423728813559325, Mean Entropy: 0.5664302110671997, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 396,  Mean reward: 5.218181818181818, Mean Entropy: 0.5645073652267456, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 397,  Mean reward: 5.95, Mean Entropy: 0.5400574207305908, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 398,  Mean reward: 4.672727272727273, Mean Entropy: 0.5983850359916687, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 399,  Mean reward: 4.9245283018867925, Mean Entropy: 0.5948411226272583, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 400,  Mean reward: 5.984126984126984, Mean Entropy: 0.4295334815979004, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.89s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 4.605769230769231, Mean Entropy: 0.6617465615272522, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 402,  Mean reward: 4.944444444444445, Mean Entropy: 0.6420706510543823, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 403,  Mean reward: 5.491379310344827, Mean Entropy: 0.5816747546195984, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 404,  Mean reward: 4.215686274509804, Mean Entropy: 0.56147301197052, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 405,  Mean reward: 5.0344827586206895, Mean Entropy: 0.4936385154724121, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 406,  Mean reward: 5.694915254237288, Mean Entropy: 0.5542769432067871, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 407,  Mean reward: 6.4375, Mean Entropy: 0.4785075783729553, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 408,  Mean reward: 6.135593220338983, Mean Entropy: 0.559898316860199, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 409,  Mean reward: 5.194444444444445, Mean Entropy: 0.5935494303703308, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 410,  Mean reward: 5.75, Mean Entropy: 0.47029489278793335, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 411,  Mean reward: 5.827868852459017, Mean Entropy: 0.42402753233909607, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 412,  Mean reward: 3.47, Mean Entropy: 0.6504870653152466, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 413,  Mean reward: 2.375, Mean Entropy: 0.6398243308067322, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 414,  Mean reward: 2.691666666666667, Mean Entropy: 0.6556078791618347, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.78s
Iteration: 415,  Mean reward: 2.865079365079365, Mean Entropy: 0.5716512203216553, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 416,  Mean reward: -0.6129032258064516, Mean Entropy: 0.6567866802215576, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 417,  Mean reward: 1.6637931034482758, Mean Entropy: 0.6737242937088013, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 418,  Mean reward: 1.2410714285714286, Mean Entropy: 0.6719920039176941, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 419,  Mean reward: 3.0901639344262297, Mean Entropy: 0.49010491371154785, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 420,  Mean reward: 4.754545454545455, Mean Entropy: 0.6084301471710205, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 421,  Mean reward: 5.0, Mean Entropy: 0.48553574085235596, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 422,  Mean reward: 5.017543859649122, Mean Entropy: 0.47976988554000854, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.94s
Iteration: 423,  Mean reward: 3.79, Mean Entropy: 0.6876139640808105, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 424,  Mean reward: 6.017241379310345, Mean Entropy: 0.5480632185935974, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 425,  Mean reward: 4.394230769230769, Mean Entropy: 0.5558393001556396, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 426,  Mean reward: 4.872727272727273, Mean Entropy: 0.45521315932273865, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 427,  Mean reward: 5.151785714285714, Mean Entropy: 0.6039853096008301, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 428,  Mean reward: 3.650943396226415, Mean Entropy: 0.6685336828231812, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 429,  Mean reward: 3.3620689655172415, Mean Entropy: 0.6601023077964783, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 430,  Mean reward: 2.1666666666666665, Mean Entropy: 0.6198465824127197, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 431,  Mean reward: 4.087301587301587, Mean Entropy: 0.4250512719154358, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 432,  Mean reward: 0.4803921568627451, Mean Entropy: 0.8105822801589966, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 433,  Mean reward: 3.542372881355932, Mean Entropy: 0.5978519916534424, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 434,  Mean reward: 0.4807692307692308, Mean Entropy: 0.7361173033714294, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 435,  Mean reward: 1.5887096774193548, Mean Entropy: 0.5798107981681824, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 436,  Mean reward: 3.111111111111111, Mean Entropy: 0.5484597682952881, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 437,  Mean reward: 1.5593220338983051, Mean Entropy: 0.760123610496521, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 438,  Mean reward: 2.5847457627118646, Mean Entropy: 0.5125131011009216, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 439,  Mean reward: 4.574074074074074, Mean Entropy: 0.5542775392532349, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 440,  Mean reward: 5.536363636363636, Mean Entropy: 0.5299903154373169, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 441,  Mean reward: 5.330188679245283, Mean Entropy: 0.5201131105422974, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 442,  Mean reward: 5.462962962962963, Mean Entropy: 0.5808582305908203, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 443,  Mean reward: 5.587719298245614, Mean Entropy: 0.5007097125053406, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 444,  Mean reward: 6.3, Mean Entropy: 0.5206466317176819, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 445,  Mean reward: 5.915254237288136, Mean Entropy: 0.5568115711212158, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 446,  Mean reward: 5.2844827586206895, Mean Entropy: 0.5775911808013916, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 447,  Mean reward: 4.132075471698113, Mean Entropy: 0.669454038143158, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 448,  Mean reward: 4.866071428571429, Mean Entropy: 0.5682744383811951, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 449,  Mean reward: 4.754545454545455, Mean Entropy: 0.542431116104126, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 450,  Mean reward: 5.5, Mean Entropy: 0.5095593333244324, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 451,  Mean reward: 4.339622641509434, Mean Entropy: 0.6465283036231995, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 452,  Mean reward: 5.732758620689655, Mean Entropy: 0.4459848403930664, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 453,  Mean reward: 1.2413793103448276, Mean Entropy: 0.6373765468597412, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 454,  Mean reward: 3.1, Mean Entropy: 0.7992396354675293, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 455,  Mean reward: 4.913793103448276, Mean Entropy: 0.47115227580070496, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 456,  Mean reward: 2.561224489795918, Mean Entropy: 0.5686696767807007, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.93s
Iteration: 457,  Mean reward: 3.8333333333333335, Mean Entropy: 0.5753389596939087, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 458,  Mean reward: 2.239130434782609, Mean Entropy: 0.6269952654838562, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 459,  Mean reward: 1.9224137931034482, Mean Entropy: 0.5710647106170654, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 460,  Mean reward: 4.0, Mean Entropy: 0.6441524624824524, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 461,  Mean reward: 4.386792452830188, Mean Entropy: 0.4944121837615967, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 462,  Mean reward: 4.25, Mean Entropy: 0.4558340907096863, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 463,  Mean reward: 4.839285714285714, Mean Entropy: 0.6207441687583923, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 464,  Mean reward: 4.625, Mean Entropy: 0.549081027507782, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 465,  Mean reward: 5.043859649122807, Mean Entropy: 0.5534137487411499, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 466,  Mean reward: 5.959016393442623, Mean Entropy: 0.37904542684555054, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 467,  Mean reward: 5.610169491525424, Mean Entropy: 0.5442345142364502, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 468,  Mean reward: 4.745283018867925, Mean Entropy: 0.5553428530693054, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 469,  Mean reward: 5.453703703703703, Mean Entropy: 0.563654899597168, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 470,  Mean reward: 4.273584905660377, Mean Entropy: 0.6294524073600769, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 471,  Mean reward: 3.9, Mean Entropy: 0.6065363883972168, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 472,  Mean reward: 5.113207547169812, Mean Entropy: 0.4315797686576843, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 473,  Mean reward: 5.291666666666667, Mean Entropy: 0.4571532905101776, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 474,  Mean reward: 5.290909090909091, Mean Entropy: 0.5512497425079346, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 475,  Mean reward: 4.956896551724138, Mean Entropy: 0.5230118036270142, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 476,  Mean reward: 4.982142857142857, Mean Entropy: 0.6122707724571228, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 477,  Mean reward: 5.027777777777778, Mean Entropy: 0.6051568388938904, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 478,  Mean reward: 4.796296296296297, Mean Entropy: 0.6135880947113037, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 479,  Mean reward: 6.584615384615384, Mean Entropy: 0.41459497809410095, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 480,  Mean reward: 4.345454545454546, Mean Entropy: 0.631058931350708, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 481,  Mean reward: 5.790322580645161, Mean Entropy: 0.43486595153808594, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 482,  Mean reward: 5.026785714285714, Mean Entropy: 0.5784165859222412, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 483,  Mean reward: 5.508928571428571, Mean Entropy: 0.5725424885749817, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 484,  Mean reward: 4.9, Mean Entropy: 0.6549025177955627, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 485,  Mean reward: 5.577586206896552, Mean Entropy: 0.51241135597229, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 486,  Mean reward: 6.576923076923077, Mean Entropy: 0.4173118770122528, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 487,  Mean reward: 6.065573770491803, Mean Entropy: 0.47456055879592896, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 488,  Mean reward: 5.294642857142857, Mean Entropy: 0.6220531463623047, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 489,  Mean reward: 4.952830188679245, Mean Entropy: 0.6351099610328674, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.96s
Iteration: 490,  Mean reward: 5.196428571428571, Mean Entropy: 0.5310277938842773, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 491,  Mean reward: 3.8529411764705883, Mean Entropy: 0.6572287082672119, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 492,  Mean reward: 6.258064516129032, Mean Entropy: 0.5089206695556641, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 493,  Mean reward: 5.408333333333333, Mean Entropy: 0.4863852262496948, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 494,  Mean reward: 5.868421052631579, Mean Entropy: 0.5130425691604614, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 495,  Mean reward: 5.155172413793103, Mean Entropy: 0.6147510409355164, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 496,  Mean reward: 4.663636363636364, Mean Entropy: 0.5923941731452942, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 497,  Mean reward: 5.413793103448276, Mean Entropy: 0.5722821950912476, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 498,  Mean reward: 5.983606557377049, Mean Entropy: 0.3771800398826599, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 499,  Mean reward: 5.07258064516129, Mean Entropy: 0.46960803866386414, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 500,  Mean reward: 3.925925925925926, Mean Entropy: 0.5959105491638184, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.76s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 4.990566037735849, Mean Entropy: 0.5362405776977539, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.74s
Iteration: 502,  Mean reward: 4.269230769230769, Mean Entropy: 0.5959197282791138, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 503,  Mean reward: 4.903846153846154, Mean Entropy: 0.5923177003860474, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 504,  Mean reward: 5.298245614035087, Mean Entropy: 0.5487784147262573, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 505,  Mean reward: 4.584905660377358, Mean Entropy: 0.6239253878593445, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.74s
Iteration: 506,  Mean reward: 4.811320754716981, Mean Entropy: 0.5719045400619507, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.73s
Iteration: 507,  Mean reward: 4.625, Mean Entropy: 0.6089812517166138, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 508,  Mean reward: 5.663636363636364, Mean Entropy: 0.58834308385849, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 509,  Mean reward: 4.12037037037037, Mean Entropy: 0.6468009948730469, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 510,  Mean reward: 1.3888888888888888, Mean Entropy: 0.7015719413757324, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 511,  Mean reward: 0.7796610169491526, Mean Entropy: 0.6800163984298706, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 512,  Mean reward: 2.101449275362319, Mean Entropy: 0.6061019897460938, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 513,  Mean reward: 1.6785714285714286, Mean Entropy: 0.6857000589370728, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 514,  Mean reward: 2.3157894736842106, Mean Entropy: 0.600600004196167, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 515,  Mean reward: 1.3333333333333333, Mean Entropy: 0.6290466785430908, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 516,  Mean reward: 2.314516129032258, Mean Entropy: 0.7125253677368164, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 517,  Mean reward: 2.3461538461538463, Mean Entropy: 0.5923980474472046, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 518,  Mean reward: 2.543103448275862, Mean Entropy: 0.6508511900901794, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 519,  Mean reward: 5.767857142857143, Mean Entropy: 0.49342942237854004, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 520,  Mean reward: 1.8369565217391304, Mean Entropy: 0.6622896790504456, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 521,  Mean reward: 1.4152542372881356, Mean Entropy: 0.8300981521606445, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 522,  Mean reward: 2.3852459016393444, Mean Entropy: 0.5364077687263489, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 523,  Mean reward: 1.280701754385965, Mean Entropy: 0.7802654504776001, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.95s
Iteration: 524,  Mean reward: 1.3416666666666666, Mean Entropy: 0.690538763999939, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 525,  Mean reward: 1.9596774193548387, Mean Entropy: 0.7579156160354614, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 526,  Mean reward: 2.236842105263158, Mean Entropy: 0.6238484382629395, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 527,  Mean reward: 3.276923076923077, Mean Entropy: 0.6849832534790039, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 528,  Mean reward: 2.1885245901639343, Mean Entropy: 0.7411601543426514, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 529,  Mean reward: 3.6904761904761907, Mean Entropy: 0.624631404876709, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 530,  Mean reward: 2.2109375, Mean Entropy: 0.6439336538314819, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 531,  Mean reward: 2.2818181818181817, Mean Entropy: 0.6458649635314941, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 532,  Mean reward: 5.422413793103448, Mean Entropy: 0.5922834873199463, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 533,  Mean reward: 3.9411764705882355, Mean Entropy: 0.5928418040275574, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 534,  Mean reward: 5.633333333333334, Mean Entropy: 0.5115272998809814, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 535,  Mean reward: 5.603448275862069, Mean Entropy: 0.5305668115615845, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 536,  Mean reward: 5.672413793103448, Mean Entropy: 0.48937973380088806, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 537,  Mean reward: 4.346153846153846, Mean Entropy: 0.6409481167793274, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 538,  Mean reward: 5.089285714285714, Mean Entropy: 0.6241688132286072, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 539,  Mean reward: 5.733333333333333, Mean Entropy: 0.5548023581504822, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.74s
Iteration: 540,  Mean reward: 5.296610169491525, Mean Entropy: 0.5800342559814453, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 541,  Mean reward: 1.0, Mean Entropy: 0.4954964816570282, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 542,  Mean reward: 4.118181818181818, Mean Entropy: 0.6527293920516968, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 543,  Mean reward: 2.7131147540983607, Mean Entropy: 0.6446983814239502, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 544,  Mean reward: 2.9642857142857144, Mean Entropy: 0.602392315864563, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 545,  Mean reward: 3.8333333333333335, Mean Entropy: 0.6152113676071167, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 546,  Mean reward: -0.008333333333333333, Mean Entropy: 0.6637942790985107, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 547,  Mean reward: 2.211864406779661, Mean Entropy: 0.5681207180023193, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 548,  Mean reward: 3.5961538461538463, Mean Entropy: 0.2041500210762024, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 549,  Mean reward: -1.0757575757575757, Mean Entropy: 0.5865501165390015, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 550,  Mean reward: 3.549019607843137, Mean Entropy: 0.2538369297981262, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 551,  Mean reward: -3.380952380952381, Mean Entropy: 0.3473796844482422, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 552,  Mean reward: -3.610294117647059, Mean Entropy: 0.6616069674491882, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 553,  Mean reward: 4.138888888888889, Mean Entropy: 0.5995853543281555, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 554,  Mean reward: 3.5729166666666665, Mean Entropy: 0.7510389685630798, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 555,  Mean reward: -1.3928571428571428, Mean Entropy: 0.8323773145675659, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 556,  Mean reward: 2.4411764705882355, Mean Entropy: 0.6260887980461121, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 557,  Mean reward: 2.5283018867924527, Mean Entropy: 0.6720541715621948, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.74s
Iteration: 558,  Mean reward: 4.8545454545454545, Mean Entropy: 0.4704941213130951, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 559,  Mean reward: 2.5588235294117645, Mean Entropy: 0.5916918516159058, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 560,  Mean reward: 1.153061224489796, Mean Entropy: 0.6491836309432983, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 561,  Mean reward: 3.51, Mean Entropy: 0.647117555141449, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 562,  Mean reward: 4.694444444444445, Mean Entropy: 0.5962456464767456, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 563,  Mean reward: 5.223214285714286, Mean Entropy: 0.6313281655311584, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 564,  Mean reward: 3.951923076923077, Mean Entropy: 0.5978958010673523, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 565,  Mean reward: 4.34375, Mean Entropy: 0.5516352653503418, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 566,  Mean reward: -2.5681818181818183, Mean Entropy: 0.36309176683425903, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 567,  Mean reward: -0.8955223880597015, Mean Entropy: 0.36979007720947266, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 568,  Mean reward: -1.8059701492537314, Mean Entropy: 0.4173264503479004, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 569,  Mean reward: -2.2573529411764706, Mean Entropy: 0.438237726688385, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 570,  Mean reward: -3.814516129032258, Mean Entropy: 0.39715129137039185, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 1.77s
Iteration: 571,  Mean reward: -1.2, Mean Entropy: 0.37207478284835815, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 572,  Mean reward: -2.0223880597014925, Mean Entropy: 0.3574088513851166, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 573,  Mean reward: -2.746153846153846, Mean Entropy: 0.3559044301509857, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 574,  Mean reward: -2.1791044776119404, Mean Entropy: 0.26338276267051697, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 575,  Mean reward: -1.7253521126760563, Mean Entropy: 0.2935211658477783, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 576,  Mean reward: -3.1901408450704225, Mean Entropy: 0.38510507345199585, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 577,  Mean reward: -1.1458333333333333, Mean Entropy: 0.3035958409309387, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 578,  Mean reward: -1.1126760563380282, Mean Entropy: 0.35855692625045776, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 579,  Mean reward: -1.8805970149253732, Mean Entropy: 0.3323034644126892, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 580,  Mean reward: -2.65, Mean Entropy: 0.3723906874656677, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 581,  Mean reward: -4.363636363636363, Mean Entropy: 0.37000784277915955, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 582,  Mean reward: -2.2357142857142858, Mean Entropy: 0.21543078124523163, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 583,  Mean reward: -2.363013698630137, Mean Entropy: 0.20928162336349487, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 584,  Mean reward: -3.563380281690141, Mean Entropy: 0.19668619334697723, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 585,  Mean reward: -4.344594594594595, Mean Entropy: 0.10432099550962448, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 586,  Mean reward: -2.1265822784810124, Mean Entropy: 0.0754360556602478, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 587,  Mean reward: -1.0128205128205128, Mean Entropy: 0.04212445020675659, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 588,  Mean reward: -0.8896103896103896, Mean Entropy: 0.024376077577471733, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 589,  Mean reward: -1.5, Mean Entropy: 0.01711665466427803, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 590,  Mean reward: -2.5, Mean Entropy: 0.014511844143271446, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 591,  Mean reward: -1.25, Mean Entropy: 0.011038444936275482, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 592,  Mean reward: -1.0, Mean Entropy: 0.010821959003806114, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 593,  Mean reward: -1.5, Mean Entropy: 0.010482687503099442, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 594,  Mean reward: -2.5, Mean Entropy: 0.011229265481233597, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 595,  Mean reward: -1.0, Mean Entropy: 0.009269892238080502, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 596,  Mean reward: -2.0, Mean Entropy: 0.009429890662431717, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 597,  Mean reward: -3.0, Mean Entropy: 0.011014706455171108, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.06s
Iteration: 598,  Mean reward: -2.25, Mean Entropy: 0.00980846956372261, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 599,  Mean reward: -2.5, Mean Entropy: 0.010324258357286453, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 600,  Mean reward: -2.9050632911392404, Mean Entropy: 0.005633372813463211, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.90s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -3.75, Mean Entropy: 0.0040587857365608215, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 602,  Mean reward: -3.0, Mean Entropy: 0.0036461949348449707, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 603,  Mean reward: -1.75, Mean Entropy: 0.002936751116067171, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 604,  Mean reward: -0.25, Mean Entropy: 0.0035207055043429136, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 605,  Mean reward: -2.0, Mean Entropy: 0.0036997301504015923, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 606,  Mean reward: -2.9050632911392404, Mean Entropy: 0.0035173329524695873, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 607,  Mean reward: -2.5, Mean Entropy: 0.00408657593652606, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 608,  Mean reward: -2.5, Mean Entropy: 0.004191435407847166, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 609,  Mean reward: -1.75, Mean Entropy: 0.0032496440690010786, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 610,  Mean reward: -1.25, Mean Entropy: 0.003432726953178644, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 611,  Mean reward: -1.0, Mean Entropy: 0.0028191618621349335, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 612,  Mean reward: -0.25, Mean Entropy: 0.002898363396525383, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 613,  Mean reward: -1.75, Mean Entropy: 0.0030353607144206762, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 614,  Mean reward: -1.5, Mean Entropy: 0.002676998032256961, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 615,  Mean reward: -2.25, Mean Entropy: 0.0029838918708264828, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 616,  Mean reward: -4.5, Mean Entropy: 0.0031182984821498394, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 617,  Mean reward: -2.5, Mean Entropy: 0.002856952603906393, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 618,  Mean reward: -0.25, Mean Entropy: 0.0023910310119390488, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 619,  Mean reward: -2.75, Mean Entropy: 0.0028003219049423933, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 620,  Mean reward: -1.25, Mean Entropy: 0.0026348517276346684, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 621,  Mean reward: -1.75, Mean Entropy: 0.0026324510108679533, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 622,  Mean reward: -1.5, Mean Entropy: 0.0025000746827572584, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 623,  Mean reward: -3.6645569620253164, Mean Entropy: 0.0016940421191975474, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 624,  Mean reward: -3.25, Mean Entropy: 0.0014260042225942016, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 625,  Mean reward: -2.0, Mean Entropy: 0.001345152733847499, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 626,  Mean reward: -1.5, Mean Entropy: 0.0013521581422537565, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 627,  Mean reward: -3.25, Mean Entropy: 0.0013690227642655373, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 628,  Mean reward: -1.75, Mean Entropy: 0.0014067627489566803, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 629,  Mean reward: -4.0, Mean Entropy: 0.0013598864898085594, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 630,  Mean reward: -1.0, Mean Entropy: 0.001461697742342949, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 631,  Mean reward: -2.5, Mean Entropy: 0.001557192299515009, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 632,  Mean reward: -2.0, Mean Entropy: 0.0014695841819047928, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 633,  Mean reward: -2.75, Mean Entropy: 0.001457663020119071, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 634,  Mean reward: -3.25, Mean Entropy: 0.0014944477006793022, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 635,  Mean reward: -2.5, Mean Entropy: 0.001445178291760385, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 636,  Mean reward: -2.5, Mean Entropy: 0.0014574140077456832, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 637,  Mean reward: -1.25, Mean Entropy: 0.0015406308230012655, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.05s
Iteration: 638,  Mean reward: -2.25, Mean Entropy: 0.0014955381629988551, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 639,  Mean reward: -3.0, Mean Entropy: 0.0014537973329424858, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.87s
Iteration: 640,  Mean reward: -2.25, Mean Entropy: 0.0014994374942034483, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 641,  Mean reward: -1.75, Mean Entropy: 0.0014658120926469564, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 642,  Mean reward: -1.5, Mean Entropy: 0.0014642280293628573, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 643,  Mean reward: -0.75, Mean Entropy: 0.0015009157359600067, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 644,  Mean reward: -4.0, Mean Entropy: 0.0014574382221326232, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 645,  Mean reward: -3.25, Mean Entropy: 0.0014509274624288082, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 646,  Mean reward: -2.5, Mean Entropy: 0.0014979973202571273, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 647,  Mean reward: -1.75, Mean Entropy: 0.0014591964427381754, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 648,  Mean reward: -4.25, Mean Entropy: 0.0014604986645281315, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 649,  Mean reward: -2.25, Mean Entropy: 0.0014866463607177138, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 650,  Mean reward: -1.75, Mean Entropy: 0.0014700418105348945, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 651,  Mean reward: -0.75, Mean Entropy: 0.001462683081626892, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 652,  Mean reward: -0.5, Mean Entropy: 0.0014754965668544173, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 653,  Mean reward: -1.25, Mean Entropy: 0.001453961944207549, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 654,  Mean reward: -1.75, Mean Entropy: 0.0014667095383629203, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 655,  Mean reward: -1.25, Mean Entropy: 0.0014909900492057204, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 656,  Mean reward: -2.75, Mean Entropy: 0.0014993061777204275, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 657,  Mean reward: 0.25, Mean Entropy: 0.0014245598576962948, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 658,  Mean reward: -3.25, Mean Entropy: 0.001547055086120963, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 659,  Mean reward: -2.0, Mean Entropy: 0.0015107732033357024, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 660,  Mean reward: -1.0, Mean Entropy: 0.001519306330010295, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 661,  Mean reward: -0.25, Mean Entropy: 0.0015232128789648414, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 662,  Mean reward: -0.75, Mean Entropy: 0.0015099598094820976, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 663,  Mean reward: -3.75, Mean Entropy: 0.0015641482314094901, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 664,  Mean reward: -1.75, Mean Entropy: 0.0015167130623012781, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 665,  Mean reward: -2.75, Mean Entropy: 0.0015609444817528129, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 666,  Mean reward: -2.0, Mean Entropy: 0.0015655141323804855, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 667,  Mean reward: -3.75, Mean Entropy: 0.0016215851064771414, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 668,  Mean reward: -1.75, Mean Entropy: 0.0015558887971565127, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 669,  Mean reward: -0.5, Mean Entropy: 0.001534012844786048, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 670,  Mean reward: -1.75, Mean Entropy: 0.0016337556298822165, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 671,  Mean reward: -2.75, Mean Entropy: 0.0015763761475682259, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 672,  Mean reward: -2.0, Mean Entropy: 0.0015562571352347732, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 673,  Mean reward: -2.5, Mean Entropy: 0.0016060599591583014, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 674,  Mean reward: -2.5, Mean Entropy: 0.0016362285241484642, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 675,  Mean reward: -3.5, Mean Entropy: 0.001652667298913002, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 676,  Mean reward: -1.0, Mean Entropy: 0.0015964014455676079, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 677,  Mean reward: -2.5, Mean Entropy: 0.001711757853627205, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.05s
Iteration: 678,  Mean reward: 0.0, Mean Entropy: 0.0015478499699383974, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 679,  Mean reward: -1.25, Mean Entropy: 0.0016171842580661178, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 680,  Mean reward: -1.25, Mean Entropy: 0.0016387845389544964, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 681,  Mean reward: -2.0, Mean Entropy: 0.0016571064479649067, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 682,  Mean reward: -1.75, Mean Entropy: 0.0017232985701411963, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 683,  Mean reward: -0.75, Mean Entropy: 0.0016729242634028196, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 684,  Mean reward: -2.9050632911392404, Mean Entropy: 0.0009467408526688814, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 685,  Mean reward: -2.5, Mean Entropy: 0.0008160040597431362, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 686,  Mean reward: -0.75, Mean Entropy: 0.0008517494425177574, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 687,  Mean reward: -2.0, Mean Entropy: 0.0008339818450622261, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 688,  Mean reward: -1.75, Mean Entropy: 0.0008890884346328676, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 689,  Mean reward: -1.5, Mean Entropy: 0.0008562218281440437, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 690,  Mean reward: -0.75, Mean Entropy: 0.0008780130883678794, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 691,  Mean reward: -2.25, Mean Entropy: 0.0008806678233668208, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 692,  Mean reward: -2.0, Mean Entropy: 0.0009240463259629905, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 693,  Mean reward: -2.0, Mean Entropy: 0.0008698470774106681, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 694,  Mean reward: 0.75, Mean Entropy: 0.0009643580997362733, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 695,  Mean reward: -1.25, Mean Entropy: 0.0008958880789577961, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 696,  Mean reward: -2.25, Mean Entropy: 0.00085032096831128, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 697,  Mean reward: -1.25, Mean Entropy: 0.00091852608602494, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 698,  Mean reward: 0.0, Mean Entropy: 0.000997854513116181, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 699,  Mean reward: -3.0, Mean Entropy: 0.0008741419296711683, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 700,  Mean reward: -3.0, Mean Entropy: 0.0009236405603587627, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -2.25, Mean Entropy: 0.0008991129579953849, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 702,  Mean reward: -1.75, Mean Entropy: 0.000932516879402101, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 703,  Mean reward: -2.0, Mean Entropy: 0.0008718956960365176, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 704,  Mean reward: -2.75, Mean Entropy: 0.000843153684400022, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 705,  Mean reward: -1.0, Mean Entropy: 0.0009472267120145261, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 706,  Mean reward: -3.75, Mean Entropy: 0.0008978352416306734, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 707,  Mean reward: -1.5, Mean Entropy: 0.0009328682790510356, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 708,  Mean reward: -1.75, Mean Entropy: 0.0009004399180412292, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 709,  Mean reward: -1.5, Mean Entropy: 0.0009774925420060754, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 710,  Mean reward: -2.0, Mean Entropy: 0.0009045799961313605, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 711,  Mean reward: -1.0, Mean Entropy: 0.0010120946681126952, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 712,  Mean reward: -1.5, Mean Entropy: 0.000955477764364332, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 713,  Mean reward: -0.5, Mean Entropy: 0.0009783494751900434, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 714,  Mean reward: -1.25, Mean Entropy: 0.0009948695078492165, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 715,  Mean reward: -1.0, Mean Entropy: 0.0009835839737206697, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 716,  Mean reward: -1.75, Mean Entropy: 0.000916495337150991, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 717,  Mean reward: -1.0, Mean Entropy: 0.0009531461400911212, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.06s
Iteration: 718,  Mean reward: -2.5, Mean Entropy: 0.0009203666122630239, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 719,  Mean reward: 0.25, Mean Entropy: 0.0010027516400441527, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 720,  Mean reward: -2.25, Mean Entropy: 0.000985773978754878, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 721,  Mean reward: -2.25, Mean Entropy: 0.0009279795340262353, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 722,  Mean reward: -3.25, Mean Entropy: 0.0008712377748452127, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 723,  Mean reward: -3.411392405063291, Mean Entropy: 0.0006684879772365093, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 724,  Mean reward: -3.25, Mean Entropy: 0.0005745889502577484, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 725,  Mean reward: -3.5, Mean Entropy: 0.0006513898260891438, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 726,  Mean reward: -1.5, Mean Entropy: 0.0006409701309166849, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 727,  Mean reward: -2.0, Mean Entropy: 0.0006201542564667761, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 728,  Mean reward: 1.75, Mean Entropy: 0.0008408479625359178, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 729,  Mean reward: -2.0, Mean Entropy: 0.000522568472661078, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 730,  Mean reward: -2.25, Mean Entropy: 0.0006198855116963387, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 731,  Mean reward: -1.75, Mean Entropy: 0.0006097746081650257, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 732,  Mean reward: -2.5, Mean Entropy: 0.0005877971998415887, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 733,  Mean reward: -0.25, Mean Entropy: 0.0007554960320703685, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 734,  Mean reward: -2.25, Mean Entropy: 0.0005533194052986801, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 735,  Mean reward: -1.25, Mean Entropy: 0.0006703523686155677, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 736,  Mean reward: -2.75, Mean Entropy: 0.0006257758941501379, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 737,  Mean reward: -1.75, Mean Entropy: 0.0005939342663623393, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 738,  Mean reward: -2.5, Mean Entropy: 0.0006071724928915501, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 739,  Mean reward: -1.5, Mean Entropy: 0.0006253539468161762, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 740,  Mean reward: -1.75, Mean Entropy: 0.0006372022908180952, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 741,  Mean reward: -1.5, Mean Entropy: 0.0005045654252171516, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 742,  Mean reward: -2.0, Mean Entropy: 0.0005574430106207728, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 743,  Mean reward: -2.5, Mean Entropy: 0.0005262201884761453, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 744,  Mean reward: -1.0, Mean Entropy: 0.0005615826230496168, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 745,  Mean reward: -2.25, Mean Entropy: 0.0005417594802565873, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 746,  Mean reward: -2.5, Mean Entropy: 0.0004915614845231175, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 747,  Mean reward: -4.0, Mean Entropy: 0.0004585449933074415, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 748,  Mean reward: -2.5, Mean Entropy: 0.0005315644666552544, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 749,  Mean reward: -1.25, Mean Entropy: 0.0005747000104747713, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 750,  Mean reward: -2.5, Mean Entropy: 0.0005006649298593402, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 751,  Mean reward: -1.75, Mean Entropy: 0.0005487382295541465, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 752,  Mean reward: -2.5, Mean Entropy: 0.0004592676123138517, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 753,  Mean reward: -2.75, Mean Entropy: 0.00046065269270911813, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 754,  Mean reward: -1.0, Mean Entropy: 0.0006263895775191486, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 755,  Mean reward: -2.5, Mean Entropy: 0.00042133344686590135, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 756,  Mean reward: -1.5, Mean Entropy: 0.000497280212584883, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 757,  Mean reward: -2.0, Mean Entropy: 0.0005774176097474992, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.04s
Iteration: 758,  Mean reward: -1.0, Mean Entropy: 0.0005207666545175016, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 759,  Mean reward: -0.75, Mean Entropy: 0.0005354618187993765, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 760,  Mean reward: -1.5, Mean Entropy: 0.0005205001216381788, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 761,  Mean reward: -1.0, Mean Entropy: 0.000509430596139282, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 762,  Mean reward: -2.25, Mean Entropy: 0.0003690035955514759, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 763,  Mean reward: -2.0, Mean Entropy: 0.00040097947930917144, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 764,  Mean reward: -1.5, Mean Entropy: 0.0004371778923086822, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 765,  Mean reward: -0.5, Mean Entropy: 0.0005209025111980736, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 766,  Mean reward: -2.5, Mean Entropy: 0.0004198238893877715, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 767,  Mean reward: -1.0, Mean Entropy: 0.0005154695245437324, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 768,  Mean reward: -0.75, Mean Entropy: 0.0004810614918824285, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 769,  Mean reward: -1.5, Mean Entropy: 0.0004665592859964818, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 770,  Mean reward: -2.75, Mean Entropy: 0.0004339498409535736, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 771,  Mean reward: -1.5, Mean Entropy: 0.0004569546435959637, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 772,  Mean reward: -2.25, Mean Entropy: 0.0003886228078044951, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 773,  Mean reward: -1.5, Mean Entropy: 0.0004216161905787885, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 774,  Mean reward: -2.75, Mean Entropy: 0.0003538925666362047, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 775,  Mean reward: -3.0, Mean Entropy: 0.00042767589911818504, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 776,  Mean reward: -1.75, Mean Entropy: 0.00043215317418798804, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 777,  Mean reward: -2.25, Mean Entropy: 0.00036323227686807513, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 778,  Mean reward: 0.5, Mean Entropy: 0.0004654954536817968, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 779,  Mean reward: -3.25, Mean Entropy: 0.00037748407339677215, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 780,  Mean reward: -1.25, Mean Entropy: 0.00044796368456445634, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 781,  Mean reward: 0.0, Mean Entropy: 0.0004729149804916233, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 782,  Mean reward: 0.0, Mean Entropy: 0.0005032160552218556, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 783,  Mean reward: -1.25, Mean Entropy: 0.00042606223723851144, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 784,  Mean reward: -2.75, Mean Entropy: 0.00041930456063710153, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 785,  Mean reward: -2.75, Mean Entropy: 0.0004022924113087356, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 786,  Mean reward: -0.25, Mean Entropy: 0.0004538201610557735, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 787,  Mean reward: -0.5, Mean Entropy: 0.00044356854050420225, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 788,  Mean reward: -1.25, Mean Entropy: 0.00042954887612722814, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 789,  Mean reward: -1.75, Mean Entropy: 0.0004098821373190731, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 790,  Mean reward: -1.75, Mean Entropy: 0.0003931801184080541, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 791,  Mean reward: -0.25, Mean Entropy: 0.00046023749746382236, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 792,  Mean reward: -2.25, Mean Entropy: 0.00033884565345942974, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 793,  Mean reward: -1.75, Mean Entropy: 0.00042016146471723914, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 794,  Mean reward: -3.25, Mean Entropy: 0.0004002961504738778, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 795,  Mean reward: -2.75, Mean Entropy: 0.0003317751397844404, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 796,  Mean reward: -1.5, Mean Entropy: 0.00043399306014180183, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 797,  Mean reward: -1.5, Mean Entropy: 0.0004389142559375614, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.06s
Iteration: 798,  Mean reward: -1.25, Mean Entropy: 0.0003978882450610399, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 799,  Mean reward: -2.0, Mean Entropy: 0.00038101564859971404, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 800,  Mean reward: -1.25, Mean Entropy: 0.00039969017961993814, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -1.75, Mean Entropy: 0.00037990626879036427, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 802,  Mean reward: -0.25, Mean Entropy: 0.00043781433487311006, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 803,  Mean reward: 0.0, Mean Entropy: 0.0004257506225258112, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 804,  Mean reward: -2.75, Mean Entropy: 0.0003746493603102863, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 805,  Mean reward: 0.5, Mean Entropy: 0.00043205905240029097, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 806,  Mean reward: -3.25, Mean Entropy: 0.00038857385516166687, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 807,  Mean reward: -0.5, Mean Entropy: 0.00040200771763920784, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 808,  Mean reward: -0.5, Mean Entropy: 0.00042466813465580344, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 809,  Mean reward: -2.0, Mean Entropy: 0.00040202352101914585, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 810,  Mean reward: -2.25, Mean Entropy: 0.0003881723969243467, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 811,  Mean reward: -1.5, Mean Entropy: 0.00038692139787599444, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.902439024390244, Mean Entropy: 0.8664340376853943, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.77s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.159090909090909, Mean Entropy: 0.9675180315971375, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 2,  Mean reward: -5.052631578947368, Mean Entropy: 1.0036189556121826, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -3.0930232558139537, Mean Entropy: 0.9602971076965332, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 4,  Mean reward: -5.848837209302325, Mean Entropy: 0.9675148725509644, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 5,  Mean reward: -4.2727272727272725, Mean Entropy: 0.9458551406860352, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 6,  Mean reward: -4.9605263157894735, Mean Entropy: 1.0036191940307617, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 7,  Mean reward: -4.7875, Mean Entropy: 1.003618597984314, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 8,  Mean reward: -5.55, Mean Entropy: 0.9602953791618347, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 9,  Mean reward: -4.6395348837209305, Mean Entropy: 0.967516303062439, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 10,  Mean reward: -2.9743589743589745, Mean Entropy: 0.9386353492736816, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 11,  Mean reward: -4.4743589743589745, Mean Entropy: 0.9386348724365234, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 12,  Mean reward: -4.2682926829268295, Mean Entropy: 0.9819561243057251, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 13,  Mean reward: -4.866666666666666, Mean Entropy: 0.9602930545806885, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.74s
Iteration: 14,  Mean reward: -3.106382978723404, Mean Entropy: 0.9891780614852905, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 15,  Mean reward: -2.9069767441860463, Mean Entropy: 0.9891788363456726, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 16,  Mean reward: -4.0, Mean Entropy: 0.924196183681488, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 17,  Mean reward: -3.3658536585365852, Mean Entropy: 0.9602968692779541, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 18,  Mean reward: -3.117021276595745, Mean Entropy: 0.924193263053894, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 19,  Mean reward: -4.025, Mean Entropy: 0.931407630443573, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.75s
Iteration: 20,  Mean reward: -5.4875, Mean Entropy: 0.9747216105461121, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 21,  Mean reward: -4.709302325581396, Mean Entropy: 0.9602784514427185, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.93s
Iteration: 22,  Mean reward: -4.7439024390243905, Mean Entropy: 0.9458253383636475, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 23,  Mean reward: -4.166666666666667, Mean Entropy: 0.9530518054962158, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 24,  Mean reward: -3.75, Mean Entropy: 0.9530242681503296, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 25,  Mean reward: -5.615384615384615, Mean Entropy: 0.9457699656486511, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 26,  Mean reward: -6.404761904761905, Mean Entropy: 0.9602469205856323, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 27,  Mean reward: -3.5454545454545454, Mean Entropy: 0.9097437858581543, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.74s
Iteration: 28,  Mean reward: -4.443181818181818, Mean Entropy: 0.9241916537284851, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 29,  Mean reward: -5.804878048780488, Mean Entropy: 0.989174485206604, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 30,  Mean reward: -3.116279069767442, Mean Entropy: 0.9675161242485046, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 31,  Mean reward: -6.6022727272727275, Mean Entropy: 0.9386352300643921, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 32,  Mean reward: -5.144736842105263, Mean Entropy: 0.9241926670074463, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 33,  Mean reward: -5.769230769230769, Mean Entropy: 0.9314001798629761, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 34,  Mean reward: -3.3333333333333335, Mean Entropy: 0.9313908815383911, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 35,  Mean reward: -4.653846153846154, Mean Entropy: 0.9458427429199219, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.78s
Iteration: 36,  Mean reward: -6.926829268292683, Mean Entropy: 0.9386155009269714, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 37,  Mean reward: -6.972222222222222, Mean Entropy: 0.9097323417663574, complete_episode_count: 36.0, Gather time: 0.50s, Train time: 1.77s
Iteration: 38,  Mean reward: -4.654761904761905, Mean Entropy: 0.916954755783081, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 39,  Mean reward: -3.175, Mean Entropy: 0.9747075438499451, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 40,  Mean reward: -3.433333333333333, Mean Entropy: 1.0324667692184448, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 41,  Mean reward: -5.2023809523809526, Mean Entropy: 0.8664023280143738, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.75s
Iteration: 42,  Mean reward: -4.476744186046512, Mean Entropy: 0.924155592918396, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 43,  Mean reward: -5.380952380952381, Mean Entropy: 0.8663861155509949, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 44,  Mean reward: -3.6666666666666665, Mean Entropy: 0.9385578036308289, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 45,  Mean reward: -4.5, Mean Entropy: 0.9096845388412476, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 46,  Mean reward: -3.7906976744186047, Mean Entropy: 0.9096629023551941, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 47,  Mean reward: -3.5365853658536586, Mean Entropy: 0.9529847502708435, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 48,  Mean reward: -3.9404761904761907, Mean Entropy: 0.9818081855773926, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 49,  Mean reward: -1.8421052631578947, Mean Entropy: 0.9024454355239868, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 50,  Mean reward: -3.1904761904761907, Mean Entropy: 0.8951716423034668, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 51,  Mean reward: -4.627906976744186, Mean Entropy: 0.9166059494018555, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 52,  Mean reward: -5.273809523809524, Mean Entropy: 0.8949064612388611, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 53,  Mean reward: -4.378048780487805, Mean Entropy: 0.8948730230331421, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 54,  Mean reward: -5.4125, Mean Entropy: 0.9667279124259949, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 55,  Mean reward: -3.8625, Mean Entropy: 0.8803073763847351, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 56,  Mean reward: -2.6666666666666665, Mean Entropy: 0.8873628377914429, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.94s
Iteration: 57,  Mean reward: -4.651162790697675, Mean Entropy: 0.9945408701896667, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 58,  Mean reward: -5.1976744186046515, Mean Entropy: 0.9438292980194092, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.75s
Iteration: 59,  Mean reward: -6.828947368421052, Mean Entropy: 0.9576146006584167, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 60,  Mean reward: -5.2375, Mean Entropy: 0.8641743659973145, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.75s
Iteration: 61,  Mean reward: -5.275, Mean Entropy: 0.907741904258728, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.75s
Iteration: 62,  Mean reward: -4.769230769230769, Mean Entropy: 0.8869404196739197, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 63,  Mean reward: -4.453488372093023, Mean Entropy: 0.8723915815353394, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 64,  Mean reward: -5.153846153846154, Mean Entropy: 0.8938021063804626, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 65,  Mean reward: -3.313953488372093, Mean Entropy: 0.8365778923034668, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 66,  Mean reward: -7.255813953488372, Mean Entropy: 0.9577888250350952, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 67,  Mean reward: -3.511904761904762, Mean Entropy: 0.8867039680480957, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 68,  Mean reward: -4.569767441860465, Mean Entropy: 0.9078354835510254, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 69,  Mean reward: -6.583333333333333, Mean Entropy: 0.9140698909759521, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 70,  Mean reward: -1.329268292682927, Mean Entropy: 0.9294611811637878, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.78s
Iteration: 71,  Mean reward: -5.9186046511627906, Mean Entropy: 0.8715483546257019, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 72,  Mean reward: -4.174418604651163, Mean Entropy: 0.9427753686904907, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 73,  Mean reward: -2.8552631578947367, Mean Entropy: 0.8935959339141846, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 74,  Mean reward: -3.2439024390243905, Mean Entropy: 0.9222902059555054, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 75,  Mean reward: -5.0638297872340425, Mean Entropy: 1.0340864658355713, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 76,  Mean reward: -1.6931818181818181, Mean Entropy: 0.962666928768158, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 77,  Mean reward: -4.021739130434782, Mean Entropy: 0.9493808746337891, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 78,  Mean reward: -4.333333333333333, Mean Entropy: 0.9837714433670044, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 79,  Mean reward: -5.2976190476190474, Mean Entropy: 0.9259504079818726, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 80,  Mean reward: -3.6136363636363638, Mean Entropy: 0.9346110224723816, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.75s
Iteration: 81,  Mean reward: -2.3295454545454546, Mean Entropy: 0.9645390510559082, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 82,  Mean reward: -3.397727272727273, Mean Entropy: 0.9471443891525269, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 83,  Mean reward: -4.380434782608695, Mean Entropy: 0.8995059728622437, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 84,  Mean reward: -4.7682926829268295, Mean Entropy: 0.9694501757621765, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.75s
Iteration: 85,  Mean reward: -5.845238095238095, Mean Entropy: 0.888807475566864, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 86,  Mean reward: -3.619565217391304, Mean Entropy: 0.8981800079345703, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 87,  Mean reward: -2.4680851063829787, Mean Entropy: 0.9583371877670288, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 88,  Mean reward: -3.227272727272727, Mean Entropy: 0.9164086580276489, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 89,  Mean reward: -2.875, Mean Entropy: 0.8803128600120544, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 90,  Mean reward: -3.2934782608695654, Mean Entropy: 0.8861948847770691, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 91,  Mean reward: -3.75531914893617, Mean Entropy: 0.8861219882965088, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.95s
Iteration: 92,  Mean reward: -3.25, Mean Entropy: 0.8555435538291931, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 93,  Mean reward: -3.7, Mean Entropy: 0.8748730421066284, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 94,  Mean reward: -2.9625, Mean Entropy: 0.8320638537406921, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 95,  Mean reward: -4.479166666666667, Mean Entropy: 0.8944792151451111, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 96,  Mean reward: -3.5609756097560976, Mean Entropy: 0.8703221082687378, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.76s
Iteration: 97,  Mean reward: -4.534090909090909, Mean Entropy: 0.8664430379867554, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 98,  Mean reward: -2.979591836734694, Mean Entropy: 0.8826319575309753, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 99,  Mean reward: -6.728260869565218, Mean Entropy: 0.8712586760520935, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 100,  Mean reward: -2.152173913043478, Mean Entropy: 0.8822400569915771, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.77s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -2.4375, Mean Entropy: 0.9256059527397156, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 102,  Mean reward: -2.9523809523809526, Mean Entropy: 0.8568636775016785, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 103,  Mean reward: -5.0, Mean Entropy: 0.8414615392684937, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 104,  Mean reward: -5.488888888888889, Mean Entropy: 0.7973968386650085, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 105,  Mean reward: -3.25, Mean Entropy: 0.8424948453903198, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 106,  Mean reward: -6.02, Mean Entropy: 0.8405628204345703, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 107,  Mean reward: -4.797872340425532, Mean Entropy: 0.8222397565841675, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 108,  Mean reward: -4.358695652173913, Mean Entropy: 0.8632920980453491, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 109,  Mean reward: -4.943181818181818, Mean Entropy: 0.8437299728393555, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.77s
Iteration: 110,  Mean reward: -4.2, Mean Entropy: 0.835971474647522, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 111,  Mean reward: -3.5, Mean Entropy: 0.7941381931304932, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 112,  Mean reward: -5.134615384615385, Mean Entropy: 0.7493022680282593, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 113,  Mean reward: -1.4285714285714286, Mean Entropy: 0.7050470113754272, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 114,  Mean reward: -2.5086206896551726, Mean Entropy: 0.7044054269790649, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 115,  Mean reward: -3.53125, Mean Entropy: 0.7166747450828552, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 116,  Mean reward: -1.25, Mean Entropy: 0.648438572883606, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.76s
Iteration: 117,  Mean reward: -3.8796296296296298, Mean Entropy: 0.5945682525634766, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 118,  Mean reward: -6.23469387755102, Mean Entropy: 0.6477250456809998, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 119,  Mean reward: -3.9038461538461537, Mean Entropy: 0.642358660697937, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 120,  Mean reward: -2.701923076923077, Mean Entropy: 0.6888021230697632, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 121,  Mean reward: -3.6470588235294117, Mean Entropy: 0.6077388525009155, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.76s
Iteration: 122,  Mean reward: -2.8859649122807016, Mean Entropy: 0.615888237953186, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.76s
Iteration: 123,  Mean reward: -4.76, Mean Entropy: 0.6410794258117676, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 124,  Mean reward: -1.4396551724137931, Mean Entropy: 0.5639874935150146, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.97s
Iteration: 125,  Mean reward: -5.284313725490196, Mean Entropy: 0.5768429040908813, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 126,  Mean reward: -1.7735849056603774, Mean Entropy: 0.6201816201210022, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 127,  Mean reward: -3.8461538461538463, Mean Entropy: 0.6039778590202332, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 128,  Mean reward: -1.471698113207547, Mean Entropy: 0.5511375665664673, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 129,  Mean reward: -2.481818181818182, Mean Entropy: 0.5539731979370117, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.75s
Iteration: 130,  Mean reward: -4.5754716981132075, Mean Entropy: 0.6153661608695984, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 131,  Mean reward: -4.12037037037037, Mean Entropy: 0.6020060777664185, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 132,  Mean reward: -4.572727272727272, Mean Entropy: 0.5739735960960388, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 133,  Mean reward: -3.048076923076923, Mean Entropy: 0.5943281650543213, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.77s
Iteration: 134,  Mean reward: -1.8773584905660377, Mean Entropy: 0.534331738948822, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.78s
Iteration: 135,  Mean reward: -3.1293103448275863, Mean Entropy: 0.5322903990745544, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 136,  Mean reward: -0.075, Mean Entropy: 0.5293536186218262, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.82s
Iteration: 137,  Mean reward: -4.712962962962963, Mean Entropy: 0.5492194890975952, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 138,  Mean reward: -2.537037037037037, Mean Entropy: 0.5370121002197266, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 139,  Mean reward: -1.74, Mean Entropy: 0.505239725112915, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.71s
Iteration: 140,  Mean reward: -2.7586206896551726, Mean Entropy: 0.5463041067123413, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.63s
Iteration: 141,  Mean reward: -4.413793103448276, Mean Entropy: 0.5291069746017456, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.65s
Iteration: 142,  Mean reward: -2.7666666666666666, Mean Entropy: 0.4797379970550537, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.65s
Iteration: 143,  Mean reward: -5.421568627450981, Mean Entropy: 0.5384184718132019, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.66s
Iteration: 144,  Mean reward: -1.696078431372549, Mean Entropy: 0.5111473798751831, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 145,  Mean reward: -4.25, Mean Entropy: 0.4947952628135681, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 146,  Mean reward: -5.245454545454545, Mean Entropy: 0.5054717063903809, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 147,  Mean reward: -0.9661016949152542, Mean Entropy: 0.48616886138916016, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 148,  Mean reward: -2.9035087719298245, Mean Entropy: 0.4707651734352112, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 149,  Mean reward: -1.5692307692307692, Mean Entropy: 0.43364962935447693, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 150,  Mean reward: -0.8615384615384616, Mean Entropy: 0.3590174615383148, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 151,  Mean reward: -3.640625, Mean Entropy: 0.28917962312698364, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 152,  Mean reward: -1.4202898550724639, Mean Entropy: 0.16521745920181274, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 153,  Mean reward: -3.804054054054054, Mean Entropy: 0.1057376116514206, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 154,  Mean reward: -3.0641025641025643, Mean Entropy: 0.05320389196276665, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 155,  Mean reward: -3.358974358974359, Mean Entropy: 0.02585088275372982, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 156,  Mean reward: -1.5, Mean Entropy: 0.020397432148456573, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 157,  Mean reward: -2.3987341772151898, Mean Entropy: 0.016756106168031693, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 158,  Mean reward: 0.25, Mean Entropy: 0.010698738507926464, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.85s
Iteration: 159,  Mean reward: -2.0, Mean Entropy: 0.012589903548359871, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.85s
Iteration: 160,  Mean reward: -2.3987341772151898, Mean Entropy: 0.008879423141479492, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 161,  Mean reward: -2.5, Mean Entropy: 0.008700041100382805, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 162,  Mean reward: -1.75, Mean Entropy: 0.007315763272345066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 163,  Mean reward: -4.25, Mean Entropy: 0.008206542581319809, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 164,  Mean reward: -1.5, Mean Entropy: 0.0063633788377046585, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 165,  Mean reward: -2.0, Mean Entropy: 0.006630315911024809, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 166,  Mean reward: -0.25, Mean Entropy: 0.006737343035638332, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 167,  Mean reward: -3.0, Mean Entropy: 0.008183861151337624, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 168,  Mean reward: -3.0, Mean Entropy: 0.009360920637845993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 169,  Mean reward: -3.0, Mean Entropy: 0.010451791808009148, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 170,  Mean reward: -4.0, Mean Entropy: 0.011825092136859894, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 171,  Mean reward: -1.75, Mean Entropy: 0.011659374460577965, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 172,  Mean reward: -1.25, Mean Entropy: 0.012485961429774761, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 173,  Mean reward: 0.25, Mean Entropy: 0.013376014307141304, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 174,  Mean reward: -2.1455696202531644, Mean Entropy: 0.01193859614431858, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 175,  Mean reward: -2.75, Mean Entropy: 0.010792678222060204, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 176,  Mean reward: -1.25, Mean Entropy: 0.00995488092303276, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 177,  Mean reward: -1.25, Mean Entropy: 0.00963670201599598, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 178,  Mean reward: -4.25, Mean Entropy: 0.012118089012801647, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 179,  Mean reward: -1.5, Mean Entropy: 0.008507788181304932, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 180,  Mean reward: -4.443037974683544, Mean Entropy: 0.007059890776872635, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 181,  Mean reward: -1.0, Mean Entropy: 0.003983713686466217, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 182,  Mean reward: -1.5, Mean Entropy: 0.003288622945547104, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 183,  Mean reward: -2.0, Mean Entropy: 0.0031142800580710173, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 184,  Mean reward: -2.25, Mean Entropy: 0.0030415579676628113, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 185,  Mean reward: -3.0, Mean Entropy: 0.0029345010407269, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 186,  Mean reward: -0.75, Mean Entropy: 0.0026374831795692444, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 187,  Mean reward: -0.75, Mean Entropy: 0.0026335599832236767, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 188,  Mean reward: -2.651898734177215, Mean Entropy: 0.0020961430855095387, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.82s
Iteration: 189,  Mean reward: -1.75, Mean Entropy: 0.0011548787588253617, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 190,  Mean reward: -1.5, Mean Entropy: 0.0011444678530097008, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 191,  Mean reward: 0.0, Mean Entropy: 0.0010831961408257484, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 192,  Mean reward: -2.75, Mean Entropy: 0.001215792028233409, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 193,  Mean reward: -1.5, Mean Entropy: 0.0010440903715789318, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 194,  Mean reward: -0.25, Mean Entropy: 0.0010695444652810693, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 195,  Mean reward: -2.5, Mean Entropy: 0.0011773935984820127, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 196,  Mean reward: -2.25, Mean Entropy: 0.0012399122351780534, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 197,  Mean reward: -1.75, Mean Entropy: 0.0011754246661439538, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 198,  Mean reward: -1.5, Mean Entropy: 0.001112992176786065, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 199,  Mean reward: -3.75, Mean Entropy: 0.0013572422321885824, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 200,  Mean reward: -3.75, Mean Entropy: 0.0012652346631512046, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.5, Mean Entropy: 0.0010039372136816382, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 202,  Mean reward: -3.25, Mean Entropy: 0.0012739485828205943, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 203,  Mean reward: -1.75, Mean Entropy: 0.001109616830945015, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 204,  Mean reward: -1.0, Mean Entropy: 0.0011380920186638832, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 205,  Mean reward: -2.25, Mean Entropy: 0.0010841439943760633, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 206,  Mean reward: -1.0, Mean Entropy: 0.0011493887286633253, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 207,  Mean reward: -3.0, Mean Entropy: 0.0012292548781260848, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 208,  Mean reward: -1.5, Mean Entropy: 0.001303554861806333, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 209,  Mean reward: -1.25, Mean Entropy: 0.000997898867353797, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 210,  Mean reward: -1.5, Mean Entropy: 0.0013086519902572036, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 211,  Mean reward: -0.5, Mean Entropy: 0.0010382349137216806, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 212,  Mean reward: 0.0, Mean Entropy: 0.0011692766565829515, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 213,  Mean reward: -3.5, Mean Entropy: 0.0013881213963031769, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 214,  Mean reward: -1.0, Mean Entropy: 0.0011425341945141554, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 215,  Mean reward: -2.75, Mean Entropy: 0.001090123550966382, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 216,  Mean reward: -2.0, Mean Entropy: 0.0012986096553504467, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 217,  Mean reward: -2.75, Mean Entropy: 0.001244881423190236, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 218,  Mean reward: -1.5, Mean Entropy: 0.0013567805290222168, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 219,  Mean reward: -3.0, Mean Entropy: 0.0013311825459823012, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.85s
Iteration: 220,  Mean reward: -2.5, Mean Entropy: 0.0012554565910249949, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 221,  Mean reward: -2.75, Mean Entropy: 0.0013128280406817794, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 222,  Mean reward: -3.25, Mean Entropy: 0.0014190420042723417, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 223,  Mean reward: -2.75, Mean Entropy: 0.0011695725843310356, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 224,  Mean reward: -2.5, Mean Entropy: 0.0013873957796022296, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 225,  Mean reward: -3.0, Mean Entropy: 0.0013604620471596718, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 226,  Mean reward: -1.75, Mean Entropy: 0.0012064615730196238, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 227,  Mean reward: -3.75, Mean Entropy: 0.001306365244090557, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 228,  Mean reward: -0.75, Mean Entropy: 0.001341052120551467, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 229,  Mean reward: -2.25, Mean Entropy: 0.0013569662114605308, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 230,  Mean reward: -1.0, Mean Entropy: 0.0013182887341827154, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 231,  Mean reward: -2.75, Mean Entropy: 0.00137245561927557, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 232,  Mean reward: -1.25, Mean Entropy: 0.001369099598377943, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 233,  Mean reward: -2.75, Mean Entropy: 0.0012305775890126824, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 234,  Mean reward: -1.5, Mean Entropy: 0.0014184180181473494, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 235,  Mean reward: -1.5, Mean Entropy: 0.0013176137581467628, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 236,  Mean reward: -1.75, Mean Entropy: 0.0014312546700239182, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 237,  Mean reward: -3.0, Mean Entropy: 0.0014081642730161548, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 238,  Mean reward: -2.5, Mean Entropy: 0.0014857010683044791, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 239,  Mean reward: -2.25, Mean Entropy: 0.0013364413753151894, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 240,  Mean reward: -2.25, Mean Entropy: 0.0014904399868100882, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 241,  Mean reward: -2.0, Mean Entropy: 0.0013069728156551719, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 242,  Mean reward: -2.25, Mean Entropy: 0.0015014286618679762, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 243,  Mean reward: 0.25, Mean Entropy: 0.001110136043280363, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 244,  Mean reward: -1.0, Mean Entropy: 0.001419177744537592, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 245,  Mean reward: -2.5, Mean Entropy: 0.0014824731042608619, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 246,  Mean reward: -2.5, Mean Entropy: 0.0014367125695571303, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 247,  Mean reward: -1.25, Mean Entropy: 0.0010418274905532598, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 248,  Mean reward: -3.25, Mean Entropy: 0.0015129692619666457, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 249,  Mean reward: -2.5, Mean Entropy: 0.001365956268273294, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 250,  Mean reward: -1.75, Mean Entropy: 0.0017390530556440353, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 251,  Mean reward: -3.0, Mean Entropy: 0.001589529449120164, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 252,  Mean reward: -1.5, Mean Entropy: 0.0013323489110916853, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 253,  Mean reward: -3.0, Mean Entropy: 0.0015281051164492965, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 254,  Mean reward: -1.25, Mean Entropy: 0.001390064018778503, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 255,  Mean reward: -4.0, Mean Entropy: 0.0015824355650693178, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 256,  Mean reward: -3.0, Mean Entropy: 0.001490010879933834, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 257,  Mean reward: -2.0, Mean Entropy: 0.0013854478020220995, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 258,  Mean reward: -3.25, Mean Entropy: 0.0016788564389571548, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 259,  Mean reward: 0.75, Mean Entropy: 0.001088567660190165, complete_episode_count: 80.0, Gather time: 0.71s, Train time: 0.83s
Iteration: 260,  Mean reward: -0.25, Mean Entropy: 0.0012272385647520423, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 261,  Mean reward: -1.0, Mean Entropy: 0.0012630249839276075, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 262,  Mean reward: -0.75, Mean Entropy: 0.001330190571025014, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 263,  Mean reward: -1.25, Mean Entropy: 0.0013625745195895433, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 264,  Mean reward: -2.25, Mean Entropy: 0.00179252156522125, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 265,  Mean reward: 0.25, Mean Entropy: 0.0013263601576909423, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 266,  Mean reward: -1.75, Mean Entropy: 0.0015793241327628493, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 267,  Mean reward: -3.25, Mean Entropy: 0.0015588965034112334, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 268,  Mean reward: 0.5, Mean Entropy: 0.0013737783301621675, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 269,  Mean reward: -2.0, Mean Entropy: 0.0014538140967488289, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 270,  Mean reward: -1.75, Mean Entropy: 0.0016244741855189204, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 271,  Mean reward: -3.0, Mean Entropy: 0.0015602342318743467, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 272,  Mean reward: -0.75, Mean Entropy: 0.001410980592481792, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 273,  Mean reward: -2.25, Mean Entropy: 0.0014936216175556183, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 274,  Mean reward: 0.0, Mean Entropy: 0.001336685731075704, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 275,  Mean reward: -2.25, Mean Entropy: 0.0016604464035481215, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 276,  Mean reward: -3.5, Mean Entropy: 0.0019813270773738623, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 277,  Mean reward: -3.0, Mean Entropy: 0.0016674702055752277, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 278,  Mean reward: -2.5, Mean Entropy: 0.001705120550468564, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 279,  Mean reward: -1.75, Mean Entropy: 0.0015816332306712866, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 280,  Mean reward: -1.25, Mean Entropy: 0.0016620077658444643, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 281,  Mean reward: -1.75, Mean Entropy: 0.0016457893652841449, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 282,  Mean reward: -2.0, Mean Entropy: 0.0018753231270238757, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 283,  Mean reward: -2.0, Mean Entropy: 0.0016551290173083544, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 284,  Mean reward: -1.5, Mean Entropy: 0.0017910426249727607, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 285,  Mean reward: -2.0, Mean Entropy: 0.001474147429689765, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 286,  Mean reward: -1.5, Mean Entropy: 0.0016488777473568916, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 287,  Mean reward: -3.0, Mean Entropy: 0.0018301544478163123, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 288,  Mean reward: 0.25, Mean Entropy: 0.0014716237783432007, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 289,  Mean reward: -1.25, Mean Entropy: 0.0015647889813408256, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 290,  Mean reward: -2.25, Mean Entropy: 0.0016982173547148705, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 291,  Mean reward: -0.5, Mean Entropy: 0.0015759170055389404, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 292,  Mean reward: 0.25, Mean Entropy: 0.0015044582542032003, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 293,  Mean reward: -3.5, Mean Entropy: 0.0018075776752084494, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 294,  Mean reward: -2.3987341772151898, Mean Entropy: 0.00098559376783669, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 295,  Mean reward: -2.25, Mean Entropy: 0.0005992042715661228, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 296,  Mean reward: -0.75, Mean Entropy: 0.0005008215084671974, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 297,  Mean reward: 0.0, Mean Entropy: 0.0004241798887960613, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 298,  Mean reward: -0.75, Mean Entropy: 0.0004011443816125393, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 299,  Mean reward: -2.5, Mean Entropy: 0.0005180388689041138, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.84s
Iteration: 300,  Mean reward: -2.75, Mean Entropy: 0.0005404024268500507, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -1.75, Mean Entropy: 0.0004960669903084636, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 302,  Mean reward: -1.0, Mean Entropy: 0.0005327907274477184, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 303,  Mean reward: -1.25, Mean Entropy: 0.0004990093875676394, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 304,  Mean reward: 0.25, Mean Entropy: 0.00045852060429751873, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 305,  Mean reward: 0.25, Mean Entropy: 0.00042833125917240977, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 306,  Mean reward: -2.0, Mean Entropy: 0.0005571746733039618, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 307,  Mean reward: -3.0, Mean Entropy: 0.0005215407581999898, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 308,  Mean reward: -3.25, Mean Entropy: 0.0006281730020418763, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 309,  Mean reward: -2.0, Mean Entropy: 0.0005428955191746354, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 310,  Mean reward: -2.75, Mean Entropy: 0.0005838947836309671, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 311,  Mean reward: -2.0, Mean Entropy: 0.0005156452534720302, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 312,  Mean reward: -1.75, Mean Entropy: 0.0005551100475713611, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 313,  Mean reward: -1.75, Mean Entropy: 0.0005346431862562895, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 314,  Mean reward: -2.0, Mean Entropy: 0.0006082325708121061, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 315,  Mean reward: -2.0, Mean Entropy: 0.0005233113188296556, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 316,  Mean reward: -0.75, Mean Entropy: 0.0005485077272169292, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 317,  Mean reward: -3.25, Mean Entropy: 0.0005767241818830371, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 318,  Mean reward: -2.25, Mean Entropy: 0.0005525187589228153, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 319,  Mean reward: -1.5, Mean Entropy: 0.0005009730812162161, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 320,  Mean reward: -2.25, Mean Entropy: 0.0004738374555017799, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 321,  Mean reward: -2.5, Mean Entropy: 0.0006169176194816828, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 322,  Mean reward: -2.75, Mean Entropy: 0.000648547662422061, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 323,  Mean reward: -1.5, Mean Entropy: 0.0005749446572735906, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 324,  Mean reward: -1.75, Mean Entropy: 0.000530493794940412, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 325,  Mean reward: -0.25, Mean Entropy: 0.00049760309047997, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 326,  Mean reward: -2.75, Mean Entropy: 0.0005698064924217761, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 327,  Mean reward: -2.0, Mean Entropy: 0.0005856595234945416, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 328,  Mean reward: -2.0, Mean Entropy: 0.0005920901312492788, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 329,  Mean reward: -2.75, Mean Entropy: 0.0006415153620764613, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 330,  Mean reward: -3.5, Mean Entropy: 0.0006685503758490086, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 331,  Mean reward: -1.5, Mean Entropy: 0.0006689891451969743, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 332,  Mean reward: -2.75, Mean Entropy: 0.0006408278131857514, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 333,  Mean reward: -1.5, Mean Entropy: 0.0005231807008385658, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 334,  Mean reward: -0.5, Mean Entropy: 0.0005743545480072498, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 335,  Mean reward: -4.5, Mean Entropy: 0.0007013340364210308, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 336,  Mean reward: -1.5, Mean Entropy: 0.0006154228467494249, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 337,  Mean reward: -2.5, Mean Entropy: 0.0006225131801329553, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 338,  Mean reward: -3.5, Mean Entropy: 0.0006925462512299418, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 339,  Mean reward: -3.0, Mean Entropy: 0.0006250347360037267, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 340,  Mean reward: -1.5, Mean Entropy: 0.0005680423928424716, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.84s
Iteration: 341,  Mean reward: -2.25, Mean Entropy: 0.0005948847392573953, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 342,  Mean reward: -0.75, Mean Entropy: 0.0005747249233536422, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 343,  Mean reward: -1.0, Mean Entropy: 0.0006555445725098252, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 344,  Mean reward: -1.0, Mean Entropy: 0.0005801717052236199, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 345,  Mean reward: -1.75, Mean Entropy: 0.0006246150005608797, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 346,  Mean reward: -3.5, Mean Entropy: 0.0006998730823397636, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 347,  Mean reward: -1.75, Mean Entropy: 0.000647601846139878, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 348,  Mean reward: -0.75, Mean Entropy: 0.0005325201200321317, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 349,  Mean reward: -1.0, Mean Entropy: 0.00048570497892796993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 350,  Mean reward: -1.75, Mean Entropy: 0.0005743179353885353, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 351,  Mean reward: -1.5, Mean Entropy: 0.0006426098989322782, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 352,  Mean reward: -3.0, Mean Entropy: 0.0008177271811291575, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 353,  Mean reward: -1.75, Mean Entropy: 0.0005920836119912565, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 354,  Mean reward: 0.0, Mean Entropy: 0.0005503272986970842, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 355,  Mean reward: 0.75, Mean Entropy: 0.0005032745539210737, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 356,  Mean reward: -2.5, Mean Entropy: 0.0006989472894929349, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 357,  Mean reward: -1.5, Mean Entropy: 0.0006255244370549917, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 358,  Mean reward: -2.25, Mean Entropy: 0.0007464975933544338, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 359,  Mean reward: -1.0, Mean Entropy: 0.0006311770994216204, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 360,  Mean reward: -0.75, Mean Entropy: 0.0006099826423451304, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 361,  Mean reward: -3.25, Mean Entropy: 0.0006806803867220879, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 362,  Mean reward: -3.0, Mean Entropy: 0.0007015494629740715, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 363,  Mean reward: -0.75, Mean Entropy: 0.0006463250028900802, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 364,  Mean reward: -1.5, Mean Entropy: 0.0006848563207313418, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 365,  Mean reward: -3.25, Mean Entropy: 0.0007780855521559715, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 366,  Mean reward: -3.25, Mean Entropy: 0.000799211673438549, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 367,  Mean reward: 0.5, Mean Entropy: 0.0006385479937307537, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 368,  Mean reward: -2.0, Mean Entropy: 0.0006515962886624038, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 369,  Mean reward: -1.0, Mean Entropy: 0.0006427529733628035, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 370,  Mean reward: -2.25, Mean Entropy: 0.0007396318251267076, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 371,  Mean reward: -2.25, Mean Entropy: 0.000670360866934061, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 372,  Mean reward: -1.0, Mean Entropy: 0.0006367973983287811, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.82s
Iteration: 373,  Mean reward: -2.5, Mean Entropy: 0.0007862354395911098, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 374,  Mean reward: -2.25, Mean Entropy: 0.0008019586093723774, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 375,  Mean reward: -0.75, Mean Entropy: 0.0006639931234531105, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 376,  Mean reward: -1.5, Mean Entropy: 0.0006535336142405868, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 377,  Mean reward: -2.5, Mean Entropy: 0.0008251398685388267, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 378,  Mean reward: -1.5, Mean Entropy: 0.0007736026309430599, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 379,  Mean reward: -1.25, Mean Entropy: 0.0006774080684408545, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 380,  Mean reward: 0.25, Mean Entropy: 0.0006671798182651401, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.83s
Iteration: 381,  Mean reward: -3.5, Mean Entropy: 0.0008662021718919277, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 382,  Mean reward: -3.25, Mean Entropy: 0.0008572655497118831, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 383,  Mean reward: -3.0, Mean Entropy: 0.0008062801789492369, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 384,  Mean reward: -2.5, Mean Entropy: 0.0007985850679688156, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 385,  Mean reward: -1.5, Mean Entropy: 0.0006550480029545724, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 386,  Mean reward: -2.0, Mean Entropy: 0.0008044370333664119, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 387,  Mean reward: -3.75, Mean Entropy: 0.0007995876367203891, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 388,  Mean reward: -0.75, Mean Entropy: 0.0007212974014692008, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 389,  Mean reward: -2.25, Mean Entropy: 0.0008571188664063811, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 390,  Mean reward: -4.25, Mean Entropy: 0.0008700437610968947, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 391,  Mean reward: -1.25, Mean Entropy: 0.0007917276816442609, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 392,  Mean reward: -2.0, Mean Entropy: 0.000753971457015723, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 393,  Mean reward: -3.5, Mean Entropy: 0.0009164573857560754, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 394,  Mean reward: 0.0, Mean Entropy: 0.0007909074774943292, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 395,  Mean reward: -1.75, Mean Entropy: 0.0007624700665473938, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 396,  Mean reward: -0.75, Mean Entropy: 0.0007242769934237003, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 397,  Mean reward: -1.75, Mean Entropy: 0.0007931466680020094, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 398,  Mean reward: 0.0, Mean Entropy: 0.0007561136735603213, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 399,  Mean reward: -1.5, Mean Entropy: 0.0008240628521889448, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 400,  Mean reward: -1.25, Mean Entropy: 0.0007875130395404994, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -0.25, Mean Entropy: 0.0007569275330752134, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 402,  Mean reward: -3.75, Mean Entropy: 0.000995791982859373, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 403,  Mean reward: -2.75, Mean Entropy: 0.0008615399710834026, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 404,  Mean reward: -2.0, Mean Entropy: 0.0008021743851713836, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 405,  Mean reward: -2.0, Mean Entropy: 0.0009015646064653993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 406,  Mean reward: -1.5, Mean Entropy: 0.0009697097120806575, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 407,  Mean reward: -1.5, Mean Entropy: 0.0008090820047073066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 408,  Mean reward: -3.5, Mean Entropy: 0.0008722562342882156, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 409,  Mean reward: -3.25, Mean Entropy: 0.0010488710831850767, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 410,  Mean reward: -1.25, Mean Entropy: 0.0008026388823054731, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 411,  Mean reward: -2.25, Mean Entropy: 0.0009280963568016887, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 412,  Mean reward: -2.25, Mean Entropy: 0.0009168144315481186, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 413,  Mean reward: -2.5, Mean Entropy: 0.0009401589632034302, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 414,  Mean reward: -1.5, Mean Entropy: 0.0009288680739700794, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 415,  Mean reward: -2.5, Mean Entropy: 0.0009502495522610843, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 416,  Mean reward: -1.75, Mean Entropy: 0.00085645099170506, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 417,  Mean reward: -3.25, Mean Entropy: 0.000935905787628144, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 418,  Mean reward: -2.5, Mean Entropy: 0.0010369172086939216, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 419,  Mean reward: -2.75, Mean Entropy: 0.001063228235580027, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.00s
Iteration: 420,  Mean reward: -0.25, Mean Entropy: 0.0008034685160964727, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 421,  Mean reward: -2.25, Mean Entropy: 0.0009197279578074813, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 422,  Mean reward: -1.25, Mean Entropy: 0.0009101772448047996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 423,  Mean reward: -2.1455696202531644, Mean Entropy: 0.00031905085779726505, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.82s
Iteration: 424,  Mean reward: -3.5, Mean Entropy: 0.00023784337099641562, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 425,  Mean reward: -1.5, Mean Entropy: 0.00017939884855877608, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 426,  Mean reward: -3.5, Mean Entropy: 0.00021477135305758566, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 427,  Mean reward: -2.75, Mean Entropy: 0.0001971339515876025, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 428,  Mean reward: -2.0, Mean Entropy: 0.0001714673126116395, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 429,  Mean reward: -1.0, Mean Entropy: 0.00018086281488649547, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 430,  Mean reward: -2.25, Mean Entropy: 0.0001879543997347355, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 431,  Mean reward: -2.5, Mean Entropy: 0.00018509337678551674, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 432,  Mean reward: -2.5, Mean Entropy: 0.00019733844965230674, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 433,  Mean reward: -1.75, Mean Entropy: 0.00017112991190515459, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 434,  Mean reward: -2.75, Mean Entropy: 0.00019445123325567693, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 435,  Mean reward: -2.25, Mean Entropy: 0.0001908649574033916, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 436,  Mean reward: -2.5, Mean Entropy: 0.0002304895024280995, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 437,  Mean reward: -3.5, Mean Entropy: 0.0001960538065759465, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 438,  Mean reward: -2.0, Mean Entropy: 0.00019886429072357714, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 439,  Mean reward: -2.25, Mean Entropy: 0.00019733759108930826, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 440,  Mean reward: -1.0, Mean Entropy: 0.00019174189947079867, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 441,  Mean reward: -1.5, Mean Entropy: 0.00017672385729383677, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 442,  Mean reward: -3.25, Mean Entropy: 0.00020978422253392637, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 443,  Mean reward: -4.0, Mean Entropy: 0.00022554106544703245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 444,  Mean reward: -1.25, Mean Entropy: 0.00018978625303134322, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 445,  Mean reward: -1.5, Mean Entropy: 0.00019480276387184858, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 446,  Mean reward: -2.0, Mean Entropy: 0.00018227266264148057, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 447,  Mean reward: -3.75, Mean Entropy: 0.0002160524600185454, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 448,  Mean reward: 0.25, Mean Entropy: 0.00017058124649338424, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 449,  Mean reward: -2.0, Mean Entropy: 0.0002012438199017197, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 450,  Mean reward: -0.5, Mean Entropy: 0.00017166612087748945, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 451,  Mean reward: 0.75, Mean Entropy: 0.0001736535778036341, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 452,  Mean reward: -1.75, Mean Entropy: 0.00019855270511470735, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 453,  Mean reward: -2.5, Mean Entropy: 0.00021616867161355913, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 454,  Mean reward: -2.75, Mean Entropy: 0.00020419601059984416, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 455,  Mean reward: -1.0, Mean Entropy: 0.00020041715470142663, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 456,  Mean reward: -2.75, Mean Entropy: 0.00021414653747342527, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 457,  Mean reward: -1.5, Mean Entropy: 0.00021867998293600976, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 458,  Mean reward: -2.25, Mean Entropy: 0.00021140526223462075, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 459,  Mean reward: -0.5, Mean Entropy: 0.0001945592084666714, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.99s
Iteration: 460,  Mean reward: -2.25, Mean Entropy: 0.00023534093634225428, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 461,  Mean reward: -4.5, Mean Entropy: 0.0002303459623362869, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 462,  Mean reward: -1.0, Mean Entropy: 0.00021406076848506927, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 463,  Mean reward: -1.75, Mean Entropy: 0.0002059167600236833, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 464,  Mean reward: -1.0, Mean Entropy: 0.00019755683024413884, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 465,  Mean reward: -2.75, Mean Entropy: 0.00023003376554697752, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 466,  Mean reward: -1.75, Mean Entropy: 0.00019413945847190917, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 467,  Mean reward: -3.0, Mean Entropy: 0.00022651534527540207, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 468,  Mean reward: -2.25, Mean Entropy: 0.00019976485054939985, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 469,  Mean reward: -2.25, Mean Entropy: 0.0002010820317082107, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 470,  Mean reward: -3.0, Mean Entropy: 0.00021528310026042163, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 471,  Mean reward: 0.25, Mean Entropy: 0.00017998437397181988, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 472,  Mean reward: -2.0, Mean Entropy: 0.0002231948747066781, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 473,  Mean reward: -1.5, Mean Entropy: 0.00021247162658255547, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 474,  Mean reward: -2.75, Mean Entropy: 0.00023409018467646092, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 475,  Mean reward: -2.25, Mean Entropy: 0.0002319956838618964, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 476,  Mean reward: 0.75, Mean Entropy: 0.00020891793246846646, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 477,  Mean reward: -1.0, Mean Entropy: 0.00021530511730816215, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 478,  Mean reward: -1.5, Mean Entropy: 0.00021978914446663111, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 479,  Mean reward: -4.25, Mean Entropy: 0.00025600899243727326, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 480,  Mean reward: -2.75, Mean Entropy: 0.0002301341446582228, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 481,  Mean reward: -1.75, Mean Entropy: 0.00020663801115006208, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 482,  Mean reward: -1.0, Mean Entropy: 0.00019729281484615058, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 483,  Mean reward: -1.75, Mean Entropy: 0.000213237275602296, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 484,  Mean reward: -1.0, Mean Entropy: 0.00021851874771527946, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 485,  Mean reward: -0.75, Mean Entropy: 0.000205154501600191, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 486,  Mean reward: -2.5, Mean Entropy: 0.00022939845803193748, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 487,  Mean reward: -2.25, Mean Entropy: 0.00021578492305707186, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 488,  Mean reward: -1.75, Mean Entropy: 0.00022557777992915362, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 489,  Mean reward: -2.25, Mean Entropy: 0.00022616161732003093, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 490,  Mean reward: -3.5, Mean Entropy: 0.00026593689108267426, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 491,  Mean reward: -2.25, Mean Entropy: 0.00022324334713630378, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 492,  Mean reward: -2.5, Mean Entropy: 0.00023331819102168083, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 493,  Mean reward: -1.25, Mean Entropy: 0.00021936457778792828, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 494,  Mean reward: -3.25, Mean Entropy: 0.00025649950839579105, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 495,  Mean reward: 0.5, Mean Entropy: 0.000192849722225219, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 496,  Mean reward: -2.0, Mean Entropy: 0.000232233403949067, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 497,  Mean reward: -2.0, Mean Entropy: 0.00022328416525851935, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 498,  Mean reward: -1.5, Mean Entropy: 0.0002185395424021408, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 499,  Mean reward: -1.75, Mean Entropy: 0.00022475914738606662, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.00s
Iteration: 500,  Mean reward: -3.0, Mean Entropy: 0.00024074767134152353, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -4.0, Mean Entropy: 0.00027112674433737993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 502,  Mean reward: -2.0, Mean Entropy: 0.0002630622766446322, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 503,  Mean reward: -1.639240506329114, Mean Entropy: 0.00014902770635671914, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 504,  Mean reward: -2.0, Mean Entropy: 0.00011931070184800774, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 505,  Mean reward: -1.75, Mean Entropy: 0.00011083677964052185, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 506,  Mean reward: -2.25, Mean Entropy: 0.00011962351709371433, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 507,  Mean reward: -1.5, Mean Entropy: 0.00011840060324175283, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 508,  Mean reward: -2.0, Mean Entropy: 0.00012656084436457604, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 509,  Mean reward: -3.0, Mean Entropy: 0.00014215130067896098, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 510,  Mean reward: -1.75, Mean Entropy: 0.00013308475899975747, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 511,  Mean reward: -1.5, Mean Entropy: 0.00014108681352809072, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 512,  Mean reward: -3.25, Mean Entropy: 0.00015337433433160186, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 513,  Mean reward: -1.0, Mean Entropy: 0.00012983685883227736, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 514,  Mean reward: -0.5, Mean Entropy: 0.00013647042214870453, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 515,  Mean reward: -3.75, Mean Entropy: 0.00016843416960909963, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 516,  Mean reward: -0.75, Mean Entropy: 0.00012586239608936012, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 517,  Mean reward: -0.75, Mean Entropy: 0.0001277950796065852, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 518,  Mean reward: -0.5, Mean Entropy: 0.00014242183533497155, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 519,  Mean reward: -1.0, Mean Entropy: 0.0001327984791714698, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 520,  Mean reward: -0.25, Mean Entropy: 0.00014375757018569857, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 521,  Mean reward: -2.0, Mean Entropy: 0.00016339407011400908, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 522,  Mean reward: -0.75, Mean Entropy: 0.0001333153632003814, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 523,  Mean reward: -2.0, Mean Entropy: 0.0001463639928260818, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 524,  Mean reward: -1.5, Mean Entropy: 0.0001591172767803073, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 525,  Mean reward: -1.25, Mean Entropy: 0.00014023861149325967, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 526,  Mean reward: -3.25, Mean Entropy: 0.00017476608627475798, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 527,  Mean reward: -2.75, Mean Entropy: 0.0001537376519991085, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 528,  Mean reward: -1.25, Mean Entropy: 0.00015168340178206563, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 529,  Mean reward: -2.5, Mean Entropy: 0.00017833858146332204, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 530,  Mean reward: -3.0, Mean Entropy: 0.00017135616508312523, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 531,  Mean reward: -2.5, Mean Entropy: 0.00016830360982567072, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 532,  Mean reward: -2.75, Mean Entropy: 0.00017304954235441983, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 533,  Mean reward: -2.75, Mean Entropy: 0.00018528086366131902, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 534,  Mean reward: -2.5, Mean Entropy: 0.0001658906548982486, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 535,  Mean reward: -1.25, Mean Entropy: 0.00016729856724850833, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 536,  Mean reward: -3.0, Mean Entropy: 0.00019195544882677495, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 537,  Mean reward: -1.25, Mean Entropy: 0.00016958317428361624, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 538,  Mean reward: -1.0, Mean Entropy: 0.00016384138143621385, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 539,  Mean reward: -0.75, Mean Entropy: 0.00015599279140587896, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.00s
Iteration: 540,  Mean reward: -1.5, Mean Entropy: 0.00014006675337441266, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 541,  Mean reward: 0.0, Mean Entropy: 0.00014743470819666982, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 542,  Mean reward: -3.0, Mean Entropy: 0.00017950267647393048, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 543,  Mean reward: -0.25, Mean Entropy: 0.00016201025573536754, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 544,  Mean reward: -1.0, Mean Entropy: 0.00017851607117336243, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 545,  Mean reward: -3.75, Mean Entropy: 0.00020901171956211329, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 546,  Mean reward: -1.5, Mean Entropy: 0.0001626983575988561, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 547,  Mean reward: -1.5, Mean Entropy: 0.00018443858425598592, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 548,  Mean reward: -1.0, Mean Entropy: 0.0001764275220921263, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 549,  Mean reward: -3.25, Mean Entropy: 0.0001957015774678439, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 550,  Mean reward: -1.5, Mean Entropy: 0.00019423254707362503, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 551,  Mean reward: -0.25, Mean Entropy: 0.00018637841276358813, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 552,  Mean reward: -3.25, Mean Entropy: 0.0002068019239231944, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 553,  Mean reward: 0.25, Mean Entropy: 0.0001877609029179439, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 554,  Mean reward: -3.25, Mean Entropy: 0.0002069286274490878, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 555,  Mean reward: -2.75, Mean Entropy: 0.0002292634453624487, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 556,  Mean reward: -0.5, Mean Entropy: 0.00018986848590429872, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 557,  Mean reward: -0.25, Mean Entropy: 0.00017566463793627918, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 558,  Mean reward: -2.25, Mean Entropy: 0.00019671197514981031, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 559,  Mean reward: -4.0, Mean Entropy: 0.00023302220506593585, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 560,  Mean reward: -3.25, Mean Entropy: 0.0002274298167321831, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 561,  Mean reward: -0.25, Mean Entropy: 0.00017842200759332627, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 562,  Mean reward: -2.25, Mean Entropy: 0.0002236147120129317, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 563,  Mean reward: -2.0, Mean Entropy: 0.00024344264238607138, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 564,  Mean reward: -2.0, Mean Entropy: 0.00018825520237442106, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 565,  Mean reward: -1.5, Mean Entropy: 0.00022086230455897748, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 566,  Mean reward: -3.25, Mean Entropy: 0.0002607357455417514, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 567,  Mean reward: -0.75, Mean Entropy: 0.00018667631957214326, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 568,  Mean reward: -2.25, Mean Entropy: 0.00022151813027448952, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 569,  Mean reward: -0.75, Mean Entropy: 0.00022410262317862362, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 570,  Mean reward: -2.5, Mean Entropy: 0.0002279851760249585, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 571,  Mean reward: -1.75, Mean Entropy: 0.00023757718736305833, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 572,  Mean reward: 0.0, Mean Entropy: 0.0002224855124950409, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 573,  Mean reward: -2.25, Mean Entropy: 0.00020880973897874355, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 574,  Mean reward: -0.25, Mean Entropy: 0.00022375970729626715, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 575,  Mean reward: -1.5, Mean Entropy: 0.00024231956922449172, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 576,  Mean reward: -1.25, Mean Entropy: 0.00019322293519508094, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 577,  Mean reward: -2.25, Mean Entropy: 0.00023920671083033085, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 578,  Mean reward: -2.5, Mean Entropy: 0.0002873194171115756, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 579,  Mean reward: -1.5, Mean Entropy: 0.0002376326301600784, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.01s
Iteration: 580,  Mean reward: -2.25, Mean Entropy: 0.0002830111770890653, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 581,  Mean reward: -2.5, Mean Entropy: 0.0002733244909904897, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 582,  Mean reward: -2.25, Mean Entropy: 0.00023570668417960405, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 583,  Mean reward: -1.75, Mean Entropy: 0.0002546871837694198, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 584,  Mean reward: -2.0, Mean Entropy: 0.0002644086489453912, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 585,  Mean reward: -1.5, Mean Entropy: 0.00022733047080691904, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 586,  Mean reward: -2.75, Mean Entropy: 0.00027181391487829387, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 587,  Mean reward: -3.0, Mean Entropy: 0.0003027312341146171, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 588,  Mean reward: -1.5, Mean Entropy: 0.0002434469060972333, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 589,  Mean reward: -2.25, Mean Entropy: 0.00024966144701465964, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 590,  Mean reward: -3.0, Mean Entropy: 0.0002867569855879992, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 591,  Mean reward: -0.5, Mean Entropy: 0.0002188905782531947, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 592,  Mean reward: -0.5, Mean Entropy: 0.000260267814155668, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 593,  Mean reward: -3.5, Mean Entropy: 0.00031595875043421984, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 594,  Mean reward: -3.25, Mean Entropy: 0.00024799894890747964, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 595,  Mean reward: -2.5, Mean Entropy: 0.0003027479397132993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 596,  Mean reward: -3.0, Mean Entropy: 0.0002556339022703469, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 597,  Mean reward: -3.25, Mean Entropy: 0.0002644576015882194, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 598,  Mean reward: -1.75, Mean Entropy: 0.00027077397680841386, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 599,  Mean reward: -1.75, Mean Entropy: 0.000288153241854161, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 600,  Mean reward: -1.75, Mean Entropy: 0.0002158602001145482, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.0, Mean Entropy: 0.00028945886879228055, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 602,  Mean reward: -1.0, Mean Entropy: 0.00027085866895504296, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 603,  Mean reward: -2.75, Mean Entropy: 0.00027468305779621005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 604,  Mean reward: -1.0, Mean Entropy: 0.00027937936829403043, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 605,  Mean reward: -2.25, Mean Entropy: 0.0002895802026614547, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 606,  Mean reward: -2.25, Mean Entropy: 0.00024872453650459647, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 607,  Mean reward: -2.25, Mean Entropy: 0.0003132391138933599, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 608,  Mean reward: -0.25, Mean Entropy: 0.0003017898998223245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 609,  Mean reward: -1.25, Mean Entropy: 0.0002667681546881795, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 610,  Mean reward: -1.25, Mean Entropy: 0.0002813551982399076, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 611,  Mean reward: -0.75, Mean Entropy: 0.00028433307306841016, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 612,  Mean reward: -0.75, Mean Entropy: 0.0002499727997928858, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 613,  Mean reward: 0.25, Mean Entropy: 0.00028540423954837024, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 614,  Mean reward: -2.25, Mean Entropy: 0.0002879690728150308, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 615,  Mean reward: -2.5, Mean Entropy: 0.00028857929282821715, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 616,  Mean reward: -3.25, Mean Entropy: 0.0003117838641628623, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 617,  Mean reward: -1.5, Mean Entropy: 0.00030786890420131385, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 618,  Mean reward: -2.25, Mean Entropy: 0.00029195347451604903, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 619,  Mean reward: -3.0, Mean Entropy: 0.00035487988498061895, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 620,  Mean reward: -1.75, Mean Entropy: 0.0003005702164955437, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 621,  Mean reward: -1.75, Mean Entropy: 0.0002675975556485355, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 622,  Mean reward: -2.25, Mean Entropy: 0.00033220858313143253, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 623,  Mean reward: 0.5, Mean Entropy: 0.00027024728478863835, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 624,  Mean reward: -3.5, Mean Entropy: 0.00033003673888742924, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 625,  Mean reward: -3.0, Mean Entropy: 0.00034698937088251114, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 626,  Mean reward: -5.25, Mean Entropy: 0.0004038343904539943, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 627,  Mean reward: 0.0, Mean Entropy: 0.00023017327475827187, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 628,  Mean reward: -5.75, Mean Entropy: 0.0004187668673694134, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 629,  Mean reward: -1.5, Mean Entropy: 0.00034258043160662055, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 630,  Mean reward: -4.25, Mean Entropy: 0.0003500950988382101, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 631,  Mean reward: -3.5, Mean Entropy: 0.0003607512335292995, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 632,  Mean reward: -1.0, Mean Entropy: 0.0003463577013462782, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 633,  Mean reward: -1.25, Mean Entropy: 0.0002984374004881829, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 634,  Mean reward: -0.75, Mean Entropy: 0.00030405045254155993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 635,  Mean reward: -2.25, Mean Entropy: 0.000388456042855978, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 636,  Mean reward: -1.5, Mean Entropy: 0.000326684326864779, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 637,  Mean reward: -1.5, Mean Entropy: 0.0003354581422172487, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 638,  Mean reward: -1.75, Mean Entropy: 0.00032841411302797496, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 639,  Mean reward: -2.25, Mean Entropy: 0.00033024369622580707, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 640,  Mean reward: -2.5, Mean Entropy: 0.00035574258072301745, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 641,  Mean reward: -0.75, Mean Entropy: 0.0003211204893887043, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 642,  Mean reward: -2.0, Mean Entropy: 0.0003053632390219718, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 643,  Mean reward: -2.0, Mean Entropy: 0.00032068570726551116, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 644,  Mean reward: -2.0, Mean Entropy: 0.00035964089329354465, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 645,  Mean reward: -4.0, Mean Entropy: 0.00037502479972317815, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 646,  Mean reward: -3.0, Mean Entropy: 0.00040546117816120386, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 647,  Mean reward: -1.75, Mean Entropy: 0.0003546222287695855, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 648,  Mean reward: -1.25, Mean Entropy: 0.00035384512739256024, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 649,  Mean reward: -1.75, Mean Entropy: 0.0004063908418174833, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 650,  Mean reward: -1.25, Mean Entropy: 0.000323402025969699, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 651,  Mean reward: -0.5, Mean Entropy: 0.00033267680555582047, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 652,  Mean reward: -2.5, Mean Entropy: 0.0003714438935276121, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 653,  Mean reward: -0.75, Mean Entropy: 0.00033631938276812434, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 654,  Mean reward: -2.5, Mean Entropy: 0.0003280286327935755, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 655,  Mean reward: -3.5, Mean Entropy: 0.00041523989057168365, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 656,  Mean reward: -0.25, Mean Entropy: 0.0003222380473744124, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 657,  Mean reward: -2.25, Mean Entropy: 0.00035808453685604036, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 658,  Mean reward: -3.25, Mean Entropy: 0.0004634719807654619, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 659,  Mean reward: -1.5, Mean Entropy: 0.0003703904221765697, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.99s
Iteration: 660,  Mean reward: -1.25, Mean Entropy: 0.00036481814458966255, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 661,  Mean reward: -1.5, Mean Entropy: 0.00037178429192863405, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 662,  Mean reward: -1.0, Mean Entropy: 0.00038636638782918453, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 663,  Mean reward: -1.25, Mean Entropy: 0.00039323559030890465, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 664,  Mean reward: -0.75, Mean Entropy: 0.00034724449506029487, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 665,  Mean reward: -1.5, Mean Entropy: 0.00042316815233789384, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 666,  Mean reward: 0.5, Mean Entropy: 0.0003311581676825881, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 667,  Mean reward: -3.0, Mean Entropy: 0.0004300618020351976, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 668,  Mean reward: -3.0, Mean Entropy: 0.00044630252523347735, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 669,  Mean reward: -3.5, Mean Entropy: 0.00045726937241852283, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 670,  Mean reward: -1.0, Mean Entropy: 0.0004101581289432943, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 671,  Mean reward: -2.75, Mean Entropy: 0.0004260956193320453, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 672,  Mean reward: -3.25, Mean Entropy: 0.00044370448449626565, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 673,  Mean reward: 0.0, Mean Entropy: 0.0004018680192530155, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 674,  Mean reward: -3.0, Mean Entropy: 0.00047121121315285563, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 675,  Mean reward: -4.25, Mean Entropy: 0.0004451643326319754, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 676,  Mean reward: -2.75, Mean Entropy: 0.000477950437925756, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 677,  Mean reward: -0.5, Mean Entropy: 0.0003892777021974325, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 678,  Mean reward: -2.5, Mean Entropy: 0.0004302992601878941, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 679,  Mean reward: -1.75, Mean Entropy: 0.00038927391869947314, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 680,  Mean reward: -1.5, Mean Entropy: 0.0003844925668090582, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 681,  Mean reward: -2.5, Mean Entropy: 0.00041575252544134855, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 682,  Mean reward: -0.75, Mean Entropy: 0.00038732303073629737, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 683,  Mean reward: -1.5, Mean Entropy: 0.0004324164183344692, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 684,  Mean reward: -3.0, Mean Entropy: 0.0004476526228245348, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 685,  Mean reward: -2.5, Mean Entropy: 0.0004219732654746622, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 686,  Mean reward: -3.75, Mean Entropy: 0.00045639052405022085, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 687,  Mean reward: -2.5, Mean Entropy: 0.0003975562285631895, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 688,  Mean reward: -2.0, Mean Entropy: 0.0004032416909467429, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 689,  Mean reward: -0.75, Mean Entropy: 0.000394558475818485, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 690,  Mean reward: -2.25, Mean Entropy: 0.000371809903299436, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 691,  Mean reward: -2.25, Mean Entropy: 0.00044688256457448006, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 692,  Mean reward: -1.5, Mean Entropy: 0.0004718386917375028, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 693,  Mean reward: -0.5, Mean Entropy: 0.0004018676409032196, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 694,  Mean reward: -3.75, Mean Entropy: 0.0004704703751485795, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 695,  Mean reward: -3.25, Mean Entropy: 0.0004965715925209224, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 696,  Mean reward: -2.5, Mean Entropy: 0.0004532812163233757, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 697,  Mean reward: -0.25, Mean Entropy: 0.0003692579339258373, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 698,  Mean reward: -0.75, Mean Entropy: 0.000416851369664073, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 699,  Mean reward: -3.5, Mean Entropy: 0.0004706243926193565, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 700,  Mean reward: -2.75, Mean Entropy: 0.0004734471149276942, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -2.0, Mean Entropy: 0.0004530241130851209, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 702,  Mean reward: -2.25, Mean Entropy: 0.00043272553011775017, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 703,  Mean reward: -1.25, Mean Entropy: 0.00047799223102629185, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 704,  Mean reward: -2.0, Mean Entropy: 0.0004809304082300514, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 705,  Mean reward: -1.5, Mean Entropy: 0.00043778037070296705, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 706,  Mean reward: -2.75, Mean Entropy: 0.00046813877997919917, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 707,  Mean reward: 0.75, Mean Entropy: 0.00039304138044826686, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 708,  Mean reward: -1.25, Mean Entropy: 0.00042875856161117554, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 709,  Mean reward: -3.75, Mean Entropy: 0.0005240129539743066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 710,  Mean reward: 1.25, Mean Entropy: 0.0004083883832208812, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.85s
Iteration: 711,  Mean reward: -1.25, Mean Entropy: 0.00043547607492655516, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 712,  Mean reward: -2.0, Mean Entropy: 0.0004616971127688885, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 713,  Mean reward: -0.5, Mean Entropy: 0.0004545646661426872, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 714,  Mean reward: 0.0, Mean Entropy: 0.0003909120277967304, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 715,  Mean reward: -2.25, Mean Entropy: 0.000499896181281656, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 716,  Mean reward: -2.75, Mean Entropy: 0.0004969764268025756, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 717,  Mean reward: -1.5, Mean Entropy: 0.00046741118421778083, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 718,  Mean reward: -1.75, Mean Entropy: 0.00046547569218091667, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 719,  Mean reward: -1.5, Mean Entropy: 0.0005286774830892682, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 720,  Mean reward: -1.0, Mean Entropy: 0.0004339480947237462, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 721,  Mean reward: -1.5, Mean Entropy: 0.0004472454311326146, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 722,  Mean reward: 1.0, Mean Entropy: 0.00041616492671892047, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 723,  Mean reward: -2.25, Mean Entropy: 0.0004385133506730199, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 724,  Mean reward: -2.75, Mean Entropy: 0.0005400435766205192, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 725,  Mean reward: -2.75, Mean Entropy: 0.0005822329549118876, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 726,  Mean reward: -3.75, Mean Entropy: 0.0005473240744322538, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 727,  Mean reward: 0.25, Mean Entropy: 0.00041473557939752936, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 728,  Mean reward: -3.25, Mean Entropy: 0.0005309180705808103, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 729,  Mean reward: -2.5, Mean Entropy: 0.0005123656010255218, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 730,  Mean reward: -2.0, Mean Entropy: 0.0005468095187097788, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 731,  Mean reward: -1.0, Mean Entropy: 0.0005292306886985898, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 732,  Mean reward: -2.75, Mean Entropy: 0.0005021760589443147, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 733,  Mean reward: -1.75, Mean Entropy: 0.0005468227900564671, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 734,  Mean reward: -1.5, Mean Entropy: 0.0005267377127893269, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 735,  Mean reward: -1.5, Mean Entropy: 0.0004553994513116777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 736,  Mean reward: -2.25, Mean Entropy: 0.0005931963096372783, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 737,  Mean reward: -1.0, Mean Entropy: 0.0005741854547522962, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 738,  Mean reward: -3.25, Mean Entropy: 0.0005357718910090625, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 739,  Mean reward: -3.0, Mean Entropy: 0.0006248531281016767, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.00s
Iteration: 740,  Mean reward: -1.25, Mean Entropy: 0.0005631980020552874, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 741,  Mean reward: -3.5, Mean Entropy: 0.0006320157554000616, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 742,  Mean reward: -0.75, Mean Entropy: 0.0005642964970320463, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 743,  Mean reward: -2.25, Mean Entropy: 0.0005984961753711104, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 744,  Mean reward: -2.5, Mean Entropy: 0.0005826758570037782, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 745,  Mean reward: 0.0, Mean Entropy: 0.0005768793635070324, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 746,  Mean reward: -2.5, Mean Entropy: 0.0006653493037447333, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 747,  Mean reward: -0.75, Mean Entropy: 0.0005669788224622607, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 748,  Mean reward: -2.0, Mean Entropy: 0.000576200895011425, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 749,  Mean reward: -3.25, Mean Entropy: 0.0006498393486253917, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 750,  Mean reward: -1.5, Mean Entropy: 0.000529000535607338, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 751,  Mean reward: -0.5, Mean Entropy: 0.0005013352492824197, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 752,  Mean reward: 0.25, Mean Entropy: 0.0005198976141400635, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 753,  Mean reward: 0.25, Mean Entropy: 0.00047423451906070113, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 754,  Mean reward: -3.0, Mean Entropy: 0.0006690301815979183, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 755,  Mean reward: -1.75, Mean Entropy: 0.0006665207329206169, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 756,  Mean reward: -1.0, Mean Entropy: 0.0005633463151752949, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 757,  Mean reward: -1.5, Mean Entropy: 0.0005537583492696285, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 758,  Mean reward: -1.75, Mean Entropy: 0.0006285581039264798, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 759,  Mean reward: -1.75, Mean Entropy: 0.0005238851299509406, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 760,  Mean reward: -1.75, Mean Entropy: 0.0005880654789507389, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 761,  Mean reward: -1.75, Mean Entropy: 0.0006390514317899942, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 762,  Mean reward: -2.0, Mean Entropy: 0.0005970814963802695, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 763,  Mean reward: -3.25, Mean Entropy: 0.000710024032741785, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 764,  Mean reward: -2.651898734177215, Mean Entropy: 0.00021987954096402973, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 765,  Mean reward: -3.0, Mean Entropy: 0.00015574063581880182, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 766,  Mean reward: -2.5, Mean Entropy: 0.00011939012620132416, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 767,  Mean reward: -0.75, Mean Entropy: 0.00011114691733382642, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 768,  Mean reward: -1.5, Mean Entropy: 0.00010871831182157621, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 769,  Mean reward: -2.0, Mean Entropy: 0.00011312263086438179, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 770,  Mean reward: -3.25, Mean Entropy: 0.00013148135622031987, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 771,  Mean reward: -3.25, Mean Entropy: 0.00013099436182528734, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 772,  Mean reward: -3.5, Mean Entropy: 0.00012657673505600542, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 773,  Mean reward: -2.25, Mean Entropy: 0.0001188141104648821, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 774,  Mean reward: -1.5, Mean Entropy: 0.00012361783592496067, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 775,  Mean reward: -0.75, Mean Entropy: 0.00011197318963240832, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 776,  Mean reward: -0.25, Mean Entropy: 0.00011557967809494585, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 777,  Mean reward: -0.75, Mean Entropy: 0.00010872489656321704, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 778,  Mean reward: -2.0, Mean Entropy: 0.000123853562399745, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 779,  Mean reward: -1.0, Mean Entropy: 0.00011842358799185604, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.98s
Iteration: 780,  Mean reward: -3.25, Mean Entropy: 0.00013479674817062914, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 781,  Mean reward: -0.75, Mean Entropy: 0.00010974821634590626, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 782,  Mean reward: 0.0, Mean Entropy: 0.00012532660912256688, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 783,  Mean reward: -0.5, Mean Entropy: 0.00012843024160247296, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 784,  Mean reward: -3.5, Mean Entropy: 0.00014662629109807312, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 785,  Mean reward: 0.0, Mean Entropy: 0.00013233281788416207, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 786,  Mean reward: -2.5, Mean Entropy: 0.0001369264064123854, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 787,  Mean reward: -0.75, Mean Entropy: 0.00013439057511277497, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 788,  Mean reward: 1.0, Mean Entropy: 0.00012589688412845135, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 789,  Mean reward: -2.25, Mean Entropy: 0.00013218994718044996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 790,  Mean reward: -2.0, Mean Entropy: 0.00014446221757680178, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 791,  Mean reward: -0.25, Mean Entropy: 0.0001331377716269344, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 792,  Mean reward: -2.3987341772151898, Mean Entropy: 0.0001044939854182303, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 793,  Mean reward: -3.5, Mean Entropy: 0.00011077948147431016, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 794,  Mean reward: -1.75, Mean Entropy: 8.620675362180918e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 795,  Mean reward: -1.25, Mean Entropy: 8.576978871133178e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 796,  Mean reward: -3.5, Mean Entropy: 0.00010610176104819402, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 797,  Mean reward: -2.5, Mean Entropy: 9.819680417422205e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 798,  Mean reward: -4.5, Mean Entropy: 0.00012265739496797323, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 799,  Mean reward: 0.25, Mean Entropy: 7.749431824777275e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 800,  Mean reward: -2.25, Mean Entropy: 0.0001109140575863421, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -3.25, Mean Entropy: 0.00010081781510962173, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 802,  Mean reward: -1.25, Mean Entropy: 0.00010749391367426142, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 803,  Mean reward: -3.0, Mean Entropy: 0.00011451193131506443, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 804,  Mean reward: -0.75, Mean Entropy: 9.97595489025116e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 805,  Mean reward: -2.75, Mean Entropy: 0.00010354442929383367, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 806,  Mean reward: -1.5, Mean Entropy: 0.00011952549539273605, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 807,  Mean reward: -2.25, Mean Entropy: 0.00010365803609602153, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 808,  Mean reward: -1.25, Mean Entropy: 0.0001077795386663638, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 809,  Mean reward: -1.0, Mean Entropy: 9.200700878864154e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 810,  Mean reward: -4.0, Mean Entropy: 0.00012625465751625597, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 811,  Mean reward: -2.25, Mean Entropy: 0.00010679903061827645, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 812,  Mean reward: -2.75, Mean Entropy: 0.0001227595203090459, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 813,  Mean reward: -2.5, Mean Entropy: 0.00012142333434894681, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 814,  Mean reward: 0.0, Mean Entropy: 0.00011545559391379356, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 815,  Mean reward: -1.75, Mean Entropy: 0.00011600480502238497, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 816,  Mean reward: -2.25, Mean Entropy: 0.0001290955115109682, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 817,  Mean reward: -3.75, Mean Entropy: 0.00013105463585816324, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 818,  Mean reward: -1.25, Mean Entropy: 0.00011702056508511305, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 819,  Mean reward: -3.0, Mean Entropy: 0.00012890883954241872, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.00s
Iteration: 820,  Mean reward: -2.75, Mean Entropy: 0.00013605569256469607, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 821,  Mean reward: -1.0, Mean Entropy: 0.00012380009866319597, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 822,  Mean reward: -1.25, Mean Entropy: 0.00011920533142983913, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 823,  Mean reward: -2.0, Mean Entropy: 0.00012091037933714688, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 824,  Mean reward: -2.75, Mean Entropy: 0.00013124890392646194, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 825,  Mean reward: -1.75, Mean Entropy: 0.0001206332744914107, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 826,  Mean reward: -1.0, Mean Entropy: 0.0001273757079616189, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 827,  Mean reward: -0.75, Mean Entropy: 0.00012106072972528636, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 828,  Mean reward: -3.25, Mean Entropy: 0.00014250556705519557, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 829,  Mean reward: -4.75, Mean Entropy: 0.0001573300687596202, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 830,  Mean reward: -2.5, Mean Entropy: 0.0001253701775567606, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 831,  Mean reward: -2.75, Mean Entropy: 0.00013058542390353978, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 832,  Mean reward: -3.0, Mean Entropy: 0.00014349368575494736, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 833,  Mean reward: -1.75, Mean Entropy: 0.00011733653809642419, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 834,  Mean reward: -3.0, Mean Entropy: 0.00016558004426769912, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 835,  Mean reward: -1.5, Mean Entropy: 0.00014136721438262612, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 836,  Mean reward: -1.75, Mean Entropy: 0.0001344385527772829, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 837,  Mean reward: -2.75, Mean Entropy: 0.00013658235548064113, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 838,  Mean reward: -2.0, Mean Entropy: 0.00015102166798897088, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 839,  Mean reward: -0.75, Mean Entropy: 0.00012997951125726104, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 840,  Mean reward: -2.75, Mean Entropy: 0.00012537708971649408, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 841,  Mean reward: -2.75, Mean Entropy: 0.0001420993939973414, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 842,  Mean reward: -1.75, Mean Entropy: 0.00014223085599951446, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 843,  Mean reward: -4.0, Mean Entropy: 0.00017326993111055344, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 844,  Mean reward: -2.5, Mean Entropy: 0.00014700833708047867, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 845,  Mean reward: -2.5, Mean Entropy: 0.00014505628496408463, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 846,  Mean reward: -1.0, Mean Entropy: 0.00014148214540909976, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 847,  Mean reward: -1.75, Mean Entropy: 0.00014768287655897439, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 848,  Mean reward: -1.75, Mean Entropy: 0.00012699524813797325, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 849,  Mean reward: -4.0, Mean Entropy: 0.00014899580855853856, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 850,  Mean reward: -2.5, Mean Entropy: 0.00015639865887351334, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 851,  Mean reward: -2.5, Mean Entropy: 0.00014674908015877008, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 852,  Mean reward: -2.0, Mean Entropy: 0.00015433809312526137, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 853,  Mean reward: -4.0, Mean Entropy: 0.00017018405196722597, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 854,  Mean reward: -3.25, Mean Entropy: 0.0001680962013779208, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 855,  Mean reward: -2.0, Mean Entropy: 0.00015811370394658297, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 856,  Mean reward: -2.75, Mean Entropy: 0.0001655223750276491, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 857,  Mean reward: 0.75, Mean Entropy: 0.0001288717467105016, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 858,  Mean reward: -0.75, Mean Entropy: 0.00013940606731921434, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 859,  Mean reward: -1.5, Mean Entropy: 0.0001443413639208302, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.99s
Iteration: 860,  Mean reward: -2.0, Mean Entropy: 0.00015515691484324634, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 861,  Mean reward: -0.25, Mean Entropy: 0.00013783729809802026, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 862,  Mean reward: -2.25, Mean Entropy: 0.00016945062088780105, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 863,  Mean reward: -3.25, Mean Entropy: 0.00014757682220079005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 864,  Mean reward: -1.25, Mean Entropy: 0.0001591918698977679, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 865,  Mean reward: -2.5, Mean Entropy: 0.00016844870697241277, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 866,  Mean reward: -2.75, Mean Entropy: 0.00017384521197527647, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 867,  Mean reward: -2.5, Mean Entropy: 0.0001633661740925163, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 868,  Mean reward: -2.25, Mean Entropy: 0.0001640816917642951, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 869,  Mean reward: -4.75, Mean Entropy: 0.00019253876234870404, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 870,  Mean reward: -1.5, Mean Entropy: 0.0001661286223679781, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 871,  Mean reward: 0.25, Mean Entropy: 0.000139607087476179, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 872,  Mean reward: -2.75, Mean Entropy: 0.00017091826885007322, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 873,  Mean reward: -3.75, Mean Entropy: 0.00017659732839092612, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 874,  Mean reward: -1.75, Mean Entropy: 0.0001543461112305522, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 875,  Mean reward: -1.75, Mean Entropy: 0.00015276181511580944, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 876,  Mean reward: -2.0, Mean Entropy: 0.00015587294183205813, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 877,  Mean reward: -3.0, Mean Entropy: 0.00016968694399110973, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 878,  Mean reward: -0.75, Mean Entropy: 0.00015929286018945277, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 879,  Mean reward: -2.5, Mean Entropy: 0.00017220378504134715, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 880,  Mean reward: -3.25, Mean Entropy: 0.00018536613788455725, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 881,  Mean reward: -2.0, Mean Entropy: 0.00015313635230995715, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 882,  Mean reward: -1.25, Mean Entropy: 0.00017959011893253773, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 883,  Mean reward: -2.5, Mean Entropy: 0.0001669755147304386, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 884,  Mean reward: -3.5, Mean Entropy: 0.00017208175268024206, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 885,  Mean reward: -2.75, Mean Entropy: 0.00017416084301657975, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 886,  Mean reward: 0.25, Mean Entropy: 0.00017257858416996896, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 887,  Mean reward: -1.5, Mean Entropy: 0.00016989998403005302, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 888,  Mean reward: -0.25, Mean Entropy: 0.00019330512441229075, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 889,  Mean reward: -2.5, Mean Entropy: 0.0001905422832351178, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 890,  Mean reward: -1.75, Mean Entropy: 0.00021795903739985079, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 891,  Mean reward: -2.25, Mean Entropy: 0.00021782045951113105, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 892,  Mean reward: -1.75, Mean Entropy: 0.00025165019906125963, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 893,  Mean reward: -0.5, Mean Entropy: 0.00021964804909657687, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 894,  Mean reward: -2.5, Mean Entropy: 0.0002751407155301422, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 895,  Mean reward: -2.25, Mean Entropy: 0.0002527329488657415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 896,  Mean reward: -3.25, Mean Entropy: 0.0003143927897326648, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 897,  Mean reward: -1.25, Mean Entropy: 0.0002346657420275733, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 898,  Mean reward: -0.75, Mean Entropy: 0.0002655565913300961, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 899,  Mean reward: -2.25, Mean Entropy: 0.00021806824952363968, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.01s
Iteration: 900,  Mean reward: -1.25, Mean Entropy: 0.00022431192337535322, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -4.0, Mean Entropy: 0.00028089649276807904, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 902,  Mean reward: -3.25, Mean Entropy: 0.000331696355715394, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 903,  Mean reward: -3.25, Mean Entropy: 0.000251339835813269, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 904,  Mean reward: -1.5, Mean Entropy: 0.000296477519441396, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 905,  Mean reward: -3.0, Mean Entropy: 0.00025487615494057536, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 906,  Mean reward: -2.25, Mean Entropy: 0.00032526731956750154, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 907,  Mean reward: -2.0, Mean Entropy: 0.0002871949109248817, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 908,  Mean reward: -3.0, Mean Entropy: 0.00033319249632768333, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 909,  Mean reward: -1.25, Mean Entropy: 0.00026193828671239316, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 910,  Mean reward: -2.0, Mean Entropy: 0.0003000554861500859, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 911,  Mean reward: -2.5, Mean Entropy: 0.00027660600608214736, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 912,  Mean reward: -0.5, Mean Entropy: 0.00028508511604741216, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 913,  Mean reward: -1.0, Mean Entropy: 0.0002554974053055048, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 914,  Mean reward: -1.5, Mean Entropy: 0.0003189323178958148, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 915,  Mean reward: -0.5, Mean Entropy: 0.00024640324409119785, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 916,  Mean reward: -2.5, Mean Entropy: 0.0003129061951767653, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 917,  Mean reward: -3.0, Mean Entropy: 0.0002968007465824485, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 918,  Mean reward: -3.75, Mean Entropy: 0.0003262710233684629, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 919,  Mean reward: -1.75, Mean Entropy: 0.0002642352774273604, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 920,  Mean reward: -2.25, Mean Entropy: 0.00029614518280141056, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 921,  Mean reward: -2.25, Mean Entropy: 0.00026123967836610973, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 922,  Mean reward: -0.5, Mean Entropy: 0.0002630979288369417, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 923,  Mean reward: -1.25, Mean Entropy: 0.00025888870004564524, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 924,  Mean reward: -1.0, Mean Entropy: 0.0003188911941833794, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 925,  Mean reward: -1.5, Mean Entropy: 0.0002881542022805661, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 926,  Mean reward: -2.0, Mean Entropy: 0.00033033161889761686, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 927,  Mean reward: -4.0, Mean Entropy: 0.00033754215110093355, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 928,  Mean reward: -2.25, Mean Entropy: 0.00034859051811508834, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 929,  Mean reward: -2.5, Mean Entropy: 0.0003341360134072602, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 930,  Mean reward: -1.25, Mean Entropy: 0.0003224154352210462, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 931,  Mean reward: -1.75, Mean Entropy: 0.00029276913846842945, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 932,  Mean reward: -3.25, Mean Entropy: 0.00035268819192424417, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 933,  Mean reward: -1.25, Mean Entropy: 0.00031485268846154213, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 934,  Mean reward: -2.5, Mean Entropy: 0.00033494780655018985, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 935,  Mean reward: -0.5, Mean Entropy: 0.0002702181227505207, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 936,  Mean reward: -1.0, Mean Entropy: 0.0003011588123627007, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 937,  Mean reward: -0.5, Mean Entropy: 0.00024302680685650557, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 938,  Mean reward: -2.75, Mean Entropy: 0.0003245153930038214, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 939,  Mean reward: -1.5, Mean Entropy: 0.00031740928534418344, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 940,  Mean reward: -3.5, Mean Entropy: 0.0003645865654107183, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 941,  Mean reward: -2.5, Mean Entropy: 0.00033144949702546, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 942,  Mean reward: -1.5, Mean Entropy: 0.0003138421452604234, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 943,  Mean reward: -1.5, Mean Entropy: 0.00030700754723511636, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 944,  Mean reward: -1.0, Mean Entropy: 0.00031541776843369007, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 945,  Mean reward: -1.0, Mean Entropy: 0.00028656268841587007, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 946,  Mean reward: 0.0, Mean Entropy: 0.00030186973162926733, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 947,  Mean reward: -0.5, Mean Entropy: 0.0002702222263906151, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 948,  Mean reward: -0.75, Mean Entropy: 0.0002982931328006089, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 949,  Mean reward: -3.0, Mean Entropy: 0.0003139758191537112, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 950,  Mean reward: -2.0, Mean Entropy: 0.0003502935287542641, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 951,  Mean reward: -0.75, Mean Entropy: 0.00028731938800774515, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 952,  Mean reward: -2.75, Mean Entropy: 0.0003721227403730154, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 953,  Mean reward: -4.0, Mean Entropy: 0.00033048581099137664, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 954,  Mean reward: 1.0, Mean Entropy: 0.00028229178860783577, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 955,  Mean reward: -1.25, Mean Entropy: 0.00029205085593275726, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 956,  Mean reward: -3.5, Mean Entropy: 0.00041970270103774965, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 957,  Mean reward: -3.0, Mean Entropy: 0.0003624644305091351, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 958,  Mean reward: -1.0, Mean Entropy: 0.0003624721139203757, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 959,  Mean reward: -3.25, Mean Entropy: 0.0003531294350977987, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 960,  Mean reward: -2.25, Mean Entropy: 0.000379947799956426, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 961,  Mean reward: -1.0, Mean Entropy: 0.0003007416962645948, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 962,  Mean reward: -0.5, Mean Entropy: 0.0003513452538754791, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 963,  Mean reward: -3.25, Mean Entropy: 0.0003870421787723899, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 964,  Mean reward: -3.75, Mean Entropy: 0.00039913656655699015, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 965,  Mean reward: -3.0, Mean Entropy: 0.00037496560253202915, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 966,  Mean reward: 0.0, Mean Entropy: 0.00028857350116595626, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 967,  Mean reward: -2.75, Mean Entropy: 0.0003729633754119277, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 968,  Mean reward: -2.0, Mean Entropy: 0.000388319487683475, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 969,  Mean reward: -1.75, Mean Entropy: 0.0003118113672826439, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 970,  Mean reward: -0.25, Mean Entropy: 0.00037460896419361234, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 971,  Mean reward: -2.5, Mean Entropy: 0.00031870335806161165, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 972,  Mean reward: -2.0, Mean Entropy: 0.00041955779306590557, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 973,  Mean reward: 0.0, Mean Entropy: 0.0003325438592582941, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 974,  Mean reward: -1.25, Mean Entropy: 0.0003745912981685251, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 975,  Mean reward: -2.5, Mean Entropy: 0.0003433475212659687, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 976,  Mean reward: -3.0, Mean Entropy: 0.00043616449693217874, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 977,  Mean reward: -3.25, Mean Entropy: 0.00041116721695289016, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 978,  Mean reward: 0.6392405063291139, Mean Entropy: 0.00011056220682803541, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 979,  Mean reward: -0.75, Mean Entropy: 6.5966734837275e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 980,  Mean reward: -1.5, Mean Entropy: 7.64749274821952e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 981,  Mean reward: -1.25, Mean Entropy: 6.170482083689421e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 982,  Mean reward: -1.0, Mean Entropy: 6.128541281213984e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 983,  Mean reward: -2.5, Mean Entropy: 6.612026481889188e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 984,  Mean reward: -2.75, Mean Entropy: 6.517271685879678e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 985,  Mean reward: -1.0, Mean Entropy: 6.468486390076578e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 986,  Mean reward: -3.75, Mean Entropy: 7.881054625613615e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 987,  Mean reward: -2.25, Mean Entropy: 6.682089588139206e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 988,  Mean reward: -2.75, Mean Entropy: 6.786543235648423e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 989,  Mean reward: -3.25, Mean Entropy: 7.262031431309879e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 990,  Mean reward: -1.75, Mean Entropy: 6.722298712702468e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 991,  Mean reward: -0.25, Mean Entropy: 6.171994027681649e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 992,  Mean reward: -3.5, Mean Entropy: 7.450190605595708e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 993,  Mean reward: -1.5, Mean Entropy: 7.09416635800153e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 994,  Mean reward: -2.0, Mean Entropy: 6.715532799717039e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 995,  Mean reward: -4.0, Mean Entropy: 7.830403774278238e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 996,  Mean reward: -1.0, Mean Entropy: 6.472358654718846e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 997,  Mean reward: -1.75, Mean Entropy: 7.077669579302892e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 998,  Mean reward: -3.5, Mean Entropy: 7.315508992178366e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 999,  Mean reward: -3.25, Mean Entropy: 7.579824887216091e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1000,  Mean reward: -1.75, Mean Entropy: 6.866276089567691e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: -1.75, Mean Entropy: 7.475180609617382e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1002,  Mean reward: -3.0, Mean Entropy: 7.121704402379692e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1003,  Mean reward: -2.75, Mean Entropy: 6.859636050648987e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1004,  Mean reward: -2.75, Mean Entropy: 7.297583943000063e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1005,  Mean reward: 0.5, Mean Entropy: 6.537857552757487e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1006,  Mean reward: -2.0, Mean Entropy: 7.20281750545837e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1007,  Mean reward: -2.5, Mean Entropy: 8.005484414752573e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1008,  Mean reward: -2.25, Mean Entropy: 6.653614400420338e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1009,  Mean reward: -0.5, Mean Entropy: 6.371083145495504e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1010,  Mean reward: -2.75, Mean Entropy: 7.424596697092056e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1011,  Mean reward: -2.0, Mean Entropy: 6.999724428169429e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1012,  Mean reward: -3.5, Mean Entropy: 7.873428694438189e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1013,  Mean reward: -0.75, Mean Entropy: 7.790987001499161e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1014,  Mean reward: -2.75, Mean Entropy: 7.763183384668082e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1015,  Mean reward: -0.75, Mean Entropy: 7.361997268162668e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1016,  Mean reward: -1.0, Mean Entropy: 6.289566226769239e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1017,  Mean reward: -1.25, Mean Entropy: 6.866559124318883e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1018,  Mean reward: -1.75, Mean Entropy: 7.749226642772555e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1019,  Mean reward: -2.5, Mean Entropy: 8.607655036030337e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 1020,  Mean reward: 0.0, Mean Entropy: 7.134937914088368e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 1021,  Mean reward: -3.0, Mean Entropy: 7.74995714891702e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1022,  Mean reward: -2.25, Mean Entropy: 7.910033309599385e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1023,  Mean reward: -2.75, Mean Entropy: 8.603767491877079e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1024,  Mean reward: -2.5, Mean Entropy: 8.173760579666123e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1025,  Mean reward: -2.5, Mean Entropy: 8.293800055980682e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1026,  Mean reward: -2.75, Mean Entropy: 8.443230763077736e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1027,  Mean reward: -2.0, Mean Entropy: 8.389818685827777e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1028,  Mean reward: -2.25, Mean Entropy: 7.828355592209846e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1029,  Mean reward: -1.5, Mean Entropy: 8.31735524116084e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1030,  Mean reward: -1.0, Mean Entropy: 8.070198236964643e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1031,  Mean reward: -1.25, Mean Entropy: 7.951294537633657e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1032,  Mean reward: -1.75, Mean Entropy: 8.33728990983218e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1033,  Mean reward: -3.75, Mean Entropy: 9.453081293031573e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1034,  Mean reward: -3.5, Mean Entropy: 9.55368741415441e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1035,  Mean reward: 0.25, Mean Entropy: 7.514293247368187e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1036,  Mean reward: 0.0, Mean Entropy: 6.962641782592982e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1037,  Mean reward: -1.5, Mean Entropy: 8.220845484174788e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1038,  Mean reward: -1.75, Mean Entropy: 8.976329991128296e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1039,  Mean reward: -1.5, Mean Entropy: 7.678016845602542e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1040,  Mean reward: -1.0, Mean Entropy: 7.49342143535614e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1041,  Mean reward: -3.0, Mean Entropy: 9.204452362610027e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1042,  Mean reward: -0.25, Mean Entropy: 7.539850048488006e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1043,  Mean reward: -1.5, Mean Entropy: 8.257226727437228e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 1044,  Mean reward: -1.0, Mean Entropy: 9.072331886272877e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1045,  Mean reward: -1.5, Mean Entropy: 8.211948443204165e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1046,  Mean reward: -1.0, Mean Entropy: 8.753490692470223e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1047,  Mean reward: 0.5, Mean Entropy: 8.438310760539025e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1048,  Mean reward: -0.75, Mean Entropy: 8.18167463876307e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 1049,  Mean reward: -1.25, Mean Entropy: 7.63054340495728e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1050,  Mean reward: -2.0, Mean Entropy: 8.513652574038133e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1051,  Mean reward: -1.0, Mean Entropy: 8.672466356074437e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 1052,  Mean reward: 0.0, Mean Entropy: 7.198162347776815e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1053,  Mean reward: -1.0, Mean Entropy: 8.660097228130326e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1054,  Mean reward: -3.75, Mean Entropy: 0.00010482113430043682, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1055,  Mean reward: -2.25, Mean Entropy: 9.634305024519563e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1056,  Mean reward: -0.75, Mean Entropy: 8.752977009862661e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1057,  Mean reward: 0.25, Mean Entropy: 8.225128840422258e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1058,  Mean reward: -0.5, Mean Entropy: 9.28941008169204e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1059,  Mean reward: -2.0, Mean Entropy: 0.000100317171018105, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.00s
Iteration: 1060,  Mean reward: -2.75, Mean Entropy: 9.964152559405193e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1061,  Mean reward: -2.0, Mean Entropy: 9.01537569006905e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1062,  Mean reward: -2.25, Mean Entropy: 0.00010433909483253956, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1063,  Mean reward: -0.5, Mean Entropy: 8.629819785710424e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1064,  Mean reward: -2.75, Mean Entropy: 0.00010627326264511794, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1065,  Mean reward: -1.25, Mean Entropy: 9.642627264838666e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 1066,  Mean reward: -2.25, Mean Entropy: 9.782626148080453e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1067,  Mean reward: -1.5, Mean Entropy: 0.00010459414625074714, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1068,  Mean reward: -3.5, Mean Entropy: 0.00010277748515363783, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1069,  Mean reward: -2.5, Mean Entropy: 0.00010516693873796612, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1070,  Mean reward: -3.75, Mean Entropy: 0.0001148708033724688, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1071,  Mean reward: -1.5, Mean Entropy: 9.92750283330679e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1072,  Mean reward: -1.5, Mean Entropy: 0.00010021973866969347, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1073,  Mean reward: -2.5, Mean Entropy: 0.00011639008152997121, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1074,  Mean reward: -2.25, Mean Entropy: 0.00010460398334544152, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1075,  Mean reward: -2.75, Mean Entropy: 0.00010596997890388593, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1076,  Mean reward: -2.0, Mean Entropy: 0.00010399980237707496, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1077,  Mean reward: -1.5, Mean Entropy: 0.00010745165491243824, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1078,  Mean reward: -2.0, Mean Entropy: 0.00010918510815827176, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1079,  Mean reward: -2.75, Mean Entropy: 0.00012075422273483127, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 1080,  Mean reward: -1.5, Mean Entropy: 9.277567733079195e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1081,  Mean reward: -1.5, Mean Entropy: 9.698553913040087e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 1082,  Mean reward: -2.5, Mean Entropy: 0.00010903750080615282, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1083,  Mean reward: -2.0, Mean Entropy: 0.0001110955054173246, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1084,  Mean reward: -2.5, Mean Entropy: 0.00010145259875571355, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1085,  Mean reward: -1.25, Mean Entropy: 0.00010805352940224111, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 1086,  Mean reward: -2.25, Mean Entropy: 0.00011444644769653678, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1087,  Mean reward: -1.5, Mean Entropy: 0.0001171963958768174, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1088,  Mean reward: -1.75, Mean Entropy: 0.00010999797814292833, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1089,  Mean reward: -4.0, Mean Entropy: 0.00012886572221759707, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1090,  Mean reward: -1.25, Mean Entropy: 0.0001146087161032483, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1091,  Mean reward: -2.5, Mean Entropy: 0.00012111007526982576, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1092,  Mean reward: -3.0, Mean Entropy: 0.00012376373342704028, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1093,  Mean reward: -1.75, Mean Entropy: 0.00010941115760942921, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1094,  Mean reward: -2.25, Mean Entropy: 0.00012368839816190302, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1095,  Mean reward: -2.5, Mean Entropy: 0.00012304393749218434, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1096,  Mean reward: -4.0, Mean Entropy: 0.0001415959413861856, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1097,  Mean reward: -3.25, Mean Entropy: 0.00013107137056067586, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1098,  Mean reward: -1.5, Mean Entropy: 0.0001236935204360634, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1099,  Mean reward: -0.75, Mean Entropy: 0.00010654980724211782, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 1100,  Mean reward: -0.75, Mean Entropy: 0.00011675594578264281, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: -1.5, Mean Entropy: 0.00012811485794372857, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1102,  Mean reward: 0.5, Mean Entropy: 0.00011322875798214227, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1103,  Mean reward: -2.5, Mean Entropy: 0.00014199415454640985, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1104,  Mean reward: -2.0, Mean Entropy: 0.0001372368133161217, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1105,  Mean reward: -3.5, Mean Entropy: 0.00013858312740921974, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1106,  Mean reward: -3.25, Mean Entropy: 0.00013183642295189202, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1107,  Mean reward: -2.25, Mean Entropy: 0.00014090182958170772, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1108,  Mean reward: -3.0, Mean Entropy: 0.0001452298165531829, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1109,  Mean reward: -1.5, Mean Entropy: 0.00013400653551798314, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1110,  Mean reward: -1.0, Mean Entropy: 0.00013578892685472965, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1111,  Mean reward: -2.5, Mean Entropy: 0.00015055194671731442, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1112,  Mean reward: 0.75, Mean Entropy: 0.00010988656140398234, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1113,  Mean reward: -2.25, Mean Entropy: 0.00014246959472075105, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1114,  Mean reward: -4.25, Mean Entropy: 0.00014914384519215673, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1115,  Mean reward: -2.75, Mean Entropy: 0.0001444538647774607, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1116,  Mean reward: -1.5, Mean Entropy: 0.0001323311444139108, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1117,  Mean reward: -1.5, Mean Entropy: 0.00013779396249447018, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1118,  Mean reward: -2.25, Mean Entropy: 0.0001428337418474257, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1119,  Mean reward: 0.75, Mean Entropy: 0.00013584265252575278, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1120,  Mean reward: -3.0, Mean Entropy: 0.00015658859047107399, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1121,  Mean reward: -2.25, Mean Entropy: 0.00014604062016587704, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1122,  Mean reward: -3.75, Mean Entropy: 0.0001543215475976467, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1123,  Mean reward: -4.5, Mean Entropy: 0.00016713740478735417, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1124,  Mean reward: -1.75, Mean Entropy: 0.00013833303819410503, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1125,  Mean reward: -2.0, Mean Entropy: 0.00015728360449429601, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1126,  Mean reward: -2.25, Mean Entropy: 0.00014652713434770703, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1127,  Mean reward: -1.5, Mean Entropy: 0.00014655062113888562, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1128,  Mean reward: -2.75, Mean Entropy: 0.0001566196297062561, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1129,  Mean reward: -2.0, Mean Entropy: 0.0001661282149143517, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1130,  Mean reward: -0.5, Mean Entropy: 0.00012552057160064578, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1131,  Mean reward: -1.75, Mean Entropy: 0.00013889512047171593, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1132,  Mean reward: -2.25, Mean Entropy: 0.00014234197442419827, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1133,  Mean reward: -1.25, Mean Entropy: 0.00014774984447285533, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1134,  Mean reward: -2.5, Mean Entropy: 0.0001489961869083345, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1135,  Mean reward: -2.0, Mean Entropy: 0.00014804030070081353, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1136,  Mean reward: -3.0, Mean Entropy: 0.00013850527466274798, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1137,  Mean reward: -2.5, Mean Entropy: 0.00015354581410065293, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1138,  Mean reward: -1.5, Mean Entropy: 0.00015019296552054584, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1139,  Mean reward: -2.75, Mean Entropy: 0.00014530097541864961, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 1140,  Mean reward: -4.0, Mean Entropy: 0.00015071986126713455, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1141,  Mean reward: -4.25, Mean Entropy: 0.00016622873954474926, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1142,  Mean reward: -3.25, Mean Entropy: 0.00015936739509925246, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1143,  Mean reward: -1.25, Mean Entropy: 0.00013555392797570676, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1144,  Mean reward: -1.5, Mean Entropy: 0.0001542641402920708, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1145,  Mean reward: -1.75, Mean Entropy: 0.00015778605302330106, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1146,  Mean reward: -1.75, Mean Entropy: 0.00014694116543978453, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1147,  Mean reward: -3.75, Mean Entropy: 0.00017054499767255038, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1148,  Mean reward: -0.75, Mean Entropy: 0.0001482105435570702, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1149,  Mean reward: -3.75, Mean Entropy: 0.00016979023348540068, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1150,  Mean reward: -2.0, Mean Entropy: 0.00015625600644852966, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1151,  Mean reward: -1.75, Mean Entropy: 0.00015831462224014103, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1152,  Mean reward: -1.25, Mean Entropy: 0.00014599369023926556, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1153,  Mean reward: -2.75, Mean Entropy: 0.00018460738647263497, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1154,  Mean reward: -1.0, Mean Entropy: 0.00016227384912781417, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1155,  Mean reward: -2.0, Mean Entropy: 0.00016707790200598538, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1156,  Mean reward: -2.75, Mean Entropy: 0.00016931898426264524, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1157,  Mean reward: -2.75, Mean Entropy: 0.0001786854991223663, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 1158,  Mean reward: -2.5, Mean Entropy: 0.00016132743621710688, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1159,  Mean reward: -4.0, Mean Entropy: 0.00019191656610928476, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1160,  Mean reward: -0.5, Mean Entropy: 0.00016439001774415374, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1161,  Mean reward: -1.75, Mean Entropy: 0.00016215590585488826, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 1162,  Mean reward: -2.25, Mean Entropy: 0.00016779416182544082, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1163,  Mean reward: -1.5, Mean Entropy: 0.00017383426893502474, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1164,  Mean reward: -2.75, Mean Entropy: 0.00016991204756777734, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1165,  Mean reward: -3.25, Mean Entropy: 0.0001997937506530434, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1166,  Mean reward: -1.75, Mean Entropy: 0.0001644647854845971, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1167,  Mean reward: 0.75, Mean Entropy: 0.00014883739640936255, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1168,  Mean reward: -1.25, Mean Entropy: 0.00015602960775140673, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1169,  Mean reward: -1.25, Mean Entropy: 0.00017031676543410867, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1170,  Mean reward: -2.0, Mean Entropy: 0.0001830908004194498, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1171,  Mean reward: -2.75, Mean Entropy: 0.00018279097275808454, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1172,  Mean reward: -3.0, Mean Entropy: 0.00018613634165376425, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1173,  Mean reward: -3.0, Mean Entropy: 0.00020243499602656811, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1174,  Mean reward: -2.5, Mean Entropy: 0.00017558151739649475, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1175,  Mean reward: -1.5, Mean Entropy: 0.00017363819642923772, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1176,  Mean reward: -0.5, Mean Entropy: 0.00015929987421259284, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1177,  Mean reward: 0.0, Mean Entropy: 0.00015315809287130833, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1178,  Mean reward: -1.5, Mean Entropy: 0.0001815085852285847, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1179,  Mean reward: -3.5, Mean Entropy: 0.0001830488326959312, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.01s
Iteration: 1180,  Mean reward: -3.0, Mean Entropy: 0.00017831141303759068, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1181,  Mean reward: -1.25, Mean Entropy: 0.00017484172713011503, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1182,  Mean reward: -2.75, Mean Entropy: 0.00018613789870869368, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1183,  Mean reward: 0.25, Mean Entropy: 0.00017331840354017913, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1184,  Mean reward: -1.75, Mean Entropy: 0.00016979982319753617, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1185,  Mean reward: -1.25, Mean Entropy: 0.0001790645474102348, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1186,  Mean reward: -4.75, Mean Entropy: 0.00018946848285850137, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1187,  Mean reward: -2.0, Mean Entropy: 0.00018841323617380112, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1188,  Mean reward: -1.25, Mean Entropy: 0.00018009354243986309, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1189,  Mean reward: -3.75, Mean Entropy: 0.00020474176562856883, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1190,  Mean reward: -1.25, Mean Entropy: 0.00018459613784216344, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1191,  Mean reward: -2.0, Mean Entropy: 0.00017842577653937042, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1192,  Mean reward: -3.25, Mean Entropy: 0.00018665175593923777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1193,  Mean reward: -1.25, Mean Entropy: 0.00019353411335032433, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1194,  Mean reward: -1.75, Mean Entropy: 0.0002119431010214612, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1195,  Mean reward: -4.0, Mean Entropy: 0.00021922843006905168, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1196,  Mean reward: -1.5, Mean Entropy: 0.00018943729810416698, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1197,  Mean reward: -2.0, Mean Entropy: 0.00021438403928186744, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1198,  Mean reward: -2.25, Mean Entropy: 0.00020536361262202263, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1199,  Mean reward: -2.25, Mean Entropy: 0.00022398500004783273, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1200,  Mean reward: -3.0, Mean Entropy: 0.00021487174672074616, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: -1.5, Mean Entropy: 0.00019995690672658384, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1202,  Mean reward: -3.75, Mean Entropy: 0.0002094241208396852, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1203,  Mean reward: -3.0, Mean Entropy: 0.00021612939599435776, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1204,  Mean reward: -1.75, Mean Entropy: 0.00022693756909575313, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1205,  Mean reward: -1.75, Mean Entropy: 0.0002168406790588051, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 1206,  Mean reward: -1.0, Mean Entropy: 0.00019695807714015245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1207,  Mean reward: -4.5, Mean Entropy: 0.0002249794633826241, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1208,  Mean reward: -0.5, Mean Entropy: 0.0001900870556710288, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 1209,  Mean reward: -1.75, Mean Entropy: 0.00019324517052154988, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 1210,  Mean reward: -2.5, Mean Entropy: 0.00022222724510356784, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.48it/s]100%|| 1/1 [00:00<00:00,  1.48it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.1 0.9 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type FE
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.29it/s]100%|| 1/1 [00:00<00:00,  1.29it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 3 action_probs [0.0 0.0 0.0 0.3 0.3 0.3 0.0 0.0] r -1.0 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  Done after 5 steps, Captured: False Reward: -6.5


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 3.50 +/- 1.50
   Lengths :[5 2]
Average return: 0.75 +/- 7.25
   Returns :[-6.5 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 0.75
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type FE
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.84it/s]100%|| 1/1 [00:00<00:00,  1.84it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type FE
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: 2.25
  std over seeds: 4.218016911614588
  per seed: [8.000 0.750 -2.000]

success_rate.......
  avg over seeds: 0.6666666666666666
  std over seeds: 0.23570226039551584
  per seed: [1.000 0.500 0.500]

