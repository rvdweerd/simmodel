#!/bin/bash
#SBATCH -p gpu_shared
#SBATCH --gpus=1
#SBATCH --job-name=gnn_ppo
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=48:00:00
#SBATCH --mem=48000M
#SBATCH --output=slurm_output_%A.out

__conda_setup="$('/home/weerdr/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "/home/weerdr/miniconda3/etc/profile.d/conda.sh" ]; then
        . "/home/weerdr/miniconda3/etc/profile.d/conda.sh"
    else
        export PATH="/home/weerdr/miniconda3/bin:$PATH"
    fi
fi
unset __conda_setup

cd $HOME/testing/sim
conda activate rlcourse

train_on="M5x5F_mixed_obs"
#train_on="MemTask-U1"
batch_size=2
#obs_mask="None"
#obs_rate=1.0
obs_mask="mix"
obs_rate=0
#obs_mask="freq"
#obs_rate=.2
emb_dim=48
#lstm_hdim=64
#lstm_type="None"
lstm_hdim=48
lstm_layers=1
emb_iterT=5
qnet="gat2"
train="True"
eval="False"
test="False"
num_seeds=5
demoruns="False"
parallel_rollouts=4
rollout_steps=75
lr=5e-4
num_step=70000
critic="q"
lstm_type="FE"
eval_deter="True"
nfm_func="NFM_ev_ec_t_dt_at_um_us"
#nfm_func="NFM_ev_ec_t_dt_at_ustack"
for seed0 in {0,5,10}
do
    python Phase3_lstm-gnn-ppo_simp.py --train_on $train_on --batch_size $batch_size --lr $lr --obs_mask $obs_mask --obs_rate $obs_rate --emb_dim $emb_dim --lstm_type $lstm_type --lstm_hdim $lstm_hdim --lstm_layers $lstm_layers --emb_iterT $emb_iterT --nfm_func $nfm_func --qnet $qnet --train $train --eval $eval --test $test --num_seeds $num_seeds --seed0 $seed0 --demoruns $demoruns --parallel_rollouts $parallel_rollouts --rollout_steps $rollout_steps --critic $critic --eval_deter $eval_deter --num_step $num_step&
done
wait