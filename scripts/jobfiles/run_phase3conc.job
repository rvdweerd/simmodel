#!/bin/bash
#SBATCH -p gpu_shared
#SBATCH --gpus=1
#SBATCH --job-name=gnn_ppo
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=6:30:00
#SBATCH --mem=48000M
#SBATCH --output=slurm_output_%A.out

__conda_setup="$('/home/weerdr/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "/home/weerdr/miniconda3/etc/profile.d/conda.sh" ]; then
        . "/home/weerdr/miniconda3/etc/profile.d/conda.sh"
    else
        export PATH="/home/weerdr/miniconda3/bin:$PATH"
    fi
fi
unset __conda_setup

cd $HOME/testing/sim
conda activate rlcourse

#train_on="M5x5F_mixed_obs"
#train_on="NWB_AMS_mixed_obs"
train_on="NWB_AMS"
#train_on="MemTask-U1"
batch_size=2
obs_mask="None"
obs_rate=1.0
#obs_mask="freq"
#obs_rate=.2
emb_dim=64
#lstm_hdim=64
lstm_hdim=64
lstm_layers=1
emb_iterT=5
nfm_func="NFM_ev_ec_t_dt_at_um_us"
#nfm_func="NFM_ev_ec_t_dt_at_ustack"
qnet="gat2"
critic="q"
train="False"
eval="False"
test="True"
test_heur="False"
num_seeds=6
demoruns="False"
num_step=25001
parallel_rollouts=4
type_obs_wrap="BasicDict"
lr=5e-4
lstm_type="None"
eval_deter="True"

for seed0 in {0,}
do
    python Phase3_lstm-gnn-ppo_simp.py --train_on $train_on --batch_size $batch_size --obs_mask $obs_mask --obs_rate $obs_rate --emb_dim $emb_dim --lstm_type $lstm_type --lstm_hdim $lstm_hdim --lstm_layers $lstm_layers --emb_iterT $emb_iterT --nfm_func $nfm_func --qnet $qnet --train $train --eval $eval --test $test --num_seeds $num_seeds --seed0 $seed0 --demoruns $demoruns --parallel_rollouts $parallel_rollouts --critic $critic --type_obs_wrap $type_obs_wrap --test_heur $test_heur --eval_deter $eval_deter --num_step $num_step&
done
wait