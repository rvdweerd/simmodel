batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: None
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_ustack
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: q
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_ustack/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
node_dim: 8
lstm_on: False
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy(
  (FE): FeatureExtractor(
    (gat): GATv2(8, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (theta6_v): Linear(in_features=24, out_features=24, bias=True)
    (theta7_v): Linear(in_features=24, out_features=24, bias=True)
    (theta5_v): Linear(in_features=48, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with LSTM switched off and GATv2 feature extraction
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 8]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 8]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.theta6_v.weight        [24, 24]     requires_grad=True
V.theta6_v.bias          [24]         requires_grad=True
V.theta7_v.weight        [24, 24]     requires_grad=True
V.theta7_v.bias          [24]         requires_grad=True
V.theta5_v.weight        [1, 48]      requires_grad=True
V.theta5_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 7970
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -2.402439024390244, Mean Entropy: 0.945857048034668, complete_episode_count: 41.0, Gather time: 6.02s, Train time: 3.26s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -3.6547619047619047, Mean Entropy: 0.9458567500114441, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 2,  Mean reward: -4.195121951219512, Mean Entropy: 0.9097554683685303, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 3,  Mean reward: -4.092105263157895, Mean Entropy: 0.9747380018234253, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 4,  Mean reward: -2.7564102564102564, Mean Entropy: 0.9530763626098633, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 5,  Mean reward: -3.7435897435897436, Mean Entropy: 0.9025322794914246, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 6,  Mean reward: -5.058139534883721, Mean Entropy: 0.9097515940666199, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 7,  Mean reward: -1.4534883720930232, Mean Entropy: 0.924189567565918, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.29s
Iteration: 8,  Mean reward: -4.2875, Mean Entropy: 0.9313949942588806, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 9,  Mean reward: -4.261904761904762, Mean Entropy: 0.9313820600509644, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 10,  Mean reward: -4.28125, Mean Entropy: 0.9819058775901794, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 11,  Mean reward: -3.925, Mean Entropy: 0.9818839430809021, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 12,  Mean reward: -6.8604651162790695, Mean Entropy: 0.8952479362487793, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 13,  Mean reward: -2.5232558139534884, Mean Entropy: 0.9385492205619812, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 14,  Mean reward: -1.315217391304348, Mean Entropy: 0.9601854085922241, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 15,  Mean reward: -4.9625, Mean Entropy: 0.8951300382614136, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 16,  Mean reward: -2.9878048780487805, Mean Entropy: 0.9886583089828491, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 17,  Mean reward: -5.059523809523809, Mean Entropy: 0.8657402992248535, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 18,  Mean reward: -6.25531914893617, Mean Entropy: 0.9729553461074829, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 19,  Mean reward: -4.535714285714286, Mean Entropy: 0.9365870356559753, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 20,  Mean reward: -3.090909090909091, Mean Entropy: 0.9009329080581665, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 21,  Mean reward: -6.8, Mean Entropy: 0.9433817267417908, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 22,  Mean reward: -4.488372093023256, Mean Entropy: 0.942992627620697, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 23,  Mean reward: -6.130952380952381, Mean Entropy: 0.9141653776168823, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.53s
Iteration: 24,  Mean reward: -3.8068181818181817, Mean Entropy: 0.928266704082489, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 25,  Mean reward: -2.466666666666667, Mean Entropy: 0.9794176816940308, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 26,  Mean reward: -3.5113636363636362, Mean Entropy: 0.9709674715995789, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 27,  Mean reward: -4.093023255813954, Mean Entropy: 0.9679245352745056, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 28,  Mean reward: -4.613636363636363, Mean Entropy: 0.9017302989959717, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 29,  Mean reward: -4.511627906976744, Mean Entropy: 0.9899520874023438, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 30,  Mean reward: -3.5, Mean Entropy: 0.9072413444519043, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 31,  Mean reward: -4.186046511627907, Mean Entropy: 0.9950108528137207, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 32,  Mean reward: -3.9791666666666665, Mean Entropy: 0.9277645349502563, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 33,  Mean reward: -4.5, Mean Entropy: 0.90190190076828, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 34,  Mean reward: -6.066666666666666, Mean Entropy: 0.9085220694541931, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 35,  Mean reward: -4.633333333333334, Mean Entropy: 0.9068827629089355, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 36,  Mean reward: -5.117021276595745, Mean Entropy: 0.8566198348999023, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 37,  Mean reward: -4.5, Mean Entropy: 0.8715866804122925, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 38,  Mean reward: -3.1777777777777776, Mean Entropy: 0.9102341532707214, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 39,  Mean reward: -3.311111111111111, Mean Entropy: 0.9431278109550476, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 40,  Mean reward: -3.7666666666666666, Mean Entropy: 0.9164785146713257, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 41,  Mean reward: -3.2934782608695654, Mean Entropy: 0.8940019607543945, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 42,  Mean reward: -5.458333333333333, Mean Entropy: 0.877368152141571, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 43,  Mean reward: -4.042553191489362, Mean Entropy: 0.8731305003166199, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 44,  Mean reward: -2.1153846153846154, Mean Entropy: 0.8435811400413513, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 45,  Mean reward: -2.5392156862745097, Mean Entropy: 0.8889440298080444, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 46,  Mean reward: -3.75, Mean Entropy: 0.6972731947898865, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 47,  Mean reward: -5.310344827586207, Mean Entropy: 0.6567750573158264, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 48,  Mean reward: -1.2916666666666667, Mean Entropy: 0.7028158903121948, complete_episode_count: 60.0, Gather time: 0.73s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 49,  Mean reward: -1.0, Mean Entropy: 0.7606167197227478, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 50,  Mean reward: -3.517857142857143, Mean Entropy: 0.6504626274108887, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 51,  Mean reward: -0.8275862068965517, Mean Entropy: 0.5926710963249207, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.29s
Iteration: 52,  Mean reward: -0.9545454545454546, Mean Entropy: 0.6933080554008484, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.28s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 53,  Mean reward: -0.3181818181818182, Mean Entropy: 0.9116498827934265, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.29s
Iteration: 54,  Mean reward: -2.5816326530612246, Mean Entropy: 0.6106241941452026, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 55,  Mean reward: -1.163265306122449, Mean Entropy: 0.7437515258789062, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 56,  Mean reward: -4.340425531914893, Mean Entropy: 0.8980368375778198, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.49s
Iteration: 57,  Mean reward: -1.1333333333333333, Mean Entropy: 0.9240556955337524, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 58,  Mean reward: -2.6904761904761907, Mean Entropy: 0.9354097247123718, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 59,  Mean reward: -4.512195121951219, Mean Entropy: 0.9057645797729492, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 60,  Mean reward: -3.9204545454545454, Mean Entropy: 0.9050056338310242, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 61,  Mean reward: -3.75, Mean Entropy: 0.9976686239242554, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 62,  Mean reward: -6.421052631578948, Mean Entropy: 0.903597354888916, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 63,  Mean reward: -5.267441860465116, Mean Entropy: 0.8640760779380798, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 64,  Mean reward: -4.761904761904762, Mean Entropy: 0.8863972425460815, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 65,  Mean reward: -2.8085106382978724, Mean Entropy: 0.957021951675415, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 66,  Mean reward: -5.571428571428571, Mean Entropy: 0.9409979581832886, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 67,  Mean reward: -5.226190476190476, Mean Entropy: 0.9354963302612305, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 68,  Mean reward: -3.234042553191489, Mean Entropy: 0.6283798217773438, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 69,  Mean reward: -2.521276595744681, Mean Entropy: 0.6933653354644775, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 70,  Mean reward: 0.018518518518518517, Mean Entropy: 0.7408413290977478, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 71,  Mean reward: -1.64, Mean Entropy: 0.68438720703125, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 72,  Mean reward: -0.8796296296296297, Mean Entropy: 0.7078806757926941, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 73,  Mean reward: -0.12280701754385964, Mean Entropy: 0.7076048851013184, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 74,  Mean reward: -1.303921568627451, Mean Entropy: 0.7170578837394714, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 75,  Mean reward: 2.0701754385964914, Mean Entropy: 0.6169816851615906, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.29s
Iteration: 76,  Mean reward: 1.4310344827586208, Mean Entropy: 0.5739504098892212, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 77,  Mean reward: -2.9423076923076925, Mean Entropy: 0.7100561857223511, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 78,  Mean reward: -1.1415094339622642, Mean Entropy: 0.6353020668029785, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 79,  Mean reward: 0.8125, Mean Entropy: 0.6287283301353455, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 80,  Mean reward: -0.9134615384615384, Mean Entropy: 0.6902582049369812, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 81,  Mean reward: -2.0, Mean Entropy: 0.6455062627792358, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 82,  Mean reward: -0.9074074074074074, Mean Entropy: 0.6623873710632324, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 83,  Mean reward: -1.576271186440678, Mean Entropy: 0.6404232382774353, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 84,  Mean reward: -0.9629629629629629, Mean Entropy: 0.6023358702659607, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 85,  Mean reward: -1.1160714285714286, Mean Entropy: 0.6120560765266418, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 86,  Mean reward: -0.9830508474576272, Mean Entropy: 0.6576113700866699, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 87,  Mean reward: -2.912280701754386, Mean Entropy: 0.5410842299461365, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 88,  Mean reward: -0.16393442622950818, Mean Entropy: 0.4888245761394501, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 89,  Mean reward: -3.5, Mean Entropy: 0.6308683156967163, complete_episode_count: 57.0, Gather time: 0.70s, Train time: 1.29s
Iteration: 90,  Mean reward: -2.3137254901960786, Mean Entropy: 0.5302678942680359, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 91,  Mean reward: -1.2923076923076924, Mean Entropy: 0.5490878820419312, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 92,  Mean reward: -2.3421052631578947, Mean Entropy: 0.5881078243255615, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 93,  Mean reward: 0.1484375, Mean Entropy: 0.5810855627059937, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 94,  Mean reward: -0.31666666666666665, Mean Entropy: 0.524911642074585, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 95,  Mean reward: -2.9375, Mean Entropy: 0.5666847825050354, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 96,  Mean reward: -0.31451612903225806, Mean Entropy: 0.5487197637557983, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 97,  Mean reward: -1.9754098360655739, Mean Entropy: 0.653515100479126, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 98,  Mean reward: -2.6293103448275863, Mean Entropy: 0.5894004702568054, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 99,  Mean reward: -4.589285714285714, Mean Entropy: 0.6579944491386414, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 100,  Mean reward: -3.0660377358490565, Mean Entropy: 0.583850622177124, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 0.14655172413793102, Mean Entropy: 0.7045383453369141, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 102,  Mean reward: -1.7452830188679245, Mean Entropy: 0.5981284976005554, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 103,  Mean reward: -3.323529411764706, Mean Entropy: 0.701246440410614, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 104,  Mean reward: -0.41818181818181815, Mean Entropy: 0.6094856262207031, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 105,  Mean reward: 1.0350877192982457, Mean Entropy: 0.7790427207946777, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 106,  Mean reward: -0.4818181818181818, Mean Entropy: 0.5974156856536865, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 107,  Mean reward: -1.5833333333333333, Mean Entropy: 0.6941045522689819, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 108,  Mean reward: -1.5851063829787233, Mean Entropy: 0.6298086643218994, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.30s
Iteration: 109,  Mean reward: -2.87, Mean Entropy: 0.73089599609375, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 110,  Mean reward: 0.8333333333333334, Mean Entropy: 0.6716367602348328, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 111,  Mean reward: 1.75, Mean Entropy: 0.613338828086853, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 112,  Mean reward: 0.7830188679245284, Mean Entropy: 0.5730614066123962, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 113,  Mean reward: -0.11, Mean Entropy: 0.5933094024658203, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 114,  Mean reward: 0.0196078431372549, Mean Entropy: 0.6928240656852722, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 115,  Mean reward: 0.05660377358490566, Mean Entropy: 0.7490939497947693, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 116,  Mean reward: 0.8725490196078431, Mean Entropy: 0.658049464225769, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 117,  Mean reward: 0.19811320754716982, Mean Entropy: 0.6532543897628784, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 118,  Mean reward: -2.033333333333333, Mean Entropy: 0.6757523417472839, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 119,  Mean reward: -1.55, Mean Entropy: 0.6120496988296509, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 120,  Mean reward: -0.8981481481481481, Mean Entropy: 0.6426286101341248, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 121,  Mean reward: 0.7884615384615384, Mean Entropy: 0.6366123557090759, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 122,  Mean reward: -1.41, Mean Entropy: 0.6971018314361572, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 123,  Mean reward: 1.8240740740740742, Mean Entropy: 0.6871850490570068, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 124,  Mean reward: -0.2, Mean Entropy: 0.6998002529144287, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 125,  Mean reward: 1.0462962962962963, Mean Entropy: 0.6210300922393799, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 126,  Mean reward: -0.4791666666666667, Mean Entropy: 0.6682330965995789, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 127,  Mean reward: 0.7358490566037735, Mean Entropy: 0.695875346660614, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 128,  Mean reward: 2.607843137254902, Mean Entropy: 0.6091359853744507, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 129,  Mean reward: -1.5980392156862746, Mean Entropy: 0.6187304258346558, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 130,  Mean reward: -2.4285714285714284, Mean Entropy: 0.7211216688156128, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 131,  Mean reward: -1.1346153846153846, Mean Entropy: 0.7375932931900024, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 132,  Mean reward: -0.20833333333333334, Mean Entropy: 0.6696588397026062, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 133,  Mean reward: 1.4245283018867925, Mean Entropy: 0.6104749441146851, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 134,  Mean reward: -2.163265306122449, Mean Entropy: 0.7639614939689636, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 135,  Mean reward: 1.6944444444444444, Mean Entropy: 0.6123695969581604, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 136,  Mean reward: -0.17346938775510204, Mean Entropy: 0.6561719179153442, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 137,  Mean reward: -0.12745098039215685, Mean Entropy: 0.7034880518913269, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 138,  Mean reward: -2.0851063829787235, Mean Entropy: 0.6573837995529175, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 139,  Mean reward: -1.07, Mean Entropy: 0.64156174659729, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 140,  Mean reward: -1.1346153846153846, Mean Entropy: 0.6763612627983093, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 141,  Mean reward: -0.96, Mean Entropy: 0.6583667993545532, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.30s
Iteration: 142,  Mean reward: 0.13725490196078433, Mean Entropy: 0.6876113414764404, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 143,  Mean reward: -2.03125, Mean Entropy: 0.6970528364181519, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 144,  Mean reward: -1.1862745098039216, Mean Entropy: 0.7637602686882019, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 145,  Mean reward: 0.5294117647058824, Mean Entropy: 0.5913763046264648, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 146,  Mean reward: -0.33636363636363636, Mean Entropy: 0.7131136655807495, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 147,  Mean reward: -2.0288461538461537, Mean Entropy: 0.6185720562934875, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 148,  Mean reward: 0.5918367346938775, Mean Entropy: 0.6461743116378784, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 149,  Mean reward: -0.48, Mean Entropy: 0.7049066424369812, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.31s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 150,  Mean reward: 2.7884615384615383, Mean Entropy: 0.6661520004272461, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 151,  Mean reward: -0.9411764705882353, Mean Entropy: 0.6618361473083496, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 152,  Mean reward: -1.202127659574468, Mean Entropy: 0.7097454071044922, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 153,  Mean reward: -1.4803921568627452, Mean Entropy: 0.7095497846603394, complete_episode_count: 51.0, Gather time: 0.72s, Train time: 1.30s
Iteration: 154,  Mean reward: 0.5480769230769231, Mean Entropy: 0.6062751412391663, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 155,  Mean reward: -2.8043478260869565, Mean Entropy: 0.6622318029403687, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 156,  Mean reward: 1.2254901960784315, Mean Entropy: 0.680863618850708, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 157,  Mean reward: 0.7169811320754716, Mean Entropy: 0.7153093814849854, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 158,  Mean reward: 0.5588235294117647, Mean Entropy: 0.6897791624069214, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 159,  Mean reward: 0.4803921568627451, Mean Entropy: 0.6034433841705322, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 160,  Mean reward: 2.3947368421052633, Mean Entropy: 0.7250287532806396, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 161,  Mean reward: 0.7767857142857143, Mean Entropy: 0.5730794668197632, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 162,  Mean reward: 1.471698113207547, Mean Entropy: 0.6291987895965576, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 163,  Mean reward: -1.3137254901960784, Mean Entropy: 0.6789471507072449, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 164,  Mean reward: -1.9042553191489362, Mean Entropy: 0.7074581980705261, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 165,  Mean reward: 0.2549019607843137, Mean Entropy: 0.6147536635398865, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 166,  Mean reward: -0.1836734693877551, Mean Entropy: 0.5678020715713501, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 167,  Mean reward: -0.10576923076923077, Mean Entropy: 0.6264276504516602, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 168,  Mean reward: -0.53, Mean Entropy: 0.6741122007369995, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 169,  Mean reward: -2.3333333333333335, Mean Entropy: 0.7374326586723328, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 170,  Mean reward: -0.3541666666666667, Mean Entropy: 0.6607484221458435, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 171,  Mean reward: -0.8979591836734694, Mean Entropy: 0.6678797602653503, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 172,  Mean reward: -1.4042553191489362, Mean Entropy: 0.630704402923584, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 173,  Mean reward: -0.36, Mean Entropy: 0.6374013423919678, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 174,  Mean reward: 0.46153846153846156, Mean Entropy: 0.694924533367157, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.30s
Iteration: 175,  Mean reward: -1.3888888888888888, Mean Entropy: 0.6193392872810364, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 176,  Mean reward: 1.7, Mean Entropy: 0.5772957801818848, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 177,  Mean reward: 1.2596153846153846, Mean Entropy: 0.5854560732841492, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 178,  Mean reward: 2.3962264150943398, Mean Entropy: 0.6744563579559326, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 179,  Mean reward: 0.6862745098039216, Mean Entropy: 0.6825133562088013, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 180,  Mean reward: -1.9673913043478262, Mean Entropy: 0.6736993193626404, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 181,  Mean reward: 1.847457627118644, Mean Entropy: 0.6873044371604919, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 182,  Mean reward: -2.2211538461538463, Mean Entropy: 0.7420434951782227, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 183,  Mean reward: -1.3, Mean Entropy: 0.6394157409667969, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 184,  Mean reward: -2.34375, Mean Entropy: 0.706992506980896, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.50s
Iteration: 185,  Mean reward: -0.6914893617021277, Mean Entropy: 0.6248599886894226, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 186,  Mean reward: 0.4636363636363636, Mean Entropy: 0.6290470957756042, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 187,  Mean reward: -0.7549019607843137, Mean Entropy: 0.6367014646530151, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 188,  Mean reward: -0.5098039215686274, Mean Entropy: 0.6062176823616028, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 189,  Mean reward: 0.9722222222222222, Mean Entropy: 0.5972363352775574, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 190,  Mean reward: 0.38, Mean Entropy: 0.6756147146224976, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.34s
Iteration: 191,  Mean reward: -1.0566037735849056, Mean Entropy: 0.7053078413009644, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 192,  Mean reward: 1.3421052631578947, Mean Entropy: 0.5643222332000732, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 193,  Mean reward: 0.646551724137931, Mean Entropy: 0.6454330682754517, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 194,  Mean reward: -1.3265306122448979, Mean Entropy: 0.6050295829772949, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 195,  Mean reward: -0.0392156862745098, Mean Entropy: 0.6708695292472839, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 196,  Mean reward: 0.6037735849056604, Mean Entropy: 0.5922967791557312, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 197,  Mean reward: 1.0576923076923077, Mean Entropy: 0.6074192523956299, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 198,  Mean reward: -2.25, Mean Entropy: 0.6722507476806641, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 199,  Mean reward: -0.54, Mean Entropy: 0.6459603309631348, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 200,  Mean reward: 0.28846153846153844, Mean Entropy: 0.6976048946380615, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 0.25471698113207547, Mean Entropy: 0.6919426321983337, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 202,  Mean reward: -0.9509803921568627, Mean Entropy: 0.6463840007781982, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 203,  Mean reward: 1.7545454545454546, Mean Entropy: 0.610139787197113, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 204,  Mean reward: 1.588235294117647, Mean Entropy: 0.6343819499015808, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 205,  Mean reward: 2.3508771929824563, Mean Entropy: 0.6034591197967529, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 206,  Mean reward: -0.46938775510204084, Mean Entropy: 0.6211955547332764, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 207,  Mean reward: 0.5490196078431373, Mean Entropy: 0.6452142596244812, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.30s
Iteration: 208,  Mean reward: -0.4326923076923077, Mean Entropy: 0.6844918131828308, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 209,  Mean reward: -0.6538461538461539, Mean Entropy: 0.5635625123977661, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 210,  Mean reward: -3.1847826086956523, Mean Entropy: 0.6828930377960205, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 211,  Mean reward: 0.4818181818181818, Mean Entropy: 0.6279681324958801, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 212,  Mean reward: -2.704081632653061, Mean Entropy: 0.6381665468215942, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 213,  Mean reward: -2.561224489795918, Mean Entropy: 0.7580991983413696, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 214,  Mean reward: -1.5, Mean Entropy: 0.645868182182312, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 215,  Mean reward: -1.2346938775510203, Mean Entropy: 0.6614210605621338, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 216,  Mean reward: -0.28431372549019607, Mean Entropy: 0.6556748151779175, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.52s
Iteration: 217,  Mean reward: -1.1979166666666667, Mean Entropy: 0.7105511426925659, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 218,  Mean reward: 1.587719298245614, Mean Entropy: 0.6571691036224365, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 219,  Mean reward: -0.5638297872340425, Mean Entropy: 0.7288698554039001, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 220,  Mean reward: 1.320754716981132, Mean Entropy: 0.6231395602226257, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 221,  Mean reward: -0.21, Mean Entropy: 0.6425905227661133, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 222,  Mean reward: -2.597826086956522, Mean Entropy: 0.7055850028991699, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 223,  Mean reward: 0.24528301886792453, Mean Entropy: 0.655377984046936, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 224,  Mean reward: 0.22549019607843138, Mean Entropy: 0.6015743017196655, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 225,  Mean reward: -0.4811320754716981, Mean Entropy: 0.6224111318588257, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 226,  Mean reward: -0.59375, Mean Entropy: 0.649836540222168, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 227,  Mean reward: 0.7115384615384616, Mean Entropy: 0.6672935485839844, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 228,  Mean reward: -0.8775510204081632, Mean Entropy: 0.6882188320159912, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 229,  Mean reward: -0.5784313725490197, Mean Entropy: 0.6550867557525635, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 230,  Mean reward: -0.0673076923076923, Mean Entropy: 0.7176846265792847, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 231,  Mean reward: -1.8076923076923077, Mean Entropy: 0.6103547811508179, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 232,  Mean reward: -0.8303571428571429, Mean Entropy: 0.6353413462638855, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 233,  Mean reward: -1.7692307692307692, Mean Entropy: 0.7301263213157654, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 234,  Mean reward: 0.71875, Mean Entropy: 0.5853981971740723, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 235,  Mean reward: 0.4716981132075472, Mean Entropy: 0.704948902130127, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 236,  Mean reward: -1.53, Mean Entropy: 0.623694658279419, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 237,  Mean reward: -0.8823529411764706, Mean Entropy: 0.6507822275161743, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 238,  Mean reward: -1.9387755102040816, Mean Entropy: 0.6438173651695251, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 239,  Mean reward: 0.4803921568627451, Mean Entropy: 0.6036978960037231, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 240,  Mean reward: 0.02830188679245283, Mean Entropy: 0.5970454812049866, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 241,  Mean reward: 0.7818181818181819, Mean Entropy: 0.6927957534790039, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 242,  Mean reward: -1.31, Mean Entropy: 0.6894021034240723, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 243,  Mean reward: -1.4285714285714286, Mean Entropy: 0.6174845099449158, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 244,  Mean reward: -0.6509433962264151, Mean Entropy: 0.6551746726036072, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 245,  Mean reward: -2.37, Mean Entropy: 0.7140594720840454, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 246,  Mean reward: 0.8818181818181818, Mean Entropy: 0.6510065197944641, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 247,  Mean reward: -3.5416666666666665, Mean Entropy: 0.6531809568405151, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 248,  Mean reward: 1.2222222222222223, Mean Entropy: 0.6466963291168213, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 249,  Mean reward: 1.0277777777777777, Mean Entropy: 0.48017609119415283, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 250,  Mean reward: -0.7924528301886793, Mean Entropy: 0.7296695709228516, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 251,  Mean reward: -0.4387755102040816, Mean Entropy: 0.6347384452819824, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 252,  Mean reward: -0.04, Mean Entropy: 0.6429057121276855, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 253,  Mean reward: 0.7653061224489796, Mean Entropy: 0.6643476486206055, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 254,  Mean reward: 0.16666666666666666, Mean Entropy: 0.6413875222206116, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 255,  Mean reward: 1.9727272727272727, Mean Entropy: 0.6843560934066772, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 256,  Mean reward: -3.186046511627907, Mean Entropy: 0.7123275995254517, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 257,  Mean reward: -0.4528301886792453, Mean Entropy: 0.7585555911064148, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 258,  Mean reward: 0.08823529411764706, Mean Entropy: 0.6942302584648132, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 259,  Mean reward: 1.1634615384615385, Mean Entropy: 0.8148168325424194, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 260,  Mean reward: -0.041666666666666664, Mean Entropy: 0.7411724925041199, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 261,  Mean reward: 0.1875, Mean Entropy: 0.6478216648101807, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 262,  Mean reward: -3.3645833333333335, Mean Entropy: 0.6843944787979126, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 263,  Mean reward: 0.18627450980392157, Mean Entropy: 0.8454717397689819, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 264,  Mean reward: -1.1956521739130435, Mean Entropy: 0.7094883918762207, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 265,  Mean reward: -2.0638297872340425, Mean Entropy: 0.7019819021224976, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 266,  Mean reward: 1.0092592592592593, Mean Entropy: 0.7994886636734009, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 267,  Mean reward: -1.4130434782608696, Mean Entropy: 0.646762490272522, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 268,  Mean reward: -1.2, Mean Entropy: 0.5851906538009644, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 269,  Mean reward: -0.66, Mean Entropy: 0.7776283025741577, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 270,  Mean reward: -1.4183673469387754, Mean Entropy: 0.6792181730270386, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 271,  Mean reward: -1.15, Mean Entropy: 0.6645926833152771, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 272,  Mean reward: -0.925531914893617, Mean Entropy: 0.6526047587394714, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 273,  Mean reward: 1.6603773584905661, Mean Entropy: 0.7302498817443848, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.29s
Iteration: 274,  Mean reward: 1.03, Mean Entropy: 0.6207513809204102, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 275,  Mean reward: -1.1326530612244898, Mean Entropy: 0.7272908091545105, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 276,  Mean reward: -0.16346153846153846, Mean Entropy: 0.8653213977813721, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 277,  Mean reward: -2.7906976744186047, Mean Entropy: 0.7940305471420288, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 278,  Mean reward: -3.8255813953488373, Mean Entropy: 0.7230161428451538, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 279,  Mean reward: -1.625, Mean Entropy: 0.9072259664535522, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 280,  Mean reward: -2.9782608695652173, Mean Entropy: 0.9291512370109558, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.52s
Iteration: 281,  Mean reward: -5.819148936170213, Mean Entropy: 0.8412432670593262, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 282,  Mean reward: -4.535714285714286, Mean Entropy: 0.770174503326416, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 283,  Mean reward: -4.208333333333333, Mean Entropy: 0.8566393256187439, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 284,  Mean reward: -2.8095238095238093, Mean Entropy: 0.7753185033798218, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 285,  Mean reward: -1.7222222222222223, Mean Entropy: 0.7164596319198608, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 286,  Mean reward: 0.09803921568627451, Mean Entropy: 0.6548293232917786, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 287,  Mean reward: 1.4150943396226414, Mean Entropy: 0.7887187004089355, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 288,  Mean reward: 0.28, Mean Entropy: 0.7780877351760864, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 289,  Mean reward: -2.3645833333333335, Mean Entropy: 0.676581621170044, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 290,  Mean reward: -0.8775510204081632, Mean Entropy: 0.6366367340087891, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 291,  Mean reward: -2.122448979591837, Mean Entropy: 0.6462403535842896, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 292,  Mean reward: -0.98, Mean Entropy: 0.6955398321151733, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 293,  Mean reward: -0.09183673469387756, Mean Entropy: 0.7316910624504089, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 294,  Mean reward: -0.35714285714285715, Mean Entropy: 0.7573832869529724, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 295,  Mean reward: -1.0113636363636365, Mean Entropy: 0.7134677171707153, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 296,  Mean reward: 2.4313725490196076, Mean Entropy: 0.6920215487480164, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 297,  Mean reward: -1.4148936170212767, Mean Entropy: 0.6897906064987183, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 298,  Mean reward: 0.8272727272727273, Mean Entropy: 0.7330142259597778, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 299,  Mean reward: -1.4583333333333333, Mean Entropy: 0.5751631855964661, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 300,  Mean reward: -2.0638297872340425, Mean Entropy: 0.6827890872955322, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -0.87, Mean Entropy: 0.7185380458831787, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 302,  Mean reward: -1.16, Mean Entropy: 0.6728643178939819, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 303,  Mean reward: -0.15, Mean Entropy: 0.5827473402023315, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 304,  Mean reward: -0.42592592592592593, Mean Entropy: 0.6867381930351257, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 305,  Mean reward: 0.6153846153846154, Mean Entropy: 0.6461110711097717, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 306,  Mean reward: -0.20754716981132076, Mean Entropy: 0.6617921590805054, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 307,  Mean reward: 1.2636363636363637, Mean Entropy: 0.6251633167266846, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 308,  Mean reward: -0.5566037735849056, Mean Entropy: 0.6980985403060913, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 309,  Mean reward: -0.5612244897959183, Mean Entropy: 0.6756352782249451, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 310,  Mean reward: -0.57, Mean Entropy: 0.5948218107223511, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 311,  Mean reward: 1.471698113207547, Mean Entropy: 0.635090708732605, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 312,  Mean reward: 1.4423076923076923, Mean Entropy: 0.6161403059959412, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 313,  Mean reward: 2.156862745098039, Mean Entropy: 0.6577068567276001, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 314,  Mean reward: 0.6176470588235294, Mean Entropy: 0.6268757581710815, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 315,  Mean reward: -3.8555555555555556, Mean Entropy: 0.6878778338432312, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 316,  Mean reward: -0.3469387755102041, Mean Entropy: 0.603387713432312, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 317,  Mean reward: 0.2604166666666667, Mean Entropy: 0.6313914656639099, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 318,  Mean reward: -0.26, Mean Entropy: 0.6113028526306152, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 319,  Mean reward: 1.537037037037037, Mean Entropy: 0.5658432841300964, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 320,  Mean reward: -1.7065217391304348, Mean Entropy: 0.6383005380630493, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 321,  Mean reward: 0.7115384615384616, Mean Entropy: 0.617917537689209, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 322,  Mean reward: -3.6145833333333335, Mean Entropy: 0.5400025248527527, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 323,  Mean reward: -3.3469387755102042, Mean Entropy: 0.6376707553863525, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 324,  Mean reward: 0.9910714285714286, Mean Entropy: 0.6034101843833923, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 325,  Mean reward: 1.4433962264150944, Mean Entropy: 0.6611301898956299, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 326,  Mean reward: -0.4230769230769231, Mean Entropy: 0.640814483165741, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 327,  Mean reward: 1.4056603773584906, Mean Entropy: 0.7313782572746277, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 328,  Mean reward: 0.06862745098039216, Mean Entropy: 0.655311644077301, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 329,  Mean reward: 2.1272727272727274, Mean Entropy: 0.5907942056655884, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 330,  Mean reward: 0.4019607843137255, Mean Entropy: 0.5945186018943787, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 331,  Mean reward: 0.9705882352941176, Mean Entropy: 0.6167075037956238, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 332,  Mean reward: 1.912280701754386, Mean Entropy: 0.5524141788482666, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 333,  Mean reward: 0.49056603773584906, Mean Entropy: 0.5521978139877319, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 334,  Mean reward: -1.7127659574468086, Mean Entropy: 0.6819256544113159, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 335,  Mean reward: 1.5588235294117647, Mean Entropy: 0.6431257724761963, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 336,  Mean reward: 2.0096153846153846, Mean Entropy: 0.5504101514816284, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 337,  Mean reward: 0.6759259259259259, Mean Entropy: 0.5551687479019165, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 338,  Mean reward: -1.9204545454545454, Mean Entropy: 0.6583284139633179, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 339,  Mean reward: 0.04, Mean Entropy: 0.6007957458496094, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 340,  Mean reward: -0.7169811320754716, Mean Entropy: 0.6385000348091125, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 341,  Mean reward: 0.39215686274509803, Mean Entropy: 0.6512356996536255, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 342,  Mean reward: 2.892857142857143, Mean Entropy: 0.5547088980674744, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 343,  Mean reward: 0.16326530612244897, Mean Entropy: 0.6046967506408691, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 344,  Mean reward: 2.3636363636363638, Mean Entropy: 0.6366847157478333, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 345,  Mean reward: 0.09183673469387756, Mean Entropy: 0.6022509336471558, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 346,  Mean reward: 0.7407407407407407, Mean Entropy: 0.6359637975692749, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 347,  Mean reward: 0.06862745098039216, Mean Entropy: 0.5591692924499512, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 348,  Mean reward: -4.954545454545454, Mean Entropy: 0.612541675567627, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 349,  Mean reward: -0.06862745098039216, Mean Entropy: 0.6810646057128906, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 350,  Mean reward: 1.8454545454545455, Mean Entropy: 0.6723721027374268, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 351,  Mean reward: -0.2830188679245283, Mean Entropy: 0.5646259784698486, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 352,  Mean reward: 1.2307692307692308, Mean Entropy: 0.6534788012504578, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 353,  Mean reward: 0.8148148148148148, Mean Entropy: 0.6052855253219604, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 354,  Mean reward: -1.0, Mean Entropy: 0.5745641589164734, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 355,  Mean reward: 0.11206896551724138, Mean Entropy: 0.6926660537719727, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 356,  Mean reward: 2.25, Mean Entropy: 0.6728988885879517, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 357,  Mean reward: -1.7173913043478262, Mean Entropy: 0.6459594964981079, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 358,  Mean reward: 1.5964912280701755, Mean Entropy: 0.5469949841499329, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 359,  Mean reward: -2.18, Mean Entropy: 0.6208397746086121, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 360,  Mean reward: -0.20754716981132076, Mean Entropy: 0.6947005987167358, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 361,  Mean reward: 1.5338983050847457, Mean Entropy: 0.6295429468154907, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 362,  Mean reward: -0.23148148148148148, Mean Entropy: 0.6645413637161255, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 363,  Mean reward: -1.6517857142857142, Mean Entropy: 0.6303521394729614, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 364,  Mean reward: -2.7346938775510203, Mean Entropy: 0.6622787714004517, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 365,  Mean reward: -1.4019607843137254, Mean Entropy: 0.6293843388557434, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 366,  Mean reward: 0.8703703703703703, Mean Entropy: 0.6423165202140808, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 367,  Mean reward: 1.2115384615384615, Mean Entropy: 0.6106770038604736, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 368,  Mean reward: 1.1440677966101696, Mean Entropy: 0.5264732241630554, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 369,  Mean reward: -0.18421052631578946, Mean Entropy: 0.635049045085907, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 370,  Mean reward: -1.8431372549019607, Mean Entropy: 0.6341725587844849, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 371,  Mean reward: -2.5294117647058822, Mean Entropy: 0.6536730527877808, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 372,  Mean reward: 0.0660377358490566, Mean Entropy: 0.6902326345443726, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 373,  Mean reward: -0.74, Mean Entropy: 0.6599417924880981, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 374,  Mean reward: -2.3229166666666665, Mean Entropy: 0.675353467464447, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.51s
Iteration: 375,  Mean reward: 0.8771929824561403, Mean Entropy: 0.6117722988128662, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 376,  Mean reward: 0.6415094339622641, Mean Entropy: 0.6490353345870972, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 377,  Mean reward: -1.6944444444444444, Mean Entropy: 0.6738710403442383, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 378,  Mean reward: 1.3035714285714286, Mean Entropy: 0.6551854610443115, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 379,  Mean reward: 1.1339285714285714, Mean Entropy: 0.7670879364013672, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 380,  Mean reward: -0.13043478260869565, Mean Entropy: 0.7154466509819031, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 381,  Mean reward: 1.2884615384615385, Mean Entropy: 0.6571276187896729, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 382,  Mean reward: -4.26595744680851, Mean Entropy: 0.6952661275863647, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 383,  Mean reward: -1.8584905660377358, Mean Entropy: 0.7219008803367615, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 384,  Mean reward: 0.3333333333333333, Mean Entropy: 0.6120191812515259, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 385,  Mean reward: -2.9545454545454546, Mean Entropy: 0.6686217188835144, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 386,  Mean reward: -3.5, Mean Entropy: 0.6235566139221191, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 387,  Mean reward: -0.9636363636363636, Mean Entropy: 0.700041651725769, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 388,  Mean reward: -1.169811320754717, Mean Entropy: 0.6169015765190125, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 389,  Mean reward: -1.1938775510204083, Mean Entropy: 0.5804777145385742, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 390,  Mean reward: -3.0, Mean Entropy: 0.6211945414543152, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 391,  Mean reward: -1.1442307692307692, Mean Entropy: 0.6527222990989685, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 392,  Mean reward: 0.0, Mean Entropy: 0.7095257043838501, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 393,  Mean reward: -0.375, Mean Entropy: 0.6570042371749878, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 394,  Mean reward: -1.2346938775510203, Mean Entropy: 0.6835318803787231, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 395,  Mean reward: -4.354166666666667, Mean Entropy: 0.7314965128898621, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 396,  Mean reward: -1.99, Mean Entropy: 0.7421921491622925, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 397,  Mean reward: 1.6090909090909091, Mean Entropy: 0.6859661936759949, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 398,  Mean reward: -2.8854166666666665, Mean Entropy: 0.7674254179000854, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 399,  Mean reward: 2.01, Mean Entropy: 0.7291642427444458, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 400,  Mean reward: -3.3555555555555556, Mean Entropy: 0.6179885268211365, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -1.15, Mean Entropy: 0.6737825274467468, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 402,  Mean reward: -0.6274509803921569, Mean Entropy: 0.5727618932723999, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 403,  Mean reward: -0.2727272727272727, Mean Entropy: 0.6590451002120972, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 404,  Mean reward: 0.330188679245283, Mean Entropy: 0.638744056224823, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 405,  Mean reward: 1.0196078431372548, Mean Entropy: 0.5954678058624268, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 406,  Mean reward: 0.14912280701754385, Mean Entropy: 0.5482856631278992, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 407,  Mean reward: -1.98, Mean Entropy: 0.6708166003227234, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 408,  Mean reward: 0.5588235294117647, Mean Entropy: 0.6161853075027466, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 409,  Mean reward: -1.5740740740740742, Mean Entropy: 0.6827483177185059, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 410,  Mean reward: -0.27450980392156865, Mean Entropy: 0.5384620428085327, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 411,  Mean reward: 0.0625, Mean Entropy: 0.7416458129882812, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 412,  Mean reward: -0.25471698113207547, Mean Entropy: 0.6547008752822876, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 413,  Mean reward: -2.35, Mean Entropy: 0.5460607409477234, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 414,  Mean reward: 0.028846153846153848, Mean Entropy: 0.6253523230552673, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 415,  Mean reward: -1.91, Mean Entropy: 0.6205517053604126, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 416,  Mean reward: 3.9916666666666667, Mean Entropy: 0.5112823247909546, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.29s
Iteration: 417,  Mean reward: -0.009259259259259259, Mean Entropy: 0.618558406829834, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 418,  Mean reward: -1.803921568627451, Mean Entropy: 0.6314896941184998, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 419,  Mean reward: -0.32727272727272727, Mean Entropy: 0.560250997543335, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 420,  Mean reward: -1.3173076923076923, Mean Entropy: 0.6302081942558289, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 421,  Mean reward: -0.6517857142857143, Mean Entropy: 0.6635131239891052, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.30s
Iteration: 422,  Mean reward: -0.7924528301886793, Mean Entropy: 0.523274302482605, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 423,  Mean reward: 0.6203703703703703, Mean Entropy: 0.5507046580314636, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 424,  Mean reward: 0.7410714285714286, Mean Entropy: 0.6536784172058105, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 425,  Mean reward: 0.5283018867924528, Mean Entropy: 0.5467897653579712, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 426,  Mean reward: 0.8055555555555556, Mean Entropy: 0.5875580310821533, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 427,  Mean reward: 1.5288461538461537, Mean Entropy: 0.6015106439590454, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 428,  Mean reward: -0.11764705882352941, Mean Entropy: 0.5737897157669067, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 429,  Mean reward: 2.3620689655172415, Mean Entropy: 0.5950380563735962, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 430,  Mean reward: 0.9537037037037037, Mean Entropy: 0.5952024459838867, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 431,  Mean reward: 0.7454545454545455, Mean Entropy: 0.6604485511779785, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 432,  Mean reward: 1.3454545454545455, Mean Entropy: 0.5966258645057678, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 433,  Mean reward: -0.5980392156862745, Mean Entropy: 0.6588467359542847, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 434,  Mean reward: 1.1296296296296295, Mean Entropy: 0.565967321395874, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 435,  Mean reward: 0.3173076923076923, Mean Entropy: 0.6056432723999023, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 436,  Mean reward: -0.29411764705882354, Mean Entropy: 0.6153066158294678, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 437,  Mean reward: -0.15384615384615385, Mean Entropy: 0.614995002746582, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.54s
Iteration: 438,  Mean reward: 0.24545454545454545, Mean Entropy: 0.6850032806396484, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.28s
Iteration: 439,  Mean reward: 2.080357142857143, Mean Entropy: 0.6385109424591064, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 440,  Mean reward: -2.9489795918367347, Mean Entropy: 0.5976938009262085, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 441,  Mean reward: -1.696078431372549, Mean Entropy: 0.5887004733085632, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 442,  Mean reward: 0.12962962962962962, Mean Entropy: 0.7843198776245117, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 443,  Mean reward: 0.4642857142857143, Mean Entropy: 0.7270001173019409, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 444,  Mean reward: -1.6, Mean Entropy: 0.7720246315002441, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 445,  Mean reward: -0.6530612244897959, Mean Entropy: 0.6446623802185059, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 446,  Mean reward: 0.5, Mean Entropy: 0.7011863589286804, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 447,  Mean reward: 0.019230769230769232, Mean Entropy: 0.5001203417778015, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 448,  Mean reward: 0.08333333333333333, Mean Entropy: 0.6597998738288879, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.78s
Iteration: 449,  Mean reward: 0.10377358490566038, Mean Entropy: 0.5690287947654724, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 450,  Mean reward: 1.1176470588235294, Mean Entropy: 0.6702457666397095, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 451,  Mean reward: 0.02, Mean Entropy: 0.6193987131118774, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 452,  Mean reward: 1.8679245283018868, Mean Entropy: 0.6504623293876648, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 453,  Mean reward: 0.9019607843137255, Mean Entropy: 0.5698588490486145, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 454,  Mean reward: 1.2352941176470589, Mean Entropy: 0.6634035706520081, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 455,  Mean reward: 0.18, Mean Entropy: 0.7785066366195679, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 456,  Mean reward: -3.0543478260869565, Mean Entropy: 0.6577529907226562, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 457,  Mean reward: 2.6315789473684212, Mean Entropy: 0.7436881065368652, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 458,  Mean reward: -0.30434782608695654, Mean Entropy: 0.76176917552948, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 459,  Mean reward: -2.9878048780487805, Mean Entropy: 0.7518288493156433, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 460,  Mean reward: -2.1444444444444444, Mean Entropy: 0.7113242149353027, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 461,  Mean reward: -0.6354166666666666, Mean Entropy: 0.5761077404022217, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 462,  Mean reward: -0.08490566037735849, Mean Entropy: 0.5794922709465027, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 463,  Mean reward: -3.223404255319149, Mean Entropy: 0.5793961882591248, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 464,  Mean reward: -0.48148148148148145, Mean Entropy: 0.6061609387397766, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 465,  Mean reward: -0.8877551020408163, Mean Entropy: 0.6079363226890564, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.30s
Iteration: 466,  Mean reward: -1.8043478260869565, Mean Entropy: 0.6751680374145508, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 467,  Mean reward: -1.56, Mean Entropy: 0.7105444669723511, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 468,  Mean reward: -1.3020833333333333, Mean Entropy: 0.7474179267883301, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 469,  Mean reward: -0.07, Mean Entropy: 0.6549034714698792, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.54s
Iteration: 470,  Mean reward: 1.55, Mean Entropy: 0.624198317527771, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.33s
Iteration: 471,  Mean reward: 0.8207547169811321, Mean Entropy: 0.6172937750816345, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 472,  Mean reward: -0.13, Mean Entropy: 0.544002890586853, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 473,  Mean reward: 0.375, Mean Entropy: 0.5830751061439514, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 474,  Mean reward: 0.15384615384615385, Mean Entropy: 0.5449326038360596, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 475,  Mean reward: 1.7169811320754718, Mean Entropy: 0.5904240012168884, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 476,  Mean reward: -1.5098039215686274, Mean Entropy: 0.6096056699752808, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 477,  Mean reward: 0.23636363636363636, Mean Entropy: 0.4758574962615967, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 478,  Mean reward: 1.8214285714285714, Mean Entropy: 0.5289560556411743, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 479,  Mean reward: 1.7407407407407407, Mean Entropy: 0.5544896125793457, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 480,  Mean reward: 1.0, Mean Entropy: 0.6274687051773071, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 481,  Mean reward: -2.306122448979592, Mean Entropy: 0.5416355133056641, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 482,  Mean reward: -0.5, Mean Entropy: 0.5548335313796997, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 483,  Mean reward: 1.7264150943396226, Mean Entropy: 0.605090320110321, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 484,  Mean reward: -0.39215686274509803, Mean Entropy: 0.5543830990791321, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 485,  Mean reward: -2.07, Mean Entropy: 0.651957631111145, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 486,  Mean reward: -1.53, Mean Entropy: 0.6107673048973083, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 487,  Mean reward: -0.3173076923076923, Mean Entropy: 0.6463921666145325, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 488,  Mean reward: -0.18269230769230768, Mean Entropy: 0.5545026659965515, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 489,  Mean reward: -1.27, Mean Entropy: 0.6277312636375427, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 490,  Mean reward: 1.669811320754717, Mean Entropy: 0.5744905471801758, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 491,  Mean reward: 2.6842105263157894, Mean Entropy: 0.6620588302612305, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 492,  Mean reward: -0.86, Mean Entropy: 0.49254393577575684, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 493,  Mean reward: 0.7211538461538461, Mean Entropy: 0.6611036062240601, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 494,  Mean reward: 0.78, Mean Entropy: 0.66047203540802, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 495,  Mean reward: -1.574468085106383, Mean Entropy: 0.6612626314163208, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 496,  Mean reward: 1.1428571428571428, Mean Entropy: 0.5649206638336182, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 497,  Mean reward: -2.7708333333333335, Mean Entropy: 0.5376595258712769, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 498,  Mean reward: 1.4166666666666667, Mean Entropy: 0.6320361495018005, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 499,  Mean reward: 1.4363636363636363, Mean Entropy: 0.550805926322937, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 500,  Mean reward: 0.3627450980392157, Mean Entropy: 0.6036739349365234, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.50s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 2.719298245614035, Mean Entropy: 0.6561228036880493, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 502,  Mean reward: 0.8173076923076923, Mean Entropy: 0.5405813455581665, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 503,  Mean reward: 1.6603773584905661, Mean Entropy: 0.609915018081665, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 504,  Mean reward: 1.962962962962963, Mean Entropy: 0.6529361009597778, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 505,  Mean reward: 1.309090909090909, Mean Entropy: 0.7337378859519958, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.32s
Iteration: 506,  Mean reward: -0.09375, Mean Entropy: 0.6062737107276917, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 507,  Mean reward: -0.6730769230769231, Mean Entropy: 0.497262179851532, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 508,  Mean reward: -0.14423076923076922, Mean Entropy: 0.6369251608848572, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 509,  Mean reward: 2.9285714285714284, Mean Entropy: 0.6318222880363464, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 510,  Mean reward: -0.17346938775510204, Mean Entropy: 0.537568986415863, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 511,  Mean reward: 1.6517857142857142, Mean Entropy: 0.535660445690155, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 512,  Mean reward: 1.6574074074074074, Mean Entropy: 0.5823603272438049, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 513,  Mean reward: 0.8269230769230769, Mean Entropy: 0.5752067565917969, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 514,  Mean reward: 1.1388888888888888, Mean Entropy: 0.5770471096038818, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 515,  Mean reward: 2.75, Mean Entropy: 0.4830726385116577, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 516,  Mean reward: 1.1203703703703705, Mean Entropy: 0.5931704044342041, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 517,  Mean reward: 2.767857142857143, Mean Entropy: 0.6344723701477051, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 518,  Mean reward: 1.6428571428571428, Mean Entropy: 0.683114230632782, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 519,  Mean reward: 0.18, Mean Entropy: 0.6328358054161072, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.29s
Iteration: 520,  Mean reward: 0.87, Mean Entropy: 0.7258751392364502, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 521,  Mean reward: -1.6521739130434783, Mean Entropy: 0.68571937084198, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 522,  Mean reward: 0.2647058823529412, Mean Entropy: 0.6604565382003784, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 523,  Mean reward: -1.28125, Mean Entropy: 0.7279521822929382, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 524,  Mean reward: 0.4888888888888889, Mean Entropy: 0.6971961259841919, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 525,  Mean reward: 2.75, Mean Entropy: 0.5831766128540039, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 526,  Mean reward: -0.6739130434782609, Mean Entropy: 0.5523157119750977, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 527,  Mean reward: -1.509433962264151, Mean Entropy: 0.5905744433403015, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 528,  Mean reward: 0.0660377358490566, Mean Entropy: 0.5459694266319275, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 529,  Mean reward: 0.7450980392156863, Mean Entropy: 0.582427442073822, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 530,  Mean reward: -0.12264150943396226, Mean Entropy: 0.5681797862052917, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 531,  Mean reward: 0.8611111111111112, Mean Entropy: 0.5968182682991028, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 532,  Mean reward: -3.4574468085106385, Mean Entropy: 0.6374266147613525, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 533,  Mean reward: 3.794642857142857, Mean Entropy: 0.4575680196285248, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 534,  Mean reward: 1.0454545454545454, Mean Entropy: 0.5404708385467529, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 535,  Mean reward: -0.49, Mean Entropy: 0.5652971267700195, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 536,  Mean reward: -0.93, Mean Entropy: 0.5837897062301636, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 537,  Mean reward: 0.6759259259259259, Mean Entropy: 0.5760113000869751, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 538,  Mean reward: 1.9375, Mean Entropy: 0.6121876239776611, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 539,  Mean reward: 2.7232142857142856, Mean Entropy: 0.5613377094268799, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 540,  Mean reward: -1.38, Mean Entropy: 0.6618358492851257, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 541,  Mean reward: 0.24528301886792453, Mean Entropy: 0.49399805068969727, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 542,  Mean reward: 1.009433962264151, Mean Entropy: 0.5834031701087952, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 543,  Mean reward: -1.4795918367346939, Mean Entropy: 0.5879837274551392, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 544,  Mean reward: 0.41, Mean Entropy: 0.577851414680481, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 545,  Mean reward: -0.7040816326530612, Mean Entropy: 0.5840328931808472, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 546,  Mean reward: -2.466666666666667, Mean Entropy: 0.5930185317993164, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 547,  Mean reward: -2.1666666666666665, Mean Entropy: 0.5536717176437378, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 548,  Mean reward: -0.058823529411764705, Mean Entropy: 0.6245760917663574, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 549,  Mean reward: 0.8076923076923077, Mean Entropy: 0.5211905241012573, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 550,  Mean reward: -2.3260869565217392, Mean Entropy: 0.6173694133758545, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 551,  Mean reward: 1.125, Mean Entropy: 0.5518720746040344, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 552,  Mean reward: -1.7708333333333333, Mean Entropy: 0.6072957515716553, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.32s
Iteration: 553,  Mean reward: 1.9912280701754386, Mean Entropy: 0.6397866010665894, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 554,  Mean reward: -0.23529411764705882, Mean Entropy: 0.6215651631355286, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 555,  Mean reward: 1.0754716981132075, Mean Entropy: 0.677017092704773, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 556,  Mean reward: -0.8235294117647058, Mean Entropy: 0.6435737013816833, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 557,  Mean reward: 0.7549019607843137, Mean Entropy: 0.6481102705001831, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 558,  Mean reward: -1.4795918367346939, Mean Entropy: 0.5330514907836914, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 559,  Mean reward: -0.69, Mean Entropy: 0.6729001998901367, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 560,  Mean reward: -0.9897959183673469, Mean Entropy: 0.6702620387077332, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 561,  Mean reward: -0.3, Mean Entropy: 0.5375179648399353, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 562,  Mean reward: -0.9021739130434783, Mean Entropy: 0.5682005882263184, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 563,  Mean reward: -0.87, Mean Entropy: 0.6139578223228455, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 564,  Mean reward: 0.45098039215686275, Mean Entropy: 0.5431613922119141, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 565,  Mean reward: 0.21153846153846154, Mean Entropy: 0.5830179452896118, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 566,  Mean reward: 1.9622641509433962, Mean Entropy: 0.6134156584739685, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 567,  Mean reward: -0.30851063829787234, Mean Entropy: 0.6121389865875244, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 568,  Mean reward: -1.076086956521739, Mean Entropy: 0.7115887999534607, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 569,  Mean reward: 2.144230769230769, Mean Entropy: 0.7056634426116943, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 570,  Mean reward: 1.2553191489361701, Mean Entropy: 0.6172705292701721, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 571,  Mean reward: 1.4038461538461537, Mean Entropy: 0.6557114720344543, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 572,  Mean reward: 0.89, Mean Entropy: 0.7446081042289734, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 573,  Mean reward: -0.7019230769230769, Mean Entropy: 0.6355817317962646, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 574,  Mean reward: 0.375, Mean Entropy: 0.6922906637191772, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 575,  Mean reward: -1.0416666666666667, Mean Entropy: 0.5897544622421265, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 576,  Mean reward: 0.046296296296296294, Mean Entropy: 0.5697164535522461, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 577,  Mean reward: -2.7551020408163267, Mean Entropy: 0.6655613780021667, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 578,  Mean reward: -0.3137254901960784, Mean Entropy: 0.6214563250541687, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 579,  Mean reward: -3.9468085106382977, Mean Entropy: 0.6350719332695007, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 580,  Mean reward: -3.2934782608695654, Mean Entropy: 0.6264671087265015, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 581,  Mean reward: -1.875, Mean Entropy: 0.7037259936332703, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 582,  Mean reward: -1.5392156862745099, Mean Entropy: 0.7001125812530518, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 583,  Mean reward: 0.33962264150943394, Mean Entropy: 0.6244109869003296, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 584,  Mean reward: -0.3958333333333333, Mean Entropy: 0.6684970855712891, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 585,  Mean reward: 0.21739130434782608, Mean Entropy: 0.7202852964401245, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 586,  Mean reward: -0.2708333333333333, Mean Entropy: 0.657060980796814, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 587,  Mean reward: 0.9905660377358491, Mean Entropy: 0.616651713848114, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 588,  Mean reward: 0.3018867924528302, Mean Entropy: 0.6255253553390503, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 589,  Mean reward: -0.3979591836734694, Mean Entropy: 0.6249237060546875, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 590,  Mean reward: 2.6982758620689653, Mean Entropy: 0.6585188508033752, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 591,  Mean reward: 0.9423076923076923, Mean Entropy: 0.6316015720367432, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 592,  Mean reward: -0.62, Mean Entropy: 0.6284265518188477, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 593,  Mean reward: 0.7452830188679245, Mean Entropy: 0.5630322694778442, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 594,  Mean reward: 0.7403846153846154, Mean Entropy: 0.6464821696281433, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 595,  Mean reward: 0.35294117647058826, Mean Entropy: 0.6384000778198242, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 596,  Mean reward: 1.7314814814814814, Mean Entropy: 0.5491980910301208, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 597,  Mean reward: 0.7352941176470589, Mean Entropy: 0.5967613458633423, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 598,  Mean reward: -0.0673076923076923, Mean Entropy: 0.6258194446563721, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 599,  Mean reward: -2.010869565217391, Mean Entropy: 0.6855133771896362, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 600,  Mean reward: 1.9803921568627452, Mean Entropy: 0.6995189189910889, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.3369565217391304, Mean Entropy: 0.6566705107688904, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 602,  Mean reward: -3.6777777777777776, Mean Entropy: 0.6114639043807983, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 603,  Mean reward: 0.92, Mean Entropy: 0.7049340009689331, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 604,  Mean reward: -1.1428571428571428, Mean Entropy: 0.6006297469139099, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 605,  Mean reward: -2.9583333333333335, Mean Entropy: 0.600020170211792, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 606,  Mean reward: -1.3829787234042554, Mean Entropy: 0.6822707653045654, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 607,  Mean reward: 1.1574074074074074, Mean Entropy: 0.6126036643981934, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 608,  Mean reward: -0.37755102040816324, Mean Entropy: 0.6145797967910767, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 609,  Mean reward: -1.0, Mean Entropy: 0.6952122449874878, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 610,  Mean reward: -4.141304347826087, Mean Entropy: 0.6452302932739258, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 611,  Mean reward: -1.9270833333333333, Mean Entropy: 0.6144800186157227, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 612,  Mean reward: -1.934782608695652, Mean Entropy: 0.6711391806602478, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 613,  Mean reward: 1.5092592592592593, Mean Entropy: 0.7027465105056763, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 614,  Mean reward: -1.5, Mean Entropy: 0.7239100933074951, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 615,  Mean reward: 1.12, Mean Entropy: 0.6754938960075378, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 616,  Mean reward: -4.282051282051282, Mean Entropy: 0.5441509485244751, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 617,  Mean reward: -3.1447368421052633, Mean Entropy: 0.6988865733146667, complete_episode_count: 38.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 618,  Mean reward: -0.26666666666666666, Mean Entropy: 0.7315904498100281, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 619,  Mean reward: -0.29069767441860467, Mean Entropy: 0.4591386020183563, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 620,  Mean reward: -5.055555555555555, Mean Entropy: 0.8046694993972778, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 621,  Mean reward: -3.2906976744186047, Mean Entropy: 0.7616311311721802, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 622,  Mean reward: -0.21739130434782608, Mean Entropy: 0.6851639151573181, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 623,  Mean reward: 0.23863636363636365, Mean Entropy: 0.579843282699585, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 624,  Mean reward: -8.333333333333334, Mean Entropy: 0.6583832502365112, complete_episode_count: 33.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 625,  Mean reward: -4.186046511627907, Mean Entropy: 0.7050430774688721, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 626,  Mean reward: -1.9024390243902438, Mean Entropy: 0.697145402431488, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 627,  Mean reward: -1.0212765957446808, Mean Entropy: 0.6921865940093994, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 628,  Mean reward: 2.660377358490566, Mean Entropy: 0.6445609927177429, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 629,  Mean reward: -2.8666666666666667, Mean Entropy: 0.6350494623184204, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 630,  Mean reward: 0.7980769230769231, Mean Entropy: 0.6504480838775635, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 631,  Mean reward: 1.4285714285714286, Mean Entropy: 0.5938462615013123, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 632,  Mean reward: 0.5849056603773585, Mean Entropy: 0.616752028465271, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 633,  Mean reward: -1.59375, Mean Entropy: 0.6235132813453674, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 634,  Mean reward: 1.3773584905660377, Mean Entropy: 0.6144744157791138, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 635,  Mean reward: -1.173913043478261, Mean Entropy: 0.6370954513549805, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 636,  Mean reward: 1.3518518518518519, Mean Entropy: 0.560165524482727, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 637,  Mean reward: 0.6153846153846154, Mean Entropy: 0.5168306827545166, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 638,  Mean reward: 0.20754716981132076, Mean Entropy: 0.5788894295692444, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 639,  Mean reward: 0.9351851851851852, Mean Entropy: 0.5833379626274109, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 640,  Mean reward: 3.2936507936507935, Mean Entropy: 0.49222397804260254, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 641,  Mean reward: 0.7547169811320755, Mean Entropy: 0.4758811891078949, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 642,  Mean reward: 2.2589285714285716, Mean Entropy: 0.5889890789985657, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 643,  Mean reward: 2.9285714285714284, Mean Entropy: 0.5313727855682373, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 644,  Mean reward: -1.3775510204081634, Mean Entropy: 0.5548684000968933, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 645,  Mean reward: 1.1296296296296295, Mean Entropy: 0.6239550113677979, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 646,  Mean reward: 1.179245283018868, Mean Entropy: 0.6201208233833313, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 647,  Mean reward: 2.2, Mean Entropy: 0.598791241645813, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 648,  Mean reward: 0.39814814814814814, Mean Entropy: 0.5574710369110107, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 649,  Mean reward: 0.4019607843137255, Mean Entropy: 0.5873503684997559, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 650,  Mean reward: -0.41836734693877553, Mean Entropy: 0.593331515789032, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 651,  Mean reward: 0.06862745098039216, Mean Entropy: 0.5824100971221924, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 652,  Mean reward: 1.9285714285714286, Mean Entropy: 0.6003305912017822, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 653,  Mean reward: -0.5784313725490197, Mean Entropy: 0.6456643342971802, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.52s
Iteration: 654,  Mean reward: -1.7826086956521738, Mean Entropy: 0.5684239864349365, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 655,  Mean reward: -0.5769230769230769, Mean Entropy: 0.5451483726501465, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 656,  Mean reward: 2.0272727272727273, Mean Entropy: 0.593543291091919, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 657,  Mean reward: 0.8076923076923077, Mean Entropy: 0.6583275198936462, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 658,  Mean reward: -0.7307692307692307, Mean Entropy: 0.6431502103805542, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 659,  Mean reward: 1.8396226415094339, Mean Entropy: 0.5802497863769531, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.52s
Iteration: 660,  Mean reward: 1.0980392156862746, Mean Entropy: 0.6715949773788452, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 661,  Mean reward: 1.15, Mean Entropy: 0.5825560092926025, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 662,  Mean reward: 0.6226415094339622, Mean Entropy: 0.6447606086730957, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 663,  Mean reward: -0.29411764705882354, Mean Entropy: 0.6233875751495361, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 664,  Mean reward: -0.5098039215686274, Mean Entropy: 0.5927820205688477, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 665,  Mean reward: -0.47115384615384615, Mean Entropy: 0.6082435846328735, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 666,  Mean reward: -1.3229166666666667, Mean Entropy: 0.6408720016479492, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 667,  Mean reward: 1.6574074074074074, Mean Entropy: 0.6227527856826782, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.29s
Iteration: 668,  Mean reward: -0.8152173913043478, Mean Entropy: 0.6886741518974304, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 669,  Mean reward: 0.49, Mean Entropy: 0.7526518702507019, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 670,  Mean reward: -1.8, Mean Entropy: 0.6349125504493713, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 671,  Mean reward: 1.2181818181818183, Mean Entropy: 0.5657539367675781, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 672,  Mean reward: 0.93, Mean Entropy: 0.6795846819877625, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 673,  Mean reward: 2.1440677966101696, Mean Entropy: 0.6457127928733826, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 674,  Mean reward: 0.5306122448979592, Mean Entropy: 0.5964028239250183, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 675,  Mean reward: -1.1354166666666667, Mean Entropy: 0.6298099756240845, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 676,  Mean reward: 1.6862745098039216, Mean Entropy: 0.6474815607070923, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 677,  Mean reward: 0.5576923076923077, Mean Entropy: 0.5579314231872559, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 678,  Mean reward: -0.41, Mean Entropy: 0.6993302702903748, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 679,  Mean reward: 0.47, Mean Entropy: 0.6236059665679932, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 680,  Mean reward: 1.5, Mean Entropy: 0.6695055961608887, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 681,  Mean reward: -1.9680851063829787, Mean Entropy: 0.6319288015365601, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 682,  Mean reward: 0.9622641509433962, Mean Entropy: 0.49424731731414795, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 683,  Mean reward: 1.75, Mean Entropy: 0.6098567843437195, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 684,  Mean reward: 0.4, Mean Entropy: 0.6291185617446899, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 685,  Mean reward: 1.3173076923076923, Mean Entropy: 0.6450355052947998, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 686,  Mean reward: 0.5, Mean Entropy: 0.6596067547798157, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 687,  Mean reward: 1.1111111111111112, Mean Entropy: 0.6867195963859558, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 688,  Mean reward: -1.7156862745098038, Mean Entropy: 0.6711761951446533, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 689,  Mean reward: -1.553191489361702, Mean Entropy: 0.6627835035324097, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 690,  Mean reward: 0.10576923076923077, Mean Entropy: 0.6021021604537964, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 691,  Mean reward: -0.29591836734693877, Mean Entropy: 0.5982720255851746, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 692,  Mean reward: -0.17647058823529413, Mean Entropy: 0.725655734539032, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 693,  Mean reward: -0.6632653061224489, Mean Entropy: 0.7108994722366333, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 694,  Mean reward: -1.21875, Mean Entropy: 0.6361818909645081, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 695,  Mean reward: 1.0849056603773586, Mean Entropy: 0.5936946272850037, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 696,  Mean reward: -0.31, Mean Entropy: 0.7069137096405029, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 697,  Mean reward: 0.37254901960784315, Mean Entropy: 0.5644863247871399, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 698,  Mean reward: 0.7547169811320755, Mean Entropy: 0.5949112176895142, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 699,  Mean reward: -0.8679245283018868, Mean Entropy: 0.6059010028839111, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 700,  Mean reward: 1.2314814814814814, Mean Entropy: 0.5763849020004272, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: 0.6442307692307693, Mean Entropy: 0.6486136317253113, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 702,  Mean reward: 0.6132075471698113, Mean Entropy: 0.6679638624191284, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 703,  Mean reward: 2.049019607843137, Mean Entropy: 0.6910035014152527, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 704,  Mean reward: 2.625, Mean Entropy: 0.5908017158508301, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 705,  Mean reward: -2.6136363636363638, Mean Entropy: 0.6284474730491638, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 706,  Mean reward: 0.8679245283018868, Mean Entropy: 0.5562740564346313, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 707,  Mean reward: 0.6090909090909091, Mean Entropy: 0.5572389364242554, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 708,  Mean reward: 1.6, Mean Entropy: 0.6026512980461121, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 709,  Mean reward: -0.39215686274509803, Mean Entropy: 0.681592583656311, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 710,  Mean reward: 1.2843137254901962, Mean Entropy: 0.6499256491661072, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 711,  Mean reward: 2.75, Mean Entropy: 0.6785217523574829, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 712,  Mean reward: -0.8260869565217391, Mean Entropy: 0.6306687593460083, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 713,  Mean reward: 0.25, Mean Entropy: 0.5107370018959045, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 714,  Mean reward: 1.6666666666666667, Mean Entropy: 0.5114776492118835, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 715,  Mean reward: -0.4019607843137255, Mean Entropy: 0.6534765958786011, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 716,  Mean reward: 1.4814814814814814, Mean Entropy: 0.6299617290496826, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 717,  Mean reward: 1.0740740740740742, Mean Entropy: 0.6702322363853455, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 718,  Mean reward: 1.8981481481481481, Mean Entropy: 0.6081258654594421, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 719,  Mean reward: -2.4270833333333335, Mean Entropy: 0.5966329574584961, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 720,  Mean reward: -0.5208333333333334, Mean Entropy: 0.6226038932800293, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 721,  Mean reward: 0.0673076923076923, Mean Entropy: 0.5977903008460999, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 722,  Mean reward: -0.35294117647058826, Mean Entropy: 0.585943877696991, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 723,  Mean reward: 0.6372549019607843, Mean Entropy: 0.6068518161773682, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 724,  Mean reward: -0.2653061224489796, Mean Entropy: 0.535901665687561, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 725,  Mean reward: 0.5961538461538461, Mean Entropy: 0.5470592379570007, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 726,  Mean reward: 0.5408163265306123, Mean Entropy: 0.654883623123169, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 727,  Mean reward: 0.12, Mean Entropy: 0.6012122631072998, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 728,  Mean reward: 0.44680851063829785, Mean Entropy: 0.6084799766540527, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 729,  Mean reward: -1.4326923076923077, Mean Entropy: 0.5819992423057556, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 730,  Mean reward: 3.542372881355932, Mean Entropy: 0.5711355209350586, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 731,  Mean reward: 2.7155172413793105, Mean Entropy: 0.6072001457214355, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 732,  Mean reward: 0.25, Mean Entropy: 0.6646462678909302, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 733,  Mean reward: -1.0686274509803921, Mean Entropy: 0.6750828623771667, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.29s
Iteration: 734,  Mean reward: -1.71875, Mean Entropy: 0.6138120889663696, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 735,  Mean reward: 0.3627450980392157, Mean Entropy: 0.6156217455863953, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 736,  Mean reward: 0.4326923076923077, Mean Entropy: 0.5537404417991638, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 737,  Mean reward: 0.5392156862745098, Mean Entropy: 0.655246376991272, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 738,  Mean reward: 0.25961538461538464, Mean Entropy: 0.6442171335220337, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 739,  Mean reward: 0.4411764705882353, Mean Entropy: 0.5399079918861389, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 740,  Mean reward: 1.5784313725490196, Mean Entropy: 0.5937905311584473, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 741,  Mean reward: 1.6481481481481481, Mean Entropy: 0.5749134421348572, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 742,  Mean reward: -0.5, Mean Entropy: 0.603979229927063, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 743,  Mean reward: 0.3877551020408163, Mean Entropy: 0.6036608815193176, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 744,  Mean reward: -1.5816326530612246, Mean Entropy: 0.5987421274185181, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 745,  Mean reward: 1.0925925925925926, Mean Entropy: 0.519758939743042, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 746,  Mean reward: -1.13, Mean Entropy: 0.6071402430534363, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 747,  Mean reward: -0.8673469387755102, Mean Entropy: 0.6023545265197754, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 748,  Mean reward: -0.59, Mean Entropy: 0.602314829826355, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 749,  Mean reward: 1.179245283018868, Mean Entropy: 0.5231958627700806, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 750,  Mean reward: -1.030612244897959, Mean Entropy: 0.5375835299491882, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 751,  Mean reward: 1.5909090909090908, Mean Entropy: 0.5967328548431396, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 752,  Mean reward: -0.04807692307692308, Mean Entropy: 0.6011005640029907, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 753,  Mean reward: -0.8163265306122449, Mean Entropy: 0.5440170764923096, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 754,  Mean reward: 2.3867924528301887, Mean Entropy: 0.5271791219711304, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 755,  Mean reward: 2.375, Mean Entropy: 0.520144522190094, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 756,  Mean reward: 0.48, Mean Entropy: 0.584479570388794, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 757,  Mean reward: -2.1122448979591835, Mean Entropy: 0.6024247407913208, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 758,  Mean reward: 0.7019230769230769, Mean Entropy: 0.5963664054870605, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 759,  Mean reward: 0.01020408163265306, Mean Entropy: 0.5382081270217896, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 760,  Mean reward: -0.049019607843137254, Mean Entropy: 0.5906166434288025, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 761,  Mean reward: -0.6770833333333334, Mean Entropy: 0.5348459482192993, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 762,  Mean reward: 0.46078431372549017, Mean Entropy: 0.5983592867851257, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 763,  Mean reward: 1.0943396226415094, Mean Entropy: 0.5895965099334717, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 764,  Mean reward: 0.8301886792452831, Mean Entropy: 0.5958588123321533, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 765,  Mean reward: -0.83, Mean Entropy: 0.584650456905365, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 766,  Mean reward: 0.29591836734693877, Mean Entropy: 0.6042543649673462, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.30s
Iteration: 767,  Mean reward: -1.1, Mean Entropy: 0.5828224420547485, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 768,  Mean reward: -0.7653061224489796, Mean Entropy: 0.5980621576309204, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 769,  Mean reward: -3.688888888888889, Mean Entropy: 0.5877906084060669, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 770,  Mean reward: -1.11, Mean Entropy: 0.570147693157196, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 771,  Mean reward: 1.1320754716981132, Mean Entropy: 0.5345737934112549, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 772,  Mean reward: -1.19, Mean Entropy: 0.5326492190361023, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 773,  Mean reward: 1.8055555555555556, Mean Entropy: 0.4906267523765564, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 774,  Mean reward: -0.057692307692307696, Mean Entropy: 0.536532461643219, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 775,  Mean reward: 0.13725490196078433, Mean Entropy: 0.5938237309455872, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 776,  Mean reward: 0.6666666666666666, Mean Entropy: 0.5764681100845337, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 777,  Mean reward: 2.1226415094339623, Mean Entropy: 0.5379133820533752, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 778,  Mean reward: 0.9811320754716981, Mean Entropy: 0.5320924520492554, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 779,  Mean reward: 1.6203703703703705, Mean Entropy: 0.5425209999084473, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 780,  Mean reward: -0.4807692307692308, Mean Entropy: 0.5536279678344727, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 781,  Mean reward: -2.19, Mean Entropy: 0.5570347309112549, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 782,  Mean reward: 0.8181818181818182, Mean Entropy: 0.5823568105697632, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 783,  Mean reward: 1.2592592592592593, Mean Entropy: 0.5691466331481934, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 784,  Mean reward: 1.4375, Mean Entropy: 0.6079034805297852, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 785,  Mean reward: -0.6634615384615384, Mean Entropy: 0.6213400363922119, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 786,  Mean reward: 0.1346153846153846, Mean Entropy: 0.5608100891113281, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 787,  Mean reward: -0.4117647058823529, Mean Entropy: 0.5621734261512756, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 788,  Mean reward: -2.6382978723404253, Mean Entropy: 0.6293220520019531, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 789,  Mean reward: 2.017857142857143, Mean Entropy: 0.5638539791107178, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 790,  Mean reward: -0.38, Mean Entropy: 0.6206642389297485, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 791,  Mean reward: -0.12, Mean Entropy: 0.6240007877349854, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 792,  Mean reward: -0.96875, Mean Entropy: 0.5696530938148499, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 793,  Mean reward: 2.2636363636363637, Mean Entropy: 0.608935534954071, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 794,  Mean reward: -0.5294117647058824, Mean Entropy: 0.5589491128921509, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 795,  Mean reward: -1.57, Mean Entropy: 0.6657412052154541, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 796,  Mean reward: -1.4, Mean Entropy: 0.5377991199493408, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 797,  Mean reward: -0.9285714285714286, Mean Entropy: 0.6327241659164429, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 798,  Mean reward: 1.5277777777777777, Mean Entropy: 0.548409640789032, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 799,  Mean reward: 0.9807692307692307, Mean Entropy: 0.5968911647796631, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.29s
Iteration: 800,  Mean reward: 1.150943396226415, Mean Entropy: 0.595876932144165, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -2.0833333333333335, Mean Entropy: 0.6179614067077637, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 802,  Mean reward: 0.9339622641509434, Mean Entropy: 0.61170494556427, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.28s
Iteration: 803,  Mean reward: 2.1, Mean Entropy: 0.6452056169509888, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 804,  Mean reward: 0.59, Mean Entropy: 0.6138822436332703, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 805,  Mean reward: 0.2692307692307692, Mean Entropy: 0.6052131056785583, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 806,  Mean reward: 2.125, Mean Entropy: 0.6166988611221313, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 807,  Mean reward: 3.9741379310344827, Mean Entropy: 0.5897808074951172, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 808,  Mean reward: -0.24468085106382978, Mean Entropy: 0.5106984376907349, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 809,  Mean reward: 1.6346153846153846, Mean Entropy: 0.6125391721725464, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 810,  Mean reward: 0.3269230769230769, Mean Entropy: 0.6099096536636353, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 811,  Mean reward: -2.03125, Mean Entropy: 0.5870620012283325, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 812,  Mean reward: 1.287037037037037, Mean Entropy: 0.626034140586853, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 813,  Mean reward: 1.3818181818181818, Mean Entropy: 0.6214737892150879, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 814,  Mean reward: 1.349056603773585, Mean Entropy: 0.5824355483055115, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 815,  Mean reward: 0.34615384615384615, Mean Entropy: 0.5747317671775818, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 816,  Mean reward: 0.06862745098039216, Mean Entropy: 0.5703702569007874, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 817,  Mean reward: -0.14705882352941177, Mean Entropy: 0.5916423201560974, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.52s
Iteration: 818,  Mean reward: -0.8061224489795918, Mean Entropy: 0.5134395360946655, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 819,  Mean reward: 2.3684210526315788, Mean Entropy: 0.5714383125305176, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 820,  Mean reward: -0.826530612244898, Mean Entropy: 0.5876210927963257, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 821,  Mean reward: 1.7019230769230769, Mean Entropy: 0.46672865748405457, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 822,  Mean reward: 0.1320754716981132, Mean Entropy: 0.508220374584198, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 823,  Mean reward: -1.0, Mean Entropy: 0.6187304854393005, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 824,  Mean reward: -1.9565217391304348, Mean Entropy: 0.5771433115005493, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 825,  Mean reward: 1.8482142857142858, Mean Entropy: 0.6130542159080505, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 826,  Mean reward: -0.09803921568627451, Mean Entropy: 0.6019154787063599, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 827,  Mean reward: -1.1734693877551021, Mean Entropy: 0.5919471979141235, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 828,  Mean reward: 2.3363636363636364, Mean Entropy: 0.6851491928100586, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 829,  Mean reward: -1.0454545454545454, Mean Entropy: 0.6488369703292847, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 830,  Mean reward: 0.16326530612244897, Mean Entropy: 0.6467574834823608, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 831,  Mean reward: -2.391304347826087, Mean Entropy: 0.6517884731292725, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 832,  Mean reward: 0.6981132075471698, Mean Entropy: 0.5724136829376221, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 833,  Mean reward: 0.07547169811320754, Mean Entropy: 0.5820803046226501, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 834,  Mean reward: -0.22641509433962265, Mean Entropy: 0.4998604655265808, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 835,  Mean reward: 2.6982758620689653, Mean Entropy: 0.5676664710044861, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 836,  Mean reward: -1.28125, Mean Entropy: 0.5965257287025452, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 837,  Mean reward: -1.6956521739130435, Mean Entropy: 0.666681170463562, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 838,  Mean reward: 0.7659574468085106, Mean Entropy: 0.6548461318016052, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 839,  Mean reward: 0.9411764705882353, Mean Entropy: 0.5371819734573364, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 840,  Mean reward: 1.3557692307692308, Mean Entropy: 0.5471471548080444, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 841,  Mean reward: 1.6428571428571428, Mean Entropy: 0.6278688311576843, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 842,  Mean reward: 0.37777777777777777, Mean Entropy: 0.48108768463134766, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 843,  Mean reward: -0.29, Mean Entropy: 0.5142254829406738, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 844,  Mean reward: 0.42452830188679247, Mean Entropy: 0.4167269468307495, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 845,  Mean reward: 2.1315789473684212, Mean Entropy: 0.39887547492980957, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 846,  Mean reward: 0.34, Mean Entropy: 0.45836660265922546, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 847,  Mean reward: -0.81, Mean Entropy: 0.4742927551269531, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 848,  Mean reward: 0.16346153846153846, Mean Entropy: 0.5403913259506226, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 849,  Mean reward: 0.7636363636363637, Mean Entropy: 0.48445257544517517, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 850,  Mean reward: -0.24509803921568626, Mean Entropy: 0.4904470443725586, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 851,  Mean reward: 0.8363636363636363, Mean Entropy: 0.48127806186676025, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 852,  Mean reward: 0.2818181818181818, Mean Entropy: 0.6184254288673401, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 853,  Mean reward: 0.1792452830188679, Mean Entropy: 0.5152302384376526, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 854,  Mean reward: 2.690909090909091, Mean Entropy: 0.6139926910400391, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 855,  Mean reward: 0.24509803921568626, Mean Entropy: 0.4940919876098633, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 856,  Mean reward: 0.6923076923076923, Mean Entropy: 0.6336122751235962, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 857,  Mean reward: -0.5961538461538461, Mean Entropy: 0.544100284576416, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 858,  Mean reward: 0.23, Mean Entropy: 0.5288199782371521, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 859,  Mean reward: 1.9326923076923077, Mean Entropy: 0.509960412979126, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 860,  Mean reward: -1.66, Mean Entropy: 0.44442272186279297, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 861,  Mean reward: -1.22, Mean Entropy: 0.4933372437953949, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 862,  Mean reward: -0.4574468085106383, Mean Entropy: 0.5401988625526428, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 863,  Mean reward: 0.17307692307692307, Mean Entropy: 0.6987211108207703, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 864,  Mean reward: 0.5909090909090909, Mean Entropy: 0.5359206795692444, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 865,  Mean reward: -0.696078431372549, Mean Entropy: 0.6089365482330322, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 866,  Mean reward: -0.9479166666666666, Mean Entropy: 0.5641450881958008, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 867,  Mean reward: 2.5701754385964914, Mean Entropy: 0.6204540729522705, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 868,  Mean reward: -5.828571428571428, Mean Entropy: 0.29166656732559204, complete_episode_count: 35.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 869,  Mean reward: -4.470588235294118, Mean Entropy: 0.7455963492393494, complete_episode_count: 34.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 870,  Mean reward: -3.8076923076923075, Mean Entropy: 0.864121675491333, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 871,  Mean reward: -3.048780487804878, Mean Entropy: 0.8885329961776733, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 872,  Mean reward: -5.056818181818182, Mean Entropy: 0.8823966383934021, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 873,  Mean reward: -4.9375, Mean Entropy: 0.8391756415367126, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 874,  Mean reward: -3.4305555555555554, Mean Entropy: 0.7919548749923706, complete_episode_count: 36.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 875,  Mean reward: -3.1794871794871793, Mean Entropy: 0.9501769542694092, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 876,  Mean reward: -1.5875, Mean Entropy: 0.9494019746780396, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 877,  Mean reward: -4.824324324324325, Mean Entropy: 0.8678722381591797, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 878,  Mean reward: -0.8522727272727273, Mean Entropy: 0.855837345123291, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 879,  Mean reward: -0.6195652173913043, Mean Entropy: 0.8568674921989441, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 880,  Mean reward: -1.875, Mean Entropy: 0.9065081477165222, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 881,  Mean reward: -1.2954545454545454, Mean Entropy: 0.8503631353378296, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 882,  Mean reward: -2.25, Mean Entropy: 0.8392906188964844, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.51s
Iteration: 883,  Mean reward: -0.5609756097560976, Mean Entropy: 0.7373247146606445, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 884,  Mean reward: 1.86, Mean Entropy: 0.6696790456771851, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 885,  Mean reward: 1.0294117647058822, Mean Entropy: 0.554562509059906, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 886,  Mean reward: -1.0833333333333333, Mean Entropy: 0.6196346879005432, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 887,  Mean reward: -0.7395833333333334, Mean Entropy: 0.7110349535942078, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 888,  Mean reward: 1.2647058823529411, Mean Entropy: 0.7496806383132935, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 889,  Mean reward: -3.951219512195122, Mean Entropy: 0.618439793586731, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 890,  Mean reward: 0.62, Mean Entropy: 0.637787938117981, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 891,  Mean reward: -0.3163265306122449, Mean Entropy: 0.6332506537437439, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 892,  Mean reward: 0.3404255319148936, Mean Entropy: 0.5974337458610535, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 893,  Mean reward: 2.4642857142857144, Mean Entropy: 0.7106909155845642, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 894,  Mean reward: -5.621621621621622, Mean Entropy: 0.7097748517990112, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 895,  Mean reward: -2.75, Mean Entropy: 0.6067059636116028, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 896,  Mean reward: -1.3229166666666667, Mean Entropy: 0.6515321731567383, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 897,  Mean reward: 0.9245283018867925, Mean Entropy: 0.5580622553825378, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 898,  Mean reward: -0.19230769230769232, Mean Entropy: 0.5543010234832764, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 899,  Mean reward: 1.412280701754386, Mean Entropy: 0.5123252272605896, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 900,  Mean reward: 3.8813559322033897, Mean Entropy: 0.5920035243034363, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -0.5, Mean Entropy: 0.3780611455440521, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 902,  Mean reward: 0.5, Mean Entropy: 0.4837613105773926, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 903,  Mean reward: 1.7407407407407407, Mean Entropy: 0.5002541542053223, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 904,  Mean reward: -0.49056603773584906, Mean Entropy: 0.6876883506774902, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 905,  Mean reward: -0.46078431372549017, Mean Entropy: 0.41784125566482544, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 906,  Mean reward: 2.008771929824561, Mean Entropy: 0.7566444277763367, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 907,  Mean reward: 0.9117647058823529, Mean Entropy: 0.6309787631034851, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 908,  Mean reward: 1.0272727272727273, Mean Entropy: 0.6450146436691284, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 909,  Mean reward: 3.017857142857143, Mean Entropy: 0.5759896039962769, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 910,  Mean reward: 1.1181818181818182, Mean Entropy: 0.6646139025688171, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 911,  Mean reward: 0.8055555555555556, Mean Entropy: 0.5127050876617432, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 912,  Mean reward: 2.2796610169491527, Mean Entropy: 0.39919161796569824, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 913,  Mean reward: 0.8584905660377359, Mean Entropy: 0.7207471132278442, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 914,  Mean reward: 0.5294117647058824, Mean Entropy: 0.5731245875358582, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.52s
Iteration: 915,  Mean reward: 1.8181818181818181, Mean Entropy: 0.5693554282188416, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 916,  Mean reward: 1.981818181818182, Mean Entropy: 0.524658203125, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -4.55, Mean Entropy: 0.9530773162841797, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.35, Mean Entropy: 0.9675179719924927, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -4.225, Mean Entropy: 0.9025352001190186, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -4.05, Mean Entropy: 0.9530734419822693, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 4,  Mean reward: -4.821428571428571, Mean Entropy: 0.9169607162475586, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 5,  Mean reward: -4.920454545454546, Mean Entropy: 0.9458388090133667, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 6,  Mean reward: -4.7894736842105265, Mean Entropy: 1.003602147102356, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 7,  Mean reward: -3.473684210526316, Mean Entropy: 0.9241863489151001, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 8,  Mean reward: -2.7222222222222223, Mean Entropy: 0.9458443522453308, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 9,  Mean reward: -5.324324324324325, Mean Entropy: 0.96027010679245, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 10,  Mean reward: -2.9146341463414633, Mean Entropy: 0.9458402395248413, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 11,  Mean reward: -6.3625, Mean Entropy: 0.9241619110107422, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 12,  Mean reward: -4.3625, Mean Entropy: 0.9097320437431335, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 13,  Mean reward: -5.4021739130434785, Mean Entropy: 0.9025036692619324, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.32s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 14,  Mean reward: -1.8255813953488371, Mean Entropy: 0.9891588091850281, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 15,  Mean reward: -4.5813953488372094, Mean Entropy: 0.9891554713249207, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 16,  Mean reward: -4.035714285714286, Mean Entropy: 0.9386110305786133, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 17,  Mean reward: -7.2375, Mean Entropy: 0.9385921955108643, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 18,  Mean reward: -5.853658536585366, Mean Entropy: 0.9529861807823181, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 19,  Mean reward: -6.083333333333333, Mean Entropy: 0.8951249122619629, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 20,  Mean reward: -3.1794871794871793, Mean Entropy: 0.974645733833313, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 21,  Mean reward: -4.804878048780488, Mean Entropy: 0.9530097842216492, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 22,  Mean reward: -3.5697674418604652, Mean Entropy: 0.931271493434906, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 23,  Mean reward: -2.7790697674418605, Mean Entropy: 0.9313115477561951, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 24,  Mean reward: -3.372093023255814, Mean Entropy: 0.895200252532959, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 25,  Mean reward: -5.2682926829268295, Mean Entropy: 0.9601194262504578, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 26,  Mean reward: -3.7386363636363638, Mean Entropy: 0.9598962068557739, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 27,  Mean reward: -4.17948717948718, Mean Entropy: 0.9095289707183838, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 28,  Mean reward: -4.546511627906977, Mean Entropy: 0.9454719424247742, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 29,  Mean reward: -4.2125, Mean Entropy: 0.9020652174949646, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 30,  Mean reward: -2.1219512195121952, Mean Entropy: 1.0032665729522705, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.32s
Iteration: 31,  Mean reward: -3.0568181818181817, Mean Entropy: 0.9455935955047607, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.54s
Iteration: 32,  Mean reward: -6.256756756756757, Mean Entropy: 0.9021567106246948, complete_episode_count: 37.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 33,  Mean reward: -4.892857142857143, Mean Entropy: 0.9600050449371338, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 34,  Mean reward: -5.243589743589744, Mean Entropy: 0.8951519727706909, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 35,  Mean reward: -1.6282051282051282, Mean Entropy: 0.9457470774650574, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 36,  Mean reward: -2.5760869565217392, Mean Entropy: 0.989043116569519, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 37,  Mean reward: -4.011111111111111, Mean Entropy: 0.9312814474105835, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 38,  Mean reward: -3.7948717948717947, Mean Entropy: 0.9023274779319763, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 39,  Mean reward: -4.558139534883721, Mean Entropy: 0.8804909586906433, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 40,  Mean reward: -6.0, Mean Entropy: 0.9379062652587891, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 41,  Mean reward: -2.977272727272727, Mean Entropy: 0.9595527648925781, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 42,  Mean reward: -2.2790697674418605, Mean Entropy: 0.9743334650993347, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 43,  Mean reward: -3.8636363636363638, Mean Entropy: 0.9889692068099976, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 44,  Mean reward: -3.7333333333333334, Mean Entropy: 0.9525408744812012, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 45,  Mean reward: -5.723684210526316, Mean Entropy: 0.9444694519042969, complete_episode_count: 38.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 46,  Mean reward: -3.869047619047619, Mean Entropy: 0.9667160511016846, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 47,  Mean reward: -3.2666666666666666, Mean Entropy: 0.9022555351257324, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 48,  Mean reward: -5.686046511627907, Mean Entropy: 0.9310634136199951, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 49,  Mean reward: -3.3205128205128207, Mean Entropy: 0.9960927963256836, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.31s
Iteration: 50,  Mean reward: -4.318181818181818, Mean Entropy: 0.9598649740219116, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 51,  Mean reward: -5.619047619047619, Mean Entropy: 0.9523062109947205, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 52,  Mean reward: -6.464285714285714, Mean Entropy: 0.9371994733810425, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 53,  Mean reward: -4.4125, Mean Entropy: 0.9577417373657227, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 54,  Mean reward: -3.4069767441860463, Mean Entropy: 0.9357044100761414, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 55,  Mean reward: -6.521739130434782, Mean Entropy: 0.9629580974578857, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 56,  Mean reward: -2.3255813953488373, Mean Entropy: 0.8856544494628906, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 57,  Mean reward: -4.5, Mean Entropy: 0.936464250087738, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 58,  Mean reward: -3.9886363636363638, Mean Entropy: 0.8861937522888184, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 59,  Mean reward: -6.546511627906977, Mean Entropy: 0.9344784617424011, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 60,  Mean reward: -3.15, Mean Entropy: 0.9575557708740234, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 61,  Mean reward: -4.012820512820513, Mean Entropy: 0.8798637390136719, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 62,  Mean reward: -5.064102564102564, Mean Entropy: 0.8940987586975098, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.34s
Iteration: 63,  Mean reward: -6.135135135135135, Mean Entropy: 0.954433798789978, complete_episode_count: 37.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 64,  Mean reward: -4.9375, Mean Entropy: 0.8932974338531494, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 65,  Mean reward: -5.865853658536586, Mean Entropy: 0.9943687319755554, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.54s
Iteration: 66,  Mean reward: -5.670731707317073, Mean Entropy: 0.993491530418396, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 67,  Mean reward: -1.8837209302325582, Mean Entropy: 0.9133296012878418, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 68,  Mean reward: -4.052631578947368, Mean Entropy: 0.9791722297668457, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 69,  Mean reward: -3.9318181818181817, Mean Entropy: 0.938132643699646, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 70,  Mean reward: -4.0125, Mean Entropy: 0.8096572160720825, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 71,  Mean reward: -3.6931818181818183, Mean Entropy: 0.8693698644638062, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 72,  Mean reward: -4.988095238095238, Mean Entropy: 0.8620104789733887, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 73,  Mean reward: -3.9555555555555557, Mean Entropy: 0.9464102983474731, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 74,  Mean reward: -2.2111111111111112, Mean Entropy: 0.8896669745445251, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 75,  Mean reward: -0.77, Mean Entropy: 0.8294360041618347, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 76,  Mean reward: -0.3979591836734694, Mean Entropy: 0.6116818785667419, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 77,  Mean reward: -0.9897959183673469, Mean Entropy: 0.648662269115448, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 78,  Mean reward: -1.1785714285714286, Mean Entropy: 0.8322650790214539, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 79,  Mean reward: -0.30851063829787234, Mean Entropy: 0.8630141615867615, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 80,  Mean reward: -2.5816326530612246, Mean Entropy: 0.7242595553398132, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 81,  Mean reward: -1.82, Mean Entropy: 0.8180227279663086, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 82,  Mean reward: -1.3942307692307692, Mean Entropy: 0.7326599359512329, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 83,  Mean reward: 1.3571428571428572, Mean Entropy: 0.78106689453125, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 84,  Mean reward: -2.4791666666666665, Mean Entropy: 0.8992422819137573, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 85,  Mean reward: -2.923076923076923, Mean Entropy: 0.8764839172363281, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 86,  Mean reward: 1.1041666666666667, Mean Entropy: 0.7944123148918152, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 87,  Mean reward: -1.0, Mean Entropy: 0.7915653586387634, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 88,  Mean reward: 0.8723404255319149, Mean Entropy: 0.7693559527397156, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 89,  Mean reward: 1.9245283018867925, Mean Entropy: 0.6285589337348938, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 90,  Mean reward: 1.9017857142857142, Mean Entropy: 0.6187633872032166, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 91,  Mean reward: 0.9489795918367347, Mean Entropy: 0.600986659526825, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 92,  Mean reward: -0.8936170212765957, Mean Entropy: 0.5723792910575867, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 93,  Mean reward: 2.2884615384615383, Mean Entropy: 0.5776812434196472, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 94,  Mean reward: -0.5714285714285714, Mean Entropy: 0.6340758800506592, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 95,  Mean reward: 0.22340425531914893, Mean Entropy: 0.6072933673858643, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 96,  Mean reward: 1.1666666666666667, Mean Entropy: 0.600806474685669, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 97,  Mean reward: 1.5196078431372548, Mean Entropy: 0.5824273824691772, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 98,  Mean reward: 0.6274509803921569, Mean Entropy: 0.5242307186126709, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 99,  Mean reward: -0.9183673469387755, Mean Entropy: 0.6318851113319397, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 100,  Mean reward: 1.4150943396226414, Mean Entropy: 0.5748227834701538, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -0.07777777777777778, Mean Entropy: 0.6111278533935547, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 102,  Mean reward: -0.8020833333333334, Mean Entropy: 0.5724641680717468, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 103,  Mean reward: 1.0288461538461537, Mean Entropy: 0.5804949998855591, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 104,  Mean reward: -0.9565217391304348, Mean Entropy: 0.5874242782592773, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 105,  Mean reward: 1.9107142857142858, Mean Entropy: 0.5605236887931824, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 106,  Mean reward: 1.3796296296296295, Mean Entropy: 0.5824316143989563, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 107,  Mean reward: 0.375, Mean Entropy: 0.5830427408218384, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 108,  Mean reward: 1.956896551724138, Mean Entropy: 0.4796527624130249, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 109,  Mean reward: 0.2549019607843137, Mean Entropy: 0.6052224040031433, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 110,  Mean reward: -2.532608695652174, Mean Entropy: 0.6331746578216553, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 111,  Mean reward: 2.205357142857143, Mean Entropy: 0.6022404432296753, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 112,  Mean reward: -0.04, Mean Entropy: 0.5695511698722839, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.30s
Iteration: 113,  Mean reward: 0.7169811320754716, Mean Entropy: 0.5622801184654236, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 114,  Mean reward: 0.18518518518518517, Mean Entropy: 0.5741586089134216, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 115,  Mean reward: -0.4215686274509804, Mean Entropy: 0.49801942706108093, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 116,  Mean reward: -2.9456521739130435, Mean Entropy: 0.5316691398620605, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 117,  Mean reward: 1.5892857142857142, Mean Entropy: 0.5275262594223022, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 118,  Mean reward: 1.3557692307692308, Mean Entropy: 0.5382078886032104, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 119,  Mean reward: 0.3113207547169811, Mean Entropy: 0.5339577198028564, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 120,  Mean reward: -0.3333333333333333, Mean Entropy: 0.5307373404502869, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 121,  Mean reward: -0.7, Mean Entropy: 0.5730973482131958, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 122,  Mean reward: -0.5204081632653061, Mean Entropy: 0.6010748147964478, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 123,  Mean reward: 1.0943396226415094, Mean Entropy: 0.614843487739563, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 124,  Mean reward: -1.7755102040816326, Mean Entropy: 0.6130242347717285, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 125,  Mean reward: 1.7142857142857142, Mean Entropy: 0.5946359038352966, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 126,  Mean reward: -2.5510204081632653, Mean Entropy: 0.583193302154541, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 127,  Mean reward: 0.8431372549019608, Mean Entropy: 0.5542120933532715, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 128,  Mean reward: 0.5416666666666666, Mean Entropy: 0.5292619466781616, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 129,  Mean reward: 0.9245283018867925, Mean Entropy: 0.5346145033836365, complete_episode_count: 53.0, Gather time: 0.72s, Train time: 1.30s
Iteration: 130,  Mean reward: 0.6470588235294118, Mean Entropy: 0.5731475949287415, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 131,  Mean reward: -1.3571428571428572, Mean Entropy: 0.6035804748535156, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 132,  Mean reward: 0.12, Mean Entropy: 0.6125496625900269, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 133,  Mean reward: 0.6862745098039216, Mean Entropy: 0.6198340058326721, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 134,  Mean reward: -0.09, Mean Entropy: 0.663395345211029, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 135,  Mean reward: 0.9537037037037037, Mean Entropy: 0.6060301065444946, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 136,  Mean reward: 1.9230769230769231, Mean Entropy: 0.6074912548065186, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 137,  Mean reward: 1.2450980392156863, Mean Entropy: 0.5885894298553467, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 138,  Mean reward: 0.5673076923076923, Mean Entropy: 0.6008706092834473, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 139,  Mean reward: -0.5102040816326531, Mean Entropy: 0.6120809316635132, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 140,  Mean reward: 2.9210526315789473, Mean Entropy: 0.599871039390564, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 141,  Mean reward: -3.2666666666666666, Mean Entropy: 0.6832353472709656, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 142,  Mean reward: -0.82, Mean Entropy: 0.6236259341239929, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 143,  Mean reward: -1.3444444444444446, Mean Entropy: 0.6553916931152344, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 144,  Mean reward: 1.7254901960784315, Mean Entropy: 0.6274997591972351, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 145,  Mean reward: -0.330188679245283, Mean Entropy: 0.6427698731422424, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.29s
Iteration: 146,  Mean reward: 0.6153846153846154, Mean Entropy: 0.6043585538864136, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 147,  Mean reward: -1.5204081632653061, Mean Entropy: 0.6470507979393005, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 148,  Mean reward: 0.27, Mean Entropy: 0.6449123620986938, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 149,  Mean reward: 1.6442307692307692, Mean Entropy: 0.5239641070365906, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 150,  Mean reward: 4.375, Mean Entropy: 0.5229596495628357, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.29s
Iteration: 151,  Mean reward: 0.11458333333333333, Mean Entropy: 0.5996670722961426, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 152,  Mean reward: 2.0636363636363635, Mean Entropy: 0.5544888377189636, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 153,  Mean reward: 1.712962962962963, Mean Entropy: 0.5628376007080078, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 154,  Mean reward: -1.4021739130434783, Mean Entropy: 0.5879552364349365, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 155,  Mean reward: -1.6222222222222222, Mean Entropy: 0.6021105051040649, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 156,  Mean reward: -0.04081632653061224, Mean Entropy: 0.5720356702804565, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 157,  Mean reward: 0.3235294117647059, Mean Entropy: 0.5984022617340088, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 158,  Mean reward: -0.5434782608695652, Mean Entropy: 0.6752411127090454, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 159,  Mean reward: -3.1219512195121952, Mean Entropy: 0.6607475280761719, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 160,  Mean reward: 1.0849056603773586, Mean Entropy: 0.5694460868835449, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 161,  Mean reward: 0.4270833333333333, Mean Entropy: 0.577568769454956, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 162,  Mean reward: -0.26595744680851063, Mean Entropy: 0.6426823735237122, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 163,  Mean reward: -3.152173913043478, Mean Entropy: 0.6793529391288757, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 164,  Mean reward: 1.5, Mean Entropy: 0.6191822290420532, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 165,  Mean reward: 0.05555555555555555, Mean Entropy: 0.5868424773216248, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 166,  Mean reward: 0.15384615384615385, Mean Entropy: 0.6117686629295349, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 167,  Mean reward: 0.5370370370370371, Mean Entropy: 0.625786542892456, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 168,  Mean reward: 0.7222222222222222, Mean Entropy: 0.6357616782188416, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 169,  Mean reward: 2.4754098360655736, Mean Entropy: 0.6293225884437561, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 170,  Mean reward: -2.4680851063829787, Mean Entropy: 0.5991492867469788, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 171,  Mean reward: -0.8823529411764706, Mean Entropy: 0.669278621673584, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 172,  Mean reward: 0.5510204081632653, Mean Entropy: 0.6025532484054565, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 173,  Mean reward: -0.057692307692307696, Mean Entropy: 0.6055502891540527, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 174,  Mean reward: -0.03773584905660377, Mean Entropy: 0.6999871730804443, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 175,  Mean reward: -0.25, Mean Entropy: 0.6160146594047546, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 176,  Mean reward: -1.7173913043478262, Mean Entropy: 0.6346902251243591, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 177,  Mean reward: -0.30612244897959184, Mean Entropy: 0.6193773150444031, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 178,  Mean reward: 0.6470588235294118, Mean Entropy: 0.5884625911712646, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.30s
Iteration: 179,  Mean reward: 1.27, Mean Entropy: 0.5886434316635132, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 180,  Mean reward: -2.7777777777777777, Mean Entropy: 0.6460564732551575, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 181,  Mean reward: 1.8416666666666666, Mean Entropy: 0.6190626621246338, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 182,  Mean reward: -4.465909090909091, Mean Entropy: 0.6236177086830139, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 183,  Mean reward: 1.6666666666666667, Mean Entropy: 0.5904605388641357, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 184,  Mean reward: 0.6666666666666666, Mean Entropy: 0.6070865988731384, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 185,  Mean reward: 0.9056603773584906, Mean Entropy: 0.5595083236694336, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 186,  Mean reward: -0.47, Mean Entropy: 0.553970217704773, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 187,  Mean reward: -1.0980392156862746, Mean Entropy: 0.5454452037811279, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 188,  Mean reward: 2.0673076923076925, Mean Entropy: 0.5580851435661316, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 189,  Mean reward: -0.29591836734693877, Mean Entropy: 0.5779430270195007, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 190,  Mean reward: 1.7924528301886793, Mean Entropy: 0.5985879898071289, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 191,  Mean reward: -1.8297872340425532, Mean Entropy: 0.6161286234855652, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.50s
Iteration: 192,  Mean reward: 3.0636363636363635, Mean Entropy: 0.5979275107383728, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 193,  Mean reward: 0.8173076923076923, Mean Entropy: 0.5086562633514404, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 194,  Mean reward: -2.021276595744681, Mean Entropy: 0.608170747756958, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 195,  Mean reward: -0.9693877551020408, Mean Entropy: 0.5592033863067627, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 196,  Mean reward: -3.1444444444444444, Mean Entropy: 0.6410608887672424, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 197,  Mean reward: 1.1727272727272726, Mean Entropy: 0.5703742504119873, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 198,  Mean reward: -0.74, Mean Entropy: 0.6053401231765747, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 199,  Mean reward: 0.37, Mean Entropy: 0.5963765382766724, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 200,  Mean reward: 1.0865384615384615, Mean Entropy: 0.6161500215530396, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 1.1764705882352942, Mean Entropy: 0.6822586059570312, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 202,  Mean reward: 2.3214285714285716, Mean Entropy: 0.5330135226249695, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 203,  Mean reward: -0.16666666666666666, Mean Entropy: 0.6018295288085938, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 204,  Mean reward: 2.2037037037037037, Mean Entropy: 0.5789818167686462, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 205,  Mean reward: -1.1770833333333333, Mean Entropy: 0.6462631225585938, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 206,  Mean reward: -2.75, Mean Entropy: 0.6615476608276367, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 207,  Mean reward: 1.0363636363636364, Mean Entropy: 0.6160540580749512, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 208,  Mean reward: 2.245614035087719, Mean Entropy: 0.5412919521331787, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 209,  Mean reward: -1.4787234042553192, Mean Entropy: 0.6294347047805786, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 210,  Mean reward: 0.125, Mean Entropy: 0.5395545363426208, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 211,  Mean reward: -1.4270833333333333, Mean Entropy: 0.5303064584732056, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 212,  Mean reward: 1.8363636363636364, Mean Entropy: 0.4919597804546356, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 213,  Mean reward: 2.4074074074074074, Mean Entropy: 0.5389655828475952, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 214,  Mean reward: 0.696078431372549, Mean Entropy: 0.5277812480926514, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 215,  Mean reward: 1.5865384615384615, Mean Entropy: 0.5424740314483643, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 216,  Mean reward: 0.7450980392156863, Mean Entropy: 0.5973271131515503, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 217,  Mean reward: 1.9642857142857142, Mean Entropy: 0.5151877403259277, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 218,  Mean reward: 0.8173076923076923, Mean Entropy: 0.5343443155288696, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 219,  Mean reward: 1.8113207547169812, Mean Entropy: 0.5189946293830872, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 220,  Mean reward: 0.7745098039215687, Mean Entropy: 0.5712319016456604, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 221,  Mean reward: 0.13725490196078433, Mean Entropy: 0.5713567137718201, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 222,  Mean reward: 0.19387755102040816, Mean Entropy: 0.5387625694274902, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 223,  Mean reward: 0.5192307692307693, Mean Entropy: 0.5671706199645996, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 224,  Mean reward: 1.5294117647058822, Mean Entropy: 0.5668188333511353, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 225,  Mean reward: 1.5769230769230769, Mean Entropy: 0.5376888513565063, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 226,  Mean reward: 1.3365384615384615, Mean Entropy: 0.5360991954803467, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 227,  Mean reward: 2.7241379310344827, Mean Entropy: 0.4800816774368286, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 228,  Mean reward: -0.5555555555555556, Mean Entropy: 0.5680456161499023, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 229,  Mean reward: 0.57, Mean Entropy: 0.5177688598632812, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 230,  Mean reward: 0.2, Mean Entropy: 0.6062654852867126, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 231,  Mean reward: 0.0784313725490196, Mean Entropy: 0.6288356184959412, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 232,  Mean reward: 1.4907407407407407, Mean Entropy: 0.560143232345581, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 233,  Mean reward: -1.03125, Mean Entropy: 0.565011739730835, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 234,  Mean reward: 2.107142857142857, Mean Entropy: 0.5510307550430298, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 235,  Mean reward: 1.7019230769230769, Mean Entropy: 0.5206500291824341, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 236,  Mean reward: 0.2647058823529412, Mean Entropy: 0.5191869735717773, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 237,  Mean reward: 0.86, Mean Entropy: 0.5595332980155945, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 238,  Mean reward: 3.087719298245614, Mean Entropy: 0.6098432540893555, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 239,  Mean reward: 0.41, Mean Entropy: 0.6188473701477051, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 240,  Mean reward: 0.03125, Mean Entropy: 0.6333739161491394, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 241,  Mean reward: 0.1, Mean Entropy: 0.564477801322937, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 242,  Mean reward: -1.3804347826086956, Mean Entropy: 0.6360152959823608, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 243,  Mean reward: 1.4038461538461537, Mean Entropy: 0.6273244619369507, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 244,  Mean reward: 1.4519230769230769, Mean Entropy: 0.5534417629241943, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 245,  Mean reward: 1.6, Mean Entropy: 0.5515475869178772, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 246,  Mean reward: 2.3363636363636364, Mean Entropy: 0.5121444463729858, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 247,  Mean reward: -0.2755102040816326, Mean Entropy: 0.5616945624351501, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 248,  Mean reward: 2.982142857142857, Mean Entropy: 0.5158895254135132, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 249,  Mean reward: 0.47115384615384615, Mean Entropy: 0.5804761648178101, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 250,  Mean reward: -0.96, Mean Entropy: 0.5859244465827942, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 251,  Mean reward: 2.3796296296296298, Mean Entropy: 0.5316238403320312, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 252,  Mean reward: 1.3584905660377358, Mean Entropy: 0.5769715905189514, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 253,  Mean reward: -0.9479166666666666, Mean Entropy: 0.5811330676078796, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 254,  Mean reward: 1.6764705882352942, Mean Entropy: 0.5243721008300781, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 255,  Mean reward: 1.7962962962962963, Mean Entropy: 0.5422167778015137, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 256,  Mean reward: 1.2685185185185186, Mean Entropy: 0.5457948446273804, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 257,  Mean reward: 0.9722222222222222, Mean Entropy: 0.5494134426116943, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 258,  Mean reward: 1.7788461538461537, Mean Entropy: 0.49139055609703064, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 259,  Mean reward: 0.4411764705882353, Mean Entropy: 0.5612751245498657, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 260,  Mean reward: -0.5918367346938775, Mean Entropy: 0.5580899119377136, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.34s
Iteration: 261,  Mean reward: 1.25, Mean Entropy: 0.46500128507614136, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 262,  Mean reward: 2.8636363636363638, Mean Entropy: 0.4528714418411255, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 263,  Mean reward: -0.5625, Mean Entropy: 0.6319383978843689, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 264,  Mean reward: 2.5357142857142856, Mean Entropy: 0.5159852504730225, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 265,  Mean reward: -0.53, Mean Entropy: 0.5451720952987671, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 266,  Mean reward: 1.65, Mean Entropy: 0.5333828926086426, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 267,  Mean reward: 2.2857142857142856, Mean Entropy: 0.49813222885131836, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 268,  Mean reward: 1.9553571428571428, Mean Entropy: 0.489177942276001, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 269,  Mean reward: 1.6574074074074074, Mean Entropy: 0.5216329097747803, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 270,  Mean reward: 2.4727272727272727, Mean Entropy: 0.548588752746582, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 271,  Mean reward: 0.6698113207547169, Mean Entropy: 0.6074180006980896, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 272,  Mean reward: 1.5555555555555556, Mean Entropy: 0.5594229698181152, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 273,  Mean reward: 3.692982456140351, Mean Entropy: 0.5840986967086792, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 274,  Mean reward: -0.1875, Mean Entropy: 0.6791736483573914, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 275,  Mean reward: 1.2884615384615385, Mean Entropy: 0.6073288917541504, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 276,  Mean reward: 1.3235294117647058, Mean Entropy: 0.6194051504135132, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 277,  Mean reward: 0.0784313725490196, Mean Entropy: 0.6352297067642212, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 278,  Mean reward: -0.2857142857142857, Mean Entropy: 0.6148980855941772, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 279,  Mean reward: -2.702127659574468, Mean Entropy: 0.6449282169342041, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 280,  Mean reward: -0.3404255319148936, Mean Entropy: 0.6064543128013611, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 281,  Mean reward: 1.5648148148148149, Mean Entropy: 0.5872547626495361, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 282,  Mean reward: 0.02040816326530612, Mean Entropy: 0.5785037279129028, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 283,  Mean reward: 1.53, Mean Entropy: 0.5501875877380371, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 284,  Mean reward: 1.9642857142857142, Mean Entropy: 0.6131231784820557, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 285,  Mean reward: 0.5, Mean Entropy: 0.5406872034072876, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 286,  Mean reward: -0.3113207547169811, Mean Entropy: 0.5167038440704346, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 287,  Mean reward: -2.6122448979591835, Mean Entropy: 0.5926613807678223, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 288,  Mean reward: -2.260869565217391, Mean Entropy: 0.6422044634819031, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 289,  Mean reward: -1.58, Mean Entropy: 0.5813223123550415, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 290,  Mean reward: 2.482142857142857, Mean Entropy: 0.5172500014305115, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 291,  Mean reward: 1.6203703703703705, Mean Entropy: 0.5825251936912537, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 292,  Mean reward: -0.5555555555555556, Mean Entropy: 0.5596606731414795, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 293,  Mean reward: 1.3, Mean Entropy: 0.5658394694328308, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 294,  Mean reward: 0.37037037037037035, Mean Entropy: 0.5867381691932678, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 295,  Mean reward: -0.6862745098039216, Mean Entropy: 0.5347387790679932, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 296,  Mean reward: -1.9895833333333333, Mean Entropy: 0.6199118494987488, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 297,  Mean reward: -2.467391304347826, Mean Entropy: 0.5653359889984131, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 298,  Mean reward: -2.51, Mean Entropy: 0.5475103855133057, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 299,  Mean reward: -0.07407407407407407, Mean Entropy: 0.526059627532959, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 300,  Mean reward: -0.6153846153846154, Mean Entropy: 0.5527850985527039, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 1.2410714285714286, Mean Entropy: 0.5805107355117798, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 302,  Mean reward: -0.3018867924528302, Mean Entropy: 0.5307642221450806, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 303,  Mean reward: 0.7105263157894737, Mean Entropy: 0.5962997674942017, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 304,  Mean reward: 0.19230769230769232, Mean Entropy: 0.4966728687286377, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 305,  Mean reward: 0.26785714285714285, Mean Entropy: 0.5873100757598877, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 306,  Mean reward: -1.6470588235294117, Mean Entropy: 0.5436885356903076, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 307,  Mean reward: -0.5204081632653061, Mean Entropy: 0.5840145945549011, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 308,  Mean reward: 3.7711864406779663, Mean Entropy: 0.5592138767242432, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.28s
Iteration: 309,  Mean reward: 2.875, Mean Entropy: 0.5805217027664185, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 310,  Mean reward: 2.5, Mean Entropy: 0.5384196043014526, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 311,  Mean reward: 1.06, Mean Entropy: 0.576251745223999, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 312,  Mean reward: -0.2608695652173913, Mean Entropy: 0.5906538367271423, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 313,  Mean reward: 1.3431372549019607, Mean Entropy: 0.5902343988418579, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 314,  Mean reward: 1.212962962962963, Mean Entropy: 0.5084879398345947, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 315,  Mean reward: 2.451923076923077, Mean Entropy: 0.5194821357727051, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 316,  Mean reward: 1.669811320754717, Mean Entropy: 0.5287359952926636, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 317,  Mean reward: -0.16666666666666666, Mean Entropy: 0.6082583665847778, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 318,  Mean reward: 1.6442307692307692, Mean Entropy: 0.5998396873474121, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 319,  Mean reward: 1.8818181818181818, Mean Entropy: 0.5777169466018677, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 320,  Mean reward: 3.2181818181818183, Mean Entropy: 0.5280393958091736, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 321,  Mean reward: 1.4363636363636363, Mean Entropy: 0.5643026232719421, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 322,  Mean reward: 1.849056603773585, Mean Entropy: 0.5914772748947144, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 323,  Mean reward: 1.4811320754716981, Mean Entropy: 0.6208634376525879, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 324,  Mean reward: 1.7980769230769231, Mean Entropy: 0.6040822267532349, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 325,  Mean reward: 1.7636363636363637, Mean Entropy: 0.6234228610992432, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 326,  Mean reward: 1.3240740740740742, Mean Entropy: 0.5623003840446472, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 327,  Mean reward: 1.17, Mean Entropy: 0.634519100189209, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 328,  Mean reward: 1.8181818181818181, Mean Entropy: 0.6214566230773926, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 329,  Mean reward: 1.2, Mean Entropy: 0.6309542655944824, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 330,  Mean reward: 2.048076923076923, Mean Entropy: 0.5838727355003357, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 331,  Mean reward: 2.5961538461538463, Mean Entropy: 0.5989366769790649, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 332,  Mean reward: 0.2980769230769231, Mean Entropy: 0.6424732208251953, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 333,  Mean reward: 0.4215686274509804, Mean Entropy: 0.6157150268554688, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 334,  Mean reward: 2.9313725490196076, Mean Entropy: 0.5687823295593262, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 335,  Mean reward: -3.6333333333333333, Mean Entropy: 0.6981527209281921, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 336,  Mean reward: 1.5, Mean Entropy: 0.6391987204551697, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 337,  Mean reward: -1.53, Mean Entropy: 0.5944127440452576, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 338,  Mean reward: 2.210526315789474, Mean Entropy: 0.5237995982170105, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 339,  Mean reward: 2.118181818181818, Mean Entropy: 0.6161599159240723, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 340,  Mean reward: -0.5, Mean Entropy: 0.6356587409973145, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 341,  Mean reward: 1.9711538461538463, Mean Entropy: 0.6990104913711548, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 342,  Mean reward: 1.3396226415094339, Mean Entropy: 0.6366496086120605, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 343,  Mean reward: 1.4313725490196079, Mean Entropy: 0.6223384141921997, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.27s
Iteration: 344,  Mean reward: 0.08163265306122448, Mean Entropy: 0.6183504462242126, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 345,  Mean reward: -0.18, Mean Entropy: 0.6882416605949402, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 346,  Mean reward: 0.6057692307692307, Mean Entropy: 0.634807288646698, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 347,  Mean reward: 1.4444444444444444, Mean Entropy: 0.5981720685958862, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 348,  Mean reward: 0.8653846153846154, Mean Entropy: 0.5516073107719421, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 349,  Mean reward: 0.7592592592592593, Mean Entropy: 0.5175401568412781, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 350,  Mean reward: 0.12962962962962962, Mean Entropy: 0.6065961122512817, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 351,  Mean reward: -0.9042553191489362, Mean Entropy: 0.5558065176010132, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 352,  Mean reward: 0.49, Mean Entropy: 0.5981904864311218, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.28s
Iteration: 353,  Mean reward: -2.6041666666666665, Mean Entropy: 0.5941461324691772, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 354,  Mean reward: 0.4897959183673469, Mean Entropy: 0.5682727694511414, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 355,  Mean reward: -1.4042553191489362, Mean Entropy: 0.6088426113128662, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 356,  Mean reward: -0.6057692307692307, Mean Entropy: 0.5971322059631348, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 357,  Mean reward: 0.8867924528301887, Mean Entropy: 0.5682950615882874, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 358,  Mean reward: 1.4901960784313726, Mean Entropy: 0.5502023696899414, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 359,  Mean reward: -1.6914893617021276, Mean Entropy: 0.5682938098907471, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 360,  Mean reward: 0.5089285714285714, Mean Entropy: 0.5557026267051697, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 361,  Mean reward: 0.03, Mean Entropy: 0.6481972932815552, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 362,  Mean reward: 2.441666666666667, Mean Entropy: 0.5934857130050659, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 363,  Mean reward: 1.5545454545454545, Mean Entropy: 0.5626323223114014, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 364,  Mean reward: 0.11538461538461539, Mean Entropy: 0.5480779409408569, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 365,  Mean reward: 0.8796296296296297, Mean Entropy: 0.5873804092407227, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 366,  Mean reward: 2.675925925925926, Mean Entropy: 0.5033861994743347, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 367,  Mean reward: 0.660377358490566, Mean Entropy: 0.5924591422080994, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 368,  Mean reward: 1.2053571428571428, Mean Entropy: 0.513698399066925, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 369,  Mean reward: 0.49019607843137253, Mean Entropy: 0.6374226212501526, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 370,  Mean reward: -1.2555555555555555, Mean Entropy: 0.6391578316688538, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 371,  Mean reward: 0.2708333333333333, Mean Entropy: 0.6536886692047119, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 372,  Mean reward: 0.19230769230769232, Mean Entropy: 0.5918797254562378, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 373,  Mean reward: 0.5769230769230769, Mean Entropy: 0.6377915143966675, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 374,  Mean reward: 1.2075471698113207, Mean Entropy: 0.5665150880813599, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 375,  Mean reward: 1.1730769230769231, Mean Entropy: 0.6068384051322937, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 376,  Mean reward: 1.4339622641509433, Mean Entropy: 0.5252323150634766, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 377,  Mean reward: -0.10377358490566038, Mean Entropy: 0.6548818945884705, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 378,  Mean reward: -0.6538461538461539, Mean Entropy: 0.6094297766685486, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 379,  Mean reward: 2.5166666666666666, Mean Entropy: 0.5395490527153015, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 380,  Mean reward: -1.7604166666666667, Mean Entropy: 0.5909246802330017, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 381,  Mean reward: 3.7033898305084745, Mean Entropy: 0.4934190511703491, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 382,  Mean reward: 0.75, Mean Entropy: 0.5842361450195312, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 383,  Mean reward: -2.6777777777777776, Mean Entropy: 0.6059929728507996, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 384,  Mean reward: -0.8469387755102041, Mean Entropy: 0.672050416469574, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 385,  Mean reward: 2.3518518518518516, Mean Entropy: 0.6629608869552612, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 386,  Mean reward: 2.745614035087719, Mean Entropy: 0.5404099225997925, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 387,  Mean reward: 0.5294117647058824, Mean Entropy: 0.6124250888824463, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 388,  Mean reward: 1.0471698113207548, Mean Entropy: 0.6375294923782349, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 389,  Mean reward: 0.42, Mean Entropy: 0.6857184767723083, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 390,  Mean reward: 0.47959183673469385, Mean Entropy: 0.6801988482475281, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 391,  Mean reward: 2.790909090909091, Mean Entropy: 0.5962926149368286, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 392,  Mean reward: 0.6274509803921569, Mean Entropy: 0.648801326751709, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 393,  Mean reward: 0.62, Mean Entropy: 0.51524817943573, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 394,  Mean reward: 3.060344827586207, Mean Entropy: 0.5494770407676697, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 395,  Mean reward: 0.9716981132075472, Mean Entropy: 0.5641657710075378, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 396,  Mean reward: -0.49, Mean Entropy: 0.5632287859916687, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 397,  Mean reward: -1.797872340425532, Mean Entropy: 0.6393154859542847, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 398,  Mean reward: 2.7777777777777777, Mean Entropy: 0.5545125007629395, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 399,  Mean reward: 1.320754716981132, Mean Entropy: 0.517876148223877, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 400,  Mean reward: 1.1730769230769231, Mean Entropy: 0.5375863313674927, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 2.9583333333333335, Mean Entropy: 0.5902012586593628, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 402,  Mean reward: 1.349056603773585, Mean Entropy: 0.5564174652099609, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 403,  Mean reward: 0.3269230769230769, Mean Entropy: 0.5241265296936035, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 404,  Mean reward: 3.3934426229508197, Mean Entropy: 0.5117834806442261, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.28s
Iteration: 405,  Mean reward: 2.694915254237288, Mean Entropy: 0.49396762251853943, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 406,  Mean reward: 0.9519230769230769, Mean Entropy: 0.4796299934387207, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 407,  Mean reward: 2.6203703703703702, Mean Entropy: 0.510191798210144, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 408,  Mean reward: 0.17708333333333334, Mean Entropy: 0.6216474771499634, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 409,  Mean reward: 1.6603773584905661, Mean Entropy: 0.5847187042236328, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 410,  Mean reward: 2.2358490566037736, Mean Entropy: 0.6058945655822754, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 411,  Mean reward: 1.2962962962962963, Mean Entropy: 0.5966903567314148, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 412,  Mean reward: 0.5392156862745098, Mean Entropy: 0.6501649618148804, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 413,  Mean reward: 2.5625, Mean Entropy: 0.49360355734825134, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 414,  Mean reward: 0.55, Mean Entropy: 0.6080406904220581, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 415,  Mean reward: 2.456896551724138, Mean Entropy: 0.5057043433189392, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 416,  Mean reward: 1.3214285714285714, Mean Entropy: 0.5513432025909424, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 417,  Mean reward: 2.0, Mean Entropy: 0.5589817762374878, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 418,  Mean reward: 2.6964285714285716, Mean Entropy: 0.4893455505371094, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.28s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 419,  Mean reward: 4.683333333333334, Mean Entropy: 0.4528384208679199, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.28s
Iteration: 420,  Mean reward: 2.3684210526315788, Mean Entropy: 0.5865702629089355, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 421,  Mean reward: -0.5612244897959183, Mean Entropy: 0.5684216022491455, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 422,  Mean reward: 1.3055555555555556, Mean Entropy: 0.5767239332199097, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 423,  Mean reward: 2.3454545454545452, Mean Entropy: 0.5288169980049133, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 424,  Mean reward: 3.4745762711864407, Mean Entropy: 0.5284981727600098, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 425,  Mean reward: 3.456896551724138, Mean Entropy: 0.30032989382743835, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 426,  Mean reward: 1.6274509803921569, Mean Entropy: 0.3661282956600189, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 427,  Mean reward: 2.5, Mean Entropy: 0.436138391494751, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 428,  Mean reward: -0.08695652173913043, Mean Entropy: 0.5357420444488525, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 429,  Mean reward: 2.303921568627451, Mean Entropy: 0.35644954442977905, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 430,  Mean reward: 1.9134615384615385, Mean Entropy: 0.37918028235435486, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 431,  Mean reward: 2.452830188679245, Mean Entropy: 0.30537253618240356, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 432,  Mean reward: 3.5727272727272728, Mean Entropy: 0.3319704532623291, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 433,  Mean reward: 3.05, Mean Entropy: 0.3403294086456299, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 434,  Mean reward: 2.234042553191489, Mean Entropy: 0.42649659514427185, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 435,  Mean reward: 2.1470588235294117, Mean Entropy: 0.4798238277435303, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 436,  Mean reward: 3.22, Mean Entropy: 0.4008050560951233, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 437,  Mean reward: 3.5098039215686274, Mean Entropy: 0.31060293316841125, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 438,  Mean reward: 2.0208333333333335, Mean Entropy: 0.5444766283035278, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 439,  Mean reward: 2.2058823529411766, Mean Entropy: 0.5119630694389343, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 440,  Mean reward: 2.8541666666666665, Mean Entropy: 0.34274566173553467, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 441,  Mean reward: 3.0714285714285716, Mean Entropy: 0.28088539838790894, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 442,  Mean reward: 3.0816326530612246, Mean Entropy: 0.44464248418807983, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 443,  Mean reward: 2.29, Mean Entropy: 0.45304179191589355, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 444,  Mean reward: -0.1956521739130435, Mean Entropy: 0.4220779836177826, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 445,  Mean reward: 1.9583333333333333, Mean Entropy: 0.4416508674621582, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 446,  Mean reward: 4.0673076923076925, Mean Entropy: 0.4136318564414978, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.28s
Iteration: 447,  Mean reward: 3.4607843137254903, Mean Entropy: 0.4788321852684021, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 448,  Mean reward: 2.561224489795918, Mean Entropy: 0.5491301417350769, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 449,  Mean reward: 2.688679245283019, Mean Entropy: 0.5945336818695068, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 450,  Mean reward: 1.0319148936170213, Mean Entropy: 0.5485614538192749, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 451,  Mean reward: 2.6595744680851063, Mean Entropy: 0.5417320728302002, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 452,  Mean reward: 2.0520833333333335, Mean Entropy: 0.4806041121482849, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 453,  Mean reward: 2.4583333333333335, Mean Entropy: 0.5308739542961121, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 454,  Mean reward: 3.46, Mean Entropy: 0.4019707143306732, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 455,  Mean reward: 4.173469387755102, Mean Entropy: 0.36566442251205444, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 456,  Mean reward: 2.648936170212766, Mean Entropy: 0.7201273441314697, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 457,  Mean reward: 3.89, Mean Entropy: 0.47924113273620605, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 458,  Mean reward: 3.7083333333333335, Mean Entropy: 0.43706995248794556, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 459,  Mean reward: 4.324561403508772, Mean Entropy: 0.5039701461791992, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 460,  Mean reward: 2.269230769230769, Mean Entropy: 0.5327683687210083, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 461,  Mean reward: 4.646551724137931, Mean Entropy: 0.35966503620147705, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 462,  Mean reward: -0.4148936170212766, Mean Entropy: 0.5765728950500488, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 463,  Mean reward: -0.1836734693877551, Mean Entropy: 0.6254645586013794, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 464,  Mean reward: 2.2830188679245285, Mean Entropy: 0.5639142990112305, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 465,  Mean reward: 1.6826923076923077, Mean Entropy: 0.6102983951568604, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 466,  Mean reward: 0.5, Mean Entropy: 0.6110788583755493, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 467,  Mean reward: 2.2264150943396226, Mean Entropy: 0.6049866676330566, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 468,  Mean reward: 3.2454545454545456, Mean Entropy: 0.5750075578689575, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 469,  Mean reward: 2.5925925925925926, Mean Entropy: 0.58431077003479, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 470,  Mean reward: 2.201923076923077, Mean Entropy: 0.6434837579727173, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 471,  Mean reward: 0.22340425531914893, Mean Entropy: 0.616686224937439, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 472,  Mean reward: 2.088235294117647, Mean Entropy: 0.5219767093658447, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 473,  Mean reward: 1.7884615384615385, Mean Entropy: 0.5390799045562744, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 474,  Mean reward: 2.2232142857142856, Mean Entropy: 0.63813316822052, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 475,  Mean reward: 3.259259259259259, Mean Entropy: 0.4991961717605591, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 476,  Mean reward: 2.17, Mean Entropy: 0.3389190435409546, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 477,  Mean reward: 1.4680851063829787, Mean Entropy: 0.3080857992172241, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 478,  Mean reward: 2.3645833333333335, Mean Entropy: 0.29865071177482605, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 479,  Mean reward: 0.5833333333333334, Mean Entropy: 0.4515632092952728, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 480,  Mean reward: 3.3627450980392157, Mean Entropy: 0.4994736611843109, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 481,  Mean reward: 2.3617021276595747, Mean Entropy: 0.4486081600189209, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 482,  Mean reward: 2.11, Mean Entropy: 0.4573374390602112, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.27s
Iteration: 483,  Mean reward: 3.7596153846153846, Mean Entropy: 0.41167983412742615, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 484,  Mean reward: 2.0, Mean Entropy: 0.3984524607658386, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 485,  Mean reward: 3.0416666666666665, Mean Entropy: 0.48789843916893005, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 486,  Mean reward: 1.0, Mean Entropy: 0.5185343027114868, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 487,  Mean reward: 0.8888888888888888, Mean Entropy: 0.5125181674957275, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 488,  Mean reward: 2.630434782608696, Mean Entropy: 0.36860164999961853, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 489,  Mean reward: 2.42, Mean Entropy: 0.3915693759918213, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 490,  Mean reward: 4.078431372549019, Mean Entropy: 0.28473353385925293, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 491,  Mean reward: 2.4313725490196076, Mean Entropy: 0.5588181018829346, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 492,  Mean reward: 3.807017543859649, Mean Entropy: 0.5518655180931091, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 493,  Mean reward: 4.728070175438597, Mean Entropy: 0.37190911173820496, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 494,  Mean reward: 3.5357142857142856, Mean Entropy: 0.3435250520706177, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 495,  Mean reward: 2.6346153846153846, Mean Entropy: 0.3240472674369812, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 496,  Mean reward: 0.16304347826086957, Mean Entropy: 0.34428995847702026, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 497,  Mean reward: 3.688679245283019, Mean Entropy: 0.35447239875793457, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 498,  Mean reward: 0.9574468085106383, Mean Entropy: 0.404207706451416, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 499,  Mean reward: 3.423076923076923, Mean Entropy: 0.15394149720668793, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 500,  Mean reward: -5.076086956521739, Mean Entropy: 0.69191575050354, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.27s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 1.3137254901960784, Mean Entropy: 0.5242896676063538, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 502,  Mean reward: 2.2142857142857144, Mean Entropy: 0.490925133228302, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 503,  Mean reward: 1.6132075471698113, Mean Entropy: 0.32343193888664246, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 504,  Mean reward: 1.683673469387755, Mean Entropy: 0.3559131622314453, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 505,  Mean reward: 3.6226415094339623, Mean Entropy: 0.46442264318466187, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 506,  Mean reward: 2.759259259259259, Mean Entropy: 0.5410864949226379, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 507,  Mean reward: 2.918181818181818, Mean Entropy: 0.5959693193435669, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 508,  Mean reward: 1.375, Mean Entropy: 0.5525221824645996, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 509,  Mean reward: 0.0, Mean Entropy: 0.3438776433467865, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 510,  Mean reward: 2.2448979591836733, Mean Entropy: 0.43624329566955566, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 511,  Mean reward: 2.34375, Mean Entropy: 0.39319396018981934, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.27s
Iteration: 512,  Mean reward: 2.989795918367347, Mean Entropy: 0.3454996943473816, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 513,  Mean reward: 4.066037735849057, Mean Entropy: 0.29552602767944336, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 514,  Mean reward: 3.7142857142857144, Mean Entropy: 0.5405939817428589, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 515,  Mean reward: 3.6403508771929824, Mean Entropy: 0.6223732829093933, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 516,  Mean reward: 1.9591836734693877, Mean Entropy: 0.4562082290649414, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 517,  Mean reward: 3.188679245283019, Mean Entropy: 0.43713635206222534, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 518,  Mean reward: 4.410714285714286, Mean Entropy: 0.39886531233787537, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 519,  Mean reward: 1.9042553191489362, Mean Entropy: 0.39277327060699463, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 520,  Mean reward: 2.9, Mean Entropy: 0.3960348069667816, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 521,  Mean reward: 2.183673469387755, Mean Entropy: 0.40731534361839294, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 522,  Mean reward: 3.479591836734694, Mean Entropy: 0.36706846952438354, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 523,  Mean reward: 3.1595744680851063, Mean Entropy: 0.3706657886505127, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 524,  Mean reward: -0.35555555555555557, Mean Entropy: 0.6086783409118652, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 525,  Mean reward: 0.8085106382978723, Mean Entropy: 0.43103736639022827, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 526,  Mean reward: 4.196078431372549, Mean Entropy: 0.45578598976135254, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 527,  Mean reward: 3.1481481481481484, Mean Entropy: 0.49080967903137207, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 528,  Mean reward: 1.7, Mean Entropy: 0.5385493636131287, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 529,  Mean reward: 2.979591836734694, Mean Entropy: 0.43851685523986816, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 530,  Mean reward: 1.7659574468085106, Mean Entropy: 0.5473171472549438, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 531,  Mean reward: 0.35294117647058826, Mean Entropy: 0.6331030130386353, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 532,  Mean reward: 0.22826086956521738, Mean Entropy: 0.6202825307846069, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 533,  Mean reward: 4.631578947368421, Mean Entropy: 0.5013647079467773, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.29s
Iteration: 534,  Mean reward: 4.591836734693878, Mean Entropy: 0.477243036031723, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 535,  Mean reward: 2.869565217391304, Mean Entropy: 0.540921688079834, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 536,  Mean reward: 0.044444444444444446, Mean Entropy: 0.5174094438552856, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 537,  Mean reward: 2.435185185185185, Mean Entropy: 0.5574953556060791, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 538,  Mean reward: 1.5377358490566038, Mean Entropy: 0.5492366552352905, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 539,  Mean reward: 2.4375, Mean Entropy: 0.6616342067718506, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 540,  Mean reward: 3.73728813559322, Mean Entropy: 0.5765313506126404, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 541,  Mean reward: 4.146551724137931, Mean Entropy: 0.5594348311424255, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 542,  Mean reward: 2.8679245283018866, Mean Entropy: 0.554155707359314, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 543,  Mean reward: 3.5258620689655173, Mean Entropy: 0.5343513488769531, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.30s
Iteration: 544,  Mean reward: 1.7169811320754718, Mean Entropy: 0.5769628286361694, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 545,  Mean reward: 0.4791666666666667, Mean Entropy: 0.5629445910453796, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 546,  Mean reward: 1.5217391304347827, Mean Entropy: 0.5286922454833984, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 547,  Mean reward: 2.1382978723404253, Mean Entropy: 0.5095121264457703, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 548,  Mean reward: 2.7745098039215685, Mean Entropy: 0.6472734212875366, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 549,  Mean reward: 3.4, Mean Entropy: 0.5558696985244751, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 550,  Mean reward: 3.9722222222222223, Mean Entropy: 0.492764413356781, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 551,  Mean reward: 3.509259259259259, Mean Entropy: 0.5736876726150513, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 552,  Mean reward: 2.0918367346938775, Mean Entropy: 0.616868257522583, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 553,  Mean reward: 3.772727272727273, Mean Entropy: 0.6231581568717957, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 554,  Mean reward: 1.7211538461538463, Mean Entropy: 0.5445274114608765, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 555,  Mean reward: 2.4150943396226414, Mean Entropy: 0.5913121700286865, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 556,  Mean reward: 2.588235294117647, Mean Entropy: 0.5850764513015747, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 557,  Mean reward: 1.5714285714285714, Mean Entropy: 0.6292413473129272, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 558,  Mean reward: 2.1, Mean Entropy: 0.6361453533172607, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 559,  Mean reward: 2.688679245283019, Mean Entropy: 0.5984508395195007, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 560,  Mean reward: 2.46, Mean Entropy: 0.5232981443405151, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 561,  Mean reward: 4.25, Mean Entropy: 0.5102202892303467, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.27s
Iteration: 562,  Mean reward: 3.660377358490566, Mean Entropy: 0.41792309284210205, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 563,  Mean reward: 3.7941176470588234, Mean Entropy: 0.40551409125328064, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 564,  Mean reward: 3.9134615384615383, Mean Entropy: 0.35943686962127686, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 565,  Mean reward: 2.358490566037736, Mean Entropy: 0.480449378490448, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 566,  Mean reward: 2.6862745098039214, Mean Entropy: 0.4522966742515564, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 567,  Mean reward: 3.230769230769231, Mean Entropy: 0.49353766441345215, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 568,  Mean reward: 1.86, Mean Entropy: 0.49347424507141113, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 569,  Mean reward: 3.2254901960784315, Mean Entropy: 0.4460793137550354, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 570,  Mean reward: 5.076271186440678, Mean Entropy: 0.5697363615036011, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 571,  Mean reward: 2.519607843137255, Mean Entropy: 0.4912572503089905, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 572,  Mean reward: 2.6132075471698113, Mean Entropy: 0.5016798973083496, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 573,  Mean reward: 3.2, Mean Entropy: 0.32036927342414856, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 574,  Mean reward: 0.723404255319149, Mean Entropy: 0.6276573538780212, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 575,  Mean reward: 0.23958333333333334, Mean Entropy: 0.52586829662323, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 576,  Mean reward: 1.316326530612245, Mean Entropy: 0.5547810792922974, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 577,  Mean reward: 0.9166666666666666, Mean Entropy: 0.5677110552787781, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.32s
Iteration: 578,  Mean reward: 3.5283018867924527, Mean Entropy: 0.5635094046592712, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 579,  Mean reward: 2.3229166666666665, Mean Entropy: 0.5084096789360046, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 580,  Mean reward: 3.9107142857142856, Mean Entropy: 0.53166663646698, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 581,  Mean reward: 2.4313725490196076, Mean Entropy: 0.6440295577049255, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 582,  Mean reward: 2.1875, Mean Entropy: 0.6328803896903992, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 583,  Mean reward: 2.727272727272727, Mean Entropy: 0.5808049440383911, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 584,  Mean reward: 3.1826923076923075, Mean Entropy: 0.4991854131221771, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 585,  Mean reward: 4.12280701754386, Mean Entropy: 0.48524269461631775, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 586,  Mean reward: 2.1792452830188678, Mean Entropy: 0.5103802680969238, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 587,  Mean reward: 3.5545454545454547, Mean Entropy: 0.2847479581832886, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 588,  Mean reward: 4.028846153846154, Mean Entropy: 0.32417500019073486, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 589,  Mean reward: 3.1176470588235294, Mean Entropy: 0.4021487236022949, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 590,  Mean reward: 4.0588235294117645, Mean Entropy: 0.4295215606689453, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 591,  Mean reward: 1.8333333333333333, Mean Entropy: 0.6182865500450134, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 592,  Mean reward: 1.53125, Mean Entropy: 0.7071250677108765, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 593,  Mean reward: 0.7551020408163265, Mean Entropy: 0.6678699851036072, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 594,  Mean reward: 0.5833333333333334, Mean Entropy: 0.5756263136863708, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 595,  Mean reward: 1.65, Mean Entropy: 0.6056731939315796, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 596,  Mean reward: 3.1826923076923075, Mean Entropy: 0.598815381526947, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 597,  Mean reward: 2.5, Mean Entropy: 0.5774177312850952, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 598,  Mean reward: 2.4326923076923075, Mean Entropy: 0.6941139101982117, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 599,  Mean reward: 4.214285714285714, Mean Entropy: 0.4318762719631195, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 600,  Mean reward: 3.2962962962962963, Mean Entropy: 0.49970531463623047, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 2.29, Mean Entropy: 0.47879931330680847, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 602,  Mean reward: 3.144230769230769, Mean Entropy: 0.3952507972717285, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 603,  Mean reward: 2.5, Mean Entropy: 0.48359793424606323, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 604,  Mean reward: 3.732142857142857, Mean Entropy: 0.5146504640579224, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 605,  Mean reward: 3.36, Mean Entropy: 0.48389506340026855, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 606,  Mean reward: 2.0217391304347827, Mean Entropy: 0.6041010618209839, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 607,  Mean reward: 2.9270833333333335, Mean Entropy: 0.4185093343257904, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 608,  Mean reward: 1.0471698113207548, Mean Entropy: 0.43384069204330444, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 609,  Mean reward: 5.645161290322581, Mean Entropy: 0.5734376311302185, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 610,  Mean reward: 1.7692307692307692, Mean Entropy: 0.4674622714519501, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 611,  Mean reward: 0.8645833333333334, Mean Entropy: 0.4546201825141907, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 612,  Mean reward: 2.4245283018867925, Mean Entropy: 0.5554459691047668, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 613,  Mean reward: 2.642857142857143, Mean Entropy: 0.4354659914970398, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 614,  Mean reward: 2.83, Mean Entropy: 0.521213710308075, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 615,  Mean reward: 4.351851851851852, Mean Entropy: 0.41903185844421387, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 616,  Mean reward: 4.830357142857143, Mean Entropy: 0.4119810461997986, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 617,  Mean reward: 3.1226415094339623, Mean Entropy: 0.46534669399261475, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 618,  Mean reward: 4.728070175438597, Mean Entropy: 0.36048781871795654, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 619,  Mean reward: 2.5784313725490198, Mean Entropy: 0.40810951590538025, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 620,  Mean reward: 3.9298245614035086, Mean Entropy: 0.44349488615989685, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 621,  Mean reward: 5.348214285714286, Mean Entropy: 0.46408817172050476, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 622,  Mean reward: 4.314814814814815, Mean Entropy: 0.36485061049461365, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 623,  Mean reward: 3.9537037037037037, Mean Entropy: 0.42497891187667847, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 624,  Mean reward: 2.685185185185185, Mean Entropy: 0.36099886894226074, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 625,  Mean reward: 3.690909090909091, Mean Entropy: 0.3381369709968567, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 626,  Mean reward: 4.131578947368421, Mean Entropy: 0.4821772575378418, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 627,  Mean reward: -0.3684210526315789, Mean Entropy: 0.5443480014801025, complete_episode_count: 38.0, Gather time: 0.48s, Train time: 1.26s
Iteration: 628,  Mean reward: 2.9375, Mean Entropy: 0.4061731696128845, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 629,  Mean reward: 4.923728813559322, Mean Entropy: 0.3502504229545593, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 630,  Mean reward: 1.5625, Mean Entropy: 0.35823583602905273, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 631,  Mean reward: 2.9537037037037037, Mean Entropy: 0.2858336567878723, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 632,  Mean reward: 3.745614035087719, Mean Entropy: 0.31705865263938904, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 633,  Mean reward: 4.350877192982456, Mean Entropy: 0.4585602283477783, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 634,  Mean reward: 3.609090909090909, Mean Entropy: 0.43182462453842163, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 635,  Mean reward: 4.412280701754386, Mean Entropy: 0.3079701364040375, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 636,  Mean reward: 4.775, Mean Entropy: 0.40962857007980347, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 637,  Mean reward: 3.7254901960784315, Mean Entropy: 0.22602832317352295, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 638,  Mean reward: 4.472727272727273, Mean Entropy: 0.3657965660095215, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 639,  Mean reward: 3.201923076923077, Mean Entropy: 0.30651164054870605, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 640,  Mean reward: 4.782258064516129, Mean Entropy: 0.27364489436149597, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 641,  Mean reward: 5.316666666666666, Mean Entropy: 0.26298317313194275, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 642,  Mean reward: 4.163793103448276, Mean Entropy: 0.2386528104543686, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 643,  Mean reward: 3.451923076923077, Mean Entropy: 0.2751308083534241, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 644,  Mean reward: 4.842105263157895, Mean Entropy: 0.29125362634658813, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 645,  Mean reward: 3.2596153846153846, Mean Entropy: 0.25194787979125977, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 646,  Mean reward: 4.672413793103448, Mean Entropy: 0.30054134130477905, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 647,  Mean reward: 2.39, Mean Entropy: 0.2924647927284241, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 648,  Mean reward: 4.0181818181818185, Mean Entropy: 0.3502790629863739, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 649,  Mean reward: 5.2155172413793105, Mean Entropy: 0.2914905250072479, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 650,  Mean reward: 4.163461538461538, Mean Entropy: 0.45575952529907227, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 651,  Mean reward: 2.6122448979591835, Mean Entropy: 0.3798736333847046, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 652,  Mean reward: 5.06140350877193, Mean Entropy: 0.3281539976596832, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 653,  Mean reward: 2.8854166666666665, Mean Entropy: 0.49780499935150146, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 654,  Mean reward: 4.037037037037037, Mean Entropy: 0.4486318528652191, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 655,  Mean reward: 2.5833333333333335, Mean Entropy: 0.41593942046165466, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 656,  Mean reward: 3.303921568627451, Mean Entropy: 0.3456796109676361, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 657,  Mean reward: 4.409090909090909, Mean Entropy: 0.284331738948822, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 658,  Mean reward: -3.6142857142857143, Mean Entropy: 0.5569958686828613, complete_episode_count: 35.0, Gather time: 0.48s, Train time: 1.27s
Iteration: 659,  Mean reward: 0.5641025641025641, Mean Entropy: 0.6229699850082397, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 660,  Mean reward: 2.611111111111111, Mean Entropy: 0.4190995693206787, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 661,  Mean reward: 1.88, Mean Entropy: 0.4687017798423767, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 662,  Mean reward: 3.0462962962962963, Mean Entropy: 0.5443482398986816, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 663,  Mean reward: 3.625, Mean Entropy: 0.36442050337791443, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 664,  Mean reward: 4.315789473684211, Mean Entropy: 0.2366483211517334, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 665,  Mean reward: 2.9285714285714284, Mean Entropy: 0.2562495470046997, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 666,  Mean reward: 3.8076923076923075, Mean Entropy: 0.29539012908935547, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 667,  Mean reward: 4.9375, Mean Entropy: 0.26869237422943115, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 668,  Mean reward: 4.848214285714286, Mean Entropy: 0.21003440022468567, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 669,  Mean reward: 3.0510204081632653, Mean Entropy: 0.34042561054229736, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 670,  Mean reward: 4.732142857142857, Mean Entropy: 0.199194997549057, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 671,  Mean reward: 4.818965517241379, Mean Entropy: 0.2447323054075241, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 672,  Mean reward: 4.333333333333333, Mean Entropy: 0.18815451860427856, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 673,  Mean reward: 3.950980392156863, Mean Entropy: 0.24213552474975586, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 674,  Mean reward: 4.827272727272727, Mean Entropy: 0.20077666640281677, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 675,  Mean reward: 4.160377358490566, Mean Entropy: 0.253741979598999, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 676,  Mean reward: 2.5652173913043477, Mean Entropy: 0.290263295173645, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 677,  Mean reward: 1.3837209302325582, Mean Entropy: 0.2725064754486084, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 678,  Mean reward: 3.78, Mean Entropy: 0.2221411168575287, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 679,  Mean reward: -4.287878787878788, Mean Entropy: 0.21098068356513977, complete_episode_count: 33.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 680,  Mean reward: -2.236111111111111, Mean Entropy: 0.5524988174438477, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.30s
Iteration: 681,  Mean reward: 0.35526315789473684, Mean Entropy: 0.5710587501525879, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 682,  Mean reward: 1.381578947368421, Mean Entropy: 0.5177287459373474, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 683,  Mean reward: -0.6351351351351351, Mean Entropy: 0.4969123303890228, complete_episode_count: 37.0, Gather time: 0.48s, Train time: 1.27s
Iteration: 684,  Mean reward: -0.28378378378378377, Mean Entropy: 0.6857261657714844, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 685,  Mean reward: 2.3780487804878048, Mean Entropy: 0.3234576880931854, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 686,  Mean reward: 2.9, Mean Entropy: 0.36293625831604004, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 687,  Mean reward: 2.3333333333333335, Mean Entropy: 0.3967827260494232, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 688,  Mean reward: 1.5, Mean Entropy: 0.34860920906066895, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 689,  Mean reward: 0.648936170212766, Mean Entropy: 0.43491697311401367, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 690,  Mean reward: 1.7916666666666667, Mean Entropy: 0.35229894518852234, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 691,  Mean reward: 1.4666666666666666, Mean Entropy: 0.4474117159843445, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 692,  Mean reward: 2.152173913043478, Mean Entropy: 0.46049779653549194, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 693,  Mean reward: 1.891304347826087, Mean Entropy: 0.4229164123535156, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 694,  Mean reward: 2.46, Mean Entropy: 0.3529512584209442, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 695,  Mean reward: 3.142857142857143, Mean Entropy: 0.4405274987220764, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 696,  Mean reward: 3.75, Mean Entropy: 0.40571150183677673, complete_episode_count: 50.0, Gather time: 0.70s, Train time: 1.27s
Iteration: 697,  Mean reward: 2.938775510204082, Mean Entropy: 0.4293336570262909, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 698,  Mean reward: 2.21875, Mean Entropy: 0.5662952661514282, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 699,  Mean reward: 2.2363636363636363, Mean Entropy: 0.3905259966850281, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 700,  Mean reward: 0.10204081632653061, Mean Entropy: 0.22115549445152283, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: 1.06, Mean Entropy: 0.21827790141105652, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 702,  Mean reward: 3.0849056603773586, Mean Entropy: 0.216642826795578, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 703,  Mean reward: 1.5625, Mean Entropy: 0.19826316833496094, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 704,  Mean reward: 1.59375, Mean Entropy: 0.3015850782394409, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 705,  Mean reward: 1.3804347826086956, Mean Entropy: 0.34023475646972656, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 706,  Mean reward: -0.11627906976744186, Mean Entropy: 0.4704795777797699, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 707,  Mean reward: 3.0, Mean Entropy: 0.42595574259757996, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 708,  Mean reward: 2.8181818181818183, Mean Entropy: 0.3837800920009613, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 709,  Mean reward: 2.5760869565217392, Mean Entropy: 0.3320443332195282, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 710,  Mean reward: 3.0306122448979593, Mean Entropy: 0.42780956625938416, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 711,  Mean reward: 0.8488372093023255, Mean Entropy: 0.41695529222488403, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 712,  Mean reward: 3.78, Mean Entropy: 0.4366602897644043, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.31s
Iteration: 713,  Mean reward: 4.769230769230769, Mean Entropy: 0.41159582138061523, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 714,  Mean reward: 3.0106382978723403, Mean Entropy: 0.44597750902175903, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 715,  Mean reward: 2.5106382978723403, Mean Entropy: 0.48788413405418396, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 716,  Mean reward: 1.9791666666666667, Mean Entropy: 0.5317373275756836, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 717,  Mean reward: 3.769230769230769, Mean Entropy: 0.20940560102462769, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 718,  Mean reward: -0.2826086956521739, Mean Entropy: 0.38650545477867126, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 719,  Mean reward: 2.3846153846153846, Mean Entropy: 0.31984996795654297, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 720,  Mean reward: 3.314814814814815, Mean Entropy: 0.2659100890159607, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 721,  Mean reward: 2.16, Mean Entropy: 0.24989303946495056, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 722,  Mean reward: -0.4090909090909091, Mean Entropy: 0.325033962726593, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 723,  Mean reward: 0.5222222222222223, Mean Entropy: 0.43374404311180115, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 724,  Mean reward: 3.0208333333333335, Mean Entropy: 0.492759108543396, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 725,  Mean reward: 1.6702127659574468, Mean Entropy: 0.41157257556915283, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 726,  Mean reward: 1.24, Mean Entropy: 0.48920154571533203, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 727,  Mean reward: 4.754237288135593, Mean Entropy: 0.4107813835144043, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 728,  Mean reward: 3.1666666666666665, Mean Entropy: 0.3682315945625305, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 729,  Mean reward: 3.06, Mean Entropy: 0.30839836597442627, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 730,  Mean reward: 2.704081632653061, Mean Entropy: 0.36255118250846863, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 731,  Mean reward: 3.0686274509803924, Mean Entropy: 0.3157994747161865, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 732,  Mean reward: 2.62, Mean Entropy: 0.3246932029724121, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 733,  Mean reward: 3.5462962962962963, Mean Entropy: 0.3456311821937561, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 734,  Mean reward: 3.981818181818182, Mean Entropy: 0.22847947478294373, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 735,  Mean reward: 4.375, Mean Entropy: 0.2088584154844284, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 736,  Mean reward: 5.291666666666667, Mean Entropy: 0.2294900119304657, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.65s
Iteration: 737,  Mean reward: 5.588709677419355, Mean Entropy: 0.24797457456588745, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 738,  Mean reward: 3.0636363636363635, Mean Entropy: 0.4346160888671875, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 739,  Mean reward: 5.0508474576271185, Mean Entropy: 0.35563167929649353, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 740,  Mean reward: 3.824074074074074, Mean Entropy: 0.399949848651886, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 741,  Mean reward: 3.712962962962963, Mean Entropy: 0.36256393790245056, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 742,  Mean reward: 6.145161290322581, Mean Entropy: 0.26060181856155396, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 743,  Mean reward: 4.028846153846154, Mean Entropy: 0.36985093355178833, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 744,  Mean reward: 4.444444444444445, Mean Entropy: 0.3166390657424927, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 745,  Mean reward: 4.964912280701754, Mean Entropy: 0.2805924415588379, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 746,  Mean reward: 4.594827586206897, Mean Entropy: 0.29567286372184753, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 747,  Mean reward: 4.089285714285714, Mean Entropy: 0.27073466777801514, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.27s
Iteration: 748,  Mean reward: 4.419642857142857, Mean Entropy: 0.17610780894756317, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 749,  Mean reward: 4.705357142857143, Mean Entropy: 0.2332867830991745, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 750,  Mean reward: 3.265957446808511, Mean Entropy: 0.35428571701049805, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 751,  Mean reward: 3.15625, Mean Entropy: 0.2619902491569519, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 752,  Mean reward: 3.9150943396226414, Mean Entropy: 0.3001061677932739, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 753,  Mean reward: 4.601851851851852, Mean Entropy: 0.306465744972229, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 754,  Mean reward: 4.657407407407407, Mean Entropy: 0.27910852432250977, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 755,  Mean reward: 2.6320754716981134, Mean Entropy: 0.2656627893447876, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 756,  Mean reward: 3.9423076923076925, Mean Entropy: 0.2161473035812378, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 757,  Mean reward: 4.413461538461538, Mean Entropy: 0.2148202508687973, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 758,  Mean reward: 4.9818181818181815, Mean Entropy: 0.1996600478887558, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 759,  Mean reward: 3.4285714285714284, Mean Entropy: 0.23958294093608856, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 760,  Mean reward: 5.944444444444445, Mean Entropy: 0.2276017665863037, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 761,  Mean reward: 5.598214285714286, Mean Entropy: 0.1910219043493271, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 762,  Mean reward: 3.78, Mean Entropy: 0.22722862660884857, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 763,  Mean reward: 1.9019607843137254, Mean Entropy: 0.4809900224208832, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 764,  Mean reward: 4.037037037037037, Mean Entropy: 0.24569058418273926, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.27s
Iteration: 765,  Mean reward: 3.9134615384615383, Mean Entropy: 0.2573169469833374, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 766,  Mean reward: 4.431372549019608, Mean Entropy: 0.25338178873062134, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 767,  Mean reward: 2.3469387755102042, Mean Entropy: 0.25251615047454834, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 768,  Mean reward: 2.4361702127659575, Mean Entropy: 0.2661592662334442, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 769,  Mean reward: 4.086538461538462, Mean Entropy: 0.19444166123867035, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 770,  Mean reward: 5.612068965517241, Mean Entropy: 0.24868497252464294, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 771,  Mean reward: 3.696078431372549, Mean Entropy: 0.22956833243370056, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 772,  Mean reward: 3.8365384615384617, Mean Entropy: 0.18455755710601807, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 773,  Mean reward: 4.5636363636363635, Mean Entropy: 0.1758822500705719, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 774,  Mean reward: 5.008928571428571, Mean Entropy: 0.1713152378797531, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 775,  Mean reward: 3.696078431372549, Mean Entropy: 0.30668866634368896, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 776,  Mean reward: 5.407407407407407, Mean Entropy: 0.2982582449913025, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 777,  Mean reward: 5.241071428571429, Mean Entropy: 0.23044830560684204, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 778,  Mean reward: 5.027777777777778, Mean Entropy: 0.38109850883483887, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 779,  Mean reward: 4.294642857142857, Mean Entropy: 0.15905779600143433, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 780,  Mean reward: 3.411764705882353, Mean Entropy: 0.3996267318725586, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 781,  Mean reward: 4.991228070175438, Mean Entropy: 0.28063952922821045, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.27s
Iteration: 782,  Mean reward: 4.444444444444445, Mean Entropy: 0.26099205017089844, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 783,  Mean reward: 3.4591836734693877, Mean Entropy: 0.42411863803863525, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 784,  Mean reward: 4.818965517241379, Mean Entropy: 0.28416991233825684, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 785,  Mean reward: 3.5660377358490565, Mean Entropy: 0.2826855182647705, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 786,  Mean reward: 4.009803921568627, Mean Entropy: 0.36133861541748047, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 787,  Mean reward: 3.193877551020408, Mean Entropy: 0.2393464893102646, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 788,  Mean reward: 5.089285714285714, Mean Entropy: 0.17835216224193573, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 789,  Mean reward: 4.927272727272728, Mean Entropy: 0.1601480394601822, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 790,  Mean reward: 4.651785714285714, Mean Entropy: 0.12201397120952606, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 791,  Mean reward: 4.885964912280702, Mean Entropy: 0.1687232106924057, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 792,  Mean reward: 3.1862745098039214, Mean Entropy: 0.24369190633296967, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 793,  Mean reward: 4.205882352941177, Mean Entropy: 0.41226714849472046, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 794,  Mean reward: 3.8962264150943398, Mean Entropy: 0.30803292989730835, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 795,  Mean reward: 4.462264150943396, Mean Entropy: 0.2642534375190735, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 796,  Mean reward: 5.364406779661017, Mean Entropy: 0.2925141155719757, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 797,  Mean reward: 4.101851851851852, Mean Entropy: 0.3037923574447632, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 798,  Mean reward: 4.830357142857143, Mean Entropy: 0.2067369669675827, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 799,  Mean reward: 3.6372549019607843, Mean Entropy: 0.18065252900123596, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 800,  Mean reward: 5.160714285714286, Mean Entropy: 0.16628895699977875, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: 4.0754716981132075, Mean Entropy: 0.30108964443206787, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 802,  Mean reward: 4.868421052631579, Mean Entropy: 0.2562762200832367, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 803,  Mean reward: 3.4705882352941178, Mean Entropy: 0.18272022902965546, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 804,  Mean reward: 4.0588235294117645, Mean Entropy: 0.189307302236557, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 805,  Mean reward: 3.2395833333333335, Mean Entropy: 0.22169485688209534, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 806,  Mean reward: 3.4285714285714284, Mean Entropy: 0.2471023052930832, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 807,  Mean reward: 4.40566037735849, Mean Entropy: 0.22780382633209229, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 808,  Mean reward: 4.803571428571429, Mean Entropy: 0.12746933102607727, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 809,  Mean reward: 3.6666666666666665, Mean Entropy: 0.21486786007881165, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 810,  Mean reward: 4.0576923076923075, Mean Entropy: 0.2138957679271698, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 811,  Mean reward: 4.568627450980392, Mean Entropy: 0.1912207454442978, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 812,  Mean reward: 3.588888888888889, Mean Entropy: 0.05940961837768555, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 813,  Mean reward: -3.5294117647058822, Mean Entropy: 0.250377893447876, complete_episode_count: 34.0, Gather time: 0.48s, Train time: 1.27s
Iteration: 814,  Mean reward: -1.162162162162162, Mean Entropy: 0.5469130277633667, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 815,  Mean reward: 1.8604651162790697, Mean Entropy: 0.23913328349590302, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 816,  Mean reward: 2.784313725490196, Mean Entropy: 0.32251521944999695, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 817,  Mean reward: 3.0925925925925926, Mean Entropy: 0.33548831939697266, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 818,  Mean reward: 3.4339622641509435, Mean Entropy: 0.31624066829681396, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 819,  Mean reward: 4.330188679245283, Mean Entropy: 0.2909538149833679, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 820,  Mean reward: 3.12, Mean Entropy: 0.3429512679576874, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 821,  Mean reward: 4.7075471698113205, Mean Entropy: 0.30724793672561646, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 822,  Mean reward: 4.176470588235294, Mean Entropy: 0.29443609714508057, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 823,  Mean reward: 5.8559322033898304, Mean Entropy: 0.2167970836162567, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 824,  Mean reward: 3.6666666666666665, Mean Entropy: 0.3496994376182556, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 825,  Mean reward: 4.712962962962963, Mean Entropy: 0.3488090932369232, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 826,  Mean reward: 3.6132075471698113, Mean Entropy: 0.3617234230041504, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 827,  Mean reward: 3.950980392156863, Mean Entropy: 0.28320902585983276, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 828,  Mean reward: 3.950980392156863, Mean Entropy: 0.25088632106781006, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 829,  Mean reward: 4.446428571428571, Mean Entropy: 0.23461946845054626, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 830,  Mean reward: 4.546296296296297, Mean Entropy: 0.21003493666648865, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 831,  Mean reward: 3.8653846153846154, Mean Entropy: 0.2418530285358429, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 832,  Mean reward: 5.719298245614035, Mean Entropy: 0.20654955506324768, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 833,  Mean reward: 4.598039215686274, Mean Entropy: 0.26082128286361694, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 834,  Mean reward: 4.336363636363636, Mean Entropy: 0.19178859889507294, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 835,  Mean reward: 3.0531914893617023, Mean Entropy: 0.2578293979167938, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 836,  Mean reward: 4.827272727272727, Mean Entropy: 0.23414820432662964, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 837,  Mean reward: 3.7745098039215685, Mean Entropy: 0.22617173194885254, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 838,  Mean reward: 4.634615384615385, Mean Entropy: 0.20593631267547607, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 839,  Mean reward: 4.627450980392157, Mean Entropy: 0.20690080523490906, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 840,  Mean reward: 3.52, Mean Entropy: 0.22753721475601196, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 841,  Mean reward: 3.1382978723404253, Mean Entropy: 0.24914103746414185, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 842,  Mean reward: 4.320754716981132, Mean Entropy: 0.17652186751365662, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 843,  Mean reward: 4.25, Mean Entropy: 0.21101567149162292, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 844,  Mean reward: 3.696078431372549, Mean Entropy: 0.19936363399028778, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 845,  Mean reward: 3.8867924528301887, Mean Entropy: 0.1705738604068756, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 846,  Mean reward: 4.898148148148148, Mean Entropy: 0.2039436548948288, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 847,  Mean reward: 5.089285714285714, Mean Entropy: 0.16745439171791077, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 848,  Mean reward: 4.758928571428571, Mean Entropy: 0.1540687531232834, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 849,  Mean reward: 4.147058823529412, Mean Entropy: 0.17753861844539642, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 850,  Mean reward: 4.264150943396227, Mean Entropy: 0.1866615116596222, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 851,  Mean reward: 5.028301886792453, Mean Entropy: 0.17415055632591248, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 852,  Mean reward: 4.598214285714286, Mean Entropy: 0.13829077780246735, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 853,  Mean reward: 4.336538461538462, Mean Entropy: 0.1901005208492279, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 854,  Mean reward: 2.4680851063829787, Mean Entropy: 0.24489331245422363, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 855,  Mean reward: 5.62280701754386, Mean Entropy: 0.17507630586624146, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 856,  Mean reward: 3.6372549019607843, Mean Entropy: 0.19011971354484558, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 857,  Mean reward: 4.349056603773585, Mean Entropy: 0.17497341334819794, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 858,  Mean reward: 3.7788461538461537, Mean Entropy: 0.19938421249389648, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 859,  Mean reward: 4.925925925925926, Mean Entropy: 0.18771140277385712, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 860,  Mean reward: 3.49, Mean Entropy: 0.22466176748275757, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 861,  Mean reward: 3.8365384615384617, Mean Entropy: 0.20103910565376282, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 862,  Mean reward: 3.81, Mean Entropy: 0.239242285490036, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 863,  Mean reward: 3.15, Mean Entropy: 0.21752893924713135, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 864,  Mean reward: 6.0344827586206895, Mean Entropy: 0.25619950890541077, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 865,  Mean reward: 4.783018867924528, Mean Entropy: 0.24233826994895935, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.27s
Iteration: 866,  Mean reward: 5.491228070175438, Mean Entropy: 0.1894739717245102, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 867,  Mean reward: 4.509090909090909, Mean Entropy: 0.18404293060302734, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 868,  Mean reward: 4.221153846153846, Mean Entropy: 0.20672234892845154, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 869,  Mean reward: 3.75, Mean Entropy: 0.26359862089157104, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 870,  Mean reward: 3.125, Mean Entropy: 0.2613590359687805, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 871,  Mean reward: 5.491228070175438, Mean Entropy: 0.22246158123016357, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 872,  Mean reward: 4.87037037037037, Mean Entropy: 0.1703563630580902, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 873,  Mean reward: 3.784313725490196, Mean Entropy: 0.20228636264801025, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 874,  Mean reward: 2.69, Mean Entropy: 0.23091062903404236, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 875,  Mean reward: 4.3431372549019605, Mean Entropy: 0.22424790263175964, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 876,  Mean reward: 2.8979591836734695, Mean Entropy: 0.25723838806152344, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 877,  Mean reward: 4.076923076923077, Mean Entropy: 0.23343150317668915, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 878,  Mean reward: 4.028846153846154, Mean Entropy: 0.22556935250759125, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 879,  Mean reward: 4.685185185185185, Mean Entropy: 0.21317815780639648, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 880,  Mean reward: 4.10377358490566, Mean Entropy: 0.19900071620941162, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 881,  Mean reward: 4.6923076923076925, Mean Entropy: 0.2139325588941574, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 882,  Mean reward: 3.5816326530612246, Mean Entropy: 0.2206108421087265, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 883,  Mean reward: 5.372727272727273, Mean Entropy: 0.2100881189107895, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 884,  Mean reward: 4.718181818181818, Mean Entropy: 0.19183039665222168, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 885,  Mean reward: 2.7127659574468086, Mean Entropy: 0.21517762541770935, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 886,  Mean reward: 4.830357142857143, Mean Entropy: 0.17316828668117523, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 887,  Mean reward: 4.9, Mean Entropy: 0.19209882616996765, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 888,  Mean reward: 3.8867924528301887, Mean Entropy: 0.21611885726451874, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 889,  Mean reward: 5.218181818181818, Mean Entropy: 0.20080670714378357, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 890,  Mean reward: 4.278846153846154, Mean Entropy: 0.19768968224525452, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 891,  Mean reward: 5.008928571428571, Mean Entropy: 0.16070851683616638, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 892,  Mean reward: 3.7788461538461537, Mean Entropy: 0.171770840883255, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 893,  Mean reward: 4.732142857142857, Mean Entropy: 0.2560724914073944, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 894,  Mean reward: 1.5833333333333333, Mean Entropy: 0.27533158659935, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 895,  Mean reward: -0.7244897959183674, Mean Entropy: 0.3282674551010132, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 896,  Mean reward: 3.2, Mean Entropy: 0.22133730351924896, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 897,  Mean reward: 1.8173076923076923, Mean Entropy: 0.3185548782348633, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 898,  Mean reward: 4.018518518518518, Mean Entropy: 0.2884872555732727, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 899,  Mean reward: 5.017857142857143, Mean Entropy: 0.38227593898773193, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 900,  Mean reward: 5.1440677966101696, Mean Entropy: 0.3292568624019623, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.27s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: 4.638888888888889, Mean Entropy: 0.2579355835914612, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 902,  Mean reward: 4.3090909090909095, Mean Entropy: 0.26495450735092163, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 903,  Mean reward: 4.342592592592593, Mean Entropy: 0.3686615228652954, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 904,  Mean reward: 5.508333333333334, Mean Entropy: 0.2896645665168762, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 905,  Mean reward: 5.068965517241379, Mean Entropy: 0.2415822446346283, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 906,  Mean reward: 4.614035087719298, Mean Entropy: 0.24708828330039978, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 907,  Mean reward: 5.614754098360656, Mean Entropy: 0.3876376748085022, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 908,  Mean reward: 3.8214285714285716, Mean Entropy: 0.3446800112724304, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 909,  Mean reward: 4.416666666666667, Mean Entropy: 0.36494606733322144, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 910,  Mean reward: 4.9, Mean Entropy: 0.2600923180580139, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 911,  Mean reward: 4.2592592592592595, Mean Entropy: 0.2686212360858917, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 912,  Mean reward: 4.912280701754386, Mean Entropy: 0.3130756616592407, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 913,  Mean reward: 4.509090909090909, Mean Entropy: 0.35297584533691406, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 914,  Mean reward: 1.9361702127659575, Mean Entropy: 0.33553799986839294, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 915,  Mean reward: 5.087719298245614, Mean Entropy: 0.34949183464050293, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 916,  Mean reward: 4.7727272727272725, Mean Entropy: 0.36548638343811035, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 917,  Mean reward: 3.423076923076923, Mean Entropy: 0.29929405450820923, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 918,  Mean reward: 3.4074074074074074, Mean Entropy: 0.31841087341308594, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 919,  Mean reward: 3.509433962264151, Mean Entropy: 0.29747211933135986, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 920,  Mean reward: 4.536363636363636, Mean Entropy: 0.22212132811546326, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 921,  Mean reward: 5.403225806451613, Mean Entropy: 0.20336243510246277, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.65s
Iteration: 922,  Mean reward: 3.8, Mean Entropy: 0.27706611156463623, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 923,  Mean reward: 5.653846153846154, Mean Entropy: 0.2574359178543091, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 924,  Mean reward: 4.3090909090909095, Mean Entropy: 0.26145702600479126, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 925,  Mean reward: 5.725, Mean Entropy: 0.1667882651090622, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 926,  Mean reward: 4.118181818181818, Mean Entropy: 0.2905615270137787, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 927,  Mean reward: 5.313559322033898, Mean Entropy: 0.2507898211479187, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 928,  Mean reward: 3.6666666666666665, Mean Entropy: 0.35237523913383484, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 929,  Mean reward: 4.883928571428571, Mean Entropy: 0.3313267230987549, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 930,  Mean reward: 3.6666666666666665, Mean Entropy: 0.35653719305992126, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 931,  Mean reward: 4.8, Mean Entropy: 0.250133752822876, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 932,  Mean reward: 5.508474576271187, Mean Entropy: 0.2339402437210083, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.28s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 933,  Mean reward: 6.306451612903226, Mean Entropy: 0.22707095742225647, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 934,  Mean reward: 3.894230769230769, Mean Entropy: 0.2505665421485901, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 935,  Mean reward: 4.40566037735849, Mean Entropy: 0.30563172698020935, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 936,  Mean reward: 3.41, Mean Entropy: 0.3620893955230713, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 937,  Mean reward: 4.5, Mean Entropy: 0.2891177237033844, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 938,  Mean reward: 3.5588235294117645, Mean Entropy: 0.341404527425766, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 939,  Mean reward: 4.745614035087719, Mean Entropy: 0.31993722915649414, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 940,  Mean reward: 4.0, Mean Entropy: 0.22267407178878784, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 941,  Mean reward: 4.320754716981132, Mean Entropy: 0.3698957860469818, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 942,  Mean reward: 2.7941176470588234, Mean Entropy: 0.38564831018447876, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 943,  Mean reward: 3.201923076923077, Mean Entropy: 0.4043625593185425, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 944,  Mean reward: 4.898148148148148, Mean Entropy: 0.1994868367910385, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 945,  Mean reward: 3.58, Mean Entropy: 0.2541351020336151, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 946,  Mean reward: 3.9711538461538463, Mean Entropy: 0.22414050996303558, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 947,  Mean reward: 4.783018867924528, Mean Entropy: 0.21825000643730164, complete_episode_count: 53.0, Gather time: 0.66s, Train time: 1.26s
Iteration: 948,  Mean reward: 5.509090909090909, Mean Entropy: 0.1969262808561325, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 949,  Mean reward: 4.7407407407407405, Mean Entropy: 0.17956827580928802, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 950,  Mean reward: 4.0673076923076925, Mean Entropy: 0.4039149284362793, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 951,  Mean reward: 3.772727272727273, Mean Entropy: 0.23840853571891785, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 952,  Mean reward: 4.40566037735849, Mean Entropy: 0.23600995540618896, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 953,  Mean reward: 4.519607843137255, Mean Entropy: 0.2637172341346741, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 954,  Mean reward: 4.571428571428571, Mean Entropy: 0.1895977258682251, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 955,  Mean reward: 2.7395833333333335, Mean Entropy: 0.25436997413635254, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 956,  Mean reward: 4.830357142857143, Mean Entropy: 0.22831961512565613, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 957,  Mean reward: 4.10377358490566, Mean Entropy: 0.19736787676811218, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 958,  Mean reward: 2.9285714285714284, Mean Entropy: 0.3278619647026062, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 959,  Mean reward: 5.439655172413793, Mean Entropy: 0.2320975512266159, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 960,  Mean reward: 5.140350877192983, Mean Entropy: 0.23862552642822266, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 961,  Mean reward: 2.816326530612245, Mean Entropy: 0.27115899324417114, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 962,  Mean reward: 4.685185185185185, Mean Entropy: 0.3164130449295044, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 963,  Mean reward: 5.0625, Mean Entropy: 0.2811838984489441, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 964,  Mean reward: 5.241379310344827, Mean Entropy: 0.23766696453094482, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 965,  Mean reward: 3.9622641509433962, Mean Entropy: 0.2759493291378021, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 966,  Mean reward: 3.2551020408163267, Mean Entropy: 0.2818378508090973, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 967,  Mean reward: 4.618181818181818, Mean Entropy: 0.22106239199638367, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 968,  Mean reward: 3.5588235294117645, Mean Entropy: 0.2527068555355072, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 969,  Mean reward: 4.859649122807017, Mean Entropy: 0.21525442600250244, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 970,  Mean reward: 3.32, Mean Entropy: 0.37030965089797974, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 971,  Mean reward: 4.803571428571429, Mean Entropy: 0.28389763832092285, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 972,  Mean reward: 4.685185185185185, Mean Entropy: 0.20046798884868622, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 973,  Mean reward: 3.6666666666666665, Mean Entropy: 0.19816862046718597, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 974,  Mean reward: 3.4591836734693877, Mean Entropy: 0.22544360160827637, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 975,  Mean reward: 4.59433962264151, Mean Entropy: 0.18509356677532196, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 976,  Mean reward: 4.117647058823529, Mean Entropy: 0.1733459234237671, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 977,  Mean reward: 3.21, Mean Entropy: 0.19615237414836884, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 978,  Mean reward: 4.6909090909090905, Mean Entropy: 0.22185902297496796, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 979,  Mean reward: 4.25, Mean Entropy: 0.20387771725654602, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 980,  Mean reward: 3.7254901960784315, Mean Entropy: 0.21078380942344666, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 981,  Mean reward: 4.24, Mean Entropy: 0.27223676443099976, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 982,  Mean reward: 3.6666666666666665, Mean Entropy: 0.2161659300327301, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 983,  Mean reward: 4.883928571428571, Mean Entropy: 0.27056750655174255, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 984,  Mean reward: 3.52, Mean Entropy: 0.16892974078655243, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.27s
Iteration: 985,  Mean reward: 3.35, Mean Entropy: 0.19087576866149902, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 986,  Mean reward: 4.62962962962963, Mean Entropy: 0.195634663105011, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 987,  Mean reward: 4.62962962962963, Mean Entropy: 0.15835312008857727, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 988,  Mean reward: 2.25, Mean Entropy: 0.24985599517822266, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 989,  Mean reward: 3.8627450980392157, Mean Entropy: 0.23916062712669373, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 990,  Mean reward: 4.25, Mean Entropy: 0.1922600269317627, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 991,  Mean reward: 3.41, Mean Entropy: 0.2371414601802826, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 992,  Mean reward: 3.43, Mean Entropy: 0.23385566473007202, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 993,  Mean reward: 5.771929824561403, Mean Entropy: 0.1885424703359604, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 994,  Mean reward: 2.6770833333333335, Mean Entropy: 0.18794433772563934, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 995,  Mean reward: 2.7282608695652173, Mean Entropy: 0.2685062289237976, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 996,  Mean reward: 4.898148148148148, Mean Entropy: 0.17521068453788757, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 997,  Mean reward: 4.8, Mean Entropy: 0.16980895400047302, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 998,  Mean reward: 5.6525423728813555, Mean Entropy: 0.21153149008750916, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 999,  Mean reward: 5.136363636363637, Mean Entropy: 0.29523664712905884, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1000,  Mean reward: 3.8363636363636364, Mean Entropy: 0.20779848098754883, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: 4.4423076923076925, Mean Entropy: 0.20836985111236572, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 1002,  Mean reward: 4.433962264150943, Mean Entropy: 0.23780226707458496, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1003,  Mean reward: 3.26, Mean Entropy: 0.23240280151367188, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1004,  Mean reward: 3.2244897959183674, Mean Entropy: 0.20667052268981934, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1005,  Mean reward: 5.0, Mean Entropy: 0.2651866674423218, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1006,  Mean reward: 4.6909090909090905, Mean Entropy: 0.17382347583770752, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1007,  Mean reward: 3.55, Mean Entropy: 0.23029206693172455, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1008,  Mean reward: 3.9705882352941178, Mean Entropy: 0.2424037903547287, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 1009,  Mean reward: 4.857142857142857, Mean Entropy: 0.20003299415111542, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1010,  Mean reward: 3.693877551020408, Mean Entropy: 0.2712809145450592, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1011,  Mean reward: 5.2631578947368425, Mean Entropy: 0.22337423264980316, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1012,  Mean reward: 5.083333333333333, Mean Entropy: 0.21646463871002197, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1013,  Mean reward: 3.6666666666666665, Mean Entropy: 0.24762670695781708, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1014,  Mean reward: 5.008928571428571, Mean Entropy: 0.18477386236190796, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1015,  Mean reward: 4.25, Mean Entropy: 0.21583950519561768, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1016,  Mean reward: 4.221153846153846, Mean Entropy: 0.23646998405456543, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1017,  Mean reward: 4.388888888888889, Mean Entropy: 0.23938331007957458, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1018,  Mean reward: 5.862903225806452, Mean Entropy: 0.18108323216438293, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 1019,  Mean reward: 5.446428571428571, Mean Entropy: 0.1109938845038414, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1020,  Mean reward: 5.4576271186440675, Mean Entropy: 0.16248495876789093, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1021,  Mean reward: 5.446428571428571, Mean Entropy: 0.1484115719795227, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1022,  Mean reward: 4.25, Mean Entropy: 0.1814591884613037, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1023,  Mean reward: 3.6153846153846154, Mean Entropy: 0.1993737518787384, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1024,  Mean reward: 4.5636363636363635, Mean Entropy: 0.18616758286952972, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1025,  Mean reward: 4.783018867924528, Mean Entropy: 0.1392849087715149, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1026,  Mean reward: 4.536363636363636, Mean Entropy: 0.16590440273284912, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 1027,  Mean reward: 4.25, Mean Entropy: 0.21446941792964935, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 1028,  Mean reward: 3.26, Mean Entropy: 0.21449273824691772, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 1029,  Mean reward: 3.4411764705882355, Mean Entropy: 0.18977832794189453, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1030,  Mean reward: 3.8627450980392157, Mean Entropy: 0.32682153582572937, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 1031,  Mean reward: 3.15, Mean Entropy: 0.3509446084499359, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 1032,  Mean reward: 2.663265306122449, Mean Entropy: 0.34290385246276855, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 1033,  Mean reward: 3.8333333333333335, Mean Entropy: 0.2380281388759613, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1034,  Mean reward: 4.9423076923076925, Mean Entropy: 0.21180659532546997, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 1035,  Mean reward: 4.877358490566038, Mean Entropy: 0.23593127727508545, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 1036,  Mean reward: 3.4285714285714284, Mean Entropy: 0.23940198123455048, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 1037,  Mean reward: 4.852941176470588, Mean Entropy: 0.26976194977760315, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1038,  Mean reward: 4.444444444444445, Mean Entropy: 0.21313250064849854, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1039,  Mean reward: 2.7604166666666665, Mean Entropy: 0.2644221782684326, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.44s
Iteration: 1040,  Mean reward: 5.432203389830509, Mean Entropy: 0.22159600257873535, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 1041,  Mean reward: 3.0, Mean Entropy: 0.2324545830488205, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 1042,  Mean reward: 2.9693877551020407, Mean Entropy: 0.26819878816604614, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1043,  Mean reward: 4.221153846153846, Mean Entropy: 0.2652924954891205, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 1044,  Mean reward: 5.366071428571429, Mean Entropy: 0.2519928216934204, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1045,  Mean reward: 4.776785714285714, Mean Entropy: 0.20732009410858154, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1046,  Mean reward: 4.866071428571429, Mean Entropy: 0.17111288011074066, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1047,  Mean reward: 4.037037037037037, Mean Entropy: 0.21652740240097046, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1048,  Mean reward: 3.393617021276596, Mean Entropy: 0.27212589979171753, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1049,  Mean reward: 5.771186440677966, Mean Entropy: 0.21065020561218262, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 1050,  Mean reward: 5.160714285714286, Mean Entropy: 0.2032807469367981, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1051,  Mean reward: 2.4, Mean Entropy: 0.23834502696990967, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1052,  Mean reward: 4.330188679245283, Mean Entropy: 0.2584354281425476, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1053,  Mean reward: 4.7407407407407405, Mean Entropy: 0.23982572555541992, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1054,  Mean reward: 4.088235294117647, Mean Entropy: 0.24508433043956757, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1055,  Mean reward: 3.0, Mean Entropy: 0.26336467266082764, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1056,  Mean reward: 4.07, Mean Entropy: 0.2476847767829895, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1057,  Mean reward: 3.75, Mean Entropy: 0.23925404250621796, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1058,  Mean reward: 3.4215686274509802, Mean Entropy: 0.19680193066596985, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1059,  Mean reward: 3.8076923076923075, Mean Entropy: 0.2938058376312256, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1060,  Mean reward: 2.9411764705882355, Mean Entropy: 0.2581116855144501, complete_episode_count: 51.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1061,  Mean reward: 4.509090909090909, Mean Entropy: 0.22563499212265015, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1062,  Mean reward: 5.116071428571429, Mean Entropy: 0.21389441192150116, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1063,  Mean reward: 5.214285714285714, Mean Entropy: 0.2353392243385315, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1064,  Mean reward: 2.3229166666666665, Mean Entropy: 0.23409804701805115, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1065,  Mean reward: 4.372727272727273, Mean Entropy: 0.20944039523601532, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1066,  Mean reward: 4.59433962264151, Mean Entropy: 0.2292851060628891, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1067,  Mean reward: 4.566037735849057, Mean Entropy: 0.1886870115995407, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1068,  Mean reward: 3.13265306122449, Mean Entropy: 0.23695163428783417, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 1069,  Mean reward: 3.163265306122449, Mean Entropy: 0.2550172805786133, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1070,  Mean reward: 3.64, Mean Entropy: 0.23275598883628845, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1071,  Mean reward: 2.9411764705882355, Mean Entropy: 0.2895166873931885, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 1072,  Mean reward: 4.0673076923076925, Mean Entropy: 0.22450165450572968, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1073,  Mean reward: 4.87962962962963, Mean Entropy: 0.25631746649742126, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1074,  Mean reward: 3.9339622641509435, Mean Entropy: 0.2215469926595688, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1075,  Mean reward: 4.1923076923076925, Mean Entropy: 0.2513842284679413, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1076,  Mean reward: 2.8229166666666665, Mean Entropy: 0.2828121483325958, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1077,  Mean reward: 2.7346938775510203, Mean Entropy: 0.2652907371520996, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1078,  Mean reward: 4.092592592592593, Mean Entropy: 0.19635921716690063, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1079,  Mean reward: 2.7448979591836733, Mean Entropy: 0.266664981842041, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1080,  Mean reward: 4.009803921568627, Mean Entropy: 0.19869212806224823, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1081,  Mean reward: 3.46, Mean Entropy: 0.24413037300109863, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1082,  Mean reward: 2.6770833333333335, Mean Entropy: 0.4051735997200012, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1083,  Mean reward: 4.836363636363636, Mean Entropy: 0.4023527503013611, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1084,  Mean reward: 3.5185185185185186, Mean Entropy: 0.28277862071990967, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1085,  Mean reward: 2.630434782608696, Mean Entropy: 0.34457454085350037, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 1086,  Mean reward: 4.37719298245614, Mean Entropy: 0.2771188020706177, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1087,  Mean reward: 6.213114754098361, Mean Entropy: 0.4048255383968353, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 1088,  Mean reward: 4.392857142857143, Mean Entropy: 0.31441688537597656, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1089,  Mean reward: 3.892156862745098, Mean Entropy: 0.3585183024406433, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1090,  Mean reward: 4.377358490566038, Mean Entropy: 0.25824666023254395, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1091,  Mean reward: 5.368421052631579, Mean Entropy: 0.1915137767791748, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1092,  Mean reward: 3.894230769230769, Mean Entropy: 0.17952939867973328, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1093,  Mean reward: 4.163461538461538, Mean Entropy: 0.2509104609489441, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1094,  Mean reward: 3.52, Mean Entropy: 0.19257281720638275, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1095,  Mean reward: 3.58, Mean Entropy: 0.21601276099681854, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1096,  Mean reward: 3.69, Mean Entropy: 0.22059836983680725, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1097,  Mean reward: 4.416666666666667, Mean Entropy: 0.1857733428478241, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1098,  Mean reward: 2.7291666666666665, Mean Entropy: 0.22545567154884338, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1099,  Mean reward: 4.62962962962963, Mean Entropy: 0.20525333285331726, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1100,  Mean reward: 3.26, Mean Entropy: 0.20528995990753174, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: 3.49, Mean Entropy: 0.231184720993042, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1102,  Mean reward: 4.9818181818181815, Mean Entropy: 0.16929328441619873, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 1103,  Mean reward: 2.34375, Mean Entropy: 0.21992942690849304, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1104,  Mean reward: 4.0, Mean Entropy: 0.18264469504356384, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1105,  Mean reward: 2.648936170212766, Mean Entropy: 0.1821933090686798, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1106,  Mean reward: 4.7727272727272725, Mean Entropy: 0.23142161965370178, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1107,  Mean reward: 3.75, Mean Entropy: 0.27828317880630493, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1108,  Mean reward: 4.830357142857143, Mean Entropy: 0.21609358489513397, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1109,  Mean reward: 3.2452830188679247, Mean Entropy: 0.17270371317863464, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1110,  Mean reward: 4.0, Mean Entropy: 0.3308521509170532, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1111,  Mean reward: 3.8333333333333335, Mean Entropy: 0.40008029341697693, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1112,  Mean reward: 3.9363636363636365, Mean Entropy: 0.16059449315071106, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 1113,  Mean reward: 4.349056603773585, Mean Entropy: 0.1334083378314972, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1114,  Mean reward: 3.7211538461538463, Mean Entropy: 0.13972637057304382, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1115,  Mean reward: 2.4680851063829787, Mean Entropy: 0.2106640785932541, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1116,  Mean reward: 5.336206896551724, Mean Entropy: 0.19298547506332397, complete_episode_count: 58.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1117,  Mean reward: 4.401960784313726, Mean Entropy: 0.14085562527179718, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1118,  Mean reward: 2.4361702127659575, Mean Entropy: 0.18142887949943542, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1119,  Mean reward: 3.0714285714285716, Mean Entropy: 0.21193477511405945, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1120,  Mean reward: 3.9285714285714284, Mean Entropy: 0.18400093913078308, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1121,  Mean reward: 4.254901960784314, Mean Entropy: 0.2251681089401245, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1122,  Mean reward: 4.685185185185185, Mean Entropy: 0.20674869418144226, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1123,  Mean reward: 4.814814814814815, Mean Entropy: 0.19665896892547607, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1124,  Mean reward: 3.489795918367347, Mean Entropy: 0.2161274552345276, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1125,  Mean reward: 4.5, Mean Entropy: 0.2380019873380661, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1126,  Mean reward: 4.147058823529412, Mean Entropy: 0.24596266448497772, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1127,  Mean reward: 4.132075471698113, Mean Entropy: 0.21742668747901917, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1128,  Mean reward: 3.49, Mean Entropy: 0.1906491369009018, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1129,  Mean reward: 4.2924528301886795, Mean Entropy: 0.24001765251159668, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1130,  Mean reward: 4.320754716981132, Mean Entropy: 0.23320427536964417, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1131,  Mean reward: 4.566037735849057, Mean Entropy: 0.20890973508358002, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1132,  Mean reward: 4.203703703703703, Mean Entropy: 0.18801549077033997, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1133,  Mean reward: 3.696078431372549, Mean Entropy: 0.2083326280117035, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1134,  Mean reward: 4.5, Mean Entropy: 0.2400924414396286, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 1135,  Mean reward: 5.160714285714286, Mean Entropy: 0.19084829092025757, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1136,  Mean reward: 6.2, Mean Entropy: 0.19352130591869354, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 1137,  Mean reward: 3.98, Mean Entropy: 0.26289790868759155, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1138,  Mean reward: 2.7551020408163267, Mean Entropy: 0.18489418923854828, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1139,  Mean reward: 4.04, Mean Entropy: 0.23844072222709656, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1140,  Mean reward: 5.0625, Mean Entropy: 0.19181177020072937, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1141,  Mean reward: 4.972222222222222, Mean Entropy: 0.1831214725971222, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1142,  Mean reward: 5.2407407407407405, Mean Entropy: 0.2061791867017746, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.27s
Iteration: 1143,  Mean reward: 2.38, Mean Entropy: 0.24516835808753967, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1144,  Mean reward: 5.241379310344827, Mean Entropy: 0.19419795274734497, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1145,  Mean reward: 1.7613636363636365, Mean Entropy: 0.3710620403289795, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 1146,  Mean reward: 4.9818181818181815, Mean Entropy: 0.274739146232605, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1147,  Mean reward: 4.678571428571429, Mean Entropy: 0.26328492164611816, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1148,  Mean reward: 3.943396226415094, Mean Entropy: 0.23970627784729004, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1149,  Mean reward: 3.15625, Mean Entropy: 0.3106802701950073, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1150,  Mean reward: 3.8137254901960786, Mean Entropy: 0.22827911376953125, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1151,  Mean reward: 3.8627450980392157, Mean Entropy: 0.3057197332382202, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1152,  Mean reward: 3.7254901960784315, Mean Entropy: 0.4090730845928192, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1153,  Mean reward: 3.3207547169811322, Mean Entropy: 0.2591111660003662, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1154,  Mean reward: 2.4591836734693877, Mean Entropy: 0.36085933446884155, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1155,  Mean reward: 5.543859649122807, Mean Entropy: 0.22295522689819336, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1156,  Mean reward: 3.7788461538461537, Mean Entropy: 0.27164608240127563, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1157,  Mean reward: 4.086538461538462, Mean Entropy: 0.28793221712112427, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1158,  Mean reward: 6.229508196721311, Mean Entropy: 0.17376725375652313, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 1159,  Mean reward: 3.0816326530612246, Mean Entropy: 0.26686736941337585, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1160,  Mean reward: 4.0, Mean Entropy: 0.2766605317592621, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1161,  Mean reward: 4.33, Mean Entropy: 0.23798593878746033, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1162,  Mean reward: 3.9711538461538463, Mean Entropy: 0.20876044034957886, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1163,  Mean reward: 3.9711538461538463, Mean Entropy: 0.23550543189048767, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1164,  Mean reward: 4.264150943396227, Mean Entropy: 0.21991586685180664, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1165,  Mean reward: 4.088235294117647, Mean Entropy: 0.24455535411834717, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1166,  Mean reward: 5.089285714285714, Mean Entropy: 0.3041267395019531, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 1167,  Mean reward: 2.595744680851064, Mean Entropy: 0.26964300870895386, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1168,  Mean reward: 4.883928571428571, Mean Entropy: 0.2871741056442261, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1169,  Mean reward: 3.9716981132075473, Mean Entropy: 0.27300652861595154, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1170,  Mean reward: 3.75, Mean Entropy: 0.2838211953639984, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 1171,  Mean reward: 3.673076923076923, Mean Entropy: 0.31463512778282166, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1172,  Mean reward: 4.160377358490566, Mean Entropy: 0.29487836360931396, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1173,  Mean reward: 3.1923076923076925, Mean Entropy: 0.36514338850975037, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1174,  Mean reward: 4.188679245283019, Mean Entropy: 0.261918306350708, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1175,  Mean reward: 3.9215686274509802, Mean Entropy: 0.31713616847991943, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1176,  Mean reward: 4.827272727272727, Mean Entropy: 0.30610641837120056, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.27s
Iteration: 1177,  Mean reward: 2.9479166666666665, Mean Entropy: 0.3296334147453308, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1178,  Mean reward: 3.9150943396226414, Mean Entropy: 0.3712503910064697, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1179,  Mean reward: 4.160377358490566, Mean Entropy: 0.290555864572525, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1180,  Mean reward: 3.888888888888889, Mean Entropy: 0.2623863220214844, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1181,  Mean reward: 4.5, Mean Entropy: 0.2939993739128113, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1182,  Mean reward: 4.964912280701754, Mean Entropy: 0.2736486792564392, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1183,  Mean reward: 4.481132075471698, Mean Entropy: 0.24974679946899414, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1184,  Mean reward: 4.278846153846154, Mean Entropy: 0.2117382138967514, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1185,  Mean reward: 3.7211538461538463, Mean Entropy: 0.20255309343338013, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1186,  Mean reward: 4.179245283018868, Mean Entropy: 0.22346386313438416, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1187,  Mean reward: 3.701923076923077, Mean Entropy: 0.1589163839817047, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.30s
Iteration: 1188,  Mean reward: 4.657407407407407, Mean Entropy: 0.24046532809734344, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1189,  Mean reward: 3.81, Mean Entropy: 0.2757011353969574, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1190,  Mean reward: 3.9166666666666665, Mean Entropy: 0.2602882385253906, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1191,  Mean reward: 4.2894736842105265, Mean Entropy: 0.21770672500133514, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1192,  Mean reward: 3.020408163265306, Mean Entropy: 0.204180046916008, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1193,  Mean reward: 2.9574468085106385, Mean Entropy: 0.2712719440460205, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1194,  Mean reward: 3.8867924528301887, Mean Entropy: 0.19950871169567108, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1195,  Mean reward: 3.2551020408163267, Mean Entropy: 0.18666034936904907, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1196,  Mean reward: 4.336538461538462, Mean Entropy: 0.1810987889766693, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1197,  Mean reward: 5.822033898305085, Mean Entropy: 0.13620568811893463, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 1198,  Mean reward: 5.090909090909091, Mean Entropy: 0.18365661799907684, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1199,  Mean reward: 4.685185185185185, Mean Entropy: 0.18414413928985596, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1200,  Mean reward: 3.892156862745098, Mean Entropy: 0.14259248971939087, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: 4.954545454545454, Mean Entropy: 0.14818263053894043, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1202,  Mean reward: 2.6808510638297873, Mean Entropy: 0.18061769008636475, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1203,  Mean reward: 4.7727272727272725, Mean Entropy: 0.18794523179531097, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1204,  Mean reward: 4.163265306122449, Mean Entropy: 0.23971299827098846, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 1205,  Mean reward: 5.372727272727273, Mean Entropy: 0.1932862401008606, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1206,  Mean reward: 3.8333333333333335, Mean Entropy: 0.21410679817199707, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1207,  Mean reward: 4.231481481481482, Mean Entropy: 0.341620534658432, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1208,  Mean reward: 4.7, Mean Entropy: 0.2275393009185791, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1209,  Mean reward: 3.6153846153846154, Mean Entropy: 0.2517843246459961, complete_episode_count: 52.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1210,  Mean reward: 5.293103448275862, Mean Entropy: 0.3027806878089905, complete_episode_count: 58.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1211,  Mean reward: 5.11864406779661, Mean Entropy: 0.18389475345611572, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1212,  Mean reward: 4.705357142857143, Mean Entropy: 0.18179762363433838, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1213,  Mean reward: 5.0625, Mean Entropy: 0.18216699361801147, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1214,  Mean reward: 3.8653846153846154, Mean Entropy: 0.20077507197856903, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1215,  Mean reward: 4.361111111111111, Mean Entropy: 0.21907231211662292, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1216,  Mean reward: 3.9215686274509802, Mean Entropy: 0.2479552924633026, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1217,  Mean reward: 3.607843137254902, Mean Entropy: 0.22629272937774658, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1218,  Mean reward: 5.321428571428571, Mean Entropy: 0.19767002761363983, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1219,  Mean reward: 3.81, Mean Entropy: 0.21023273468017578, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1220,  Mean reward: 3.950980392156863, Mean Entropy: 0.20049190521240234, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1221,  Mean reward: 4.0, Mean Entropy: 0.1878071278333664, complete_episode_count: 52.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 1222,  Mean reward: 2.404255319148936, Mean Entropy: 0.22293543815612793, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1223,  Mean reward: 3.5096153846153846, Mean Entropy: 0.3455685079097748, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1224,  Mean reward: 0.8877551020408163, Mean Entropy: 0.35113438963890076, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1225,  Mean reward: 0.8627450980392157, Mean Entropy: 0.34977108240127563, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1226,  Mean reward: 2.0849056603773586, Mean Entropy: 0.3401089906692505, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1227,  Mean reward: 3.0272727272727273, Mean Entropy: 0.3327145278453827, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1228,  Mean reward: 4.074074074074074, Mean Entropy: 0.26687654852867126, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 1229,  Mean reward: 2.84, Mean Entropy: 0.2187023162841797, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1230,  Mean reward: 5.072727272727272, Mean Entropy: 0.3162335157394409, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1231,  Mean reward: 5.728813559322034, Mean Entropy: 0.3043965697288513, complete_episode_count: 59.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1232,  Mean reward: 4.314814814814815, Mean Entropy: 0.2871169447898865, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1233,  Mean reward: 4.479591836734694, Mean Entropy: 0.4467793405056, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1234,  Mean reward: 2.6020408163265305, Mean Entropy: 0.37244758009910583, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1235,  Mean reward: 2.5377358490566038, Mean Entropy: 0.3201039135456085, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1236,  Mean reward: 5.854838709677419, Mean Entropy: 0.21403905749320984, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 1237,  Mean reward: 4.568965517241379, Mean Entropy: 0.2504293620586395, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1238,  Mean reward: 3.5294117647058822, Mean Entropy: 0.1969958245754242, complete_episode_count: 51.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1239,  Mean reward: 4.330357142857143, Mean Entropy: 0.27692604064941406, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1240,  Mean reward: 2.85, Mean Entropy: 0.30167651176452637, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1241,  Mean reward: 4.089285714285714, Mean Entropy: 0.2750820517539978, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1242,  Mean reward: 4.5, Mean Entropy: 0.41250157356262207, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1243,  Mean reward: 3.323529411764706, Mean Entropy: 0.2617705762386322, complete_episode_count: 51.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1244,  Mean reward: 3.9705882352941178, Mean Entropy: 0.15980345010757446, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1245,  Mean reward: 3.1382978723404253, Mean Entropy: 0.22558081150054932, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1246,  Mean reward: 5.035714285714286, Mean Entropy: 0.17567496001720428, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1247,  Mean reward: 4.264150943396227, Mean Entropy: 0.36720919609069824, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1248,  Mean reward: 4.919642857142857, Mean Entropy: 0.3956352174282074, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1249,  Mean reward: 5.036363636363636, Mean Entropy: 0.2793656289577484, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1250,  Mean reward: 5.610169491525424, Mean Entropy: 0.3549482822418213, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1251,  Mean reward: 4.363636363636363, Mean Entropy: 0.4519220292568207, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1252,  Mean reward: 5.027272727272727, Mean Entropy: 0.4333484172821045, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1253,  Mean reward: 3.875, Mean Entropy: 0.38774213194847107, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1254,  Mean reward: 2.1595744680851063, Mean Entropy: 0.2698090970516205, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1255,  Mean reward: 3.8076923076923075, Mean Entropy: 0.21250377595424652, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1256,  Mean reward: 4.62037037037037, Mean Entropy: 0.36701852083206177, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1257,  Mean reward: 4.894736842105263, Mean Entropy: 0.3240744471549988, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1258,  Mean reward: 4.651785714285714, Mean Entropy: 0.38184377551078796, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1259,  Mean reward: 2.9285714285714284, Mean Entropy: 0.3596075475215912, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 1260,  Mean reward: 4.705357142857143, Mean Entropy: 0.29513949155807495, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1261,  Mean reward: 2.7127659574468086, Mean Entropy: 0.282010942697525, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1262,  Mean reward: 4.431372549019608, Mean Entropy: 0.13444772362709045, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1263,  Mean reward: 2.7916666666666665, Mean Entropy: 0.15811651945114136, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1264,  Mean reward: 4.7727272727272725, Mean Entropy: 0.1414041370153427, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1265,  Mean reward: 4.153846153846154, Mean Entropy: 0.16502246260643005, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1266,  Mean reward: 4.433962264150943, Mean Entropy: 0.1255033314228058, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1267,  Mean reward: 3.6666666666666665, Mean Entropy: 0.15173180401325226, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1268,  Mean reward: 4.509090909090909, Mean Entropy: 0.13887977600097656, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1269,  Mean reward: 4.10377358490566, Mean Entropy: 0.14675293862819672, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1270,  Mean reward: 3.2395833333333335, Mean Entropy: 0.21463316679000854, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1271,  Mean reward: 3.892156862745098, Mean Entropy: 0.17165295779705048, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1272,  Mean reward: 3.5588235294117645, Mean Entropy: 0.152552992105484, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1273,  Mean reward: 2.806122448979592, Mean Entropy: 0.15761283040046692, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1274,  Mean reward: 4.231481481481482, Mean Entropy: 0.27670609951019287, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1275,  Mean reward: 4.554545454545455, Mean Entropy: 0.15914003551006317, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1276,  Mean reward: 4.776785714285714, Mean Entropy: 0.14573459327220917, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 1277,  Mean reward: 4.471153846153846, Mean Entropy: 0.12075553834438324, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1278,  Mean reward: 3.696078431372549, Mean Entropy: 0.14158597588539124, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1279,  Mean reward: 4.0, Mean Entropy: 0.1623057723045349, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1280,  Mean reward: 2.5, Mean Entropy: 0.133054718375206, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1281,  Mean reward: 4.721153846153846, Mean Entropy: 0.17643219232559204, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1282,  Mean reward: 4.991379310344827, Mean Entropy: 0.14259901642799377, complete_episode_count: 58.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1283,  Mean reward: 4.127450980392157, Mean Entropy: 0.1436363160610199, complete_episode_count: 51.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1284,  Mean reward: 4.0576923076923075, Mean Entropy: 0.16236135363578796, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1285,  Mean reward: 4.59433962264151, Mean Entropy: 0.14685748517513275, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1286,  Mean reward: 4.264150943396227, Mean Entropy: 0.15823523700237274, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1287,  Mean reward: 4.163461538461538, Mean Entropy: 0.1635242998600006, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1288,  Mean reward: 4.509433962264151, Mean Entropy: 0.18634429574012756, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1289,  Mean reward: 4.413461538461538, Mean Entropy: 0.16511237621307373, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 1290,  Mean reward: 4.2592592592592595, Mean Entropy: 0.14526158571243286, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1291,  Mean reward: 3.8076923076923075, Mean Entropy: 0.28169387578964233, complete_episode_count: 52.0, Gather time: 0.67s, Train time: 1.26s
Iteration: 1292,  Mean reward: 2.0, Mean Entropy: 0.32024896144866943, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1293,  Mean reward: 4.10377358490566, Mean Entropy: 0.1772550642490387, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1294,  Mean reward: 3.7211538461538463, Mean Entropy: 0.18880197405815125, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1295,  Mean reward: 4.413461538461538, Mean Entropy: 0.19646112620830536, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1296,  Mean reward: 4.62962962962963, Mean Entropy: 0.18048594892024994, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1297,  Mean reward: 3.9711538461538463, Mean Entropy: 0.1252756118774414, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1298,  Mean reward: 4.566037735849057, Mean Entropy: 0.18835985660552979, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1299,  Mean reward: 3.6666666666666665, Mean Entropy: 0.19127309322357178, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1300,  Mean reward: 4.388888888888889, Mean Entropy: 0.20620936155319214, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
rec seq len 2
actor lr 0.0005
Iteration: 1301,  Mean reward: 3.8627450980392157, Mean Entropy: 0.18363039195537567, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1302,  Mean reward: 4.444444444444445, Mean Entropy: 0.1881866455078125, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1303,  Mean reward: 4.62962962962963, Mean Entropy: 0.20430994033813477, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1304,  Mean reward: 4.176470588235294, Mean Entropy: 0.221871018409729, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1305,  Mean reward: 3.8076923076923075, Mean Entropy: 0.20586225390434265, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1306,  Mean reward: 2.9591836734693877, Mean Entropy: 0.23689362406730652, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1307,  Mean reward: 1.7826086956521738, Mean Entropy: 0.2540649175643921, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1308,  Mean reward: 3.29, Mean Entropy: 0.2479589879512787, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1309,  Mean reward: 1.9673913043478262, Mean Entropy: 0.24192377924919128, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1310,  Mean reward: 4.444444444444445, Mean Entropy: 0.26555442810058594, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1311,  Mean reward: 2.479591836734694, Mean Entropy: 0.24967873096466064, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1312,  Mean reward: 4.844827586206897, Mean Entropy: 0.21858762204647064, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1313,  Mean reward: 4.132075471698113, Mean Entropy: 0.23558354377746582, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1314,  Mean reward: 4.622641509433962, Mean Entropy: 0.19358953833580017, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1315,  Mean reward: 3.4607843137254903, Mean Entropy: 0.20857200026512146, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1316,  Mean reward: 4.25, Mean Entropy: 0.2292342483997345, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 1317,  Mean reward: 2.217391304347826, Mean Entropy: 0.2651716470718384, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1318,  Mean reward: 3.980392156862745, Mean Entropy: 0.23134085536003113, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1319,  Mean reward: 4.287037037037037, Mean Entropy: 0.19868133962154388, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1320,  Mean reward: 3.688679245283019, Mean Entropy: 0.21058616042137146, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1321,  Mean reward: 4.5, Mean Entropy: 0.20264852046966553, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1322,  Mean reward: 4.718181818181818, Mean Entropy: 0.2120516151189804, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 1323,  Mean reward: 4.5, Mean Entropy: 0.20578180253505707, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1324,  Mean reward: 3.520408163265306, Mean Entropy: 0.2308637797832489, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1325,  Mean reward: 5.140350877192983, Mean Entropy: 0.19839560985565186, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1326,  Mean reward: 2.78125, Mean Entropy: 0.5115364789962769, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1327,  Mean reward: 3.4705882352941178, Mean Entropy: 0.2685917317867279, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1328,  Mean reward: 3.7788461538461537, Mean Entropy: 0.2648364305496216, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1329,  Mean reward: 2.5520833333333335, Mean Entropy: 0.2745160460472107, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1330,  Mean reward: 4.2924528301886795, Mean Entropy: 0.21661315858364105, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1331,  Mean reward: 4.176470588235294, Mean Entropy: 0.2349989414215088, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1332,  Mean reward: 3.6666666666666665, Mean Entropy: 0.2043023556470871, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1333,  Mean reward: 5.508474576271187, Mean Entropy: 0.16885089874267578, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1334,  Mean reward: 3.75, Mean Entropy: 0.27469682693481445, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1335,  Mean reward: 4.5636363636363635, Mean Entropy: 0.22134897112846375, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1336,  Mean reward: 3.6481481481481484, Mean Entropy: 0.2863540053367615, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1337,  Mean reward: 3.41, Mean Entropy: 0.25607937574386597, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1338,  Mean reward: 4.1923076923076925, Mean Entropy: 0.3138856887817383, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1339,  Mean reward: 4.827272727272727, Mean Entropy: 0.20743408799171448, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1340,  Mean reward: 4.4, Mean Entropy: 0.2569599747657776, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 1341,  Mean reward: 5.0636363636363635, Mean Entropy: 0.15845248103141785, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1342,  Mean reward: 3.35, Mean Entropy: 0.21684256196022034, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1343,  Mean reward: -3.338235294117647, Mean Entropy: 0.47406405210494995, complete_episode_count: 34.0, Gather time: 0.48s, Train time: 1.26s
Iteration: 1344,  Mean reward: 0.2375, Mean Entropy: 0.4501211643218994, complete_episode_count: 40.0, Gather time: 0.48s, Train time: 1.26s
Iteration: 1345,  Mean reward: 0.881578947368421, Mean Entropy: 0.5453751087188721, complete_episode_count: 38.0, Gather time: 0.48s, Train time: 1.27s
Iteration: 1346,  Mean reward: 1.8717948717948718, Mean Entropy: 0.5081565380096436, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1347,  Mean reward: 1.0, Mean Entropy: 0.5435251593589783, complete_episode_count: 39.0, Gather time: 0.48s, Train time: 1.27s
Iteration: 1348,  Mean reward: 2.2023809523809526, Mean Entropy: 0.5570024847984314, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1349,  Mean reward: -0.2972972972972973, Mean Entropy: 0.4991167485713959, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1350,  Mean reward: 2.2261904761904763, Mean Entropy: 0.49058446288108826, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1351,  Mean reward: -0.2625, Mean Entropy: 0.4934164881706238, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1352,  Mean reward: -0.1527777777777778, Mean Entropy: 0.5576808452606201, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1353,  Mean reward: 1.6904761904761905, Mean Entropy: 0.5297278761863708, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1354,  Mean reward: 3.2790697674418605, Mean Entropy: 0.6068986654281616, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.41s
Iteration: 1355,  Mean reward: 3.1818181818181817, Mean Entropy: 0.38780325651168823, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1356,  Mean reward: 1.8804347826086956, Mean Entropy: 0.4233191907405853, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.24s
Iteration: 1357,  Mean reward: 3.011111111111111, Mean Entropy: 0.48106420040130615, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 1358,  Mean reward: 2.8444444444444446, Mean Entropy: 0.5198639631271362, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1359,  Mean reward: 3.1630434782608696, Mean Entropy: 0.5178470611572266, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1360,  Mean reward: 1.670731707317073, Mean Entropy: 0.42760032415390015, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1361,  Mean reward: 4.428571428571429, Mean Entropy: 0.42274630069732666, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1362,  Mean reward: 2.4456521739130435, Mean Entropy: 0.4679906368255615, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1363,  Mean reward: 2.840909090909091, Mean Entropy: 0.5272058248519897, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1364,  Mean reward: 1.5348837209302326, Mean Entropy: 0.6423752307891846, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 1365,  Mean reward: 1.8372093023255813, Mean Entropy: 0.5573437809944153, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1366,  Mean reward: 3.297872340425532, Mean Entropy: 0.5514523983001709, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1367,  Mean reward: 4.846153846153846, Mean Entropy: 0.5590381622314453, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1368,  Mean reward: 4.1875, Mean Entropy: 0.5060858726501465, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1369,  Mean reward: 2.0, Mean Entropy: 0.5056082606315613, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1370,  Mean reward: 3.0, Mean Entropy: 0.4672432839870453, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1371,  Mean reward: 3.622448979591837, Mean Entropy: 0.4788852334022522, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1372,  Mean reward: 4.4245283018867925, Mean Entropy: 0.3804287910461426, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1373,  Mean reward: 3.0425531914893615, Mean Entropy: 0.4971911311149597, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1374,  Mean reward: 2.488888888888889, Mean Entropy: 0.5036633014678955, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 1375,  Mean reward: 2.851063829787234, Mean Entropy: 0.554694414138794, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 1376,  Mean reward: 2.8333333333333335, Mean Entropy: 0.5070106387138367, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1377,  Mean reward: 1.1395348837209303, Mean Entropy: 0.42165541648864746, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1378,  Mean reward: 4.066037735849057, Mean Entropy: 0.3353763520717621, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1379,  Mean reward: 2.6018518518518516, Mean Entropy: 0.5275939702987671, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1380,  Mean reward: 3.0943396226415096, Mean Entropy: 0.4728105068206787, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1381,  Mean reward: 3.287037037037037, Mean Entropy: 0.42322850227355957, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1382,  Mean reward: 3.390909090909091, Mean Entropy: 0.4087223708629608, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1383,  Mean reward: 3.6491228070175437, Mean Entropy: 0.4409036338329315, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1384,  Mean reward: 3.9909090909090907, Mean Entropy: 0.4370300769805908, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1385,  Mean reward: 3.7636363636363637, Mean Entropy: 0.4325947165489197, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1386,  Mean reward: 4.298076923076923, Mean Entropy: 0.3832537531852722, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 1387,  Mean reward: 5.0673076923076925, Mean Entropy: 0.30161410570144653, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1388,  Mean reward: 4.5754716981132075, Mean Entropy: 0.3188092112541199, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1389,  Mean reward: 2.490566037735849, Mean Entropy: 0.30542945861816406, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1390,  Mean reward: 3.2244897959183674, Mean Entropy: 0.2582665681838989, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1391,  Mean reward: 3.54, Mean Entropy: 0.21131645143032074, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 1392,  Mean reward: 5.5, Mean Entropy: 0.2688385248184204, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1393,  Mean reward: 2.9081632653061225, Mean Entropy: 0.2895638346672058, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1394,  Mean reward: 3.816326530612245, Mean Entropy: 0.2663167119026184, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 1395,  Mean reward: 4.367924528301887, Mean Entropy: 0.2353355586528778, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1396,  Mean reward: 2.7857142857142856, Mean Entropy: 0.25703009963035583, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1397,  Mean reward: 5.745614035087719, Mean Entropy: 0.20811407268047333, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1398,  Mean reward: 3.75, Mean Entropy: 0.27143561840057373, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1399,  Mean reward: 4.231481481481482, Mean Entropy: 0.24328741431236267, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1400,  Mean reward: 2.8979591836734695, Mean Entropy: 0.2846119999885559, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.26s
rec seq len 2
actor lr 0.0005
Iteration: 1401,  Mean reward: 3.7551020408163267, Mean Entropy: 0.21321898698806763, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1402,  Mean reward: 3.38, Mean Entropy: 0.24421684443950653, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1403,  Mean reward: 2.7291666666666665, Mean Entropy: 0.18974030017852783, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 1404,  Mean reward: 5.543103448275862, Mean Entropy: 0.14451856911182404, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1405,  Mean reward: 3.803921568627451, Mean Entropy: 0.21800653636455536, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1406,  Mean reward: 4.433962264150943, Mean Entropy: 0.13375310599803925, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1407,  Mean reward: 4.803571428571429, Mean Entropy: 0.14197760820388794, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1408,  Mean reward: 3.75, Mean Entropy: 0.11709731817245483, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.29s
Iteration: 1409,  Mean reward: 5.163793103448276, Mean Entropy: 0.22440873086452484, complete_episode_count: 58.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1410,  Mean reward: 3.7254901960784315, Mean Entropy: 0.22223035991191864, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1411,  Mean reward: 4.0, Mean Entropy: 0.12396720051765442, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 1412,  Mean reward: 4.235294117647059, Mean Entropy: 0.18573734164237976, complete_episode_count: 51.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1413,  Mean reward: 4.5636363636363635, Mean Entropy: 0.13104218244552612, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1414,  Mean reward: 4.590909090909091, Mean Entropy: 0.14852510392665863, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 1415,  Mean reward: 4.1923076923076925, Mean Entropy: 0.19331884384155273, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1416,  Mean reward: 4.537735849056604, Mean Entropy: 0.20563672482967377, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1417,  Mean reward: 5.0625, Mean Entropy: 0.2398151159286499, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1418,  Mean reward: 3.69811320754717, Mean Entropy: 0.1813141107559204, complete_episode_count: 53.0, Gather time: 0.65s, Train time: 1.25s
Iteration: 1419,  Mean reward: 3.35, Mean Entropy: 0.22390882670879364, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1420,  Mean reward: 4.9818181818181815, Mean Entropy: 0.15577083826065063, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1421,  Mean reward: 4.231481481481482, Mean Entropy: 0.1334913969039917, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1422,  Mean reward: 4.590909090909091, Mean Entropy: 0.153700590133667, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1423,  Mean reward: 4.2924528301886795, Mean Entropy: 0.15877343714237213, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1424,  Mean reward: 2.25, Mean Entropy: 0.20446668565273285, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 1425,  Mean reward: 3.4705882352941178, Mean Entropy: 0.3915725350379944, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1426,  Mean reward: 3.01, Mean Entropy: 0.2954248785972595, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 1427,  Mean reward: 5.926229508196721, Mean Entropy: 0.21143841743469238, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 1428,  Mean reward: 4.7, Mean Entropy: 0.29043927788734436, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1429,  Mean reward: 3.8055555555555554, Mean Entropy: 0.31638064980506897, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 1430,  Mean reward: 4.587719298245614, Mean Entropy: 0.33483657240867615, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 1431,  Mean reward: 2.95, Mean Entropy: 0.25286564230918884, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1432,  Mean reward: 4.388888888888889, Mean Entropy: 0.244041308760643, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 1433,  Mean reward: 5.663793103448276, Mean Entropy: 0.1963338851928711, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.25s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -3.3974358974358974, Mean Entropy: 0.9025321006774902, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.25s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -2.697674418604651, Mean Entropy: 0.9386173486709595, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 2,  Mean reward: -5.230769230769231, Mean Entropy: 0.9458044767379761, complete_episode_count: 39.0, Gather time: 0.48s, Train time: 1.25s
Iteration: 3,  Mean reward: -6.012195121951219, Mean Entropy: 0.916872501373291, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 4,  Mean reward: -5.3, Mean Entropy: 0.8662067651748657, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 5,  Mean reward: -4.575, Mean Entropy: 0.9815804362297058, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 6,  Mean reward: -6.4875, Mean Entropy: 1.0100817680358887, complete_episode_count: 40.0, Gather time: 0.48s, Train time: 1.25s
Iteration: 7,  Mean reward: -4.1, Mean Entropy: 0.9164499044418335, complete_episode_count: 40.0, Gather time: 0.48s, Train time: 1.25s
Iteration: 8,  Mean reward: -4.906976744186046, Mean Entropy: 0.9955713152885437, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 9,  Mean reward: -7.475, Mean Entropy: 0.9875130653381348, complete_episode_count: 40.0, Gather time: 0.48s, Train time: 1.26s
Iteration: 10,  Mean reward: -5.144444444444445, Mean Entropy: 0.959117591381073, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 11,  Mean reward: -5.0131578947368425, Mean Entropy: 0.9016762971878052, complete_episode_count: 38.0, Gather time: 0.48s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 12,  Mean reward: -2.1818181818181817, Mean Entropy: 0.9524383544921875, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.25s
Iteration: 13,  Mean reward: -3.911111111111111, Mean Entropy: 0.9452087879180908, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 14,  Mean reward: -3.55, Mean Entropy: 0.9451488256454468, complete_episode_count: 40.0, Gather time: 0.48s, Train time: 1.25s
Iteration: 15,  Mean reward: -5.073170731707317, Mean Entropy: 0.9446324110031128, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 16,  Mean reward: -5.595238095238095, Mean Entropy: 0.928861141204834, complete_episode_count: 42.0, Gather time: 0.48s, Train time: 1.41s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 17,  Mean reward: -1.9285714285714286, Mean Entropy: 1.0007342100143433, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 18,  Mean reward: -4.627659574468085, Mean Entropy: 0.9569732546806335, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 19,  Mean reward: -3.9878048780487805, Mean Entropy: 0.9406367540359497, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 20,  Mean reward: -4.556818181818182, Mean Entropy: 0.9341367483139038, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 21,  Mean reward: -6.3375, Mean Entropy: 0.9611592292785645, complete_episode_count: 40.0, Gather time: 0.48s, Train time: 1.25s
Iteration: 22,  Mean reward: -5.7727272727272725, Mean Entropy: 0.9522125720977783, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 23,  Mean reward: -6.571428571428571, Mean Entropy: 0.9534017443656921, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 24,  Mean reward: -3.3815789473684212, Mean Entropy: 0.924952507019043, complete_episode_count: 38.0, Gather time: 0.48s, Train time: 1.26s
Iteration: 25,  Mean reward: -5.0, Mean Entropy: 0.9156966209411621, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.29s
Iteration: 26,  Mean reward: -5.511363636363637, Mean Entropy: 0.9462116956710815, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 27,  Mean reward: -5.619047619047619, Mean Entropy: 0.9103999137878418, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 28,  Mean reward: -3.663265306122449, Mean Entropy: 0.9816926717758179, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 29,  Mean reward: -3.1951219512195124, Mean Entropy: 0.9270148277282715, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 30,  Mean reward: -2.8777777777777778, Mean Entropy: 0.9609960913658142, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 31,  Mean reward: -1.6666666666666667, Mean Entropy: 0.9553803205490112, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 32,  Mean reward: -3.776595744680851, Mean Entropy: 0.9489580988883972, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 33,  Mean reward: -3.152173913043478, Mean Entropy: 0.9080413579940796, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 34,  Mean reward: -4.948717948717949, Mean Entropy: 0.9974836111068726, complete_episode_count: 39.0, Gather time: 0.48s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 35,  Mean reward: 1.2291666666666667, Mean Entropy: 0.937086284160614, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 36,  Mean reward: -4.058139534883721, Mean Entropy: 0.8739463686943054, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 37,  Mean reward: -0.45454545454545453, Mean Entropy: 0.8986023664474487, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 38,  Mean reward: -5.293478260869565, Mean Entropy: 0.9772318601608276, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 39,  Mean reward: -1.975, Mean Entropy: 0.8753937482833862, complete_episode_count: 40.0, Gather time: 0.48s, Train time: 1.26s
Iteration: 40,  Mean reward: -2.5, Mean Entropy: 0.9169551134109497, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 41,  Mean reward: -1.9444444444444444, Mean Entropy: 0.8530564308166504, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 42,  Mean reward: 0.8611111111111112, Mean Entropy: 0.7651559710502625, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.28s
Iteration: 43,  Mean reward: 0.8653846153846154, Mean Entropy: 0.6949519515037537, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 44,  Mean reward: 1.5338983050847457, Mean Entropy: 0.6989821791648865, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.26s
Iteration: 45,  Mean reward: -0.5196078431372549, Mean Entropy: 0.7260435819625854, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 46,  Mean reward: -2.7555555555555555, Mean Entropy: 0.7514281272888184, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 47,  Mean reward: 0.09090909090909091, Mean Entropy: 0.7512316107749939, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 48,  Mean reward: 1.990566037735849, Mean Entropy: 0.8402187824249268, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 49,  Mean reward: -5.404255319148936, Mean Entropy: 0.7879063487052917, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.41s
Iteration: 50,  Mean reward: 1.2, Mean Entropy: 0.9448121786117554, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 51,  Mean reward: -3.5232558139534884, Mean Entropy: 0.9565846920013428, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 52,  Mean reward: -0.13953488372093023, Mean Entropy: 0.8425313830375671, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.28s
Iteration: 53,  Mean reward: -4.2745098039215685, Mean Entropy: 0.9121770858764648, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 54,  Mean reward: -0.5909090909090909, Mean Entropy: 0.7210383415222168, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 55,  Mean reward: 0.5, Mean Entropy: 0.6505964994430542, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 56,  Mean reward: 1.3148148148148149, Mean Entropy: 0.6977366209030151, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 57,  Mean reward: 0.37, Mean Entropy: 0.6945330500602722, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 58,  Mean reward: -0.7314814814814815, Mean Entropy: 0.7123059034347534, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.25s
Iteration: 59,  Mean reward: -0.19444444444444445, Mean Entropy: 0.6890971064567566, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.29s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 60,  Mean reward: 3.0462962962962963, Mean Entropy: 0.7151724696159363, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.26s
Iteration: 61,  Mean reward: 0.5204081632653061, Mean Entropy: 0.687263011932373, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 62,  Mean reward: -0.2857142857142857, Mean Entropy: 0.6950573921203613, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.27s
Iteration: 63,  Mean reward: 0.9361702127659575, Mean Entropy: 0.6720783114433289, complete_episode_count: 47.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 64,  Mean reward: 1.4791666666666667, Mean Entropy: 0.6873996257781982, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.27s
Iteration: 65,  Mean reward: 0.5098039215686274, Mean Entropy: 0.7114989757537842, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 66,  Mean reward: 2.0816326530612246, Mean Entropy: 0.6674625277519226, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 67,  Mean reward: 3.787037037037037, Mean Entropy: 0.6951801776885986, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.26s
Iteration: 68,  Mean reward: 1.684782608695652, Mean Entropy: 0.785153865814209, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 69,  Mean reward: 0.7272727272727273, Mean Entropy: 0.8559589385986328, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 70,  Mean reward: -2.730769230769231, Mean Entropy: 0.837221622467041, complete_episode_count: 39.0, Gather time: 0.48s, Train time: 1.26s
Iteration: 71,  Mean reward: -0.14285714285714285, Mean Entropy: 0.8246192932128906, complete_episode_count: 49.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 72,  Mean reward: -0.03125, Mean Entropy: 0.8434521555900574, complete_episode_count: 48.0, Gather time: 0.49s, Train time: 1.26s
Iteration: 73,  Mean reward: 1.9042553191489362, Mean Entropy: 0.7905017733573914, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.26s
Iteration: 74,  Mean reward: -1.1195652173913044, Mean Entropy: 0.7143346667289734, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.25s
Iteration: 75,  Mean reward: 3.46, Mean Entropy: 0.7140822410583496, complete_episode_count: 50.0, Gather time: 0.49s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 76,  Mean reward: 3.8207547169811322, Mean Entropy: 0.5322324633598328, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.28s
Iteration: 77,  Mean reward: 3.75, Mean Entropy: 0.5381299257278442, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.26s
Iteration: 78,  Mean reward: 3.787037037037037, Mean Entropy: 0.5423144698143005, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.25s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 79,  Mean reward: 4.401785714285714, Mean Entropy: 0.5352746844291687, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 80,  Mean reward: 2.3823529411764706, Mean Entropy: 0.4923485219478607, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 81,  Mean reward: 5.214285714285714, Mean Entropy: 0.5967363119125366, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 82,  Mean reward: 3.8796296296296298, Mean Entropy: 0.4798358380794525, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.27s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 83,  Mean reward: 5.298245614035087, Mean Entropy: 0.4914054870605469, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.26s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 84,  Mean reward: 5.9772727272727275, Mean Entropy: 0.3517386317253113, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.63s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 85,  Mean reward: 6.120689655172414, Mean Entropy: 0.4720275402069092, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.26s
Iteration: 86,  Mean reward: 5.758064516129032, Mean Entropy: 0.33558958768844604, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.25s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 87,  Mean reward: 6.4765625, Mean Entropy: 0.40075212717056274, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 88,  Mean reward: 6.150793650793651, Mean Entropy: 0.41221174597740173, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 89,  Mean reward: 6.242424242424242, Mean Entropy: 0.2249140441417694, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.65s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 90,  Mean reward: 7.54054054054054, Mean Entropy: 0.0162937194108963, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.63s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 91,  Mean reward: 7.7215189873417724, Mean Entropy: 0.0459657721221447, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.63s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 92,  Mean reward: 8.0, Mean Entropy: 0.024116335436701775, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 93,  Mean reward: 7.7215189873417724, Mean Entropy: 0.011302009224891663, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 94,  Mean reward: 8.0, Mean Entropy: 0.014947543852031231, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 95,  Mean reward: 8.0, Mean Entropy: 0.01534541230648756, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 96,  Mean reward: 8.0, Mean Entropy: 0.030243169516324997, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 97,  Mean reward: 7.717948717948718, Mean Entropy: 0.013902626931667328, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 98,  Mean reward: 8.0, Mean Entropy: 0.010687175206840038, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 99,  Mean reward: 7.7215189873417724, Mean Entropy: 0.004250604659318924, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 100,  Mean reward: 8.0, Mean Entropy: 0.008248433470726013, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 8.0, Mean Entropy: 0.013111057691276073, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 102,  Mean reward: 8.0, Mean Entropy: 0.013533629477024078, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 103,  Mean reward: 7.7215189873417724, Mean Entropy: 0.014808529987931252, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 104,  Mean reward: 8.0, Mean Entropy: 0.002838350832462311, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 105,  Mean reward: 8.0, Mean Entropy: 0.0037768655456602573, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 106,  Mean reward: 8.0, Mean Entropy: 0.003635384840890765, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 107,  Mean reward: 8.0, Mean Entropy: 0.004679936449974775, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 108,  Mean reward: 7.7215189873417724, Mean Entropy: 0.014764806255698204, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 109,  Mean reward: 8.0, Mean Entropy: 0.0004950683214701712, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 110,  Mean reward: 8.0, Mean Entropy: 0.0008533548098057508, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 111,  Mean reward: 8.0, Mean Entropy: 0.0010030033299699426, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 112,  Mean reward: 8.0, Mean Entropy: 0.0014268930535763502, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 113,  Mean reward: 8.0, Mean Entropy: 0.0013324692845344543, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 114,  Mean reward: 8.0, Mean Entropy: 0.0011199010768905282, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 115,  Mean reward: 8.0, Mean Entropy: 0.0011673076078295708, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 116,  Mean reward: 8.0, Mean Entropy: 0.0010907072573900223, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 117,  Mean reward: 8.0, Mean Entropy: 0.0018073044484481215, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.79s
Iteration: 118,  Mean reward: 8.0, Mean Entropy: 0.0013361657038331032, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 119,  Mean reward: 8.0, Mean Entropy: 0.00179306510835886, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 120,  Mean reward: 8.0, Mean Entropy: 0.002145602833479643, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 121,  Mean reward: 8.0, Mean Entropy: 0.002269660821184516, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 122,  Mean reward: 8.0, Mean Entropy: 0.0027509555220603943, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 123,  Mean reward: 8.0, Mean Entropy: 0.002963240724056959, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 124,  Mean reward: 8.0, Mean Entropy: 0.002405043924227357, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 125,  Mean reward: 8.0, Mean Entropy: 0.0053048282861709595, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 126,  Mean reward: 8.0, Mean Entropy: 0.005613365676254034, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 127,  Mean reward: 8.0, Mean Entropy: 0.00735311908647418, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 128,  Mean reward: 8.0, Mean Entropy: 0.010662047192454338, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 129,  Mean reward: 8.0, Mean Entropy: 0.01021520420908928, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 130,  Mean reward: 8.0, Mean Entropy: 0.010352451354265213, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 131,  Mean reward: 8.0, Mean Entropy: 0.013660814613103867, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 132,  Mean reward: 7.698717948717949, Mean Entropy: 0.020661145448684692, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 133,  Mean reward: 8.0, Mean Entropy: 0.0011062824632972479, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 134,  Mean reward: 8.0, Mean Entropy: 0.003585905535146594, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 135,  Mean reward: 8.0, Mean Entropy: 0.0016433902783319354, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 136,  Mean reward: 8.0, Mean Entropy: 0.002913606120273471, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 137,  Mean reward: 7.7215189873417724, Mean Entropy: 0.001262545119971037, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 138,  Mean reward: 8.0, Mean Entropy: 0.0004349700757302344, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 139,  Mean reward: 8.0, Mean Entropy: 0.0002895573852583766, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 140,  Mean reward: 8.0, Mean Entropy: 0.000616303295828402, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 141,  Mean reward: 8.0, Mean Entropy: 0.0009663938544690609, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 142,  Mean reward: 8.0, Mean Entropy: 0.0009502213215455413, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 143,  Mean reward: 8.0, Mean Entropy: 0.00095308170421049, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 144,  Mean reward: 8.0, Mean Entropy: 0.0008754610898904502, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 145,  Mean reward: 8.0, Mean Entropy: 0.001025568344630301, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 146,  Mean reward: 8.0, Mean Entropy: 0.0022820234298706055, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 147,  Mean reward: 8.0, Mean Entropy: 0.003948351368308067, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 148,  Mean reward: 8.0, Mean Entropy: 0.007416171487420797, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 149,  Mean reward: 8.0, Mean Entropy: 0.007060515694320202, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 150,  Mean reward: 7.7215189873417724, Mean Entropy: 0.010896681807935238, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 151,  Mean reward: 8.0, Mean Entropy: 0.0008919038227759302, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 152,  Mean reward: 8.0, Mean Entropy: 0.0002769312122836709, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 153,  Mean reward: 8.0, Mean Entropy: 0.0006540179019793868, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 154,  Mean reward: 8.0, Mean Entropy: 0.0008395875338464975, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 155,  Mean reward: 8.0, Mean Entropy: 0.0010672783246263862, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 156,  Mean reward: 8.0, Mean Entropy: 0.0013354610418900847, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 157,  Mean reward: 8.0, Mean Entropy: 0.0014030340826138854, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 158,  Mean reward: 8.0, Mean Entropy: 0.0022035427391529083, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 159,  Mean reward: 8.0, Mean Entropy: 0.001861547352746129, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 160,  Mean reward: 8.0, Mean Entropy: 0.0015652452129870653, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 161,  Mean reward: 8.0, Mean Entropy: 0.0013122775126248598, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 162,  Mean reward: 8.0, Mean Entropy: 0.0013662520796060562, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 163,  Mean reward: 8.0, Mean Entropy: 0.001227717031724751, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 164,  Mean reward: 8.0, Mean Entropy: 0.0012656825128942728, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 165,  Mean reward: 8.0, Mean Entropy: 0.0012392729986459017, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 166,  Mean reward: 8.0, Mean Entropy: 0.0013996174093335867, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 167,  Mean reward: 8.0, Mean Entropy: 0.0013062841026112437, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 168,  Mean reward: 8.0, Mean Entropy: 0.0013357712887227535, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 169,  Mean reward: 8.0, Mean Entropy: 0.00124279351439327, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 170,  Mean reward: 8.0, Mean Entropy: 0.0012657631887122989, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 171,  Mean reward: 8.0, Mean Entropy: 0.0011370768770575523, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 172,  Mean reward: 8.0, Mean Entropy: 0.001229847432114184, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.63s
Iteration: 173,  Mean reward: 8.0, Mean Entropy: 0.0010791858658194542, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 174,  Mean reward: 8.0, Mean Entropy: 0.0010881191119551659, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 175,  Mean reward: 8.0, Mean Entropy: 0.0009879288263618946, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 176,  Mean reward: 8.0, Mean Entropy: 0.0010911368299275637, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 177,  Mean reward: 8.0, Mean Entropy: 0.0009647474507801235, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 178,  Mean reward: 8.0, Mean Entropy: 0.001025860896334052, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 179,  Mean reward: 8.0, Mean Entropy: 0.0009615856106393039, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 180,  Mean reward: 8.0, Mean Entropy: 0.0010353416437283158, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 181,  Mean reward: 7.7215189873417724, Mean Entropy: 0.0008076557423919439, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 182,  Mean reward: 8.0, Mean Entropy: 0.00041724456241354346, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 183,  Mean reward: 8.0, Mean Entropy: 0.0004277772968634963, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 184,  Mean reward: 8.0, Mean Entropy: 0.0009921686723828316, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 185,  Mean reward: 8.0, Mean Entropy: 0.001068881363607943, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 186,  Mean reward: 8.0, Mean Entropy: 0.0022803680039942265, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 187,  Mean reward: 8.0, Mean Entropy: 0.0023983358405530453, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 188,  Mean reward: 8.0, Mean Entropy: 0.0016974098980426788, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 189,  Mean reward: 8.0, Mean Entropy: 0.0022525619715452194, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 190,  Mean reward: 8.0, Mean Entropy: 0.0023317821323871613, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 191,  Mean reward: 8.0, Mean Entropy: 0.0019416779978200793, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 192,  Mean reward: 8.0, Mean Entropy: 0.0027448523323982954, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 193,  Mean reward: 8.0, Mean Entropy: 0.0026220623403787613, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.79s
Iteration: 194,  Mean reward: 8.0, Mean Entropy: 0.00268487143330276, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 195,  Mean reward: 8.0, Mean Entropy: 0.0027459003031253815, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 196,  Mean reward: 8.0, Mean Entropy: 0.0034656133502721786, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 197,  Mean reward: 8.0, Mean Entropy: 0.003819970181211829, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 198,  Mean reward: 8.0, Mean Entropy: 0.004012518562376499, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 199,  Mean reward: 8.0, Mean Entropy: 0.0037155835889279842, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 200,  Mean reward: 8.0, Mean Entropy: 0.002548862947151065, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 8.0, Mean Entropy: 0.0025016427971422672, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 202,  Mean reward: 8.0, Mean Entropy: 0.0028781245928257704, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 203,  Mean reward: 8.0, Mean Entropy: 0.0038914280012249947, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 204,  Mean reward: 8.0, Mean Entropy: 0.003612034721300006, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 205,  Mean reward: 8.0, Mean Entropy: 0.004069511778652668, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 206,  Mean reward: 8.0, Mean Entropy: 0.004120619036257267, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 207,  Mean reward: 8.0, Mean Entropy: 0.004133975133299828, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 208,  Mean reward: 8.0, Mean Entropy: 0.0036617571022361517, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 209,  Mean reward: 8.0, Mean Entropy: 0.004403925966471434, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 210,  Mean reward: 8.0, Mean Entropy: 0.004266571253538132, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 211,  Mean reward: 8.0, Mean Entropy: 0.003971932455897331, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 212,  Mean reward: 8.0, Mean Entropy: 0.005751488264650106, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 213,  Mean reward: 8.0, Mean Entropy: 0.004580807872116566, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 214,  Mean reward: 8.0, Mean Entropy: 0.0036109022330492735, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 215,  Mean reward: 8.0, Mean Entropy: 0.004142426420003176, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 216,  Mean reward: 8.0, Mean Entropy: 0.004727421328425407, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 217,  Mean reward: 8.0, Mean Entropy: 0.0033515989780426025, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 218,  Mean reward: 8.0, Mean Entropy: 0.0034108906984329224, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 219,  Mean reward: 8.0, Mean Entropy: 0.0027607674710452557, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 220,  Mean reward: 8.0, Mean Entropy: 0.0027683665975928307, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 221,  Mean reward: 8.0, Mean Entropy: 0.0026983050629496574, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 222,  Mean reward: 8.0, Mean Entropy: 0.002041610423475504, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 223,  Mean reward: 8.0, Mean Entropy: 0.0028294185176491737, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 224,  Mean reward: 8.0, Mean Entropy: 0.002612559124827385, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 225,  Mean reward: 8.0, Mean Entropy: 0.0025998088531196117, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.63s
Iteration: 226,  Mean reward: 8.0, Mean Entropy: 0.001870343810878694, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 227,  Mean reward: 8.0, Mean Entropy: 0.0022022135090082884, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 228,  Mean reward: 8.0, Mean Entropy: 0.001923611736856401, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 229,  Mean reward: 8.0, Mean Entropy: 0.0011702608317136765, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 230,  Mean reward: 8.0, Mean Entropy: 0.0006853536469861865, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 231,  Mean reward: 8.0, Mean Entropy: 0.0005357804475352168, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.81s
Iteration: 232,  Mean reward: 8.0, Mean Entropy: 0.0010276264511048794, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 233,  Mean reward: 8.0, Mean Entropy: 0.0007512451848015189, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 234,  Mean reward: 8.0, Mean Entropy: 0.0008525317534804344, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 235,  Mean reward: 8.0, Mean Entropy: 0.0006786667509004474, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 236,  Mean reward: 8.0, Mean Entropy: 0.0006268370198085904, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 237,  Mean reward: 8.0, Mean Entropy: 0.0005862804246135056, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 238,  Mean reward: 8.0, Mean Entropy: 0.0008263218915089965, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 239,  Mean reward: 8.0, Mean Entropy: 0.0005093907820992172, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 240,  Mean reward: 8.0, Mean Entropy: 0.0007008014945313334, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 241,  Mean reward: 8.0, Mean Entropy: 0.0006668322603218257, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 242,  Mean reward: 8.0, Mean Entropy: 0.00047029461711645126, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.65s
Iteration: 243,  Mean reward: 8.0, Mean Entropy: 0.0004514853935688734, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 244,  Mean reward: 8.0, Mean Entropy: 0.00031883391784504056, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 245,  Mean reward: 8.0, Mean Entropy: 0.0003069148806389421, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 246,  Mean reward: 8.0, Mean Entropy: 0.0002496849629096687, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 247,  Mean reward: 8.0, Mean Entropy: 0.00030689570121467113, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 248,  Mean reward: 8.0, Mean Entropy: 0.0002223443443654105, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 249,  Mean reward: 8.0, Mean Entropy: 0.00022032688139006495, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 250,  Mean reward: 8.0, Mean Entropy: 0.00020884477999061346, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 251,  Mean reward: 8.0, Mean Entropy: 0.00015797775995451957, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.63s
Iteration: 252,  Mean reward: 8.0, Mean Entropy: 0.00018247342086397111, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 253,  Mean reward: 8.0, Mean Entropy: 0.00016927439719438553, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 254,  Mean reward: 8.0, Mean Entropy: 0.00017079872486647218, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 255,  Mean reward: 8.0, Mean Entropy: 0.00018654605082701892, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 256,  Mean reward: 8.0, Mean Entropy: 0.0001682961592450738, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 257,  Mean reward: 8.0, Mean Entropy: 0.00017616359400562942, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 258,  Mean reward: 8.0, Mean Entropy: 0.00016768649220466614, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 259,  Mean reward: 8.0, Mean Entropy: 0.00018298198119737208, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 260,  Mean reward: 8.0, Mean Entropy: 0.00012539359158836305, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 261,  Mean reward: 8.0, Mean Entropy: 0.00019108090782538056, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 262,  Mean reward: 8.0, Mean Entropy: 0.00016731528739910573, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 263,  Mean reward: 8.0, Mean Entropy: 0.00015325422282330692, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 264,  Mean reward: 8.0, Mean Entropy: 0.00013839382154401392, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 265,  Mean reward: 8.0, Mean Entropy: 0.00019629168673418462, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 266,  Mean reward: 8.0, Mean Entropy: 0.00016515934839844704, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 267,  Mean reward: 8.0, Mean Entropy: 0.00014865840785205364, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 268,  Mean reward: 8.0, Mean Entropy: 0.00021645586821250618, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 269,  Mean reward: 8.0, Mean Entropy: 0.00023939769016578794, complete_episode_count: 80.0, Gather time: 0.68s, Train time: 0.63s
Iteration: 270,  Mean reward: 8.0, Mean Entropy: 0.000163811186212115, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 271,  Mean reward: 8.0, Mean Entropy: 0.00022649960010312498, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 272,  Mean reward: 8.0, Mean Entropy: 0.00019520979549270123, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 273,  Mean reward: 8.0, Mean Entropy: 0.00013298036355990916, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 274,  Mean reward: 8.0, Mean Entropy: 0.00020128735923208296, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.64s
Iteration: 275,  Mean reward: 8.0, Mean Entropy: 0.00022271383204497397, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 276,  Mean reward: 8.0, Mean Entropy: 0.00018678040942177176, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 277,  Mean reward: 8.0, Mean Entropy: 0.0002343121450394392, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 278,  Mean reward: 8.0, Mean Entropy: 0.00026420928770676255, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 279,  Mean reward: 8.0, Mean Entropy: 0.00016603540279902518, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 280,  Mean reward: 8.0, Mean Entropy: 0.0002137715637218207, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 281,  Mean reward: 8.0, Mean Entropy: 0.00019608004367910326, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 282,  Mean reward: 8.0, Mean Entropy: 0.0001767335197655484, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 283,  Mean reward: 8.0, Mean Entropy: 0.00020283687626942992, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 284,  Mean reward: 8.0, Mean Entropy: 0.00027528073405846953, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 285,  Mean reward: 8.0, Mean Entropy: 0.0001650088670430705, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 286,  Mean reward: 8.0, Mean Entropy: 0.00019415834685787559, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 287,  Mean reward: 8.0, Mean Entropy: 0.00020148069597780704, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 288,  Mean reward: 8.0, Mean Entropy: 0.0001552890898892656, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 289,  Mean reward: 8.0, Mean Entropy: 0.00021203383221291006, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 290,  Mean reward: 8.0, Mean Entropy: 0.00014533572539221495, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 291,  Mean reward: 8.0, Mean Entropy: 0.0001652194478083402, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 292,  Mean reward: 8.0, Mean Entropy: 0.00015305995475500822, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 293,  Mean reward: 8.0, Mean Entropy: 0.00016309384955093265, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 294,  Mean reward: 8.0, Mean Entropy: 0.0001484305685153231, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 295,  Mean reward: 8.0, Mean Entropy: 0.00017439600196667016, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 296,  Mean reward: 8.0, Mean Entropy: 0.00019391695968806744, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 297,  Mean reward: 8.0, Mean Entropy: 0.00016959184722509235, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 298,  Mean reward: 8.0, Mean Entropy: 0.00016088462143670768, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 299,  Mean reward: 8.0, Mean Entropy: 0.00020316288282629102, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 300,  Mean reward: 8.0, Mean Entropy: 0.00013652522466145456, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 8.0, Mean Entropy: 0.0001431821146979928, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 302,  Mean reward: 8.0, Mean Entropy: 0.00017904452397488058, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 303,  Mean reward: 8.0, Mean Entropy: 0.00013812929682899266, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 304,  Mean reward: 8.0, Mean Entropy: 0.00014550809282809496, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 305,  Mean reward: 8.0, Mean Entropy: 0.00017134944209828973, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 306,  Mean reward: 8.0, Mean Entropy: 0.00015565508510917425, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 307,  Mean reward: 8.0, Mean Entropy: 0.00015687369159422815, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 308,  Mean reward: 8.0, Mean Entropy: 0.00015428868937306106, complete_episode_count: 80.0, Gather time: 0.67s, Train time: 0.63s
Iteration: 309,  Mean reward: 8.0, Mean Entropy: 0.00016164597764145583, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 310,  Mean reward: 8.0, Mean Entropy: 0.0001418854226358235, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 311,  Mean reward: 8.0, Mean Entropy: 0.00012389515177346766, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 312,  Mean reward: 8.0, Mean Entropy: 0.00011795182217611, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 313,  Mean reward: 8.0, Mean Entropy: 0.00012499824515543878, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 314,  Mean reward: 8.0, Mean Entropy: 0.0001541336823720485, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 315,  Mean reward: 8.0, Mean Entropy: 0.00015986179641913623, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 316,  Mean reward: 8.0, Mean Entropy: 0.00014359956549014896, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 317,  Mean reward: 8.0, Mean Entropy: 0.00016209899331443012, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 318,  Mean reward: 8.0, Mean Entropy: 0.00016010465333238244, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 319,  Mean reward: 8.0, Mean Entropy: 0.00013459313777275383, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 320,  Mean reward: 8.0, Mean Entropy: 0.00012349200551398098, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 321,  Mean reward: 8.0, Mean Entropy: 0.00015021744184195995, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 322,  Mean reward: 8.0, Mean Entropy: 0.0001275827962672338, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 323,  Mean reward: 8.0, Mean Entropy: 0.00012116743891965598, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 324,  Mean reward: 8.0, Mean Entropy: 0.00011887901928275824, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 325,  Mean reward: 8.0, Mean Entropy: 0.00011556597746675834, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 326,  Mean reward: 8.0, Mean Entropy: 0.00012214682647027075, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 327,  Mean reward: 8.0, Mean Entropy: 0.00013568966824095696, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 328,  Mean reward: 8.0, Mean Entropy: 0.00012661307118833065, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 329,  Mean reward: 8.0, Mean Entropy: 0.00011036681098630652, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 330,  Mean reward: 8.0, Mean Entropy: 0.00010597646178212017, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 331,  Mean reward: 8.0, Mean Entropy: 0.00010151604510610923, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 332,  Mean reward: 8.0, Mean Entropy: 0.00011138893023598939, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 333,  Mean reward: 8.0, Mean Entropy: 0.0001098635548260063, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 334,  Mean reward: 8.0, Mean Entropy: 9.716065687825903e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 335,  Mean reward: 8.0, Mean Entropy: 0.0001015623493003659, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 336,  Mean reward: 8.0, Mean Entropy: 0.00010246075544273481, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 337,  Mean reward: 8.0, Mean Entropy: 0.00010744079918367788, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 338,  Mean reward: 8.0, Mean Entropy: 0.000102070058346726, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 339,  Mean reward: 8.0, Mean Entropy: 9.871047222986817e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 340,  Mean reward: 8.0, Mean Entropy: 0.00010958025086438283, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 341,  Mean reward: 8.0, Mean Entropy: 0.00010322542948415503, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 342,  Mean reward: 8.0, Mean Entropy: 0.0001222773571498692, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 343,  Mean reward: 8.0, Mean Entropy: 0.00011314744187984616, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 344,  Mean reward: 8.0, Mean Entropy: 0.0001134597696363926, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 345,  Mean reward: 8.0, Mean Entropy: 0.00010358126019127667, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 346,  Mean reward: 8.0, Mean Entropy: 0.00011504501162562519, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 347,  Mean reward: 8.0, Mean Entropy: 0.00010147029388463125, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 348,  Mean reward: 8.0, Mean Entropy: 0.00010064509115181863, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 349,  Mean reward: 8.0, Mean Entropy: 0.00011665093916235492, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 350,  Mean reward: 8.0, Mean Entropy: 0.00010076398029923439, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 351,  Mean reward: 8.0, Mean Entropy: 8.413111208938062e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 352,  Mean reward: 8.0, Mean Entropy: 0.00012200630590086803, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 353,  Mean reward: 8.0, Mean Entropy: 0.00013901051715947688, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 354,  Mean reward: 8.0, Mean Entropy: 8.314119622809812e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.64s
Iteration: 355,  Mean reward: 8.0, Mean Entropy: 0.00010313808161299676, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 356,  Mean reward: 8.0, Mean Entropy: 0.00010335593833588064, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 357,  Mean reward: 8.0, Mean Entropy: 9.000631689559668e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 358,  Mean reward: 8.0, Mean Entropy: 0.00012093204713892192, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 359,  Mean reward: 8.0, Mean Entropy: 0.00011182583693880588, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 360,  Mean reward: 8.0, Mean Entropy: 9.481326560489833e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 361,  Mean reward: 8.0, Mean Entropy: 0.0001251541543751955, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 362,  Mean reward: 8.0, Mean Entropy: 0.00011321122292429209, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 363,  Mean reward: 8.0, Mean Entropy: 9.394418157171458e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 364,  Mean reward: 8.0, Mean Entropy: 0.00012390216579660773, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 365,  Mean reward: 8.0, Mean Entropy: 0.00011295106378383934, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 366,  Mean reward: 8.0, Mean Entropy: 9.609118569642305e-05, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 367,  Mean reward: 8.0, Mean Entropy: 0.00013232359196990728, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 368,  Mean reward: 8.0, Mean Entropy: 0.00012134811549913138, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 369,  Mean reward: 8.0, Mean Entropy: 0.00011203443864360452, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 370,  Mean reward: 8.0, Mean Entropy: 0.00012561862240545452, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 371,  Mean reward: 8.0, Mean Entropy: 0.00013390433741733432, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 372,  Mean reward: 8.0, Mean Entropy: 0.00010476321767782792, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 373,  Mean reward: 8.0, Mean Entropy: 0.00012101025640731677, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 374,  Mean reward: 8.0, Mean Entropy: 0.00014159746933728456, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 375,  Mean reward: 8.0, Mean Entropy: 9.5820301794447e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 376,  Mean reward: 8.0, Mean Entropy: 0.00011606949556153268, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 377,  Mean reward: 8.0, Mean Entropy: 0.00014520264812745154, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 378,  Mean reward: 8.0, Mean Entropy: 0.00011304017243674025, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 379,  Mean reward: 8.0, Mean Entropy: 0.00012324299314059317, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 380,  Mean reward: 8.0, Mean Entropy: 0.00013736807159148157, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.63s
Iteration: 381,  Mean reward: 8.0, Mean Entropy: 0.00012006569158984348, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 382,  Mean reward: 8.0, Mean Entropy: 0.0001160609390353784, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 383,  Mean reward: 8.0, Mean Entropy: 0.00014640079461969435, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.80s
Iteration: 384,  Mean reward: 8.0, Mean Entropy: 0.00013397191651165485, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 385,  Mean reward: 8.0, Mean Entropy: 0.00013102777302265167, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 386,  Mean reward: 8.0, Mean Entropy: 0.00013494245649781078, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 0.00012191424320917577, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.63s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 0.00011243341577937827, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 389,  Mean reward: 8.0, Mean Entropy: 0.00011522056593094021, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 390,  Mean reward: 8.0, Mean Entropy: 0.00013489346019923687, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 0.00013483354996424168, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 392,  Mean reward: 8.0, Mean Entropy: 0.000133596797240898, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 0.00011733655992429703, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 0.00013975892215967178, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 395,  Mean reward: 8.0, Mean Entropy: 0.00013255010708235204, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 396,  Mean reward: 8.0, Mean Entropy: 0.0001283435121877119, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 0.000147082784678787, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 0.00014832534361630678, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 0.00016611660248599946, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 0.00014402391389012337, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 8.0, Mean Entropy: 0.00018617107707541436, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 402,  Mean reward: 8.0, Mean Entropy: 0.0001571395987411961, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 403,  Mean reward: 8.0, Mean Entropy: 0.00015417089161928743, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 404,  Mean reward: 8.0, Mean Entropy: 0.00015616386372130364, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 405,  Mean reward: 8.0, Mean Entropy: 0.0001835026778280735, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 0.00015884742606431246, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 407,  Mean reward: 8.0, Mean Entropy: 0.00017081880650948733, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 0.0001640907721593976, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 0.00017218783614225686, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 410,  Mean reward: 8.0, Mean Entropy: 0.0001362142647849396, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 411,  Mean reward: 8.0, Mean Entropy: 0.00015730768791399896, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 412,  Mean reward: 8.0, Mean Entropy: 0.0001467620168114081, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 413,  Mean reward: 8.0, Mean Entropy: 0.00016201427206397057, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 0.00013027910608798265, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 415,  Mean reward: 8.0, Mean Entropy: 0.00015491721569560468, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 416,  Mean reward: 8.0, Mean Entropy: 0.0001284170284634456, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 417,  Mean reward: 8.0, Mean Entropy: 0.00017284852219745517, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 418,  Mean reward: 8.0, Mean Entropy: 0.00014447967987507582, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 419,  Mean reward: 8.0, Mean Entropy: 0.00014373839076142758, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 0.00013461674097925425, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 421,  Mean reward: 8.0, Mean Entropy: 0.00016718699771445245, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.79s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.0001452973228879273, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 423,  Mean reward: 8.0, Mean Entropy: 0.0001535320043331012, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 424,  Mean reward: 8.0, Mean Entropy: 0.00014486847794614732, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 0.00014607823686674237, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 0.00012980884639546275, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 427,  Mean reward: 8.0, Mean Entropy: 0.00013824168127030134, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 428,  Mean reward: 8.0, Mean Entropy: 0.00013135140761733055, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 429,  Mean reward: 8.0, Mean Entropy: 0.0001413211866747588, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 430,  Mean reward: 8.0, Mean Entropy: 0.0001349668309558183, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 0.00015506077033933252, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 432,  Mean reward: 8.0, Mean Entropy: 0.00014130776980891824, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 433,  Mean reward: 8.0, Mean Entropy: 0.0001790394017007202, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 434,  Mean reward: 8.0, Mean Entropy: 0.00013636040966957808, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 435,  Mean reward: 8.0, Mean Entropy: 0.0001766918576322496, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 436,  Mean reward: 8.0, Mean Entropy: 0.0001233764924108982, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 437,  Mean reward: 8.0, Mean Entropy: 0.00017486404976807535, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 438,  Mean reward: 8.0, Mean Entropy: 0.00014911899052094668, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 0.00014757411554455757, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 440,  Mean reward: 8.0, Mean Entropy: 0.00013262225547805429, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 441,  Mean reward: 8.0, Mean Entropy: 0.00014515983639284968, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 0.00014958267274778336, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 443,  Mean reward: 8.0, Mean Entropy: 0.00017403328092768788, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 444,  Mean reward: 8.0, Mean Entropy: 0.00019755595712922513, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 445,  Mean reward: 8.0, Mean Entropy: 0.00020441498782020062, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 446,  Mean reward: 8.0, Mean Entropy: 0.0001859245094237849, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 447,  Mean reward: 8.0, Mean Entropy: 0.00017179494898300618, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 448,  Mean reward: 8.0, Mean Entropy: 0.00017061553080566227, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.63s
Iteration: 449,  Mean reward: 8.0, Mean Entropy: 0.0001939104840857908, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 450,  Mean reward: 8.0, Mean Entropy: 0.0001694866077741608, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 451,  Mean reward: 8.0, Mean Entropy: 0.00017541504348628223, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 452,  Mean reward: 8.0, Mean Entropy: 0.00013985284022055566, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 453,  Mean reward: 8.0, Mean Entropy: 0.00016062297800090164, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 454,  Mean reward: 8.0, Mean Entropy: 0.00017099268734455109, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 455,  Mean reward: 8.0, Mean Entropy: 0.00020544647122733295, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 456,  Mean reward: 8.0, Mean Entropy: 0.00018392309721093625, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 457,  Mean reward: 8.0, Mean Entropy: 0.00021291901066433638, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.63s
Iteration: 458,  Mean reward: 8.0, Mean Entropy: 0.00021486752666532993, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 459,  Mean reward: 8.0, Mean Entropy: 0.0001804055500542745, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.79s
Iteration: 460,  Mean reward: 8.0, Mean Entropy: 0.00020324983051978052, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 461,  Mean reward: 8.0, Mean Entropy: 0.00019590373267419636, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 462,  Mean reward: 8.0, Mean Entropy: 0.00019211074686609209, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 463,  Mean reward: 8.0, Mean Entropy: 0.00017145328456535935, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 464,  Mean reward: 8.0, Mean Entropy: 0.0001567850704304874, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 465,  Mean reward: 8.0, Mean Entropy: 0.00020065074204467237, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 466,  Mean reward: 8.0, Mean Entropy: 0.00018603456555865705, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 467,  Mean reward: 8.0, Mean Entropy: 0.00016873980348464102, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 468,  Mean reward: 8.0, Mean Entropy: 0.00020536819647531956, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 469,  Mean reward: 8.0, Mean Entropy: 0.00013040570775046945, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 470,  Mean reward: 8.0, Mean Entropy: 0.00014690429088659585, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 471,  Mean reward: 8.0, Mean Entropy: 0.0001754069235175848, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 472,  Mean reward: 8.0, Mean Entropy: 0.0001682907750364393, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 473,  Mean reward: 8.0, Mean Entropy: 0.00013121479423716664, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 474,  Mean reward: 8.0, Mean Entropy: 0.00012010087084490806, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 475,  Mean reward: 8.0, Mean Entropy: 0.000147147016832605, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 476,  Mean reward: 8.0, Mean Entropy: 0.00013022578787058592, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 477,  Mean reward: 8.0, Mean Entropy: 0.00013064808445051312, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 478,  Mean reward: 8.0, Mean Entropy: 0.0001597103546373546, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 479,  Mean reward: 8.0, Mean Entropy: 0.0001331344392383471, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 480,  Mean reward: 8.0, Mean Entropy: 0.00015481817536056042, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 481,  Mean reward: 8.0, Mean Entropy: 0.00014442615793086588, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 482,  Mean reward: 8.0, Mean Entropy: 0.00012541405158117414, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 483,  Mean reward: 8.0, Mean Entropy: 0.00012039770081173629, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.64s
Iteration: 484,  Mean reward: 8.0, Mean Entropy: 0.00011654355330392718, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 485,  Mean reward: 8.0, Mean Entropy: 0.00012389713083393872, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 486,  Mean reward: 8.0, Mean Entropy: 0.00013349756773095578, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 487,  Mean reward: 8.0, Mean Entropy: 0.00012850215716753155, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 488,  Mean reward: 8.0, Mean Entropy: 0.0001259389246115461, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 489,  Mean reward: 8.0, Mean Entropy: 0.00015298783546313643, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 490,  Mean reward: 8.0, Mean Entropy: 0.00014737782476004213, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 491,  Mean reward: 8.0, Mean Entropy: 0.00014081064728088677, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 492,  Mean reward: 8.0, Mean Entropy: 0.0001427356619387865, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 493,  Mean reward: 8.0, Mean Entropy: 0.00013072960427962244, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 494,  Mean reward: 8.0, Mean Entropy: 0.00018818036187440157, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 495,  Mean reward: 8.0, Mean Entropy: 0.0001762979372870177, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 496,  Mean reward: 8.0, Mean Entropy: 0.0001368802331853658, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 497,  Mean reward: 8.0, Mean Entropy: 0.00018596660811454058, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.80s
Iteration: 498,  Mean reward: 8.0, Mean Entropy: 0.00013901684724260122, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 499,  Mean reward: 8.0, Mean Entropy: 0.00011166798503836617, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 500,  Mean reward: 8.0, Mean Entropy: 0.0001227525353897363, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 8.0, Mean Entropy: 0.0001406694937031716, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 502,  Mean reward: 8.0, Mean Entropy: 0.00011540194100234658, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 503,  Mean reward: 8.0, Mean Entropy: 0.0001408258976880461, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 504,  Mean reward: 8.0, Mean Entropy: 0.0001472073490731418, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 505,  Mean reward: 8.0, Mean Entropy: 0.00013623209088109434, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 506,  Mean reward: 8.0, Mean Entropy: 0.00014585487951990217, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 507,  Mean reward: 8.0, Mean Entropy: 0.00016256197704933584, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 508,  Mean reward: 8.0, Mean Entropy: 0.00014021486276760697, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 509,  Mean reward: 8.0, Mean Entropy: 0.0001359628076897934, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 510,  Mean reward: 8.0, Mean Entropy: 0.0001461801293771714, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.64s
Iteration: 511,  Mean reward: 8.0, Mean Entropy: 0.00015848010662011802, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 512,  Mean reward: 8.0, Mean Entropy: 0.00011731639824574813, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 513,  Mean reward: 8.0, Mean Entropy: 0.00013280316488817334, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 514,  Mean reward: 8.0, Mean Entropy: 0.00012119894381612539, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 515,  Mean reward: 8.0, Mean Entropy: 0.00011133858060929924, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 516,  Mean reward: 8.0, Mean Entropy: 0.00010676424426492304, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 517,  Mean reward: 8.0, Mean Entropy: 0.00010438716708449647, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 518,  Mean reward: 8.0, Mean Entropy: 9.556581790093333e-05, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 519,  Mean reward: 8.0, Mean Entropy: 0.00010942386870738119, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 520,  Mean reward: 8.0, Mean Entropy: 0.00012390667689032853, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 521,  Mean reward: 8.0, Mean Entropy: 0.00010513793677091599, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 522,  Mean reward: 8.0, Mean Entropy: 0.00014876193017698824, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 523,  Mean reward: 8.0, Mean Entropy: 0.00012686318950727582, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 524,  Mean reward: 8.0, Mean Entropy: 0.0001287760678678751, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 525,  Mean reward: 8.0, Mean Entropy: 0.00015317343058995903, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 526,  Mean reward: 8.0, Mean Entropy: 0.00013880303595215082, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 527,  Mean reward: 8.0, Mean Entropy: 0.0001243933365913108, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 528,  Mean reward: 8.0, Mean Entropy: 0.00014017293869983405, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 529,  Mean reward: 8.0, Mean Entropy: 0.00014027883298695087, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 530,  Mean reward: 8.0, Mean Entropy: 0.0001264589373022318, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 531,  Mean reward: 8.0, Mean Entropy: 0.0001186074732686393, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 532,  Mean reward: 8.0, Mean Entropy: 0.00017293941345997155, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 533,  Mean reward: 8.0, Mean Entropy: 0.00019172461179550737, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 534,  Mean reward: 8.0, Mean Entropy: 0.00025634397752583027, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 535,  Mean reward: 8.0, Mean Entropy: 0.00023896146740298718, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.79s
Iteration: 536,  Mean reward: 8.0, Mean Entropy: 0.00016996858175843954, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.63s
Iteration: 537,  Mean reward: 8.0, Mean Entropy: 0.0001529379514977336, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 538,  Mean reward: 8.0, Mean Entropy: 0.00013193589984439313, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 539,  Mean reward: 8.0, Mean Entropy: 0.00012117915321141481, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 540,  Mean reward: 8.0, Mean Entropy: 0.00015466564218513668, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 541,  Mean reward: 8.0, Mean Entropy: 0.00016078483895398676, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 542,  Mean reward: 8.0, Mean Entropy: 0.00018075216212309897, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 543,  Mean reward: 8.0, Mean Entropy: 0.00015270621224772185, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 544,  Mean reward: 8.0, Mean Entropy: 0.00012306599819567055, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 545,  Mean reward: 8.0, Mean Entropy: 0.00016517691256012768, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 546,  Mean reward: 8.0, Mean Entropy: 0.0001387197116855532, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 547,  Mean reward: 8.0, Mean Entropy: 0.00012820026313420385, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 548,  Mean reward: 8.0, Mean Entropy: 0.0001348775258520618, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 549,  Mean reward: 8.0, Mean Entropy: 0.00014438855578191578, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 550,  Mean reward: 8.0, Mean Entropy: 8.565414464101195e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 551,  Mean reward: 8.0, Mean Entropy: 0.00010867083619814366, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 552,  Mean reward: 8.0, Mean Entropy: 0.0001601889234734699, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 553,  Mean reward: 8.0, Mean Entropy: 0.0001245613966602832, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 554,  Mean reward: 8.0, Mean Entropy: 0.00010115814802702516, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 555,  Mean reward: 8.0, Mean Entropy: 0.0001647374010644853, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 556,  Mean reward: 8.0, Mean Entropy: 0.00010881756315939128, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 557,  Mean reward: 8.0, Mean Entropy: 0.00011363843077560887, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.0001688114134594798, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 9.689817670732737e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 560,  Mean reward: 8.0, Mean Entropy: 0.00010437034507049248, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 561,  Mean reward: 8.0, Mean Entropy: 0.00015423685545101762, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 562,  Mean reward: 8.0, Mean Entropy: 0.0001313115790253505, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 563,  Mean reward: 8.0, Mean Entropy: 9.930309897754341e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 564,  Mean reward: 8.0, Mean Entropy: 0.00016556144692003727, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 565,  Mean reward: 8.0, Mean Entropy: 0.00012265329132787883, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 566,  Mean reward: 8.0, Mean Entropy: 0.00010715566168073565, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 567,  Mean reward: 8.0, Mean Entropy: 0.00012784963473677635, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 568,  Mean reward: 8.0, Mean Entropy: 8.999412966659293e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 569,  Mean reward: 8.0, Mean Entropy: 9.89527179626748e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 570,  Mean reward: 8.0, Mean Entropy: 0.00013386548380367458, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 571,  Mean reward: 8.0, Mean Entropy: 0.00012922151654493064, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 572,  Mean reward: 8.0, Mean Entropy: 9.9068449344486e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 573,  Mean reward: 8.0, Mean Entropy: 0.00015248361160047352, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.80s
Iteration: 574,  Mean reward: 8.0, Mean Entropy: 0.00011079043906647712, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.63s
Iteration: 575,  Mean reward: 8.0, Mean Entropy: 0.00011686883226502687, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 576,  Mean reward: 8.0, Mean Entropy: 0.00011886974971275777, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 577,  Mean reward: 8.0, Mean Entropy: 0.00010872427810681984, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 578,  Mean reward: 8.0, Mean Entropy: 9.813683573156595e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 579,  Mean reward: 8.0, Mean Entropy: 0.00012868677731603384, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.64s
Iteration: 580,  Mean reward: 8.0, Mean Entropy: 0.00010621986439218745, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 581,  Mean reward: 8.0, Mean Entropy: 0.0001224549487233162, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 582,  Mean reward: 8.0, Mean Entropy: 0.00014240531891118735, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.65s
Iteration: 583,  Mean reward: 8.0, Mean Entropy: 0.0001133638434112072, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 584,  Mean reward: 8.0, Mean Entropy: 0.00011792895384132862, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 585,  Mean reward: 8.0, Mean Entropy: 0.0001605026045581326, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 586,  Mean reward: 8.0, Mean Entropy: 0.00015347808948718011, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.64s
Iteration: 587,  Mean reward: 8.0, Mean Entropy: 0.00014641604502685368, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 588,  Mean reward: 8.0, Mean Entropy: 0.00017589516937732697, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.63s
Iteration: 589,  Mean reward: 8.0, Mean Entropy: 0.0001515724288765341, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 590,  Mean reward: 8.0, Mean Entropy: 0.0002165631449315697, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 591,  Mean reward: 8.0, Mean Entropy: 0.00023554118524771184, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Iteration: 592,  Mean reward: 8.0, Mean Entropy: 0.00021803808340337127, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.64s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=8, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=8, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.23it/s]100%|██████████| 1/1 [00:00<00:00,  1.23it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 3 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.5 s_ (3, 1)
  s:             (3, 1) a 3 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 4 action_probs [0.0 0.0 0.0 0.1 0.9 0.0 0.0 0.0] r -1.0 s_ (4, 2)
  s:             (4, 2) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 5 steps, Captured: True Reward: -16.5


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.1 0.9 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 3.50 +/- 1.50
   Lengths :[5 2]
Average return: -4.25 +/- 12.25
   Returns :[-16.5 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -4.25
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_ustack
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_ustack/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=8, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=8, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.64it/s]100%|██████████| 1/1 [00:00<00:00,  1.64it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 4 action_probs [0.0 0.0 0.0 0.1 0.8 0.1 0.0 0.0] r -1.5 s_ (4, 2)
  s:             (4, 2) a 3 action_probs [0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0] r -1.0 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  Done after 5 steps, Captured: False Reward: -6.5


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 3.50 +/- 1.50
   Lengths :[5 2]
Average return: 0.75 +/- 7.25
   Returns :[-6.5 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 0.75
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_ustack
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_ustack/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=8, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, node_dim=8, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.87it/s]100%|██████████| 1/1 [00:00<00:00,  1.87it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_ustack
edge_blocking True
solve_select solvable
qnet gat2
critic q
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-q/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_ustack/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: 1.5
  std over seeds: 5.029082089871538
  per seed: [-4.250 0.750 8.000]

success_rate.......
  avg over seeds: 0.6666666666666666
  std over seeds: 0.23570226039551584
  per seed: [0.500 0.500 1.000]

