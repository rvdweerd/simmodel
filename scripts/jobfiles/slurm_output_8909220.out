batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: Dual
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: v
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
critic: v
node_dim: 7
lstm_on: True
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy(
  (FE): FeatureExtractor(
    (gat): GATv2(7, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (lstm): LSTM(24, 24)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (lstm): LSTM(24, 24)
    (theta8_v): Linear(in_features=24, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with A+C LSTMs and GATv2 feature extraction
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.lstm.weight_ih_l0     [96, 24]     requires_grad=True
PI.lstm.weight_hh_l0     [96, 24]     requires_grad=True
PI.lstm.bias_ih_l0       [96]         requires_grad=True
PI.lstm.bias_hh_l0       [96]         requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.lstm.weight_ih_l0      [96, 24]     requires_grad=True
V.lstm.weight_hh_l0      [96, 24]     requires_grad=True
V.lstm.bias_ih_l0        [96]         requires_grad=True
V.lstm.bias_hh_l0        [96]         requires_grad=True
V.theta8_v.weight        [1, 24]      requires_grad=True
V.theta8_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 16298
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -2.302325581395349, Mean Entropy: 0.9675179719924927, complete_episode_count: 43.0, Gather time: 5.76s, Train time: 3.51s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -5.136363636363637, Mean Entropy: 0.9747382402420044, complete_episode_count: 44.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 2,  Mean reward: -3.0952380952380953, Mean Entropy: 0.9530773162841797, complete_episode_count: 42.0, Gather time: 0.60s, Train time: 1.55s
Iteration: 3,  Mean reward: -7.2625, Mean Entropy: 0.9819585680961609, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.54s
Iteration: 4,  Mean reward: -6.604651162790698, Mean Entropy: 0.9530773162841797, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 5,  Mean reward: -4.775, Mean Entropy: 0.9747382402420044, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 6,  Mean reward: -0.5957446808510638, Mean Entropy: 0.8953150510787964, complete_episode_count: 47.0, Gather time: 0.60s, Train time: 1.54s
Iteration: 7,  Mean reward: -7.171052631578948, Mean Entropy: 0.9314162135124207, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 8,  Mean reward: -6.404761904761905, Mean Entropy: 0.9458567500114441, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 9,  Mean reward: -4.7682926829268295, Mean Entropy: 0.9097546339035034, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 10,  Mean reward: -1.2045454545454546, Mean Entropy: 0.9241952896118164, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 11,  Mean reward: -6.409090909090909, Mean Entropy: 0.9169756174087524, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 12,  Mean reward: -1.7613636363636365, Mean Entropy: 0.9963985681533813, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 13,  Mean reward: -4.6976744186046515, Mean Entropy: 0.9963981509208679, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 14,  Mean reward: -2.7023809523809526, Mean Entropy: 0.9675168395042419, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 15,  Mean reward: -2.902439024390244, Mean Entropy: 0.9675154089927673, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 16,  Mean reward: -4.7976190476190474, Mean Entropy: 0.9314109086990356, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 17,  Mean reward: -4.878048780487805, Mean Entropy: 0.960284948348999, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 18,  Mean reward: -2.3068181818181817, Mean Entropy: 0.974716305732727, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 19,  Mean reward: -3.75, Mean Entropy: 0.9097404479980469, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 20,  Mean reward: -4.4523809523809526, Mean Entropy: 0.9963399767875671, complete_episode_count: 42.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 21,  Mean reward: -4.511904761904762, Mean Entropy: 0.9241148233413696, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 22,  Mean reward: -2.5568181818181817, Mean Entropy: 1.0105608701705933, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 23,  Mean reward: -5.5, Mean Entropy: 0.9308089017868042, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.79s
Iteration: 24,  Mean reward: -7.214285714285714, Mean Entropy: 0.9310152530670166, complete_episode_count: 42.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 25,  Mean reward: -4.939024390243903, Mean Entropy: 0.9655553102493286, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 26,  Mean reward: -4.209302325581396, Mean Entropy: 0.9071881175041199, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 27,  Mean reward: -4.666666666666667, Mean Entropy: 0.9733365774154663, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 28,  Mean reward: -5.726415094339623, Mean Entropy: 0.9948004484176636, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 29,  Mean reward: -3.8181818181818183, Mean Entropy: 0.9882634878158569, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 30,  Mean reward: -4.298076923076923, Mean Entropy: 0.9394776225090027, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 31,  Mean reward: -4.67, Mean Entropy: 0.9564062356948853, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 32,  Mean reward: -3.5, Mean Entropy: 0.9076846837997437, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 33,  Mean reward: -5.0, Mean Entropy: 0.7685432434082031, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.69s
Iteration: 34,  Mean reward: -1.9919354838709677, Mean Entropy: 0.8246415853500366, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 35,  Mean reward: -2.1271186440677967, Mean Entropy: 0.7115164995193481, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 36,  Mean reward: -1.9615384615384615, Mean Entropy: 0.7437605261802673, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 37,  Mean reward: -2.871212121212121, Mean Entropy: 0.709459662437439, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 38,  Mean reward: -2.6666666666666665, Mean Entropy: 0.6268629431724548, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 39,  Mean reward: -0.6911764705882353, Mean Entropy: 0.8223028182983398, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 40,  Mean reward: -3.306451612903226, Mean Entropy: 0.7595227956771851, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 41,  Mean reward: -2.5785714285714287, Mean Entropy: 0.7152522802352905, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 42,  Mean reward: -4.216666666666667, Mean Entropy: 0.6845940947532654, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 43,  Mean reward: 0.5384615384615384, Mean Entropy: 0.7499618530273438, complete_episode_count: 65.0, Gather time: 0.74s, Train time: 0.78s
Iteration: 44,  Mean reward: -2.08955223880597, Mean Entropy: 0.6426502466201782, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 45,  Mean reward: -2.8333333333333335, Mean Entropy: 0.6221811771392822, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 46,  Mean reward: -1.4057971014492754, Mean Entropy: 0.6216721534729004, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 47,  Mean reward: -3.36, Mean Entropy: 0.5821447372436523, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 48,  Mean reward: -0.7205882352941176, Mean Entropy: 0.6694858074188232, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 49,  Mean reward: -1.3958333333333333, Mean Entropy: 0.5961155891418457, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 50,  Mean reward: 1.5298507462686568, Mean Entropy: 0.537788987159729, complete_episode_count: 67.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 51,  Mean reward: 0.6690140845070423, Mean Entropy: 0.4938705563545227, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 52,  Mean reward: -0.04929577464788732, Mean Entropy: 0.47351786494255066, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 53,  Mean reward: 0.25, Mean Entropy: 0.533012866973877, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 54,  Mean reward: 2.4305555555555554, Mean Entropy: 0.4851522445678711, complete_episode_count: 72.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 55,  Mean reward: 1.0416666666666667, Mean Entropy: 0.4694109857082367, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 56,  Mean reward: 2.084507042253521, Mean Entropy: 0.5093421936035156, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 57,  Mean reward: 0.9420289855072463, Mean Entropy: 0.5650540590286255, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 58,  Mean reward: 2.955223880597015, Mean Entropy: 0.5325275659561157, complete_episode_count: 67.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 59,  Mean reward: 1.7463768115942029, Mean Entropy: 0.5613579750061035, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 60,  Mean reward: 1.6544117647058822, Mean Entropy: 0.5255113244056702, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 61,  Mean reward: 2.65625, Mean Entropy: 0.5891749858856201, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 1.00s
Iteration: 62,  Mean reward: 1.046875, Mean Entropy: 0.6478942632675171, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 63,  Mean reward: 1.540983606557377, Mean Entropy: 0.5700847506523132, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 1.54s
Iteration: 64,  Mean reward: 2.4130434782608696, Mean Entropy: 0.612345278263092, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 65,  Mean reward: 1.1825396825396826, Mean Entropy: 0.6244663596153259, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 66,  Mean reward: -0.29365079365079366, Mean Entropy: 0.5926535129547119, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 67,  Mean reward: 1.628787878787879, Mean Entropy: 0.5703406929969788, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 68,  Mean reward: 2.146153846153846, Mean Entropy: 0.5098875761032104, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 69,  Mean reward: 3.376923076923077, Mean Entropy: 0.40855321288108826, complete_episode_count: 65.0, Gather time: 0.63s, Train time: 0.82s
Iteration: 70,  Mean reward: 2.2928571428571427, Mean Entropy: 0.38974854350090027, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 71,  Mean reward: 2.0, Mean Entropy: 0.5731768012046814, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 72,  Mean reward: 0.3790322580645161, Mean Entropy: 0.6293514966964722, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 73,  Mean reward: 2.2205882352941178, Mean Entropy: 0.48351120948791504, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 74,  Mean reward: 2.2426470588235294, Mean Entropy: 0.5564997792243958, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 75,  Mean reward: 1.2890625, Mean Entropy: 0.5562571287155151, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 76,  Mean reward: 4.143939393939394, Mean Entropy: 0.45871782302856445, complete_episode_count: 66.0, Gather time: 0.62s, Train time: 0.75s
Iteration: 77,  Mean reward: 1.9794520547945205, Mean Entropy: 0.5212846994400024, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 78,  Mean reward: 3.463235294117647, Mean Entropy: 0.4649699628353119, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 79,  Mean reward: 1.4044117647058822, Mean Entropy: 0.5274219512939453, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 80,  Mean reward: 0.27611940298507465, Mean Entropy: 0.5694559812545776, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 81,  Mean reward: 3.707692307692308, Mean Entropy: 0.6105844378471375, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 82,  Mean reward: 2.661290322580645, Mean Entropy: 0.532045304775238, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 83,  Mean reward: 2.971014492753623, Mean Entropy: 0.5500686168670654, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 84,  Mean reward: 1.5076923076923077, Mean Entropy: 0.5211730599403381, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 85,  Mean reward: 3.515151515151515, Mean Entropy: 0.49593186378479004, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 86,  Mean reward: 4.243243243243243, Mean Entropy: 0.4233698844909668, complete_episode_count: 74.0, Gather time: 0.62s, Train time: 0.75s
Iteration: 87,  Mean reward: 3.4527027027027026, Mean Entropy: 0.4067550599575043, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 88,  Mean reward: 3.472972972972973, Mean Entropy: 0.3981606662273407, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 89,  Mean reward: 3.3904109589041096, Mean Entropy: 0.3639141023159027, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 90,  Mean reward: 3.4675324675324677, Mean Entropy: 0.2988331615924835, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 91,  Mean reward: 4.7368421052631575, Mean Entropy: 0.28868091106414795, complete_episode_count: 76.0, Gather time: 0.64s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 92,  Mean reward: 5.358974358974359, Mean Entropy: 0.2715739607810974, complete_episode_count: 78.0, Gather time: 0.65s, Train time: 0.82s
Iteration: 93,  Mean reward: 4.966666666666667, Mean Entropy: 0.29363131523132324, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 94,  Mean reward: 5.473333333333334, Mean Entropy: 0.2656920552253723, complete_episode_count: 75.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 95,  Mean reward: 5.026315789473684, Mean Entropy: 0.24985456466674805, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 96,  Mean reward: 5.084415584415584, Mean Entropy: 0.2125738263130188, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 97,  Mean reward: 5.910256410256411, Mean Entropy: 0.1994587630033493, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 98,  Mean reward: 6.304054054054054, Mean Entropy: 0.21232931315898895, complete_episode_count: 74.0, Gather time: 0.62s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 99,  Mean reward: 7.25, Mean Entropy: 0.13061273097991943, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 100,  Mean reward: 7.474683544303797, Mean Entropy: 0.13676008582115173, complete_episode_count: 79.0, Gather time: 0.63s, Train time: 0.97s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 7.474683544303797, Mean Entropy: 0.17385880649089813, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 102,  Mean reward: 7.448717948717949, Mean Entropy: 0.09894940257072449, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 103,  Mean reward: 6.67948717948718, Mean Entropy: 0.068511001765728, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 104,  Mean reward: 7.75, Mean Entropy: 0.053941063582897186, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 105,  Mean reward: 7.25, Mean Entropy: 0.04255010932683945, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 106,  Mean reward: 7.705128205128205, Mean Entropy: 0.12890943884849548, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 107,  Mean reward: 7.75, Mean Entropy: 0.10570021718740463, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 108,  Mean reward: 7.941558441558442, Mean Entropy: 0.05925862491130829, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 109,  Mean reward: 7.75, Mean Entropy: 0.11820467561483383, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 110,  Mean reward: 7.657894736842105, Mean Entropy: 0.11360593140125275, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 111,  Mean reward: 6.25, Mean Entropy: 0.0422557108104229, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 112,  Mean reward: 8.0, Mean Entropy: 0.030725467950105667, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 113,  Mean reward: 7.75, Mean Entropy: 0.023055914789438248, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 114,  Mean reward: 8.0, Mean Entropy: 0.019330304116010666, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 115,  Mean reward: 7.981012658227848, Mean Entropy: 0.021266063675284386, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 116,  Mean reward: 8.0, Mean Entropy: 0.031991779804229736, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 117,  Mean reward: 7.727848101265823, Mean Entropy: 0.025585226714611053, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 118,  Mean reward: 7.961538461538462, Mean Entropy: 0.02184969000518322, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 119,  Mean reward: 7.75, Mean Entropy: 0.018226278945803642, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 120,  Mean reward: 8.0, Mean Entropy: 0.01443114411085844, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 121,  Mean reward: 8.0, Mean Entropy: 0.012579098343849182, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 122,  Mean reward: 8.0, Mean Entropy: 0.012357231229543686, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 123,  Mean reward: 8.0, Mean Entropy: 0.011494363658130169, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 124,  Mean reward: 8.0, Mean Entropy: 0.010243913158774376, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 125,  Mean reward: 8.0, Mean Entropy: 0.01063881628215313, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 126,  Mean reward: 8.0, Mean Entropy: 0.010189925320446491, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 127,  Mean reward: 8.0, Mean Entropy: 0.009765435941517353, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 128,  Mean reward: 8.0, Mean Entropy: 0.008926755748689175, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 129,  Mean reward: 8.0, Mean Entropy: 0.009395076893270016, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 130,  Mean reward: 8.0, Mean Entropy: 0.00902293436229229, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 131,  Mean reward: 8.0, Mean Entropy: 0.008176283910870552, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 132,  Mean reward: 8.0, Mean Entropy: 0.008536032401025295, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 133,  Mean reward: 8.0, Mean Entropy: 0.00831296294927597, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 134,  Mean reward: 8.0, Mean Entropy: 0.008178578689694405, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 135,  Mean reward: 8.0, Mean Entropy: 0.007715105079114437, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 136,  Mean reward: 7.981012658227848, Mean Entropy: 0.00973972212523222, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 137,  Mean reward: 8.0, Mean Entropy: 0.011278476566076279, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 138,  Mean reward: 8.0, Mean Entropy: 0.008767390623688698, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.12s
Iteration: 139,  Mean reward: 7.75, Mean Entropy: 0.22725477814674377, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.78s
Iteration: 140,  Mean reward: 6.425373134328358, Mean Entropy: 0.16718046367168427, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 141,  Mean reward: 3.930379746835443, Mean Entropy: 0.0370354950428009, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 142,  Mean reward: 7.5, Mean Entropy: 0.09198781102895737, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 143,  Mean reward: 7.474683544303797, Mean Entropy: 0.012756414711475372, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 144,  Mean reward: 7.75, Mean Entropy: 0.012341714464128017, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 145,  Mean reward: 7.981012658227848, Mean Entropy: 0.024830777198076248, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 146,  Mean reward: 8.0, Mean Entropy: 0.5640627145767212, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 147,  Mean reward: 0.55, Mean Entropy: 0.680294930934906, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.58s
Iteration: 148,  Mean reward: -0.5373134328358209, Mean Entropy: 0.6637663841247559, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 149,  Mean reward: -1.0857142857142856, Mean Entropy: 0.5666642785072327, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 150,  Mean reward: 1.1666666666666667, Mean Entropy: 0.4212743043899536, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 151,  Mean reward: 2.26, Mean Entropy: 0.45064133405685425, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 152,  Mean reward: 2.6901408450704225, Mean Entropy: 0.3687470555305481, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 153,  Mean reward: 3.84, Mean Entropy: 0.33251190185546875, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 154,  Mean reward: 3.985294117647059, Mean Entropy: 0.4299851357936859, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 155,  Mean reward: 3.7083333333333335, Mean Entropy: 0.39884400367736816, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 156,  Mean reward: 2.4583333333333335, Mean Entropy: 0.4303264617919922, complete_episode_count: 72.0, Gather time: 0.62s, Train time: 0.74s
Iteration: 157,  Mean reward: 2.236111111111111, Mean Entropy: 0.37784910202026367, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 158,  Mean reward: 1.8958333333333333, Mean Entropy: 0.39069828391075134, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 159,  Mean reward: 2.2205882352941178, Mean Entropy: 0.3774986267089844, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 160,  Mean reward: 0.7638888888888888, Mean Entropy: 0.40752753615379333, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 161,  Mean reward: 1.5405405405405406, Mean Entropy: 0.36394643783569336, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 162,  Mean reward: 2.141891891891892, Mean Entropy: 0.32054540514945984, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 163,  Mean reward: 3.239130434782609, Mean Entropy: 0.355207622051239, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 164,  Mean reward: 2.992857142857143, Mean Entropy: 0.32598018646240234, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 165,  Mean reward: 3.493150684931507, Mean Entropy: 0.3189658224582672, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 166,  Mean reward: 3.5933333333333333, Mean Entropy: 0.2853301465511322, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 167,  Mean reward: 3.25, Mean Entropy: 0.27737581729888916, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 168,  Mean reward: 4.694444444444445, Mean Entropy: 0.29183220863342285, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 169,  Mean reward: 7.898648648648648, Mean Entropy: 0.07648590952157974, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 170,  Mean reward: 7.0, Mean Entropy: 0.020653847604990005, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 171,  Mean reward: 7.981012658227848, Mean Entropy: 0.03161470219492912, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 172,  Mean reward: 7.75, Mean Entropy: 0.01067414041608572, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 173,  Mean reward: 8.0, Mean Entropy: 0.005027410574257374, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 174,  Mean reward: 8.0, Mean Entropy: 0.004715644288808107, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 175,  Mean reward: 8.0, Mean Entropy: 0.004161243326961994, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 176,  Mean reward: 8.0, Mean Entropy: 0.002658053068444133, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.01s
Iteration: 177,  Mean reward: 8.0, Mean Entropy: 0.0028294739313423634, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 178,  Mean reward: 8.0, Mean Entropy: 0.0023101952392607927, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 179,  Mean reward: 8.0, Mean Entropy: 0.0021323205437511206, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 180,  Mean reward: 8.0, Mean Entropy: 0.002395437564700842, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 181,  Mean reward: 8.0, Mean Entropy: 0.0018951086094602942, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 182,  Mean reward: 8.0, Mean Entropy: 0.0019267778843641281, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 183,  Mean reward: 8.0, Mean Entropy: 0.001918717985972762, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 184,  Mean reward: 8.0, Mean Entropy: 0.001655462896451354, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 185,  Mean reward: 8.0, Mean Entropy: 0.0014317238237708807, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 186,  Mean reward: 8.0, Mean Entropy: 0.001519202021881938, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 187,  Mean reward: 8.0, Mean Entropy: 0.0015027178451418877, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 188,  Mean reward: 8.0, Mean Entropy: 0.0013266311725601554, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 189,  Mean reward: 8.0, Mean Entropy: 0.0013370038941502571, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 190,  Mean reward: 8.0, Mean Entropy: 0.001387048396281898, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 191,  Mean reward: 8.0, Mean Entropy: 0.0014317450113594532, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 192,  Mean reward: 8.0, Mean Entropy: 0.0018203777726739645, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 193,  Mean reward: 8.0, Mean Entropy: 0.0025086209643632174, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 194,  Mean reward: 8.0, Mean Entropy: 0.012399250641465187, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 195,  Mean reward: 7.75, Mean Entropy: 0.0014994058292359114, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 196,  Mean reward: 8.0, Mean Entropy: 0.4528864622116089, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 197,  Mean reward: 3.287878787878788, Mean Entropy: 0.6953414678573608, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 198,  Mean reward: 1.4014084507042253, Mean Entropy: 0.5457193851470947, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 199,  Mean reward: 2.3, Mean Entropy: 0.49623554944992065, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 200,  Mean reward: 4.113333333333333, Mean Entropy: 0.43845027685165405, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 1.7013888888888888, Mean Entropy: 0.5533134341239929, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 202,  Mean reward: 3.4527027027027026, Mean Entropy: 0.36688828468322754, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.93s
Iteration: 203,  Mean reward: 3.3066666666666666, Mean Entropy: 0.4018971920013428, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 204,  Mean reward: 2.2837837837837838, Mean Entropy: 0.4666014611721039, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 205,  Mean reward: 4.273333333333333, Mean Entropy: 0.17858460545539856, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 206,  Mean reward: 6.67948717948718, Mean Entropy: 0.02019783854484558, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 207,  Mean reward: 8.0, Mean Entropy: 0.007751786150038242, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 208,  Mean reward: 8.0, Mean Entropy: 0.020760904997587204, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 209,  Mean reward: 7.25, Mean Entropy: 0.016291821375489235, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.82s
Iteration: 210,  Mean reward: 8.0, Mean Entropy: 0.4154728055000305, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 211,  Mean reward: 1.5916666666666666, Mean Entropy: 0.6309486627578735, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 212,  Mean reward: 0.6194029850746269, Mean Entropy: 0.5820741057395935, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 213,  Mean reward: 3.3493150684931505, Mean Entropy: 0.34591144323349, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 214,  Mean reward: 4.673611111111111, Mean Entropy: 0.20128273963928223, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.95s
Iteration: 215,  Mean reward: 5.955696202531645, Mean Entropy: 0.07096342742443085, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 216,  Mean reward: 7.5, Mean Entropy: 0.008415058255195618, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 217,  Mean reward: 8.0, Mean Entropy: 0.0010229945182800293, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 218,  Mean reward: 8.0, Mean Entropy: 0.001970029203221202, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 219,  Mean reward: 8.0, Mean Entropy: 0.0057809255085885525, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 220,  Mean reward: 7.75, Mean Entropy: 0.007134683430194855, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 221,  Mean reward: 8.0, Mean Entropy: 0.06476148962974548, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 222,  Mean reward: 6.67948717948718, Mean Entropy: 0.007953990250825882, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 223,  Mean reward: 8.0, Mean Entropy: 0.07036517560482025, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 224,  Mean reward: 6.75, Mean Entropy: 0.003054080531001091, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 225,  Mean reward: 8.0, Mean Entropy: 0.027097171172499657, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 226,  Mean reward: 7.75, Mean Entropy: 0.016607515513896942, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 227,  Mean reward: 7.5, Mean Entropy: 0.005757886916399002, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 228,  Mean reward: 8.0, Mean Entropy: 0.047875504940748215, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 229,  Mean reward: 8.0, Mean Entropy: 0.5246838331222534, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 230,  Mean reward: -1.2619047619047619, Mean Entropy: 0.7299053072929382, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 231,  Mean reward: -3.640625, Mean Entropy: 0.6848828792572021, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 232,  Mean reward: -0.6428571428571429, Mean Entropy: 0.6970049738883972, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 233,  Mean reward: 0.3, Mean Entropy: 0.603545069694519, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 234,  Mean reward: 2.3203125, Mean Entropy: 0.6016128063201904, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 235,  Mean reward: 0.2803030303030303, Mean Entropy: 0.5336353182792664, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 236,  Mean reward: 4.845070422535211, Mean Entropy: 0.23849737644195557, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 237,  Mean reward: 3.6025641025641026, Mean Entropy: 0.22518198192119598, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 238,  Mean reward: 6.208860759493671, Mean Entropy: 0.10064105689525604, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 239,  Mean reward: 8.0, Mean Entropy: 0.125139981508255, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 240,  Mean reward: 4.75, Mean Entropy: 0.18148159980773926, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 241,  Mean reward: 5.584415584415584, Mean Entropy: 0.1933038830757141, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 242,  Mean reward: 6.5, Mean Entropy: 0.08487975597381592, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 243,  Mean reward: 7.685897435897436, Mean Entropy: 0.08748582005500793, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 244,  Mean reward: 7.901315789473684, Mean Entropy: 0.023669391870498657, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 245,  Mean reward: 7.961538461538462, Mean Entropy: 0.1651589721441269, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 246,  Mean reward: 6.883116883116883, Mean Entropy: 0.025241045281291008, complete_episode_count: 77.0, Gather time: 0.69s, Train time: 0.82s
Iteration: 247,  Mean reward: 7.708860759493671, Mean Entropy: 0.0010159574449062347, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 248,  Mean reward: 8.0, Mean Entropy: 0.04893144592642784, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 249,  Mean reward: 8.0, Mean Entropy: 0.11123731732368469, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 250,  Mean reward: 6.185897435897436, Mean Entropy: 0.0009967437945306301, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 251,  Mean reward: 8.0, Mean Entropy: 0.004890909418463707, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 252,  Mean reward: 7.981012658227848, Mean Entropy: 0.0015751754399389029, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.92s
Iteration: 253,  Mean reward: 8.0, Mean Entropy: 0.004363652318716049, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 254,  Mean reward: 8.0, Mean Entropy: 0.009277606382966042, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 255,  Mean reward: 8.0, Mean Entropy: 0.03573271632194519, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 256,  Mean reward: 7.981012658227848, Mean Entropy: 0.09253750741481781, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 257,  Mean reward: -3.409090909090909, Mean Entropy: 0.5483757853507996, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 258,  Mean reward: 3.3285714285714287, Mean Entropy: 0.29458779096603394, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 259,  Mean reward: 6.828947368421052, Mean Entropy: 0.122406005859375, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 260,  Mean reward: 7.5, Mean Entropy: 0.007341439835727215, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 261,  Mean reward: 8.0, Mean Entropy: 0.19516144692897797, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 262,  Mean reward: 4.326086956521739, Mean Entropy: 0.3394205570220947, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 263,  Mean reward: 5.324675324675325, Mean Entropy: 0.012162654660642147, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 264,  Mean reward: 8.0, Mean Entropy: 0.05009869486093521, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 265,  Mean reward: 8.0, Mean Entropy: 0.127708300948143, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 266,  Mean reward: 4.25, Mean Entropy: 0.004202382639050484, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 267,  Mean reward: 8.0, Mean Entropy: 0.002287858398631215, complete_episode_count: 80.0, Gather time: 0.65s, Train time: 0.76s
Iteration: 268,  Mean reward: 8.0, Mean Entropy: 0.008216220885515213, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 269,  Mean reward: 8.0, Mean Entropy: 0.4202858805656433, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 270,  Mean reward: 1.8189655172413792, Mean Entropy: 0.6640546321868896, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 271,  Mean reward: 3.064516129032258, Mean Entropy: 0.554965615272522, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 272,  Mean reward: 4.554054054054054, Mean Entropy: 0.3847366273403168, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 273,  Mean reward: 5.732394366197183, Mean Entropy: 0.3573767840862274, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 274,  Mean reward: 7.006756756756757, Mean Entropy: 0.16084685921669006, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 275,  Mean reward: 7.04, Mean Entropy: 0.01674671098589897, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 276,  Mean reward: 7.9423076923076925, Mean Entropy: 0.0010727079352363944, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 277,  Mean reward: 8.0, Mean Entropy: 0.0026078736409544945, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 278,  Mean reward: 7.981012658227848, Mean Entropy: 0.0027172639966011047, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 279,  Mean reward: 8.0, Mean Entropy: 0.18419921398162842, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.76s
Iteration: 280,  Mean reward: 6.5, Mean Entropy: 0.014232742600142956, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 281,  Mean reward: 8.0, Mean Entropy: 0.2457299679517746, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 282,  Mean reward: 5.52054794520548, Mean Entropy: 0.21040713787078857, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 283,  Mean reward: 7.655405405405405, Mean Entropy: 0.06450128555297852, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 284,  Mean reward: 7.75, Mean Entropy: 0.011381130665540695, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 285,  Mean reward: 8.0, Mean Entropy: 0.17444291710853577, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 286,  Mean reward: 4.217105263157895, Mean Entropy: 0.19379472732543945, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 287,  Mean reward: 4.943037974683544, Mean Entropy: 0.1352919042110443, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 288,  Mean reward: 7.5, Mean Entropy: 0.07357411086559296, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 289,  Mean reward: 7.474683544303797, Mean Entropy: 0.14231964945793152, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 290,  Mean reward: 7.9423076923076925, Mean Entropy: 0.14905191957950592, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 291,  Mean reward: 6.0, Mean Entropy: 0.09666943550109863, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 292,  Mean reward: 6.715189873417722, Mean Entropy: 0.02362976223230362, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 293,  Mean reward: 8.0, Mean Entropy: 0.17567133903503418, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 294,  Mean reward: 5.634615384615385, Mean Entropy: 0.18495160341262817, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 295,  Mean reward: 7.326666666666667, Mean Entropy: 0.08480663597583771, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 296,  Mean reward: 7.941558441558442, Mean Entropy: 0.005657976493239403, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 297,  Mean reward: 8.0, Mean Entropy: 0.42590898275375366, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 298,  Mean reward: 0.5526315789473685, Mean Entropy: 0.64397132396698, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 299,  Mean reward: 1.1825396825396826, Mean Entropy: 0.6299129724502563, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 300,  Mean reward: -3.6557377049180326, Mean Entropy: 0.7379450798034668, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 0.76s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -4.088709677419355, Mean Entropy: 0.748853862285614, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 302,  Mean reward: 0.6307692307692307, Mean Entropy: 1.0436049699783325, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 303,  Mean reward: -1.73, Mean Entropy: 0.8456950187683105, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 304,  Mean reward: -4.694444444444445, Mean Entropy: 0.9494923949241638, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 305,  Mean reward: -3.0113636363636362, Mean Entropy: 0.910637378692627, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 306,  Mean reward: -3.339622641509434, Mean Entropy: 0.9555240869522095, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 307,  Mean reward: -0.9166666666666666, Mean Entropy: 0.8980234861373901, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 308,  Mean reward: -2.3596491228070176, Mean Entropy: 0.8612194061279297, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.48s
Iteration: 309,  Mean reward: -1.6929824561403508, Mean Entropy: 0.8139710426330566, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 310,  Mean reward: -0.868421052631579, Mean Entropy: 0.7502491474151611, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 311,  Mean reward: -1.6, Mean Entropy: 0.8061763644218445, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 312,  Mean reward: 2.9649122807017543, Mean Entropy: 0.7223925590515137, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 313,  Mean reward: 1.6101694915254237, Mean Entropy: 0.48622581362724304, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 314,  Mean reward: 2.7733333333333334, Mean Entropy: 0.3641897439956665, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 315,  Mean reward: 0.5333333333333333, Mean Entropy: 0.5518810749053955, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 316,  Mean reward: -0.4696969696969697, Mean Entropy: 0.539615273475647, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 317,  Mean reward: 5.753521126760563, Mean Entropy: 0.500150203704834, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 318,  Mean reward: 5.229166666666667, Mean Entropy: 0.31558912992477417, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 319,  Mean reward: 5.8441558441558445, Mean Entropy: 0.3625389337539673, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 320,  Mean reward: 4.021428571428571, Mean Entropy: 0.07133209705352783, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 321,  Mean reward: -1.25, Mean Entropy: 0.5312022566795349, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 322,  Mean reward: -3.025, Mean Entropy: 0.7530779838562012, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 323,  Mean reward: -4.224137931034483, Mean Entropy: 0.7979444861412048, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 324,  Mean reward: -1.5583333333333333, Mean Entropy: 0.7079594135284424, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 325,  Mean reward: -1.7076923076923076, Mean Entropy: 0.6855615377426147, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.92s
Iteration: 326,  Mean reward: -1.876923076923077, Mean Entropy: 0.708803117275238, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 327,  Mean reward: -4.258064516129032, Mean Entropy: 0.8122929334640503, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 328,  Mean reward: -3.261904761904762, Mean Entropy: 0.7326011657714844, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 329,  Mean reward: -3.871212121212121, Mean Entropy: 0.664071261882782, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 330,  Mean reward: -4.315384615384615, Mean Entropy: 0.6871992945671082, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 331,  Mean reward: -0.5606060606060606, Mean Entropy: 0.7103540301322937, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 332,  Mean reward: -2.134920634920635, Mean Entropy: 0.721960723400116, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 333,  Mean reward: -2.65625, Mean Entropy: 0.7221209406852722, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 334,  Mean reward: -4.4453125, Mean Entropy: 0.7450279593467712, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 335,  Mean reward: -3.6311475409836067, Mean Entropy: 0.7336190938949585, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 336,  Mean reward: -2.357142857142857, Mean Entropy: 0.7448534369468689, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 337,  Mean reward: -5.274647887323944, Mean Entropy: 0.6229833364486694, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 338,  Mean reward: -2.2681159420289854, Mean Entropy: 0.6506250500679016, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 339,  Mean reward: -2.976923076923077, Mean Entropy: 0.5622478723526001, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 340,  Mean reward: -2.361842105263158, Mean Entropy: 0.4803277850151062, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 341,  Mean reward: -1.2066666666666668, Mean Entropy: 0.4724998474121094, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 342,  Mean reward: -5.689873417721519, Mean Entropy: 0.5612286329269409, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 343,  Mean reward: -3.5555555555555554, Mean Entropy: 0.7555809020996094, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 344,  Mean reward: -2.1666666666666665, Mean Entropy: 0.7100545763969421, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 345,  Mean reward: -2.1048387096774195, Mean Entropy: 0.7330085635185242, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 346,  Mean reward: -0.19230769230769232, Mean Entropy: 0.6877044439315796, complete_episode_count: 65.0, Gather time: 0.75s, Train time: 0.77s
Iteration: 347,  Mean reward: -2.265625, Mean Entropy: 0.7341920733451843, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 348,  Mean reward: -4.275, Mean Entropy: 0.7464424967765808, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 349,  Mean reward: -4.0, Mean Entropy: 0.7225670218467712, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 350,  Mean reward: -2.703125, Mean Entropy: 0.721211850643158, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 351,  Mean reward: -2.921875, Mean Entropy: 0.675377368927002, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 352,  Mean reward: -2.7222222222222223, Mean Entropy: 0.7211048007011414, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 353,  Mean reward: -1.4296875, Mean Entropy: 0.7325815558433533, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 354,  Mean reward: -1.7222222222222223, Mean Entropy: 0.721333920955658, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.81s
Iteration: 355,  Mean reward: -1.1825396825396826, Mean Entropy: 0.7216514348983765, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 356,  Mean reward: -1.3046875, Mean Entropy: 0.699033260345459, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 357,  Mean reward: -3.088235294117647, Mean Entropy: 0.7102820873260498, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 358,  Mean reward: -1.9444444444444444, Mean Entropy: 0.7216863036155701, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 359,  Mean reward: -1.598360655737705, Mean Entropy: 0.7216004133224487, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 360,  Mean reward: -2.484375, Mean Entropy: 0.7101188898086548, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 361,  Mean reward: -3.757575757575758, Mean Entropy: 0.6757199764251709, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 362,  Mean reward: -3.338709677419355, Mean Entropy: 0.7439338564872742, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 363,  Mean reward: -1.6363636363636365, Mean Entropy: 0.6638244986534119, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 364,  Mean reward: -3.7419354838709675, Mean Entropy: 0.7551608085632324, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 365,  Mean reward: -3.8968253968253967, Mean Entropy: 0.7093881368637085, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.96s
Iteration: 366,  Mean reward: -3.6507936507936507, Mean Entropy: 0.6980915069580078, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.87s
Iteration: 367,  Mean reward: -2.7983870967741935, Mean Entropy: 0.7324624061584473, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 368,  Mean reward: -3.4285714285714284, Mean Entropy: 0.7095283269882202, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 369,  Mean reward: -3.953125, Mean Entropy: 0.755351185798645, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 370,  Mean reward: -1.3333333333333333, Mean Entropy: 0.7782194018363953, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 371,  Mean reward: -2.346774193548387, Mean Entropy: 0.7429441213607788, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 372,  Mean reward: -0.5, Mean Entropy: 0.45678719878196716, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 373,  Mean reward: -2.75, Mean Entropy: 0.5245170593261719, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 374,  Mean reward: -2.536231884057971, Mean Entropy: 0.6173926591873169, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 375,  Mean reward: -0.6714285714285714, Mean Entropy: 0.5269457101821899, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 376,  Mean reward: -1.78, Mean Entropy: 0.3577379584312439, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 377,  Mean reward: -1.25, Mean Entropy: 0.4600277841091156, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.83s
Iteration: 378,  Mean reward: -2.532051282051282, Mean Entropy: 0.5370938181877136, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 379,  Mean reward: -2.6194029850746268, Mean Entropy: 0.6238390803337097, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 380,  Mean reward: -3.6056338028169015, Mean Entropy: 0.6520347595214844, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 381,  Mean reward: -3.6597222222222223, Mean Entropy: 0.5859606266021729, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 382,  Mean reward: -2.463768115942029, Mean Entropy: 0.6801693439483643, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 383,  Mean reward: -1.2205882352941178, Mean Entropy: 0.590630054473877, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 384,  Mean reward: -1.412162162162162, Mean Entropy: 0.3678940236568451, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 385,  Mean reward: -2.25, Mean Entropy: 0.32885581254959106, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 386,  Mean reward: -1.75, Mean Entropy: 0.3502364158630371, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 387,  Mean reward: -1.8924050632911393, Mean Entropy: 0.3521270751953125, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 388,  Mean reward: -3.25, Mean Entropy: 0.3421027660369873, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 389,  Mean reward: -2.75, Mean Entropy: 0.33214348554611206, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 390,  Mean reward: -1.25, Mean Entropy: 0.4043130576610565, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 391,  Mean reward: -0.6688311688311688, Mean Entropy: 0.37803587317466736, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 392,  Mean reward: -2.25, Mean Entropy: 0.5225896835327148, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 393,  Mean reward: -1.792857142857143, Mean Entropy: 0.629156231880188, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 394,  Mean reward: 0.20945945945945946, Mean Entropy: 0.4654012620449066, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 395,  Mean reward: -0.12025316455696203, Mean Entropy: 0.5038780570030212, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 396,  Mean reward: -1.5723684210526316, Mean Entropy: 0.44695794582366943, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 397,  Mean reward: -1.8924050632911393, Mean Entropy: 0.445717990398407, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 398,  Mean reward: -2.727272727272727, Mean Entropy: 0.5643370151519775, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.76s
Iteration: 399,  Mean reward: -3.6769230769230767, Mean Entropy: 0.6919949054718018, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 400,  Mean reward: -2.6538461538461537, Mean Entropy: 0.7318296432495117, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -4.204918032786885, Mean Entropy: 0.8143255710601807, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 402,  Mean reward: -2.153225806451613, Mean Entropy: 0.725084662437439, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 403,  Mean reward: -1.5606060606060606, Mean Entropy: 0.6778217554092407, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 404,  Mean reward: -3.4846153846153847, Mean Entropy: 0.708651065826416, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 405,  Mean reward: -2.441666666666667, Mean Entropy: 0.7409467101097107, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 406,  Mean reward: -5.598484848484849, Mean Entropy: 0.7044321894645691, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 407,  Mean reward: -2.142857142857143, Mean Entropy: 0.7580363154411316, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 408,  Mean reward: -3.6746031746031744, Mean Entropy: 0.7212445139884949, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 409,  Mean reward: -3.8688524590163933, Mean Entropy: 0.7665531039237976, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 410,  Mean reward: -0.44696969696969696, Mean Entropy: 0.6866297721862793, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 411,  Mean reward: -1.0, Mean Entropy: 0.7304109334945679, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 412,  Mean reward: -1.4296875, Mean Entropy: 0.6738568544387817, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 413,  Mean reward: 1.2611940298507462, Mean Entropy: 0.46150100231170654, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 414,  Mean reward: 3.0538461538461537, Mean Entropy: 0.4862043261528015, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 415,  Mean reward: 2.4714285714285715, Mean Entropy: 0.5195476412773132, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 416,  Mean reward: 2.753623188405797, Mean Entropy: 0.4904453754425049, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 417,  Mean reward: 4.3933333333333335, Mean Entropy: 0.18788431584835052, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 418,  Mean reward: 3.5, Mean Entropy: 0.18535786867141724, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 419,  Mean reward: 5.891025641025641, Mean Entropy: 0.16251985728740692, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 420,  Mean reward: 7.474683544303797, Mean Entropy: 0.18042632937431335, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 421,  Mean reward: 7.402597402597403, Mean Entropy: 0.1695796251296997, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 422,  Mean reward: 7.618421052631579, Mean Entropy: 0.1503087282180786, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 423,  Mean reward: 7.981012658227848, Mean Entropy: 0.2571834921836853, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 424,  Mean reward: 6.966216216216216, Mean Entropy: 0.2647334635257721, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 425,  Mean reward: 6.854166666666667, Mean Entropy: 0.36785265803337097, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 426,  Mean reward: 6.253846153846154, Mean Entropy: 0.4195573329925537, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 427,  Mean reward: 6.949275362318841, Mean Entropy: 0.3819034993648529, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 428,  Mean reward: 5.96875, Mean Entropy: 0.3873112201690674, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 429,  Mean reward: 6.440298507462686, Mean Entropy: 0.339519739151001, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 430,  Mean reward: 6.276923076923077, Mean Entropy: 0.3686663508415222, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 431,  Mean reward: 6.319672131147541, Mean Entropy: 0.4125101566314697, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 432,  Mean reward: 6.595238095238095, Mean Entropy: 0.3177810609340668, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 433,  Mean reward: 7.053846153846154, Mean Entropy: 0.2679674029350281, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 434,  Mean reward: 6.811594202898551, Mean Entropy: 0.3009090721607208, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 435,  Mean reward: 6.758064516129032, Mean Entropy: 0.34394773840904236, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 436,  Mean reward: 7.0, Mean Entropy: 0.39767253398895264, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 437,  Mean reward: 6.884615384615385, Mean Entropy: 0.4175759255886078, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 438,  Mean reward: 5.841269841269841, Mean Entropy: 0.41209346055984497, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 439,  Mean reward: 7.242424242424242, Mean Entropy: 0.35388457775115967, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 440,  Mean reward: 7.409090909090909, Mean Entropy: 0.2996063232421875, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 441,  Mean reward: 6.484848484848484, Mean Entropy: 0.36709147691726685, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 442,  Mean reward: 6.185483870967742, Mean Entropy: 0.4208357036113739, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 443,  Mean reward: 7.264705882352941, Mean Entropy: 0.32438337802886963, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 444,  Mean reward: 6.634920634920635, Mean Entropy: 0.34914106130599976, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.92s
Iteration: 445,  Mean reward: 7.037313432835821, Mean Entropy: 0.4239077568054199, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 446,  Mean reward: 7.635714285714286, Mean Entropy: 0.28927621245384216, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 447,  Mean reward: 6.887096774193548, Mean Entropy: 0.3691592812538147, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 448,  Mean reward: 7.123076923076923, Mean Entropy: 0.36798804998397827, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 449,  Mean reward: 6.477941176470588, Mean Entropy: 0.307303786277771, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 450,  Mean reward: 6.130769230769231, Mean Entropy: 0.3746277689933777, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 451,  Mean reward: 7.464285714285714, Mean Entropy: 0.3061003088951111, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 452,  Mean reward: 7.25, Mean Entropy: 0.2461595982313156, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 453,  Mean reward: 6.463235294117647, Mean Entropy: 0.3142925500869751, complete_episode_count: 68.0, Gather time: 0.75s, Train time: 0.77s
Iteration: 454,  Mean reward: 7.203125, Mean Entropy: 0.36411261558532715, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 455,  Mean reward: 6.731343283582089, Mean Entropy: 0.32366296648979187, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 456,  Mean reward: 7.291666666666667, Mean Entropy: 0.35155054926872253, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 457,  Mean reward: 5.063492063492063, Mean Entropy: 0.4444957673549652, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 458,  Mean reward: 7.1923076923076925, Mean Entropy: 0.3307082951068878, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 459,  Mean reward: 7.076923076923077, Mean Entropy: 0.3535749316215515, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 460,  Mean reward: 6.7, Mean Entropy: 0.4116687774658203, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 461,  Mean reward: 7.113636363636363, Mean Entropy: 0.2902326285839081, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 462,  Mean reward: 7.2835820895522385, Mean Entropy: 0.28579282760620117, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 463,  Mean reward: 7.1, Mean Entropy: 0.320672869682312, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 464,  Mean reward: 6.204545454545454, Mean Entropy: 0.2889882028102875, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 465,  Mean reward: 7.4, Mean Entropy: 0.29719918966293335, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 466,  Mean reward: 7.161538461538462, Mean Entropy: 0.24490754306316376, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 467,  Mean reward: 6.992647058823529, Mean Entropy: 0.29807889461517334, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 468,  Mean reward: 7.022388059701493, Mean Entropy: 0.29534441232681274, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 469,  Mean reward: 5.358333333333333, Mean Entropy: 0.36620569229125977, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 470,  Mean reward: 7.6571428571428575, Mean Entropy: 0.2526167631149292, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 471,  Mean reward: 6.9921875, Mean Entropy: 0.3318684697151184, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 472,  Mean reward: 7.3671875, Mean Entropy: 0.36596640944480896, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 473,  Mean reward: 7.2727272727272725, Mean Entropy: 0.4191243350505829, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 474,  Mean reward: 6.1, Mean Entropy: 0.4058254659175873, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 475,  Mean reward: 6.677966101694915, Mean Entropy: 0.4425429105758667, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 476,  Mean reward: 6.461538461538462, Mean Entropy: 0.35385164618492126, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 477,  Mean reward: 6.714285714285714, Mean Entropy: 0.4124935567378998, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 478,  Mean reward: 7.521739130434782, Mean Entropy: 0.3309977948665619, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 479,  Mean reward: 7.0, Mean Entropy: 0.30152449011802673, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 480,  Mean reward: 6.573770491803279, Mean Entropy: 0.3850257396697998, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 481,  Mean reward: 7.3161764705882355, Mean Entropy: 0.44628581404685974, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 482,  Mean reward: 5.659090909090909, Mean Entropy: 0.38330671191215515, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 483,  Mean reward: 7.430555555555555, Mean Entropy: 0.2880512475967407, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 484,  Mean reward: 6.084615384615384, Mean Entropy: 0.5658403635025024, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.93s
Iteration: 485,  Mean reward: 5.388059701492537, Mean Entropy: 0.29587146639823914, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 486,  Mean reward: 6.555555555555555, Mean Entropy: 0.33065444231033325, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 487,  Mean reward: 6.838461538461538, Mean Entropy: 0.3785324990749359, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 488,  Mean reward: 6.7265625, Mean Entropy: 0.3765558898448944, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 489,  Mean reward: 6.880597014925373, Mean Entropy: 0.31507885456085205, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 490,  Mean reward: 6.420634920634921, Mean Entropy: 0.46201619505882263, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 491,  Mean reward: 5.061538461538461, Mean Entropy: 0.3993402421474457, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 492,  Mean reward: 7.0661764705882355, Mean Entropy: 0.308769166469574, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 493,  Mean reward: 7.360294117647059, Mean Entropy: 0.3235001564025879, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 494,  Mean reward: 4.849206349206349, Mean Entropy: 0.4170137345790863, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 495,  Mean reward: 7.426470588235294, Mean Entropy: 0.24652759730815887, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 496,  Mean reward: 6.17741935483871, Mean Entropy: 0.3200830817222595, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 497,  Mean reward: 7.007462686567164, Mean Entropy: 0.29790550470352173, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 498,  Mean reward: 7.015151515151516, Mean Entropy: 0.2746918797492981, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 499,  Mean reward: 6.246153846153846, Mean Entropy: 0.38297176361083984, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 500,  Mean reward: 7.2, Mean Entropy: 0.2999456524848938, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 6.548387096774194, Mean Entropy: 0.3652079105377197, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 502,  Mean reward: 6.809523809523809, Mean Entropy: 0.38416776061058044, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 503,  Mean reward: 7.007692307692308, Mean Entropy: 0.3301481008529663, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 504,  Mean reward: 7.409090909090909, Mean Entropy: 0.2997860312461853, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 505,  Mean reward: 5.898305084745763, Mean Entropy: 0.3820146918296814, complete_episode_count: 59.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 506,  Mean reward: 7.074626865671642, Mean Entropy: 0.34869417548179626, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 507,  Mean reward: 7.076923076923077, Mean Entropy: 0.2952161431312561, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 508,  Mean reward: 6.9453125, Mean Entropy: 0.36597177386283875, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 509,  Mean reward: 6.4921875, Mean Entropy: 0.516883373260498, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 510,  Mean reward: 6.111111111111111, Mean Entropy: 0.4937770366668701, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 511,  Mean reward: 5.475, Mean Entropy: 0.912319540977478, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 512,  Mean reward: -2.47, Mean Entropy: 1.078155517578125, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 513,  Mean reward: -5.317073170731708, Mean Entropy: 0.9241824150085449, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 514,  Mean reward: -3.9404761904761907, Mean Entropy: 0.960290789604187, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.58s
Iteration: 515,  Mean reward: -3.658536585365854, Mean Entropy: 0.8953127861022949, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 516,  Mean reward: -3.292682926829268, Mean Entropy: 0.9314039945602417, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 517,  Mean reward: -3.0609756097560976, Mean Entropy: 0.9891566038131714, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 518,  Mean reward: -4.128205128205129, Mean Entropy: 0.902496337890625, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 519,  Mean reward: -4.488095238095238, Mean Entropy: 0.9386072158813477, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 520,  Mean reward: -5.621951219512195, Mean Entropy: 0.9675135612487793, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 521,  Mean reward: -5.948717948717949, Mean Entropy: 0.9386336207389832, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 522,  Mean reward: -2.857142857142857, Mean Entropy: 0.9096765518188477, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.72s
Iteration: 523,  Mean reward: -4.67948717948718, Mean Entropy: 0.9168795347213745, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 524,  Mean reward: -5.804878048780488, Mean Entropy: 0.9385029077529907, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 525,  Mean reward: -4.2368421052631575, Mean Entropy: 0.8951517939567566, complete_episode_count: 38.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 526,  Mean reward: -3.951219512195122, Mean Entropy: 0.9023213386535645, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 527,  Mean reward: -5.392857142857143, Mean Entropy: 0.9816438555717468, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 528,  Mean reward: -3.7, Mean Entropy: 0.9598247408866882, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 529,  Mean reward: -3.188888888888889, Mean Entropy: 0.981552243232727, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 530,  Mean reward: -3.5813953488372094, Mean Entropy: 1.0179007053375244, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 531,  Mean reward: -5.012195121951219, Mean Entropy: 0.9891626834869385, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 532,  Mean reward: -4.85, Mean Entropy: 0.9517544507980347, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 533,  Mean reward: -4.892857142857143, Mean Entropy: 0.9058961868286133, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 534,  Mean reward: -3.5875, Mean Entropy: 0.8592783808708191, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 535,  Mean reward: -3.2804878048780486, Mean Entropy: 0.9093931913375854, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 536,  Mean reward: -3.3846153846153846, Mean Entropy: 0.7982949018478394, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 537,  Mean reward: 3.8085106382978724, Mean Entropy: 0.7439128160476685, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 538,  Mean reward: 3.7, Mean Entropy: 0.6891548037528992, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 539,  Mean reward: 3.9166666666666665, Mean Entropy: 0.6529415845870972, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 540,  Mean reward: 4.975, Mean Entropy: 0.575842559337616, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.52s
Iteration: 541,  Mean reward: 4.396825396825397, Mean Entropy: 0.5563805103302002, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 542,  Mean reward: 4.875, Mean Entropy: 0.5191918611526489, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 543,  Mean reward: 5.333333333333333, Mean Entropy: 0.6481422781944275, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 544,  Mean reward: 5.666666666666667, Mean Entropy: 0.5509788990020752, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 545,  Mean reward: 6.0661764705882355, Mean Entropy: 0.39023077487945557, complete_episode_count: 68.0, Gather time: 0.75s, Train time: 0.77s
Iteration: 546,  Mean reward: 4.090909090909091, Mean Entropy: 0.3934718072414398, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 547,  Mean reward: 7.359154929577465, Mean Entropy: 0.3207474946975708, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 548,  Mean reward: 7.3125, Mean Entropy: 0.2842121720314026, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 549,  Mean reward: 7.6066666666666665, Mean Entropy: 0.21559831500053406, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 550,  Mean reward: 7.84, Mean Entropy: 0.214277982711792, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 551,  Mean reward: 7.902597402597403, Mean Entropy: 0.11908008903265, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 552,  Mean reward: 7.981012658227848, Mean Entropy: 0.12219232320785522, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 553,  Mean reward: 7.6381578947368425, Mean Entropy: 0.09750869125127792, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 554,  Mean reward: 7.705128205128205, Mean Entropy: 0.07897095382213593, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 555,  Mean reward: 7.961538461538462, Mean Entropy: 0.03957096487283707, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 556,  Mean reward: 7.981012658227848, Mean Entropy: 0.025079837068915367, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 557,  Mean reward: 7.981012658227848, Mean Entropy: 0.04828156530857086, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.0239965058863163, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.93s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 0.01476141344755888, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 560,  Mean reward: 7.75, Mean Entropy: 0.04599139839410782, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 561,  Mean reward: 7.5, Mean Entropy: 0.0757034569978714, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 562,  Mean reward: 7.402597402597403, Mean Entropy: 0.19388386607170105, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 563,  Mean reward: 7.164383561643835, Mean Entropy: 0.2907634675502777, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 564,  Mean reward: 5.757142857142857, Mean Entropy: 0.18641361594200134, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 565,  Mean reward: 7.5675675675675675, Mean Entropy: 0.06466864049434662, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 566,  Mean reward: 7.961538461538462, Mean Entropy: 0.057802777737379074, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 567,  Mean reward: 7.981012658227848, Mean Entropy: 0.12481123208999634, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 568,  Mean reward: 7.881578947368421, Mean Entropy: 0.08012741059064865, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 569,  Mean reward: 7.981012658227848, Mean Entropy: 0.3362559676170349, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 570,  Mean reward: -3.2662337662337664, Mean Entropy: 0.1936107575893402, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 571,  Mean reward: 1.1455696202531647, Mean Entropy: 0.30230021476745605, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 572,  Mean reward: 1.3223684210526316, Mean Entropy: 0.3789216876029968, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 573,  Mean reward: 5.726666666666667, Mean Entropy: 0.3067791163921356, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 574,  Mean reward: 1.7465753424657535, Mean Entropy: 0.5140030384063721, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 575,  Mean reward: -1.6736111111111112, Mean Entropy: 0.5243800282478333, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 576,  Mean reward: 2.9675324675324677, Mean Entropy: 0.18037621676921844, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 577,  Mean reward: 7.236486486486487, Mean Entropy: 0.3848254382610321, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 578,  Mean reward: 1.3805970149253732, Mean Entropy: 0.4039795398712158, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 579,  Mean reward: 5.824675324675325, Mean Entropy: 0.16038335859775543, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 580,  Mean reward: 7.922077922077922, Mean Entropy: 0.06429654359817505, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 581,  Mean reward: 7.981012658227848, Mean Entropy: 0.037999123334884644, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 582,  Mean reward: 7.981012658227848, Mean Entropy: 0.053983625024557114, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.73s
Iteration: 583,  Mean reward: 7.173076923076923, Mean Entropy: 0.15277911722660065, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 584,  Mean reward: 7.5, Mean Entropy: 0.119107186794281, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 585,  Mean reward: 7.9423076923076925, Mean Entropy: 0.034480080008506775, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 586,  Mean reward: 7.9423076923076925, Mean Entropy: 0.013203807175159454, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 587,  Mean reward: 7.961538461538462, Mean Entropy: 0.01219453290104866, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 588,  Mean reward: 8.0, Mean Entropy: 0.6522324085235596, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 589,  Mean reward: -3.6818181818181817, Mean Entropy: 0.7057594656944275, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 590,  Mean reward: -1.0076923076923077, Mean Entropy: 0.7649469375610352, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 591,  Mean reward: -2.507936507936508, Mean Entropy: 1.0685455799102783, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 592,  Mean reward: -3.94, Mean Entropy: 1.0159046649932861, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.57s
Iteration: 593,  Mean reward: -3.9651162790697674, Mean Entropy: 0.8857969045639038, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 594,  Mean reward: 0.3645833333333333, Mean Entropy: 0.7544487714767456, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 595,  Mean reward: 2.5267857142857144, Mean Entropy: 0.70842045545578, complete_episode_count: 56.0, Gather time: 0.58s, Train time: 1.69s
Iteration: 596,  Mean reward: 3.3666666666666667, Mean Entropy: 0.4894087314605713, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 597,  Mean reward: 6.753623188405797, Mean Entropy: 0.26597416400909424, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 598,  Mean reward: 7.359154929577465, Mean Entropy: 0.15910908579826355, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 599,  Mean reward: 7.881578947368421, Mean Entropy: 0.23117247223854065, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 600,  Mean reward: 6.895833333333333, Mean Entropy: 0.22439377009868622, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 6.695945945945946, Mean Entropy: 0.15690064430236816, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 602,  Mean reward: 7.685897435897436, Mean Entropy: 0.04651692137122154, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 603,  Mean reward: 7.901315789473684, Mean Entropy: 0.035406745970249176, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 604,  Mean reward: 7.708860759493671, Mean Entropy: 0.04907584935426712, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 605,  Mean reward: 7.727848101265823, Mean Entropy: 0.017131362110376358, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.05191347002983093, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 607,  Mean reward: 7.961538461538462, Mean Entropy: 0.021125836297869682, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 608,  Mean reward: 8.0, Mean Entropy: 0.039604805409908295, complete_episode_count: 80.0, Gather time: 0.65s, Train time: 0.79s
Iteration: 609,  Mean reward: 7.981012658227848, Mean Entropy: 0.02741118334233761, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 610,  Mean reward: 7.75, Mean Entropy: 0.04537179321050644, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 611,  Mean reward: 7.962025316455696, Mean Entropy: 0.022943805903196335, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 612,  Mean reward: 7.75, Mean Entropy: 0.022262703627347946, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -4.688888888888889, Mean Entropy: 0.9241962432861328, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.49s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.65, Mean Entropy: 0.9458569288253784, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -3.953488372093023, Mean Entropy: 0.9097553491592407, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 3,  Mean reward: -7.5227272727272725, Mean Entropy: 0.9458566904067993, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 4,  Mean reward: -4.902439024390244, Mean Entropy: 0.9097557067871094, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 5,  Mean reward: -5.568181818181818, Mean Entropy: 0.9386353492736816, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 6,  Mean reward: -5.4375, Mean Entropy: 0.9314162731170654, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 7,  Mean reward: -4.666666666666667, Mean Entropy: 0.938633143901825, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 8,  Mean reward: -6.5, Mean Entropy: 0.9747132658958435, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 9,  Mean reward: -4.548780487804878, Mean Entropy: 0.953021228313446, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.53s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 10,  Mean reward: -2.3152173913043477, Mean Entropy: 0.9890777468681335, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 11,  Mean reward: -2.965909090909091, Mean Entropy: 0.902389645576477, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 12,  Mean reward: -3.0365853658536586, Mean Entropy: 0.90940260887146, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 13,  Mean reward: -5.151162790697675, Mean Entropy: 0.9165242910385132, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.56s
Iteration: 14,  Mean reward: -5.056818181818182, Mean Entropy: 0.9308730363845825, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 15,  Mean reward: -4.728260869565218, Mean Entropy: 0.959703803062439, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 16,  Mean reward: -3.7857142857142856, Mean Entropy: 0.9784552454948425, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 17,  Mean reward: -2.033333333333333, Mean Entropy: 0.9716591835021973, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.70s
Iteration: 18,  Mean reward: -3.7, Mean Entropy: 0.9686676859855652, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 19,  Mean reward: -3.659090909090909, Mean Entropy: 0.9692355990409851, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 20,  Mean reward: -6.2682926829268295, Mean Entropy: 0.9294483065605164, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 21,  Mean reward: -3.5208333333333335, Mean Entropy: 0.8612076044082642, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 22,  Mean reward: -1.434782608695652, Mean Entropy: 0.9009830355644226, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 23,  Mean reward: -4.488636363636363, Mean Entropy: 0.9125567674636841, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 24,  Mean reward: -2.426829268292683, Mean Entropy: 0.8789889216423035, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 25,  Mean reward: -4.566666666666666, Mean Entropy: 0.925137460231781, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 26,  Mean reward: -2.8068181818181817, Mean Entropy: 0.894438624382019, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 27,  Mean reward: -3.183673469387755, Mean Entropy: 0.8193590044975281, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 28,  Mean reward: -3.6666666666666665, Mean Entropy: 0.8408564329147339, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 29,  Mean reward: -2.6176470588235294, Mean Entropy: 0.729418158531189, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 30,  Mean reward: -3.574074074074074, Mean Entropy: 0.8641254305839539, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 31,  Mean reward: 0.42592592592592593, Mean Entropy: 0.7681773900985718, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.51s
Iteration: 32,  Mean reward: -1.12, Mean Entropy: 0.7627142667770386, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 33,  Mean reward: -2.625, Mean Entropy: 0.8301579356193542, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 34,  Mean reward: 0.9150943396226415, Mean Entropy: 0.83042973279953, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.57s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 35,  Mean reward: 1.47, Mean Entropy: 0.7088190317153931, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 36,  Mean reward: 2.1530612244897958, Mean Entropy: 0.6612610220909119, complete_episode_count: 49.0, Gather time: 0.59s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 37,  Mean reward: 4.339622641509434, Mean Entropy: 0.6447103023529053, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 38,  Mean reward: 3.7636363636363637, Mean Entropy: 0.6494588255882263, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 39,  Mean reward: 4.916666666666667, Mean Entropy: 0.6520662903785706, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 40,  Mean reward: 3.236842105263158, Mean Entropy: 0.6788220405578613, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 41,  Mean reward: 3.3425925925925926, Mean Entropy: 0.5984932780265808, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 42,  Mean reward: 6.8, Mean Entropy: 0.507609486579895, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 43,  Mean reward: 6.896825396825397, Mean Entropy: 0.41913992166519165, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 44,  Mean reward: 6.483333333333333, Mean Entropy: 0.389974445104599, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 45,  Mean reward: 6.944444444444445, Mean Entropy: 0.3180909752845764, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.73s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 46,  Mean reward: 7.409090909090909, Mean Entropy: 0.37340259552001953, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 47,  Mean reward: 7.286764705882353, Mean Entropy: 0.3578159213066101, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 48,  Mean reward: 7.164179104477612, Mean Entropy: 0.3410812318325043, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 49,  Mean reward: 7.454545454545454, Mean Entropy: 0.31902197003364563, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 50,  Mean reward: 7.152777777777778, Mean Entropy: 0.3109423518180847, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.91s
Iteration: 51,  Mean reward: 6.865671641791045, Mean Entropy: 0.29999005794525146, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 52,  Mean reward: 7.608695652173913, Mean Entropy: 0.25348037481307983, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 53,  Mean reward: 7.27536231884058, Mean Entropy: 0.3350266218185425, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 54,  Mean reward: 7.458904109589041, Mean Entropy: 0.23523300886154175, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 55,  Mean reward: 7.140845070422535, Mean Entropy: 0.2486613690853119, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 56,  Mean reward: 6.916666666666667, Mean Entropy: 0.2345377653837204, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 57,  Mean reward: 7.797297297297297, Mean Entropy: 0.2866743803024292, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 58,  Mean reward: 6.910958904109589, Mean Entropy: 0.27829355001449585, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 59,  Mean reward: 6.635714285714286, Mean Entropy: 0.21389852464199066, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 60,  Mean reward: 7.8175675675675675, Mean Entropy: 0.1460440456867218, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 61,  Mean reward: 7.8618421052631575, Mean Entropy: 0.12012302875518799, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 62,  Mean reward: 7.922077922077922, Mean Entropy: 0.08732552826404572, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 63,  Mean reward: 7.923076923076923, Mean Entropy: 0.054474011063575745, complete_episode_count: 78.0, Gather time: 0.96s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 64,  Mean reward: 7.941558441558442, Mean Entropy: 0.06756442785263062, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 65,  Mean reward: 7.980769230769231, Mean Entropy: 0.04554787278175354, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 66,  Mean reward: 7.75, Mean Entropy: 0.1426316797733307, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 67,  Mean reward: 7.148648648648648, Mean Entropy: 0.16744253039360046, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 68,  Mean reward: 7.901315789473684, Mean Entropy: 0.03713463991880417, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 69,  Mean reward: 8.0, Mean Entropy: 0.029371287673711777, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 70,  Mean reward: 7.961538461538462, Mean Entropy: 0.040531136095523834, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 71,  Mean reward: 8.0, Mean Entropy: 0.06310287117958069, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 72,  Mean reward: 7.961538461538462, Mean Entropy: 0.04913751408457756, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 73,  Mean reward: 7.981012658227848, Mean Entropy: 0.03247128054499626, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 74,  Mean reward: 7.981012658227848, Mean Entropy: 0.05002579092979431, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 75,  Mean reward: 8.0, Mean Entropy: 0.026118431240320206, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 76,  Mean reward: -2.0, Mean Entropy: 0.03362324461340904, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 77,  Mean reward: -3.411392405063291, Mean Entropy: 0.03612887114286423, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 78,  Mean reward: -1.0, Mean Entropy: 0.03715255483984947, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 79,  Mean reward: -1.948051948051948, Mean Entropy: 0.02456553280353546, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 80,  Mean reward: -1.0, Mean Entropy: 0.028582222759723663, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 81,  Mean reward: -2.0, Mean Entropy: 0.10952087491750717, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 82,  Mean reward: -1.6883116883116882, Mean Entropy: 0.33006778359413147, complete_episode_count: 77.0, Gather time: 0.63s, Train time: 0.77s
Iteration: 83,  Mean reward: 1.3987341772151898, Mean Entropy: 0.1958971917629242, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 84,  Mean reward: 7.142857142857143, Mean Entropy: 0.054028552025556564, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 85,  Mean reward: 8.0, Mean Entropy: 0.04916022717952728, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 86,  Mean reward: 8.0, Mean Entropy: 0.03863288834691048, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 87,  Mean reward: 7.75, Mean Entropy: 0.04347584769129753, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 88,  Mean reward: 8.0, Mean Entropy: 0.06679429113864899, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 89,  Mean reward: 7.9423076923076925, Mean Entropy: 0.060537658631801605, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 90,  Mean reward: 8.0, Mean Entropy: 0.5107095837593079, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 91,  Mean reward: 0.22972972972972974, Mean Entropy: 0.1935916244983673, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 92,  Mean reward: 5.802631578947368, Mean Entropy: 0.14908427000045776, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 93,  Mean reward: 7.961538461538462, Mean Entropy: 0.07349465787410736, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 94,  Mean reward: 8.0, Mean Entropy: 0.038838040083646774, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 95,  Mean reward: 8.0, Mean Entropy: 0.7932958602905273, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 96,  Mean reward: -2.4274193548387095, Mean Entropy: 0.34936994314193726, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 97,  Mean reward: 7.961538461538462, Mean Entropy: 0.43143877387046814, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 98,  Mean reward: -1.028169014084507, Mean Entropy: 0.22092662751674652, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 99,  Mean reward: 8.0, Mean Entropy: 0.03166131302714348, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 100,  Mean reward: 8.0, Mean Entropy: 0.6405478715896606, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 2.507142857142857, Mean Entropy: 0.22858643531799316, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 102,  Mean reward: 7.5, Mean Entropy: 0.12576943635940552, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 103,  Mean reward: 7.42948717948718, Mean Entropy: 0.08335685729980469, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 104,  Mean reward: 7.922077922077922, Mean Entropy: 0.22574695944786072, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 105,  Mean reward: 5.094594594594595, Mean Entropy: 0.3569519519805908, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 106,  Mean reward: 5.706666666666667, Mean Entropy: 0.0343523845076561, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 107,  Mean reward: 8.0, Mean Entropy: 0.31657081842422485, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 108,  Mean reward: 4.395833333333333, Mean Entropy: 0.6928496360778809, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 109,  Mean reward: 3.1384615384615384, Mean Entropy: 0.45336803793907166, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 110,  Mean reward: 6.863636363636363, Mean Entropy: 0.08355643600225449, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 111,  Mean reward: 7.981012658227848, Mean Entropy: 0.21843120455741882, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 112,  Mean reward: 4.480263157894737, Mean Entropy: 0.09168128669261932, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 113,  Mean reward: 7.922077922077922, Mean Entropy: 0.07993795722723007, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 114,  Mean reward: 7.961538461538462, Mean Entropy: 0.028665728867053986, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 115,  Mean reward: 8.0, Mean Entropy: 0.6103459596633911, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 116,  Mean reward: 2.5303030303030303, Mean Entropy: 0.4014619290828705, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 117,  Mean reward: 3.987012987012987, Mean Entropy: 0.04776381701231003, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 118,  Mean reward: 8.0, Mean Entropy: 0.03188149631023407, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 119,  Mean reward: 8.0, Mean Entropy: 0.02192002162337303, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 120,  Mean reward: 8.0, Mean Entropy: 0.4908845126628876, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 121,  Mean reward: 3.246376811594203, Mean Entropy: 0.6324387788772583, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 122,  Mean reward: 3.652173913043478, Mean Entropy: 0.4895756244659424, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 123,  Mean reward: 5.197183098591549, Mean Entropy: 0.2865371108055115, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 124,  Mean reward: 5.993333333333333, Mean Entropy: 0.06903388351202011, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 125,  Mean reward: 7.9423076923076925, Mean Entropy: 0.036095742136240005, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 126,  Mean reward: 7.981012658227848, Mean Entropy: 0.16479068994522095, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 127,  Mean reward: 4.689873417721519, Mean Entropy: 0.1398025006055832, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 128,  Mean reward: 6.935897435897436, Mean Entropy: 0.014254230074584484, complete_episode_count: 78.0, Gather time: 0.78s, Train time: 0.78s
Iteration: 129,  Mean reward: 8.0, Mean Entropy: 0.027610789984464645, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 130,  Mean reward: 8.0, Mean Entropy: 0.015027686953544617, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 131,  Mean reward: 8.0, Mean Entropy: 0.010440042242407799, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 132,  Mean reward: 8.0, Mean Entropy: 0.020709343254566193, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 133,  Mean reward: 7.981012658227848, Mean Entropy: 0.008469536900520325, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 134,  Mean reward: 8.0, Mean Entropy: 0.007334679365158081, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 135,  Mean reward: 7.981012658227848, Mean Entropy: 0.018622344359755516, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 136,  Mean reward: 7.981012658227848, Mean Entropy: 0.011029484681785107, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 137,  Mean reward: 8.0, Mean Entropy: 0.04806346818804741, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 138,  Mean reward: 7.708860759493671, Mean Entropy: 0.017685849219560623, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 139,  Mean reward: 8.0, Mean Entropy: 0.01793571189045906, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 140,  Mean reward: 8.0, Mean Entropy: 0.09123969078063965, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.78s
Iteration: 141,  Mean reward: 2.25, Mean Entropy: 0.37065911293029785, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 142,  Mean reward: 1.8333333333333333, Mean Entropy: 0.49793124198913574, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 143,  Mean reward: 3.7739726027397262, Mean Entropy: 0.2758539617061615, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 144,  Mean reward: 6.342105263157895, Mean Entropy: 0.12046496570110321, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 145,  Mean reward: 7.727848101265823, Mean Entropy: 0.04815475642681122, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 146,  Mean reward: 8.0, Mean Entropy: 0.010228198021650314, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 147,  Mean reward: 8.0, Mean Entropy: 0.1755775511264801, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 148,  Mean reward: 5.706666666666667, Mean Entropy: 0.058862101286649704, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 149,  Mean reward: 8.0, Mean Entropy: 0.02448951080441475, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 150,  Mean reward: 8.0, Mean Entropy: 0.17686805129051208, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 151,  Mean reward: 5.084415584415584, Mean Entropy: 0.37449750304222107, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 152,  Mean reward: 6.126760563380282, Mean Entropy: 0.312836229801178, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 153,  Mean reward: 6.0394736842105265, Mean Entropy: 0.0633757933974266, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 154,  Mean reward: 8.0, Mean Entropy: 0.4511650800704956, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 155,  Mean reward: 0.42063492063492064, Mean Entropy: 0.7927154302597046, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 156,  Mean reward: -3.09375, Mean Entropy: 0.7974646091461182, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 157,  Mean reward: -0.35344827586206895, Mean Entropy: 0.7378933429718018, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 158,  Mean reward: 1.7954545454545454, Mean Entropy: 0.7199481725692749, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 159,  Mean reward: 1.9357142857142857, Mean Entropy: 0.8513533473014832, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 160,  Mean reward: -1.3055555555555556, Mean Entropy: 0.9791185855865479, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 161,  Mean reward: -4.5, Mean Entropy: 0.8952271342277527, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 162,  Mean reward: -3.802325581395349, Mean Entropy: 0.945826530456543, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 163,  Mean reward: -5.157894736842105, Mean Entropy: 0.9241933822631836, complete_episode_count: 38.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 164,  Mean reward: -6.976190476190476, Mean Entropy: 0.9314095973968506, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.70s
Iteration: 165,  Mean reward: -3.5238095238095237, Mean Entropy: 0.8880901336669922, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 166,  Mean reward: -4.512195121951219, Mean Entropy: 0.9747270345687866, complete_episode_count: 41.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 167,  Mean reward: -4.8076923076923075, Mean Entropy: 0.9314032793045044, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 168,  Mean reward: -7.511904761904762, Mean Entropy: 0.9458536505699158, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 169,  Mean reward: -7.225, Mean Entropy: 0.9458550810813904, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 170,  Mean reward: -7.608108108108108, Mean Entropy: 0.9458553194999695, complete_episode_count: 37.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 171,  Mean reward: -4.430232558139535, Mean Entropy: 0.9458309412002563, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 172,  Mean reward: -3.263157894736842, Mean Entropy: 0.9528381824493408, complete_episode_count: 38.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 173,  Mean reward: -4.512820512820513, Mean Entropy: 0.9311851263046265, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 174,  Mean reward: -3.5444444444444443, Mean Entropy: 0.9526316523551941, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 175,  Mean reward: -4.658536585365853, Mean Entropy: 0.960141658782959, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 176,  Mean reward: -4.463414634146342, Mean Entropy: 0.9747085571289062, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 177,  Mean reward: -4.4875, Mean Entropy: 0.8591008186340332, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 178,  Mean reward: -3.8974358974358974, Mean Entropy: 0.9313890337944031, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 179,  Mean reward: -5.714285714285714, Mean Entropy: 0.974432110786438, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 180,  Mean reward: -5.871794871794871, Mean Entropy: 0.9958836436271667, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 181,  Mean reward: -4.5, Mean Entropy: 1.0177720785140991, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 182,  Mean reward: -3.761904761904762, Mean Entropy: 1.0107700824737549, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 183,  Mean reward: -6.011363636363637, Mean Entropy: 0.9595796465873718, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 184,  Mean reward: -5.022222222222222, Mean Entropy: 0.9522907137870789, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 185,  Mean reward: -7.104651162790698, Mean Entropy: 0.8298847675323486, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 186,  Mean reward: -3.926829268292683, Mean Entropy: 0.9657899141311646, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 187,  Mean reward: -4.290697674418604, Mean Entropy: 0.9598631262779236, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 188,  Mean reward: -6.3625, Mean Entropy: 0.9499702453613281, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 189,  Mean reward: -4.337209302325581, Mean Entropy: 0.8924757838249207, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 190,  Mean reward: -4.329545454545454, Mean Entropy: 0.9245103597640991, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 191,  Mean reward: -7.0, Mean Entropy: 0.8773414492607117, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 192,  Mean reward: -2.1808510638297873, Mean Entropy: 0.9067026972770691, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 193,  Mean reward: -2.619565217391304, Mean Entropy: 0.8406292796134949, complete_episode_count: 46.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 194,  Mean reward: -6.1395348837209305, Mean Entropy: 0.8878622055053711, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 195,  Mean reward: -5.27906976744186, Mean Entropy: 0.848326563835144, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 196,  Mean reward: -1.3863636363636365, Mean Entropy: 0.8915714025497437, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 197,  Mean reward: -1.4787234042553192, Mean Entropy: 0.954729437828064, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 198,  Mean reward: -3.2717391304347827, Mean Entropy: 0.9516984820365906, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.69s
Iteration: 199,  Mean reward: -5.4186046511627906, Mean Entropy: 0.8174961805343628, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 200,  Mean reward: -4.73404255319149, Mean Entropy: 0.8562060594558716, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.52s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -5.315217391304348, Mean Entropy: 0.7564563751220703, complete_episode_count: 46.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 202,  Mean reward: -4.846938775510204, Mean Entropy: 0.7732219099998474, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 203,  Mean reward: -3.701923076923077, Mean Entropy: 0.7310504913330078, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 204,  Mean reward: -4.25, Mean Entropy: 0.745631217956543, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 205,  Mean reward: -3.9489795918367347, Mean Entropy: 0.8250142931938171, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 206,  Mean reward: -0.8269230769230769, Mean Entropy: 0.8821743726730347, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 207,  Mean reward: -2.630434782608696, Mean Entropy: 0.8857426047325134, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 208,  Mean reward: -3.15625, Mean Entropy: 0.9387803077697754, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 209,  Mean reward: -5.8625, Mean Entropy: 0.8776904940605164, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 210,  Mean reward: -3.197674418604651, Mean Entropy: 0.835681676864624, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 211,  Mean reward: -3.5096153846153846, Mean Entropy: 0.7411598563194275, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 212,  Mean reward: -2.0754716981132075, Mean Entropy: 0.7715764045715332, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 213,  Mean reward: -1.9607843137254901, Mean Entropy: 0.6952512264251709, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 214,  Mean reward: -5.26530612244898, Mean Entropy: 0.6131124496459961, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 215,  Mean reward: -2.212962962962963, Mean Entropy: 0.7170284986495972, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 216,  Mean reward: -2.5104166666666665, Mean Entropy: 0.7254759073257446, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 217,  Mean reward: -1.8181818181818181, Mean Entropy: 0.8832630515098572, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 218,  Mean reward: -4.77906976744186, Mean Entropy: 0.5916936993598938, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 219,  Mean reward: -3.1578947368421053, Mean Entropy: 0.574047863483429, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 220,  Mean reward: -4.833333333333333, Mean Entropy: 0.5960770845413208, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 221,  Mean reward: -3.3846153846153846, Mean Entropy: 0.5770954489707947, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 222,  Mean reward: -5.1875, Mean Entropy: 0.5398819446563721, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 223,  Mean reward: -5.8125, Mean Entropy: 0.5826783776283264, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 224,  Mean reward: -2.016949152542373, Mean Entropy: 0.7459416389465332, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 225,  Mean reward: -5.811320754716981, Mean Entropy: 0.8017222285270691, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 226,  Mean reward: -5.130434782608695, Mean Entropy: 0.7236047983169556, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 227,  Mean reward: -1.3818181818181818, Mean Entropy: 0.7903923392295837, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 228,  Mean reward: -1.2456140350877194, Mean Entropy: 0.7280575037002563, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 229,  Mean reward: -2.440677966101695, Mean Entropy: 0.7035357356071472, complete_episode_count: 59.0, Gather time: 0.58s, Train time: 1.71s
Iteration: 230,  Mean reward: -1.1796875, Mean Entropy: 0.8264278173446655, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 231,  Mean reward: -3.3796296296296298, Mean Entropy: 0.7199163436889648, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.54s
Iteration: 232,  Mean reward: -2.6792452830188678, Mean Entropy: 0.8166495561599731, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 233,  Mean reward: -5.87037037037037, Mean Entropy: 0.5735318660736084, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 234,  Mean reward: 0.09154929577464789, Mean Entropy: 0.5207632184028625, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 235,  Mean reward: -1.6126760563380282, Mean Entropy: 0.4571625888347626, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 236,  Mean reward: 0.9066666666666666, Mean Entropy: 0.2055414915084839, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 237,  Mean reward: 5.993333333333333, Mean Entropy: 0.14449253678321838, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 238,  Mean reward: 3.2222222222222223, Mean Entropy: 0.47288811206817627, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 239,  Mean reward: -1.5803571428571428, Mean Entropy: 0.5074869990348816, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 240,  Mean reward: -4.907692307692308, Mean Entropy: 0.4417147636413574, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 241,  Mean reward: -3.1587301587301586, Mean Entropy: 0.3560747504234314, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 242,  Mean reward: -2.664285714285714, Mean Entropy: 0.4882359504699707, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 243,  Mean reward: -1.5076923076923077, Mean Entropy: 0.5635025501251221, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 244,  Mean reward: -0.7946428571428571, Mean Entropy: 0.5413440465927124, complete_episode_count: 56.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 245,  Mean reward: -0.2076923076923077, Mean Entropy: 0.5740808844566345, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 246,  Mean reward: 3.75, Mean Entropy: 0.48316606879234314, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 247,  Mean reward: 4.880281690140845, Mean Entropy: 0.2955048084259033, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 248,  Mean reward: -2.657142857142857, Mean Entropy: 0.36108893156051636, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 249,  Mean reward: -0.1917808219178082, Mean Entropy: 0.29760581254959106, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 250,  Mean reward: -3.2837837837837838, Mean Entropy: 0.33816275000572205, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 251,  Mean reward: -3.5142857142857142, Mean Entropy: 0.5198289155960083, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 252,  Mean reward: 2.1363636363636362, Mean Entropy: 0.38945087790489197, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 253,  Mean reward: 3.536231884057971, Mean Entropy: 0.3965536952018738, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 254,  Mean reward: 4.051470588235294, Mean Entropy: 0.37943196296691895, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 255,  Mean reward: 4.098484848484849, Mean Entropy: 0.3227348327636719, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 256,  Mean reward: 4.889705882352941, Mean Entropy: 0.29489296674728394, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 257,  Mean reward: 5.4338235294117645, Mean Entropy: 0.31137579679489136, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 258,  Mean reward: 5.35, Mean Entropy: 0.26695650815963745, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 259,  Mean reward: 7.729166666666667, Mean Entropy: 0.1657571643590927, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 260,  Mean reward: 7.458904109589041, Mean Entropy: 0.15975186228752136, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 261,  Mean reward: 7.729166666666667, Mean Entropy: 0.18209800124168396, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 262,  Mean reward: 7.580882352941177, Mean Entropy: 0.17736098170280457, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 263,  Mean reward: 7.725352112676056, Mean Entropy: 0.23147469758987427, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 264,  Mean reward: 7.064285714285714, Mean Entropy: 0.22407189011573792, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 265,  Mean reward: 4.3283582089552235, Mean Entropy: 0.3692358732223511, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 266,  Mean reward: 6.45, Mean Entropy: 0.1668987274169922, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 267,  Mean reward: 7.708333333333333, Mean Entropy: 0.17288784682750702, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 268,  Mean reward: 2.573529411764706, Mean Entropy: 0.2999435365200043, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 269,  Mean reward: 1.9140625, Mean Entropy: 0.19533556699752808, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 270,  Mean reward: 3.8582089552238807, Mean Entropy: 0.23948848247528076, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 271,  Mean reward: 7.485074626865671, Mean Entropy: 0.2638072669506073, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 272,  Mean reward: 7.547297297297297, Mean Entropy: 0.20164060592651367, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 273,  Mean reward: 7.451388888888889, Mean Entropy: 0.18532046675682068, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 274,  Mean reward: 7.6571428571428575, Mean Entropy: 0.1276334971189499, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 275,  Mean reward: 7.635714285714286, Mean Entropy: 0.3877254128456116, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 276,  Mean reward: 2.9318181818181817, Mean Entropy: 0.5858261585235596, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 277,  Mean reward: -0.1206896551724138, Mean Entropy: 0.6391363143920898, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 278,  Mean reward: -0.5873015873015873, Mean Entropy: 0.3734142780303955, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 279,  Mean reward: -1.3114754098360655, Mean Entropy: 0.34116309881210327, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 280,  Mean reward: 3.8333333333333335, Mean Entropy: 0.2898338735103607, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 281,  Mean reward: 7.614285714285714, Mean Entropy: 0.21589331328868866, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 282,  Mean reward: 7.565217391304348, Mean Entropy: 0.1611933559179306, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 283,  Mean reward: 7.007246376811594, Mean Entropy: 0.21431003510951996, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 284,  Mean reward: 7.392857142857143, Mean Entropy: 0.19014200568199158, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 285,  Mean reward: 7.608695652173913, Mean Entropy: 0.14943504333496094, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 286,  Mean reward: 7.5588235294117645, Mean Entropy: 0.1664147973060608, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 287,  Mean reward: 7.773972602739726, Mean Entropy: 0.27192771434783936, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 288,  Mean reward: 2.0403225806451615, Mean Entropy: 0.5415881872177124, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 289,  Mean reward: 1.2537313432835822, Mean Entropy: 0.30726510286331177, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 290,  Mean reward: 0.3384615384615385, Mean Entropy: 0.15140078961849213, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 291,  Mean reward: 7.661971830985915, Mean Entropy: 0.16644181311130524, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 292,  Mean reward: 7.635714285714286, Mean Entropy: 0.15961652994155884, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 293,  Mean reward: 7.307142857142857, Mean Entropy: 0.15605610609054565, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 294,  Mean reward: 7.683098591549296, Mean Entropy: 0.14339342713356018, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 295,  Mean reward: 4.395833333333333, Mean Entropy: 0.22185549139976501, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 296,  Mean reward: 5.9, Mean Entropy: 0.13049298524856567, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 297,  Mean reward: 7.529850746268656, Mean Entropy: 0.15463142096996307, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 298,  Mean reward: 7.661971830985915, Mean Entropy: 0.15546296536922455, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 299,  Mean reward: 7.75, Mean Entropy: 0.15492115914821625, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 300,  Mean reward: 7.756756756756757, Mean Entropy: 0.1240563616156578, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 7.704225352112676, Mean Entropy: 0.30566510558128357, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 302,  Mean reward: -1.5725806451612903, Mean Entropy: 0.6404244899749756, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 303,  Mean reward: -1.076271186440678, Mean Entropy: 0.5407072305679321, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 304,  Mean reward: 0.20689655172413793, Mean Entropy: 0.34288114309310913, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 305,  Mean reward: 2.9420289855072466, Mean Entropy: 0.219000905752182, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 306,  Mean reward: 1.7681159420289856, Mean Entropy: 0.1523578017950058, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.94s
Iteration: 307,  Mean reward: 1.3602941176470589, Mean Entropy: 0.25934702157974243, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 308,  Mean reward: 0.6417910447761194, Mean Entropy: 0.43914228677749634, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 309,  Mean reward: 4.456521739130435, Mean Entropy: 0.3591585159301758, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 310,  Mean reward: 5.971830985915493, Mean Entropy: 0.22659818828105927, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 311,  Mean reward: 6.963235294117647, Mean Entropy: 0.1725359857082367, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 312,  Mean reward: 7.608695652173913, Mean Entropy: 0.17532716691493988, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 313,  Mean reward: 7.683098591549296, Mean Entropy: 0.12648692727088928, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 314,  Mean reward: 7.608695652173913, Mean Entropy: 0.11986379325389862, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 315,  Mean reward: 7.704225352112676, Mean Entropy: 0.13805758953094482, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 316,  Mean reward: 7.580882352941177, Mean Entropy: 0.15312953293323517, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 317,  Mean reward: 7.5928571428571425, Mean Entropy: 0.16273561120033264, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 318,  Mean reward: 7.507462686567164, Mean Entropy: 0.14038708806037903, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 319,  Mean reward: 7.608695652173913, Mean Entropy: 0.11768297851085663, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 320,  Mean reward: 7.640845070422535, Mean Entropy: 0.13745325803756714, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 321,  Mean reward: 7.683098591549296, Mean Entropy: 0.10798775404691696, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 322,  Mean reward: 0.9044117647058824, Mean Entropy: 0.40753501653671265, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 323,  Mean reward: 1.9924242424242424, Mean Entropy: 0.22376355528831482, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 324,  Mean reward: 7.683098591549296, Mean Entropy: 0.10145212709903717, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 325,  Mean reward: 7.630434782608695, Mean Entropy: 0.2271035760641098, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 326,  Mean reward: 1.9402985074626866, Mean Entropy: 0.39495885372161865, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 327,  Mean reward: 2.985074626865672, Mean Entropy: 0.18746136128902435, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 328,  Mean reward: 7.286764705882353, Mean Entropy: 0.1522015929222107, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 329,  Mean reward: 7.098591549295775, Mean Entropy: 0.1326727718114853, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 330,  Mean reward: 7.708333333333333, Mean Entropy: 0.18726882338523865, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 331,  Mean reward: 0.855072463768116, Mean Entropy: 0.16941499710083008, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 332,  Mean reward: 1.0441176470588236, Mean Entropy: 0.24141070246696472, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 333,  Mean reward: 7.586956521739131, Mean Entropy: 0.14903073012828827, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 334,  Mean reward: 7.773972602739726, Mean Entropy: 0.1531178057193756, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 335,  Mean reward: 7.565217391304348, Mean Entropy: 0.14211276173591614, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 336,  Mean reward: 6.610294117647059, Mean Entropy: 0.15392330288887024, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 337,  Mean reward: 7.565217391304348, Mean Entropy: 0.10941275954246521, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 338,  Mean reward: 7.704225352112676, Mean Entropy: 0.14556075632572174, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 339,  Mean reward: 7.635714285714286, Mean Entropy: 0.11712130159139633, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 340,  Mean reward: 7.586956521739131, Mean Entropy: 0.15443043410778046, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 341,  Mean reward: 7.6875, Mean Entropy: 0.1534576117992401, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 342,  Mean reward: 7.661971830985915, Mean Entropy: 0.17632339894771576, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 343,  Mean reward: 7.7534246575342465, Mean Entropy: 0.15302734076976776, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 344,  Mean reward: 7.704225352112676, Mean Entropy: 0.25044047832489014, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 345,  Mean reward: 7.297101449275362, Mean Entropy: 0.1342272162437439, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 346,  Mean reward: 7.6875, Mean Entropy: 0.2131977379322052, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.97s
Iteration: 347,  Mean reward: 7.359154929577465, Mean Entropy: 0.1220635399222374, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 348,  Mean reward: 7.630434782608695, Mean Entropy: 0.13159549236297607, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 349,  Mean reward: 7.608695652173913, Mean Entropy: 0.14723840355873108, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 350,  Mean reward: 7.454545454545454, Mean Entropy: 0.1823510378599167, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 351,  Mean reward: 7.6875, Mean Entropy: 0.1502346396446228, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 352,  Mean reward: 7.586956521739131, Mean Entropy: 0.14595866203308105, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 353,  Mean reward: 7.635714285714286, Mean Entropy: 0.13846957683563232, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 354,  Mean reward: 7.614285714285714, Mean Entropy: 0.15404319763183594, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 355,  Mean reward: 7.543478260869565, Mean Entropy: 0.12147846072912216, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 356,  Mean reward: 7.725352112676056, Mean Entropy: 0.14660786092281342, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 357,  Mean reward: 7.586956521739131, Mean Entropy: 0.16925612092018127, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 358,  Mean reward: 7.431818181818182, Mean Entropy: 0.16028450429439545, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 359,  Mean reward: 7.640845070422535, Mean Entropy: 0.14866751432418823, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 360,  Mean reward: 7.5928571428571425, Mean Entropy: 0.163192018866539, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 361,  Mean reward: 7.514705882352941, Mean Entropy: 0.15205177664756775, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 362,  Mean reward: 7.586956521739131, Mean Entropy: 0.14614877104759216, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 363,  Mean reward: 7.704225352112676, Mean Entropy: 0.14280393719673157, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 364,  Mean reward: 7.797297297297297, Mean Entropy: 0.1315908282995224, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 365,  Mean reward: 7.514705882352941, Mean Entropy: 0.14159798622131348, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 366,  Mean reward: 7.602941176470588, Mean Entropy: 0.1446232795715332, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 367,  Mean reward: 7.5928571428571425, Mean Entropy: 0.16784334182739258, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 368,  Mean reward: 7.586956521739131, Mean Entropy: 0.12260725349187851, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 369,  Mean reward: 7.297101449275362, Mean Entropy: 0.12660536170005798, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 370,  Mean reward: 7.536764705882353, Mean Entropy: 0.14250531792640686, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 371,  Mean reward: 7.708333333333333, Mean Entropy: 0.11262809485197067, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 372,  Mean reward: 7.86, Mean Entropy: 0.0801531970500946, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 373,  Mean reward: 7.623376623376624, Mean Entropy: 0.007227152585983276, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 374,  Mean reward: 8.0, Mean Entropy: 0.025086022913455963, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 375,  Mean reward: 7.25, Mean Entropy: 0.001938476925715804, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 376,  Mean reward: 8.0, Mean Entropy: 0.013352090492844582, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 377,  Mean reward: 7.75, Mean Entropy: 0.003490281291306019, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 378,  Mean reward: 8.0, Mean Entropy: 0.0005138273118063807, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 379,  Mean reward: 8.0, Mean Entropy: 0.0006411981303244829, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 380,  Mean reward: 8.0, Mean Entropy: 0.0028268126770853996, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 381,  Mean reward: 8.0, Mean Entropy: 0.017519615590572357, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 382,  Mean reward: 7.75, Mean Entropy: 0.008026177063584328, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 383,  Mean reward: 8.0, Mean Entropy: 0.14869822561740875, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 384,  Mean reward: 6.462025316455696, Mean Entropy: 0.10111000388860703, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 385,  Mean reward: 6.935897435897436, Mean Entropy: 0.015177195891737938, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 386,  Mean reward: 8.0, Mean Entropy: 0.0074907392263412476, complete_episode_count: 80.0, Gather time: 0.76s, Train time: 0.78s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 0.0296127051115036, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 0.4296252727508545, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 389,  Mean reward: 3.2027027027027026, Mean Entropy: 0.2934558689594269, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 390,  Mean reward: 6.383116883116883, Mean Entropy: 0.0611531063914299, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 0.09061788767576218, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 392,  Mean reward: 6.935897435897436, Mean Entropy: 0.016903720796108246, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 0.0008166597108356655, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 7.06870632711798e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 395,  Mean reward: 8.0, Mean Entropy: 5.986628093523905e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 396,  Mean reward: 8.0, Mean Entropy: 3.6308043490862474e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 4.3782813008874655e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 4.8971087380778044e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 4.25573016400449e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 3.968813325627707e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 8.0, Mean Entropy: 4.414461363921873e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 402,  Mean reward: 8.0, Mean Entropy: 4.2795825720531866e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 403,  Mean reward: 8.0, Mean Entropy: 5.79479310545139e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 404,  Mean reward: 8.0, Mean Entropy: 6.185879465192556e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 405,  Mean reward: 8.0, Mean Entropy: 8.329882984980941e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 0.00012965315545443445, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 407,  Mean reward: 8.0, Mean Entropy: 0.00024303111422341317, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 0.0011238257866352797, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 0.1243181824684143, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 410,  Mean reward: 6.059210526315789, Mean Entropy: 0.4757528305053711, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 411,  Mean reward: 4.007246376811594, Mean Entropy: 0.4196072816848755, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 412,  Mean reward: 4.927536231884058, Mean Entropy: 0.3174992501735687, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 413,  Mean reward: 6.445945945945946, Mean Entropy: 0.10131068527698517, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 414,  Mean reward: 7.448717948717949, Mean Entropy: 0.04682523012161255, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 415,  Mean reward: 7.727848101265823, Mean Entropy: 0.002300779800862074, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 416,  Mean reward: 8.0, Mean Entropy: 0.017387453466653824, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 417,  Mean reward: 7.961538461538462, Mean Entropy: 0.001958663808181882, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 418,  Mean reward: 8.0, Mean Entropy: 0.0029988265596330166, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 419,  Mean reward: 8.0, Mean Entropy: 0.004981501959264278, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 0.029259931296110153, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 421,  Mean reward: 7.5, Mean Entropy: 0.005283647682517767, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.0013898536562919617, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 423,  Mean reward: 8.0, Mean Entropy: 0.001986036542803049, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.95s
Iteration: 424,  Mean reward: 8.0, Mean Entropy: 0.0036179362796247005, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 0.005054935347288847, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 0.020753255113959312, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.83s
Iteration: 427,  Mean reward: 7.75, Mean Entropy: 0.007970355451107025, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 428,  Mean reward: 8.0, Mean Entropy: 0.02905813418328762, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 429,  Mean reward: 7.5, Mean Entropy: 0.017448604106903076, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 430,  Mean reward: 7.75, Mean Entropy: 0.0022266528103500605, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 0.10144307464361191, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 432,  Mean reward: 7.86, Mean Entropy: 0.028126422315835953, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 433,  Mean reward: 8.0, Mean Entropy: 0.025029536336660385, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 434,  Mean reward: 8.0, Mean Entropy: 0.11024996638298035, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 435,  Mean reward: 7.474683544303797, Mean Entropy: 0.15243202447891235, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 436,  Mean reward: 7.1923076923076925, Mean Entropy: 0.1151496097445488, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 437,  Mean reward: 7.962025316455696, Mean Entropy: 0.06679810583591461, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 438,  Mean reward: 7.902597402597403, Mean Entropy: 0.028327925130724907, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 0.08963444828987122, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 440,  Mean reward: 7.1923076923076925, Mean Entropy: 0.09502957761287689, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 441,  Mean reward: 6.5, Mean Entropy: 0.01479911245405674, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 0.039417050778865814, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 443,  Mean reward: 7.25, Mean Entropy: 0.011355086229741573, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 444,  Mean reward: 8.0, Mean Entropy: 0.011392354965209961, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 445,  Mean reward: 8.0, Mean Entropy: 0.07618657499551773, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 446,  Mean reward: 6.5, Mean Entropy: 0.07867967337369919, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 447,  Mean reward: 7.25, Mean Entropy: 0.014629870653152466, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 448,  Mean reward: 8.0, Mean Entropy: 0.008036080747842789, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 449,  Mean reward: 8.0, Mean Entropy: 0.0035085929557681084, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 450,  Mean reward: 8.0, Mean Entropy: 0.0020625186152756214, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 451,  Mean reward: 8.0, Mean Entropy: 0.004727210849523544, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 452,  Mean reward: 8.0, Mean Entropy: 0.02132205292582512, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 453,  Mean reward: 8.0, Mean Entropy: 0.07053681463003159, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 454,  Mean reward: 6.5, Mean Entropy: 0.03996516764163971, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 455,  Mean reward: 7.5, Mean Entropy: 0.006464102771133184, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 456,  Mean reward: 8.0, Mean Entropy: 0.002246411517262459, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 457,  Mean reward: 8.0, Mean Entropy: 0.0015405867015942931, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 458,  Mean reward: 8.0, Mean Entropy: 0.002224371302872896, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 459,  Mean reward: 8.0, Mean Entropy: 0.15777802467346191, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 460,  Mean reward: 7.614285714285714, Mean Entropy: 0.13121500611305237, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 461,  Mean reward: 7.901315789473684, Mean Entropy: 0.007231724914163351, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 462,  Mean reward: 8.0, Mean Entropy: 0.0024255949538201094, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 463,  Mean reward: 8.0, Mean Entropy: 0.0034033479169011116, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 464,  Mean reward: 7.75, Mean Entropy: 0.02095760405063629, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 465,  Mean reward: 7.962025316455696, Mean Entropy: 0.007775416597723961, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 466,  Mean reward: 8.0, Mean Entropy: 0.006092920899391174, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 467,  Mean reward: 8.0, Mean Entropy: 0.012932589277625084, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 468,  Mean reward: 7.981012658227848, Mean Entropy: 0.0028774775564670563, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 469,  Mean reward: 8.0, Mean Entropy: 0.0016464653890579939, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 470,  Mean reward: 8.0, Mean Entropy: 0.002203742740675807, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 471,  Mean reward: 8.0, Mean Entropy: 0.0033330796286463737, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 472,  Mean reward: 7.981012658227848, Mean Entropy: 0.0012115671997889876, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 473,  Mean reward: 8.0, Mean Entropy: 0.0009301042882725596, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 474,  Mean reward: 8.0, Mean Entropy: 0.0015219891211017966, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 475,  Mean reward: 8.0, Mean Entropy: 0.0020700213499367237, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 476,  Mean reward: 8.0, Mean Entropy: 0.003061921801418066, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 477,  Mean reward: 8.0, Mean Entropy: 0.006776377558708191, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 478,  Mean reward: 8.0, Mean Entropy: 0.024857981130480766, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 479,  Mean reward: 7.25, Mean Entropy: 0.007328844629228115, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 480,  Mean reward: 8.0, Mean Entropy: 0.005009962245821953, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 481,  Mean reward: 8.0, Mean Entropy: 0.008646978065371513, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 482,  Mean reward: 7.75, Mean Entropy: 0.0030184010975062847, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 483,  Mean reward: 8.0, Mean Entropy: 0.002338837832212448, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 484,  Mean reward: 8.0, Mean Entropy: 0.0025450082030147314, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 485,  Mean reward: 8.0, Mean Entropy: 0.00224312674254179, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 486,  Mean reward: 8.0, Mean Entropy: 0.0022571254521608353, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 487,  Mean reward: 8.0, Mean Entropy: 0.001737498096190393, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 488,  Mean reward: 8.0, Mean Entropy: 0.0016059314366430044, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 489,  Mean reward: 8.0, Mean Entropy: 0.001021329895593226, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 490,  Mean reward: 8.0, Mean Entropy: 0.0012391051277518272, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 491,  Mean reward: 8.0, Mean Entropy: 0.0009623865480534732, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 492,  Mean reward: 8.0, Mean Entropy: 0.0009703485993668437, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 493,  Mean reward: 8.0, Mean Entropy: 0.0015287294518202543, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 494,  Mean reward: 8.0, Mean Entropy: 0.007725318428128958, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 495,  Mean reward: 8.0, Mean Entropy: 0.0021405040752142668, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 496,  Mean reward: 7.981012658227848, Mean Entropy: 0.0014685054775327444, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 497,  Mean reward: 8.0, Mean Entropy: 0.001752957934513688, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 498,  Mean reward: 8.0, Mean Entropy: 0.0018479421269148588, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 499,  Mean reward: 8.0, Mean Entropy: 0.00205793883651495, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 500,  Mean reward: 8.0, Mean Entropy: 0.0021152347326278687, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 8.0, Mean Entropy: 0.0020936126820743084, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.77s
Iteration: 502,  Mean reward: 8.0, Mean Entropy: 0.0024074141401797533, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 503,  Mean reward: 8.0, Mean Entropy: 0.0028445040807127953, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 504,  Mean reward: 8.0, Mean Entropy: 0.004350751638412476, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 505,  Mean reward: 8.0, Mean Entropy: 0.00878775492310524, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 506,  Mean reward: 8.0, Mean Entropy: 0.018316704779863358, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 507,  Mean reward: 7.25, Mean Entropy: 0.005819213576614857, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 508,  Mean reward: 8.0, Mean Entropy: 0.002430541208013892, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 509,  Mean reward: 8.0, Mean Entropy: 0.0024434335064142942, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 510,  Mean reward: 8.0, Mean Entropy: 0.0036224396899342537, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 511,  Mean reward: 8.0, Mean Entropy: 0.0022978568449616432, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 512,  Mean reward: 8.0, Mean Entropy: 0.0018222881481051445, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 513,  Mean reward: 8.0, Mean Entropy: 0.0024737457279115915, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 514,  Mean reward: 8.0, Mean Entropy: 0.001451273332349956, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 515,  Mean reward: 8.0, Mean Entropy: 0.001439828658476472, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 516,  Mean reward: 8.0, Mean Entropy: 0.0027077775448560715, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 517,  Mean reward: 8.0, Mean Entropy: 0.0030134585686028004, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 518,  Mean reward: 8.0, Mean Entropy: 0.0011330051347613335, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 519,  Mean reward: 8.0, Mean Entropy: 0.0012393590295687318, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 520,  Mean reward: 8.0, Mean Entropy: 0.0011640437878668308, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 521,  Mean reward: 8.0, Mean Entropy: 0.001582944649271667, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 522,  Mean reward: 8.0, Mean Entropy: 0.0016358805587515235, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.82s
Iteration: 523,  Mean reward: 8.0, Mean Entropy: 0.0012249178253114223, complete_episode_count: 80.0, Gather time: 0.74s, Train time: 0.79s
Iteration: 524,  Mean reward: 8.0, Mean Entropy: 0.0016258114483207464, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 525,  Mean reward: 8.0, Mean Entropy: 0.00334936729632318, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 526,  Mean reward: 8.0, Mean Entropy: 0.0020270268432796, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 527,  Mean reward: 8.0, Mean Entropy: 0.0015677337069064379, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 528,  Mean reward: 8.0, Mean Entropy: 0.001284708734601736, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 529,  Mean reward: 8.0, Mean Entropy: 0.0013607507571578026, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 530,  Mean reward: 8.0, Mean Entropy: 0.0013356551062315702, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 531,  Mean reward: 8.0, Mean Entropy: 0.0017052296316251159, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 532,  Mean reward: 8.0, Mean Entropy: 0.0016369232907891273, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 533,  Mean reward: 8.0, Mean Entropy: 0.0018103427719324827, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 534,  Mean reward: 8.0, Mean Entropy: 0.00165105692576617, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 535,  Mean reward: 8.0, Mean Entropy: 0.002651254180818796, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 536,  Mean reward: 8.0, Mean Entropy: 0.002539796754717827, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 537,  Mean reward: 8.0, Mean Entropy: 0.002479519695043564, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 538,  Mean reward: 8.0, Mean Entropy: 0.0025596769992262125, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 539,  Mean reward: 8.0, Mean Entropy: 0.002366598229855299, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 540,  Mean reward: 8.0, Mean Entropy: 0.002292775548994541, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 541,  Mean reward: 8.0, Mean Entropy: 0.0023585688322782516, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 542,  Mean reward: 8.0, Mean Entropy: 0.005393678322434425, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 543,  Mean reward: 8.0, Mean Entropy: 0.005685669835656881, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 544,  Mean reward: 8.0, Mean Entropy: 0.0026785675436258316, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 545,  Mean reward: 8.0, Mean Entropy: 0.002113920636475086, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 546,  Mean reward: 8.0, Mean Entropy: 0.002228989265859127, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 547,  Mean reward: 8.0, Mean Entropy: 0.001947882934473455, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 548,  Mean reward: 8.0, Mean Entropy: 0.0013889460824429989, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 549,  Mean reward: 8.0, Mean Entropy: 0.001079984474927187, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 550,  Mean reward: 8.0, Mean Entropy: 0.001396417384967208, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 551,  Mean reward: 8.0, Mean Entropy: 0.001428239862434566, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 552,  Mean reward: 8.0, Mean Entropy: 0.000984740792773664, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 553,  Mean reward: 8.0, Mean Entropy: 0.0008839878137223423, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 554,  Mean reward: 8.0, Mean Entropy: 0.0009083309560082853, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 555,  Mean reward: 8.0, Mean Entropy: 0.0010372400283813477, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 556,  Mean reward: 8.0, Mean Entropy: 0.0008620434673503041, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 557,  Mean reward: 8.0, Mean Entropy: 0.0008545581949874759, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.0007963086245581508, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 0.0008101607672870159, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 560,  Mean reward: 8.0, Mean Entropy: 0.0006592649733647704, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 561,  Mean reward: 8.0, Mean Entropy: 0.0005986534524708986, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 562,  Mean reward: 8.0, Mean Entropy: 0.0005942147690802813, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 563,  Mean reward: 8.0, Mean Entropy: 0.0007717594271525741, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 564,  Mean reward: 8.0, Mean Entropy: 0.0005172683740966022, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 565,  Mean reward: 8.0, Mean Entropy: 0.0006407744367606938, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 566,  Mean reward: 8.0, Mean Entropy: 0.0005993517115712166, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 567,  Mean reward: 8.0, Mean Entropy: 0.0005437210202217102, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 568,  Mean reward: 8.0, Mean Entropy: 0.0004901948850601912, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 569,  Mean reward: 8.0, Mean Entropy: 0.00048409978626295924, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.7875, Mean Entropy: 0.9025353193283081, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.52s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.107142857142857, Mean Entropy: 0.9386361837387085, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -3.141304347826087, Mean Entropy: 0.9314103126525879, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 3,  Mean reward: -3.7111111111111112, Mean Entropy: 0.8808689117431641, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 4,  Mean reward: -4.280487804878049, Mean Entropy: 0.960289716720581, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.67s
Iteration: 5,  Mean reward: -4.595238095238095, Mean Entropy: 0.9530668258666992, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 6,  Mean reward: -5.825581395348837, Mean Entropy: 0.9241663813591003, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 7,  Mean reward: -4.423913043478261, Mean Entropy: 0.8808002471923828, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 8,  Mean reward: -6.602564102564102, Mean Entropy: 0.9891351461410522, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 9,  Mean reward: -4.023255813953488, Mean Entropy: 0.9241313338279724, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 10,  Mean reward: -4.595238095238095, Mean Entropy: 0.9456137418746948, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 11,  Mean reward: -3.5625, Mean Entropy: 0.9023417234420776, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 12,  Mean reward: -6.2682926829268295, Mean Entropy: 1.025040864944458, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 13,  Mean reward: -4.6891891891891895, Mean Entropy: 0.988480269908905, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 14,  Mean reward: -6.476190476190476, Mean Entropy: 0.8949103355407715, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 15,  Mean reward: -4.226190476190476, Mean Entropy: 0.9166571497917175, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 16,  Mean reward: -5.674418604651163, Mean Entropy: 0.8802539706230164, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 17,  Mean reward: -3.590909090909091, Mean Entropy: 0.9375056624412537, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 18,  Mean reward: -5.193181818181818, Mean Entropy: 0.9969179630279541, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 19,  Mean reward: -5.125, Mean Entropy: 0.9541850090026855, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 20,  Mean reward: -2.182926829268293, Mean Entropy: 0.9374658465385437, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 21,  Mean reward: -4.7926829268292686, Mean Entropy: 0.9021196365356445, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 22,  Mean reward: -5.410256410256411, Mean Entropy: 0.9096918106079102, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 23,  Mean reward: -6.337837837837838, Mean Entropy: 0.9385678172111511, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 24,  Mean reward: -2.5833333333333335, Mean Entropy: 0.9600259065628052, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 25,  Mean reward: -1.9864864864864864, Mean Entropy: 0.8729370832443237, complete_episode_count: 37.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 26,  Mean reward: -5.451219512195122, Mean Entropy: 0.8925401568412781, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 27,  Mean reward: -1.7023809523809523, Mean Entropy: 0.9401795864105225, complete_episode_count: 42.0, Gather time: 0.62s, Train time: 1.50s
Iteration: 28,  Mean reward: -3.4146341463414633, Mean Entropy: 0.943452000617981, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 29,  Mean reward: -4.6477272727272725, Mean Entropy: 0.9579910039901733, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 30,  Mean reward: -5.144444444444445, Mean Entropy: 0.893932044506073, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 31,  Mean reward: -3.7325581395348837, Mean Entropy: 0.9313712120056152, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 32,  Mean reward: -3.966666666666667, Mean Entropy: 0.8751182556152344, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 33,  Mean reward: -1.6, Mean Entropy: 0.8966724872589111, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 34,  Mean reward: -2.148936170212766, Mean Entropy: 0.8588391542434692, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 35,  Mean reward: -1.7653061224489797, Mean Entropy: 0.8498474955558777, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 36,  Mean reward: -2.7444444444444445, Mean Entropy: 0.8228170275688171, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 37,  Mean reward: 0.5851063829787234, Mean Entropy: 0.8574409484863281, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 38,  Mean reward: -1.755813953488372, Mean Entropy: 0.8644067645072937, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.66s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 39,  Mean reward: 1.2346938775510203, Mean Entropy: 0.8253486156463623, complete_episode_count: 49.0, Gather time: 0.59s, Train time: 1.46s
Iteration: 40,  Mean reward: -5.524390243902439, Mean Entropy: 0.9307646751403809, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 41,  Mean reward: -4.177777777777778, Mean Entropy: 0.922897219657898, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 42,  Mean reward: -3.951219512195122, Mean Entropy: 0.9782425761222839, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 43,  Mean reward: -2.0113636363636362, Mean Entropy: 0.9053985476493835, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 44,  Mean reward: -1.069767441860465, Mean Entropy: 0.8473910093307495, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 45,  Mean reward: -1.5319148936170213, Mean Entropy: 0.8582110404968262, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 46,  Mean reward: -0.125, Mean Entropy: 0.8718308210372925, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 47,  Mean reward: -4.628205128205129, Mean Entropy: 0.8935467600822449, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 48,  Mean reward: -4.7631578947368425, Mean Entropy: 0.9596971273422241, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 49,  Mean reward: -6.769230769230769, Mean Entropy: 0.8880715370178223, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 50,  Mean reward: -4.346153846153846, Mean Entropy: 0.9097504615783691, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 51,  Mean reward: -4.4324324324324325, Mean Entropy: 0.9241418242454529, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 52,  Mean reward: -3.9047619047619047, Mean Entropy: 0.8735579252243042, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 53,  Mean reward: -4.75, Mean Entropy: 0.8879752159118652, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 54,  Mean reward: -4.817073170731708, Mean Entropy: 0.9023324847221375, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 55,  Mean reward: -4.573170731707317, Mean Entropy: 0.8651816844940186, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 56,  Mean reward: -2.9767441860465116, Mean Entropy: 0.9136428833007812, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 57,  Mean reward: -3.7045454545454546, Mean Entropy: 0.997307300567627, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 58,  Mean reward: -1.1818181818181819, Mean Entropy: 0.9215126037597656, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 59,  Mean reward: -3.8043478260869565, Mean Entropy: 0.8848837614059448, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 60,  Mean reward: 0.6195652173913043, Mean Entropy: 0.8130673170089722, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 61,  Mean reward: -0.6979166666666666, Mean Entropy: 0.8060662150382996, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 62,  Mean reward: -0.7978723404255319, Mean Entropy: 0.8785907626152039, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 63,  Mean reward: -4.333333333333333, Mean Entropy: 0.8557472229003906, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 64,  Mean reward: -2.7738095238095237, Mean Entropy: 0.9484970569610596, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 65,  Mean reward: 0.32978723404255317, Mean Entropy: 0.9398145079612732, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 66,  Mean reward: -3.425531914893617, Mean Entropy: 0.9723376035690308, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 67,  Mean reward: -2.3214285714285716, Mean Entropy: 0.8738951683044434, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 68,  Mean reward: -0.2553191489361702, Mean Entropy: 0.9191884994506836, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 69,  Mean reward: -0.5098039215686274, Mean Entropy: 0.8273452520370483, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 70,  Mean reward: 2.0576923076923075, Mean Entropy: 0.6897763013839722, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 71,  Mean reward: 3.0526315789473686, Mean Entropy: 0.5274307727813721, complete_episode_count: 57.0, Gather time: 0.60s, Train time: 1.68s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 72,  Mean reward: 3.789473684210526, Mean Entropy: 0.5941230058670044, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 73,  Mean reward: 4.60377358490566, Mean Entropy: 0.7141953706741333, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 74,  Mean reward: 1.6826923076923077, Mean Entropy: 0.6683270335197449, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 75,  Mean reward: 1.8703703703703705, Mean Entropy: 0.7072689533233643, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 76,  Mean reward: 3.6481481481481484, Mean Entropy: 0.6451252698898315, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 77,  Mean reward: 2.0918367346938775, Mean Entropy: 0.670975387096405, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 78,  Mean reward: 4.651785714285714, Mean Entropy: 0.6253403425216675, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.50s
Iteration: 79,  Mean reward: 1.3272727272727274, Mean Entropy: 0.5764619708061218, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 80,  Mean reward: 4.027777777777778, Mean Entropy: 0.5878832340240479, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 81,  Mean reward: 4.603448275862069, Mean Entropy: 0.6790685653686523, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 82,  Mean reward: 4.186274509803922, Mean Entropy: 0.6193081140518188, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 83,  Mean reward: 3.358490566037736, Mean Entropy: 0.6077635884284973, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 84,  Mean reward: 5.25, Mean Entropy: 0.6851100921630859, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 85,  Mean reward: 3.4035087719298245, Mean Entropy: 0.61226886510849, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 86,  Mean reward: 3.4479166666666665, Mean Entropy: 0.729820191860199, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 87,  Mean reward: 1.5, Mean Entropy: 0.7270780801773071, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 88,  Mean reward: 2.823529411764706, Mean Entropy: 0.7112735509872437, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 89,  Mean reward: 2.4903846153846154, Mean Entropy: 0.5815714597702026, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 90,  Mean reward: 3.61, Mean Entropy: 0.8424010276794434, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 91,  Mean reward: 1.36, Mean Entropy: 0.7414271235466003, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.47s
Iteration: 92,  Mean reward: 2.7547169811320753, Mean Entropy: 0.6247960925102234, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 93,  Mean reward: 3.07, Mean Entropy: 0.6249210238456726, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 94,  Mean reward: 4.0964912280701755, Mean Entropy: 0.5477858781814575, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 95,  Mean reward: 4.157407407407407, Mean Entropy: 0.6090956926345825, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 96,  Mean reward: 3.6636363636363636, Mean Entropy: 0.6143091917037964, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 97,  Mean reward: 4.833333333333333, Mean Entropy: 0.7149671912193298, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 98,  Mean reward: 4.0, Mean Entropy: 0.6327404379844666, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 99,  Mean reward: 4.33, Mean Entropy: 0.6909767985343933, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 100,  Mean reward: 4.556603773584905, Mean Entropy: 0.5959593057632446, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.50s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 1.6666666666666667, Mean Entropy: 0.6504995822906494, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 102,  Mean reward: 3.0408163265306123, Mean Entropy: 0.6239736080169678, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.70s
Iteration: 103,  Mean reward: 4.820754716981132, Mean Entropy: 0.6301119923591614, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 104,  Mean reward: 3.297872340425532, Mean Entropy: 0.6781468391418457, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 105,  Mean reward: 5.640350877192983, Mean Entropy: 0.6157240867614746, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 106,  Mean reward: 2.033333333333333, Mean Entropy: 0.5067901015281677, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 107,  Mean reward: 1.2410714285714286, Mean Entropy: 0.5565211772918701, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 108,  Mean reward: 4.336538461538462, Mean Entropy: 0.6702281832695007, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 109,  Mean reward: 2.490740740740741, Mean Entropy: 0.6997551918029785, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 110,  Mean reward: -1.1226415094339623, Mean Entropy: 0.839492917060852, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 111,  Mean reward: 1.6470588235294117, Mean Entropy: 0.8833909034729004, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 112,  Mean reward: -5.311111111111111, Mean Entropy: 0.8989911079406738, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 113,  Mean reward: -6.104651162790698, Mean Entropy: 0.6146844625473022, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 114,  Mean reward: 2.574468085106383, Mean Entropy: 0.7053477168083191, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 115,  Mean reward: 0.7021276595744681, Mean Entropy: 0.7469058036804199, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 116,  Mean reward: -0.011111111111111112, Mean Entropy: 0.7201402187347412, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 117,  Mean reward: 4.923913043478261, Mean Entropy: 0.7175073623657227, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 118,  Mean reward: 1.0434782608695652, Mean Entropy: 0.7396568059921265, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 119,  Mean reward: 0.75, Mean Entropy: 0.6819235682487488, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 120,  Mean reward: 2.683673469387755, Mean Entropy: 0.7062815427780151, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 121,  Mean reward: 4.10377358490566, Mean Entropy: 0.5542299151420593, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 122,  Mean reward: 3.5283018867924527, Mean Entropy: 0.6055505275726318, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 123,  Mean reward: 3.96875, Mean Entropy: 0.6837818026542664, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 124,  Mean reward: 4.788461538461538, Mean Entropy: 0.6083608865737915, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 125,  Mean reward: 6.107843137254902, Mean Entropy: 0.6492220163345337, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.48s
Iteration: 126,  Mean reward: 5.913793103448276, Mean Entropy: 0.505606472492218, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 127,  Mean reward: 6.139344262295082, Mean Entropy: 0.45515260100364685, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 1.47s
Iteration: 128,  Mean reward: 6.116071428571429, Mean Entropy: 0.5157514214515686, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 129,  Mean reward: 6.532258064516129, Mean Entropy: 0.3043820858001709, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 130,  Mean reward: 6.354838709677419, Mean Entropy: 0.40643101930618286, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 131,  Mean reward: 6.666666666666667, Mean Entropy: 0.4408368468284607, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 132,  Mean reward: 5.942622950819672, Mean Entropy: 0.29128193855285645, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 133,  Mean reward: 6.754237288135593, Mean Entropy: 0.3642158508300781, complete_episode_count: 59.0, Gather time: 1.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 134,  Mean reward: 7.145161290322581, Mean Entropy: 0.3135264813899994, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.94s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 135,  Mean reward: 7.470588235294118, Mean Entropy: 0.25996527075767517, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 136,  Mean reward: 7.346153846153846, Mean Entropy: 0.2142437994480133, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.72s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 137,  Mean reward: 7.7534246575342465, Mean Entropy: 0.21473436057567596, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 138,  Mean reward: 7.756944444444445, Mean Entropy: 0.1955835074186325, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 139,  Mean reward: 7.8618421052631575, Mean Entropy: 0.14957034587860107, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 140,  Mean reward: 7.787671232876712, Mean Entropy: 0.16196313500404358, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 141,  Mean reward: 7.693333333333333, Mean Entropy: 0.09248535335063934, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 142,  Mean reward: 7.881578947368421, Mean Entropy: 0.13115425407886505, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 143,  Mean reward: 7.962025316455696, Mean Entropy: 0.08621078729629517, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 144,  Mean reward: 7.88, Mean Entropy: 0.08671017736196518, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 145,  Mean reward: 7.961538461538462, Mean Entropy: 0.0638871043920517, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 146,  Mean reward: 7.955128205128205, Mean Entropy: 0.0978960320353508, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 147,  Mean reward: 7.86, Mean Entropy: 0.04702363163232803, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 148,  Mean reward: 7.981012658227848, Mean Entropy: 0.05613843724131584, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.71s
Iteration: 149,  Mean reward: 7.961538461538462, Mean Entropy: 0.041833311319351196, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 150,  Mean reward: 7.961538461538462, Mean Entropy: 0.028449539095163345, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 151,  Mean reward: 8.0, Mean Entropy: 0.027714941650629044, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 152,  Mean reward: 8.0, Mean Entropy: 0.05182233080267906, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 153,  Mean reward: 7.962025316455696, Mean Entropy: 0.04370377957820892, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 154,  Mean reward: 8.0, Mean Entropy: 0.17975366115570068, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 155,  Mean reward: 7.533783783783784, Mean Entropy: 0.05160859227180481, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 156,  Mean reward: 7.980769230769231, Mean Entropy: 0.037734292447566986, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 157,  Mean reward: 8.0, Mean Entropy: 0.12711438536643982, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 158,  Mean reward: 7.207692307692308, Mean Entropy: 0.09448304027318954, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 159,  Mean reward: 7.923076923076923, Mean Entropy: 0.09650688618421555, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 160,  Mean reward: 7.961538461538462, Mean Entropy: 0.06375429034233093, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 161,  Mean reward: 7.9423076923076925, Mean Entropy: 0.07971187680959702, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 162,  Mean reward: 8.0, Mean Entropy: 0.19754156470298767, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 163,  Mean reward: 7.431506849315069, Mean Entropy: 0.17445093393325806, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 164,  Mean reward: 7.79054054054054, Mean Entropy: 0.08436349034309387, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 165,  Mean reward: 7.82, Mean Entropy: 0.11501263827085495, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 166,  Mean reward: 7.901315789473684, Mean Entropy: 0.1280575692653656, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 167,  Mean reward: 7.84, Mean Entropy: 0.08010640740394592, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 168,  Mean reward: 7.935064935064935, Mean Entropy: 0.1413625329732895, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 169,  Mean reward: 7.228571428571429, Mean Entropy: 0.08698618412017822, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 170,  Mean reward: 7.915584415584416, Mean Entropy: 0.02424476109445095, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 171,  Mean reward: 7.981012658227848, Mean Entropy: 0.010790718719363213, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 172,  Mean reward: 8.0, Mean Entropy: 0.013920260593295097, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.90s
Iteration: 173,  Mean reward: 8.0, Mean Entropy: 0.008948281407356262, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 174,  Mean reward: 8.0, Mean Entropy: 0.01353270560503006, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 175,  Mean reward: 8.0, Mean Entropy: 0.014870706014335155, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.70s
Iteration: 176,  Mean reward: 7.961538461538462, Mean Entropy: 0.04155104607343674, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 177,  Mean reward: 7.974683544303797, Mean Entropy: 0.018099863082170486, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 178,  Mean reward: 8.0, Mean Entropy: 0.050469111651182175, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 179,  Mean reward: 7.941558441558442, Mean Entropy: 0.04099854454398155, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 180,  Mean reward: 7.981012658227848, Mean Entropy: 0.21396899223327637, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 181,  Mean reward: 5.92741935483871, Mean Entropy: 0.278236985206604, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 182,  Mean reward: 7.902597402597403, Mean Entropy: 0.07045041769742966, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 183,  Mean reward: 7.9423076923076925, Mean Entropy: 0.048466622829437256, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 184,  Mean reward: 7.955128205128205, Mean Entropy: 0.017281539738178253, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 185,  Mean reward: 8.0, Mean Entropy: 0.008284933865070343, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 186,  Mean reward: 8.0, Mean Entropy: 0.011137227527797222, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 187,  Mean reward: 7.981012658227848, Mean Entropy: 0.012889310717582703, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 188,  Mean reward: 8.0, Mean Entropy: 0.007784147746860981, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 189,  Mean reward: 7.82051282051282, Mean Entropy: 0.015385381877422333, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 190,  Mean reward: 8.0, Mean Entropy: 0.011605635285377502, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 191,  Mean reward: 8.0, Mean Entropy: 0.011677511036396027, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 192,  Mean reward: 8.0, Mean Entropy: 0.015732109546661377, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 193,  Mean reward: 8.0, Mean Entropy: 0.02310296706855297, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 194,  Mean reward: 7.981012658227848, Mean Entropy: 0.029983391985297203, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 195,  Mean reward: 8.0, Mean Entropy: 0.052924782037734985, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 196,  Mean reward: 7.922077922077922, Mean Entropy: 0.06557607650756836, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 197,  Mean reward: 7.981012658227848, Mean Entropy: 0.018429651856422424, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 198,  Mean reward: 8.0, Mean Entropy: 0.021378949284553528, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 199,  Mean reward: 7.981012658227848, Mean Entropy: 0.02153429388999939, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 200,  Mean reward: 8.0, Mean Entropy: 0.016828808933496475, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 8.0, Mean Entropy: 0.03126884624361992, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 202,  Mean reward: 8.0, Mean Entropy: 0.08379596471786499, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 203,  Mean reward: 7.881578947368421, Mean Entropy: 0.055160194635391235, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 204,  Mean reward: 7.981012658227848, Mean Entropy: 0.026844045147299767, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 205,  Mean reward: 8.0, Mean Entropy: 0.054799191653728485, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 206,  Mean reward: 7.961538461538462, Mean Entropy: 0.04997146129608154, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 207,  Mean reward: 7.981012658227848, Mean Entropy: 0.027413666248321533, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 208,  Mean reward: 7.981012658227848, Mean Entropy: 0.012630043551325798, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 209,  Mean reward: 8.0, Mean Entropy: 0.009421157650649548, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 210,  Mean reward: 8.0, Mean Entropy: 0.009725244715809822, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.93s
Iteration: 211,  Mean reward: 8.0, Mean Entropy: 0.011195551604032516, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 212,  Mean reward: 8.0, Mean Entropy: 0.013034715317189693, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 213,  Mean reward: 7.981012658227848, Mean Entropy: 0.008546194061636925, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 214,  Mean reward: 8.0, Mean Entropy: 0.007451359182596207, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 215,  Mean reward: 8.0, Mean Entropy: 0.009510263800621033, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 216,  Mean reward: 8.0, Mean Entropy: 0.010060916654765606, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 217,  Mean reward: 7.981012658227848, Mean Entropy: 0.008805686607956886, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 218,  Mean reward: 8.0, Mean Entropy: 0.011800534091889858, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 219,  Mean reward: 8.0, Mean Entropy: 0.018388448283076286, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 220,  Mean reward: 8.0, Mean Entropy: 0.024794256314635277, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 221,  Mean reward: 8.0, Mean Entropy: 0.045953959226608276, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 222,  Mean reward: 7.961538461538462, Mean Entropy: 0.03880711644887924, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 223,  Mean reward: 8.0, Mean Entropy: 0.15305712819099426, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 224,  Mean reward: 7.057971014492754, Mean Entropy: 0.1155160665512085, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 225,  Mean reward: 8.0, Mean Entropy: 0.05574673414230347, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 226,  Mean reward: 7.961538461538462, Mean Entropy: 0.05426657944917679, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 227,  Mean reward: 8.0, Mean Entropy: 0.060203246772289276, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 228,  Mean reward: 7.961538461538462, Mean Entropy: 0.06615282595157623, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 229,  Mean reward: 7.922077922077922, Mean Entropy: 0.06752632558345795, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 230,  Mean reward: 8.0, Mean Entropy: 0.05746164172887802, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 231,  Mean reward: 7.52027027027027, Mean Entropy: 0.11346016824245453, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 232,  Mean reward: 8.0, Mean Entropy: 0.062212005257606506, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 233,  Mean reward: 7.941558441558442, Mean Entropy: 0.01995348557829857, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 234,  Mean reward: 7.981012658227848, Mean Entropy: 0.006767679005861282, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 235,  Mean reward: 8.0, Mean Entropy: 0.0029585021547973156, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 236,  Mean reward: 8.0, Mean Entropy: 0.0019269322510808706, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 237,  Mean reward: 8.0, Mean Entropy: 0.0031227674335241318, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 238,  Mean reward: 8.0, Mean Entropy: 0.003961528651416302, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 239,  Mean reward: 8.0, Mean Entropy: 0.005133218131959438, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 240,  Mean reward: 8.0, Mean Entropy: 0.010328738018870354, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 241,  Mean reward: 7.981012658227848, Mean Entropy: 0.007685612887144089, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 242,  Mean reward: 8.0, Mean Entropy: 0.004765243735164404, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 243,  Mean reward: 8.0, Mean Entropy: 0.005514179822057486, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 244,  Mean reward: 7.974683544303797, Mean Entropy: 0.04005054011940956, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.71s
Iteration: 245,  Mean reward: 7.961538461538462, Mean Entropy: 0.020456209778785706, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 246,  Mean reward: 8.0, Mean Entropy: 0.021722665056586266, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 247,  Mean reward: 8.0, Mean Entropy: 0.04128023236989975, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 248,  Mean reward: 7.961538461538462, Mean Entropy: 0.0355052724480629, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.92s
Iteration: 249,  Mean reward: 8.0, Mean Entropy: 0.21467304229736328, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 250,  Mean reward: 1.7796610169491525, Mean Entropy: 0.23599573969841003, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 251,  Mean reward: 7.623376623376624, Mean Entropy: 0.061606697738170624, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 252,  Mean reward: 8.0, Mean Entropy: 0.11729496717453003, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 253,  Mean reward: 7.602941176470588, Mean Entropy: 0.10804003477096558, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 254,  Mean reward: 7.981012658227848, Mean Entropy: 0.008937227539718151, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 255,  Mean reward: 8.0, Mean Entropy: 0.012227259576320648, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 256,  Mean reward: 8.0, Mean Entropy: 0.023149430751800537, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 257,  Mean reward: 8.0, Mean Entropy: 0.11677992343902588, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 258,  Mean reward: 7.486486486486487, Mean Entropy: 0.12400001287460327, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 259,  Mean reward: 7.9423076923076925, Mean Entropy: 0.034070681780576706, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 260,  Mean reward: 8.0, Mean Entropy: 0.04487457126379013, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 261,  Mean reward: 7.7368421052631575, Mean Entropy: 0.04487600177526474, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 262,  Mean reward: 8.0, Mean Entropy: 0.013312743976712227, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 263,  Mean reward: 7.981012658227848, Mean Entropy: 0.01100139319896698, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 264,  Mean reward: 7.981012658227848, Mean Entropy: 0.0037576481699943542, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 265,  Mean reward: 8.0, Mean Entropy: 0.002739356830716133, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 266,  Mean reward: 8.0, Mean Entropy: 0.002480358351022005, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 267,  Mean reward: 8.0, Mean Entropy: 0.0019892945419996977, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 268,  Mean reward: 8.0, Mean Entropy: 0.002558546606451273, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 269,  Mean reward: 8.0, Mean Entropy: 0.003256383817642927, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 270,  Mean reward: 8.0, Mean Entropy: 0.0034940734039992094, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 271,  Mean reward: 8.0, Mean Entropy: 0.00339932506904006, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 272,  Mean reward: 8.0, Mean Entropy: 0.008334603160619736, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 273,  Mean reward: 8.0, Mean Entropy: 0.05885925143957138, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 274,  Mean reward: 7.961538461538462, Mean Entropy: 0.07100405544042587, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 275,  Mean reward: 7.981012658227848, Mean Entropy: 0.156919464468956, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 276,  Mean reward: 6.302816901408451, Mean Entropy: 0.03477579355239868, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 277,  Mean reward: 8.0, Mean Entropy: 0.02717963419854641, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 278,  Mean reward: 7.974683544303797, Mean Entropy: 0.045875608921051025, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 279,  Mean reward: 7.961538461538462, Mean Entropy: 0.04240185022354126, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 280,  Mean reward: 7.962025316455696, Mean Entropy: 0.02971050702035427, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.86s
Iteration: 281,  Mean reward: 7.981012658227848, Mean Entropy: 0.013407565653324127, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 282,  Mean reward: 7.981012658227848, Mean Entropy: 0.01026986539363861, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 283,  Mean reward: 8.0, Mean Entropy: 0.011697359383106232, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 284,  Mean reward: 8.0, Mean Entropy: 0.035846345126628876, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 285,  Mean reward: 7.941558441558442, Mean Entropy: 0.04827791452407837, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 286,  Mean reward: 8.0, Mean Entropy: 0.019768040627241135, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.91s
Iteration: 287,  Mean reward: 8.0, Mean Entropy: 0.20388492941856384, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 288,  Mean reward: 7.457142857142857, Mean Entropy: 0.16701161861419678, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 289,  Mean reward: 7.86, Mean Entropy: 0.029118238016963005, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 290,  Mean reward: 8.0, Mean Entropy: 0.12369909137487411, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 291,  Mean reward: 7.881578947368421, Mean Entropy: 0.08937245607376099, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 292,  Mean reward: 7.9423076923076925, Mean Entropy: 0.0968291163444519, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 293,  Mean reward: 7.605263157894737, Mean Entropy: 0.06441396474838257, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 294,  Mean reward: 7.961538461538462, Mean Entropy: 0.028559153899550438, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 295,  Mean reward: 8.0, Mean Entropy: 0.03502694517374039, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 296,  Mean reward: 7.981012658227848, Mean Entropy: 0.032242268323898315, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 297,  Mean reward: 7.961538461538462, Mean Entropy: 0.013483159244060516, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 298,  Mean reward: 8.0, Mean Entropy: 0.009515400975942612, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 299,  Mean reward: 8.0, Mean Entropy: 0.010907444171607494, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 300,  Mean reward: 7.981012658227848, Mean Entropy: 0.004168995656073093, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 7.981012658227848, Mean Entropy: 0.0014922420959919691, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 302,  Mean reward: 8.0, Mean Entropy: 0.0011015980271622539, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 303,  Mean reward: 8.0, Mean Entropy: 0.0009144414216279984, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 304,  Mean reward: 8.0, Mean Entropy: 0.0008733766153454781, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 305,  Mean reward: 8.0, Mean Entropy: 0.0007592263864353299, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 306,  Mean reward: 8.0, Mean Entropy: 0.0009402378345839679, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 307,  Mean reward: 8.0, Mean Entropy: 0.0009655570611357689, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 308,  Mean reward: 8.0, Mean Entropy: 0.0009013063972815871, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 309,  Mean reward: 8.0, Mean Entropy: 0.0009314889903180301, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 310,  Mean reward: 8.0, Mean Entropy: 0.0009455840918235481, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 311,  Mean reward: 8.0, Mean Entropy: 0.0012196180177852511, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 312,  Mean reward: 8.0, Mean Entropy: 0.001373711391352117, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 313,  Mean reward: 8.0, Mean Entropy: 0.002507858444005251, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 314,  Mean reward: 8.0, Mean Entropy: 0.00933723896741867, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 315,  Mean reward: 8.0, Mean Entropy: 0.2405831664800644, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 316,  Mean reward: 7.0546875, Mean Entropy: 0.004350667353719473, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 317,  Mean reward: 7.981012658227848, Mean Entropy: 0.00536485156044364, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 318,  Mean reward: 7.981012658227848, Mean Entropy: 0.002910679206252098, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 319,  Mean reward: 8.0, Mean Entropy: 0.0016322077717632055, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 320,  Mean reward: 8.0, Mean Entropy: 0.0010223810095340014, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 321,  Mean reward: 8.0, Mean Entropy: 0.0010288041085004807, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 322,  Mean reward: 8.0, Mean Entropy: 0.0007200014661066234, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 323,  Mean reward: 8.0, Mean Entropy: 0.0008580292342230678, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 324,  Mean reward: 8.0, Mean Entropy: 0.0008326463284902275, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 325,  Mean reward: 8.0, Mean Entropy: 0.0007157537620514631, complete_episode_count: 80.0, Gather time: 0.65s, Train time: 0.74s
Iteration: 326,  Mean reward: 8.0, Mean Entropy: 0.0009165660012513399, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 327,  Mean reward: 8.0, Mean Entropy: 0.0009497327264398336, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 328,  Mean reward: 8.0, Mean Entropy: 0.0009233138407580554, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 329,  Mean reward: 8.0, Mean Entropy: 0.0023004505783319473, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 330,  Mean reward: 8.0, Mean Entropy: 0.005512806586921215, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 331,  Mean reward: 8.0, Mean Entropy: 0.1821889877319336, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 332,  Mean reward: 6.983606557377049, Mean Entropy: 0.12485842406749725, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 333,  Mean reward: 7.75, Mean Entropy: 0.05247887223958969, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 334,  Mean reward: 8.0, Mean Entropy: 0.21828600764274597, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 335,  Mean reward: 6.5396825396825395, Mean Entropy: 0.20269887149333954, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 336,  Mean reward: 7.881578947368421, Mean Entropy: 0.04034678637981415, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 337,  Mean reward: 8.0, Mean Entropy: 0.007489349227398634, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 338,  Mean reward: 8.0, Mean Entropy: 0.13892363011837006, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 339,  Mean reward: 7.328571428571428, Mean Entropy: 0.14352652430534363, complete_episode_count: 70.0, Gather time: 0.64s, Train time: 0.80s
Iteration: 340,  Mean reward: 7.84, Mean Entropy: 0.10999307036399841, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 341,  Mean reward: 7.597402597402597, Mean Entropy: 0.06352923065423965, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 342,  Mean reward: 7.928571428571429, Mean Entropy: 0.01726289466023445, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 343,  Mean reward: 8.0, Mean Entropy: 0.178223118185997, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 344,  Mean reward: 7.837837837837838, Mean Entropy: 0.12542402744293213, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 345,  Mean reward: 7.729166666666667, Mean Entropy: 0.2379562109708786, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 346,  Mean reward: 7.119047619047619, Mean Entropy: 0.04225839301943779, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.86s
Iteration: 347,  Mean reward: 7.961538461538462, Mean Entropy: 0.05474633723497391, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 348,  Mean reward: 7.902597402597403, Mean Entropy: 0.014401555061340332, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 349,  Mean reward: 8.0, Mean Entropy: 0.06251221895217896, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 350,  Mean reward: 7.923076923076923, Mean Entropy: 0.03171452134847641, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 351,  Mean reward: 7.980769230769231, Mean Entropy: 0.020908113569021225, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 352,  Mean reward: 7.981012658227848, Mean Entropy: 0.028105448931455612, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 353,  Mean reward: 7.981012658227848, Mean Entropy: 0.0014343520160764456, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 354,  Mean reward: 8.0, Mean Entropy: 0.0007287319749593735, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 355,  Mean reward: 8.0, Mean Entropy: 0.0006145339575596154, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 356,  Mean reward: 8.0, Mean Entropy: 0.0007636998780071735, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 357,  Mean reward: 8.0, Mean Entropy: 0.0007966157281771302, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 358,  Mean reward: 8.0, Mean Entropy: 0.0007268100744113326, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 359,  Mean reward: 8.0, Mean Entropy: 0.0007986437412910163, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 360,  Mean reward: 8.0, Mean Entropy: 0.001338971545919776, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 361,  Mean reward: 8.0, Mean Entropy: 0.001468744594603777, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 362,  Mean reward: 8.0, Mean Entropy: 0.001795171294361353, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 363,  Mean reward: 8.0, Mean Entropy: 0.0027542412281036377, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 364,  Mean reward: 8.0, Mean Entropy: 0.011160781607031822, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 365,  Mean reward: 8.0, Mean Entropy: 0.08977216482162476, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 366,  Mean reward: 7.366197183098592, Mean Entropy: 0.11285382509231567, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 367,  Mean reward: 7.901315789473684, Mean Entropy: 0.025036951526999474, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 368,  Mean reward: 7.981012658227848, Mean Entropy: 0.007233345415443182, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 369,  Mean reward: 8.0, Mean Entropy: 0.1975838840007782, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 370,  Mean reward: 7.309523809523809, Mean Entropy: 0.1225014179944992, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 371,  Mean reward: 7.902597402597403, Mean Entropy: 0.1502058058977127, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 372,  Mean reward: 7.858108108108108, Mean Entropy: 0.07749557495117188, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 373,  Mean reward: 7.961538461538462, Mean Entropy: 0.07521810382604599, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 374,  Mean reward: 7.955128205128205, Mean Entropy: 0.05627208203077316, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 375,  Mean reward: 7.941558441558442, Mean Entropy: 0.03592311590909958, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 376,  Mean reward: 7.981012658227848, Mean Entropy: 0.0232959296554327, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 377,  Mean reward: 8.0, Mean Entropy: 0.12509912252426147, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 378,  Mean reward: 7.618421052631579, Mean Entropy: 0.09809938073158264, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 379,  Mean reward: 7.883116883116883, Mean Entropy: 0.05312373489141464, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 380,  Mean reward: 7.981012658227848, Mean Entropy: 0.040731824934482574, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 381,  Mean reward: 7.961538461538462, Mean Entropy: 0.06212658807635307, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 382,  Mean reward: 7.9423076923076925, Mean Entropy: 0.07553137838840485, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 383,  Mean reward: 7.961538461538462, Mean Entropy: 0.06601957231760025, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 384,  Mean reward: 7.730263157894737, Mean Entropy: 0.02756347879767418, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 385,  Mean reward: 8.0, Mean Entropy: 0.011649839580059052, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 386,  Mean reward: 7.981012658227848, Mean Entropy: 0.0010167800355702639, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 0.0005714419530704618, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 0.0004858328029513359, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 389,  Mean reward: 8.0, Mean Entropy: 0.0005695290747098625, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 390,  Mean reward: 8.0, Mean Entropy: 0.0007906283717602491, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 0.0013679687399417162, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 392,  Mean reward: 8.0, Mean Entropy: 0.002263444708660245, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 0.011728249490261078, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 0.07963308691978455, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 395,  Mean reward: 7.902597402597403, Mean Entropy: 0.053284063935279846, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 396,  Mean reward: 7.941558441558442, Mean Entropy: 0.006375109311193228, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 0.0011714339489117265, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 0.0022247284650802612, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 0.018016453832387924, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 0.07708920538425446, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.93s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 7.902597402597403, Mean Entropy: 0.05626719444990158, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 402,  Mean reward: 7.981012658227848, Mean Entropy: 0.06639081239700317, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 403,  Mean reward: 7.981012658227848, Mean Entropy: 0.03377857431769371, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 404,  Mean reward: 7.981012658227848, Mean Entropy: 0.04073014855384827, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 405,  Mean reward: 7.961538461538462, Mean Entropy: 0.012002321891486645, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 0.023924848064780235, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 407,  Mean reward: 7.961538461538462, Mean Entropy: 0.005261266138404608, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 0.0019307562615722418, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 0.005803605075925589, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 410,  Mean reward: 8.0, Mean Entropy: 0.014963465742766857, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 411,  Mean reward: 7.981012658227848, Mean Entropy: 0.01286675687879324, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 412,  Mean reward: 8.0, Mean Entropy: 0.02318969927728176, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 413,  Mean reward: 8.0, Mean Entropy: 0.04121846333146095, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 0.031580328941345215, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 415,  Mean reward: 8.0, Mean Entropy: 0.03349093720316887, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 416,  Mean reward: 7.981012658227848, Mean Entropy: 0.023960355669260025, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 417,  Mean reward: 8.0, Mean Entropy: 0.033069826662540436, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 418,  Mean reward: 7.981012658227848, Mean Entropy: 0.023510819301009178, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 419,  Mean reward: 7.962025316455696, Mean Entropy: 0.014699186198413372, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 0.01793232001364231, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 421,  Mean reward: 8.0, Mean Entropy: 0.02705179713666439, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.0921013355255127, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 423,  Mean reward: 7.573333333333333, Mean Entropy: 0.10377514362335205, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 424,  Mean reward: 7.921052631578948, Mean Entropy: 0.02119717374444008, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 0.022341288626194, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 0.07232724130153656, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 427,  Mean reward: 7.922077922077922, Mean Entropy: 0.060214266180992126, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 428,  Mean reward: 7.941558441558442, Mean Entropy: 0.04179573059082031, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 429,  Mean reward: 7.961538461538462, Mean Entropy: 0.015734529122710228, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 430,  Mean reward: 7.980769230769231, Mean Entropy: 0.0013176355278119445, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 4.0474769775755703e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 432,  Mean reward: 8.0, Mean Entropy: 5.1551258366089314e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 433,  Mean reward: 8.0, Mean Entropy: 8.400129445362836e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 434,  Mean reward: 8.0, Mean Entropy: 7.922663644421846e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 435,  Mean reward: 8.0, Mean Entropy: 6.050846423022449e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 436,  Mean reward: 8.0, Mean Entropy: 6.155909795779735e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 437,  Mean reward: 8.0, Mean Entropy: 5.459978638100438e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 438,  Mean reward: 8.0, Mean Entropy: 5.7626333727966994e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 5.5788634199416265e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 440,  Mean reward: 8.0, Mean Entropy: 6.0610655054915696e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 441,  Mean reward: 8.0, Mean Entropy: 4.921148865832947e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 3.6369310691952705e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 443,  Mean reward: 8.0, Mean Entropy: 4.815119245904498e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 444,  Mean reward: 8.0, Mean Entropy: 4.130677552893758e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 445,  Mean reward: 8.0, Mean Entropy: 2.983187005156651e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 446,  Mean reward: 8.0, Mean Entropy: 3.854636452160776e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 447,  Mean reward: 8.0, Mean Entropy: 3.009658757946454e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 448,  Mean reward: 8.0, Mean Entropy: 3.23244821629487e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 449,  Mean reward: 8.0, Mean Entropy: 3.123685019090772e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 450,  Mean reward: 8.0, Mean Entropy: 3.758823368116282e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 451,  Mean reward: 8.0, Mean Entropy: 3.7045996577944607e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 452,  Mean reward: 8.0, Mean Entropy: 3.802352512138896e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 453,  Mean reward: 8.0, Mean Entropy: 3.2422656659036875e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 454,  Mean reward: 8.0, Mean Entropy: 3.055266279261559e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 455,  Mean reward: 8.0, Mean Entropy: 3.507463407004252e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 456,  Mean reward: 8.0, Mean Entropy: 2.8527760150609538e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 457,  Mean reward: 8.0, Mean Entropy: 4.104050458408892e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 458,  Mean reward: 8.0, Mean Entropy: 3.7482088373508304e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 459,  Mean reward: 8.0, Mean Entropy: 3.9557031414005905e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 460,  Mean reward: 8.0, Mean Entropy: 3.657939305412583e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.70s
Iteration: 461,  Mean reward: 8.0, Mean Entropy: 3.0038207114557736e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 462,  Mean reward: 8.0, Mean Entropy: 4.013365105492994e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 463,  Mean reward: 8.0, Mean Entropy: 4.3108913814648986e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 464,  Mean reward: 8.0, Mean Entropy: 3.989031392848119e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 465,  Mean reward: 8.0, Mean Entropy: 4.34017856605351e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 466,  Mean reward: 8.0, Mean Entropy: 4.611790063790977e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 467,  Mean reward: 8.0, Mean Entropy: 4.327455098973587e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 468,  Mean reward: 8.0, Mean Entropy: 3.538024611771107e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 469,  Mean reward: 8.0, Mean Entropy: 4.897657345281914e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 470,  Mean reward: 8.0, Mean Entropy: 6.24012463958934e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 471,  Mean reward: 8.0, Mean Entropy: 6.232651503523812e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 472,  Mean reward: 8.0, Mean Entropy: 6.491367821581662e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 473,  Mean reward: 8.0, Mean Entropy: 6.167704123072326e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 474,  Mean reward: 8.0, Mean Entropy: 6.451595982071012e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 475,  Mean reward: 8.0, Mean Entropy: 0.00010551762534305453, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 476,  Mean reward: 8.0, Mean Entropy: 0.000121135642984882, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 477,  Mean reward: 8.0, Mean Entropy: 0.00012666350812651217, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 478,  Mean reward: 8.0, Mean Entropy: 0.00014494868810288608, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 479,  Mean reward: 8.0, Mean Entropy: 0.00016870074614416808, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 480,  Mean reward: 8.0, Mean Entropy: 0.0002026623988058418, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 481,  Mean reward: 8.0, Mean Entropy: 0.0002670046524144709, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 482,  Mean reward: 8.0, Mean Entropy: 0.0004192765336483717, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 483,  Mean reward: 8.0, Mean Entropy: 0.0007423992501571774, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 484,  Mean reward: 8.0, Mean Entropy: 0.001630963757634163, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 485,  Mean reward: 8.0, Mean Entropy: 0.0038763955235481262, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 486,  Mean reward: 8.0, Mean Entropy: 0.015189595520496368, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 487,  Mean reward: 8.0, Mean Entropy: 0.04433499276638031, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 488,  Mean reward: 7.980769230769231, Mean Entropy: 0.018490448594093323, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 489,  Mean reward: 8.0, Mean Entropy: 0.00011117913527414203, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 490,  Mean reward: 8.0, Mean Entropy: 0.00015243643429130316, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 491,  Mean reward: 8.0, Mean Entropy: 0.00024286039115395397, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 492,  Mean reward: 8.0, Mean Entropy: 0.0003044071781914681, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 493,  Mean reward: 8.0, Mean Entropy: 0.0002196751011069864, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 494,  Mean reward: 8.0, Mean Entropy: 0.00023542786948382854, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 495,  Mean reward: 8.0, Mean Entropy: 0.0001325943594565615, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 496,  Mean reward: 8.0, Mean Entropy: 0.00014485439169220626, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 497,  Mean reward: 8.0, Mean Entropy: 0.00016924572992138565, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 498,  Mean reward: 8.0, Mean Entropy: 0.0001454589219065383, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 499,  Mean reward: 8.0, Mean Entropy: 9.392783977091312e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 500,  Mean reward: 8.0, Mean Entropy: 9.728343866299838e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 8.0, Mean Entropy: 8.29245982458815e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 502,  Mean reward: 8.0, Mean Entropy: 8.037588850129396e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 503,  Mean reward: 8.0, Mean Entropy: 8.11441132100299e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 504,  Mean reward: 8.0, Mean Entropy: 7.048244879115373e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 505,  Mean reward: 8.0, Mean Entropy: 7.982902752701193e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 506,  Mean reward: 8.0, Mean Entropy: 6.294589547906071e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 507,  Mean reward: 8.0, Mean Entropy: 6.008569471305236e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 508,  Mean reward: 8.0, Mean Entropy: 4.792767867911607e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 509,  Mean reward: 8.0, Mean Entropy: 5.270111432764679e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 510,  Mean reward: 8.0, Mean Entropy: 4.4038108171662316e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 511,  Mean reward: 8.0, Mean Entropy: 5.2862065786030143e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 512,  Mean reward: 8.0, Mean Entropy: 4.529363286565058e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 513,  Mean reward: 8.0, Mean Entropy: 4.171087493887171e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 514,  Mean reward: 8.0, Mean Entropy: 4.441308192326687e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 515,  Mean reward: 8.0, Mean Entropy: 3.977609958383255e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 516,  Mean reward: 8.0, Mean Entropy: 4.7575256758136675e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 517,  Mean reward: 8.0, Mean Entropy: 3.1064715585671365e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 518,  Mean reward: 8.0, Mean Entropy: 4.2438143282197416e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 519,  Mean reward: 8.0, Mean Entropy: 4.345586057752371e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 520,  Mean reward: 8.0, Mean Entropy: 3.627654223237187e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 521,  Mean reward: 8.0, Mean Entropy: 3.42680505127646e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 522,  Mean reward: 8.0, Mean Entropy: 3.998800821136683e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 523,  Mean reward: 8.0, Mean Entropy: 3.995070801465772e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 524,  Mean reward: 8.0, Mean Entropy: 4.4509637518785894e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 525,  Mean reward: 8.0, Mean Entropy: 3.1668048904975876e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 526,  Mean reward: 8.0, Mean Entropy: 4.1121220419881865e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 527,  Mean reward: 8.0, Mean Entropy: 3.6595949495676905e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 528,  Mean reward: 8.0, Mean Entropy: 4.466708924155682e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 529,  Mean reward: 8.0, Mean Entropy: 4.8066558520076796e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 530,  Mean reward: 8.0, Mean Entropy: 3.5230252251494676e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 531,  Mean reward: 8.0, Mean Entropy: 4.0392707887804136e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 532,  Mean reward: 8.0, Mean Entropy: 4.082082159584388e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 533,  Mean reward: 8.0, Mean Entropy: 4.2997708078473806e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 534,  Mean reward: 8.0, Mean Entropy: 4.531761078396812e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 535,  Mean reward: 8.0, Mean Entropy: 3.79136145056691e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 536,  Mean reward: 8.0, Mean Entropy: 4.574909689836204e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.85s
Iteration: 537,  Mean reward: 8.0, Mean Entropy: 4.3105326767545193e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 538,  Mean reward: 8.0, Mean Entropy: 3.291579560027458e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 539,  Mean reward: 8.0, Mean Entropy: 5.519636761164293e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 540,  Mean reward: 8.0, Mean Entropy: 4.690560672315769e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 541,  Mean reward: 8.0, Mean Entropy: 5.296451490721665e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 542,  Mean reward: 8.0, Mean Entropy: 4.851002449868247e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 543,  Mean reward: 8.0, Mean Entropy: 5.4894404456717893e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.71s
Iteration: 544,  Mean reward: 8.0, Mean Entropy: 5.9639307437464595e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 545,  Mean reward: 8.0, Mean Entropy: 5.809164576930925e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 546,  Mean reward: 8.0, Mean Entropy: 7.1747723268345e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 547,  Mean reward: 8.0, Mean Entropy: 5.434850390884094e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 548,  Mean reward: 8.0, Mean Entropy: 6.422594742616639e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 549,  Mean reward: 8.0, Mean Entropy: 8.627748320577666e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 550,  Mean reward: 8.0, Mean Entropy: 5.478132516145706e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.81s
Iteration: 551,  Mean reward: 8.0, Mean Entropy: 0.00012037437409162521, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 552,  Mean reward: 8.0, Mean Entropy: 0.00011467721196822822, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 553,  Mean reward: 8.0, Mean Entropy: 9.498011786490679e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 554,  Mean reward: 8.0, Mean Entropy: 0.00013532328011933714, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 555,  Mean reward: 8.0, Mean Entropy: 0.00020631166989915073, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 556,  Mean reward: 8.0, Mean Entropy: 0.0003160018823109567, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 557,  Mean reward: 8.0, Mean Entropy: 0.0004362639447208494, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.0007086399709805846, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 0.002858672058209777, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 560,  Mean reward: 8.0, Mean Entropy: 0.013618726283311844, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 561,  Mean reward: 8.0, Mean Entropy: 0.028787806630134583, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 562,  Mean reward: 8.0, Mean Entropy: 0.131511852145195, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 563,  Mean reward: 7.5, Mean Entropy: 0.04145308956503868, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 564,  Mean reward: 8.0, Mean Entropy: 0.20002858340740204, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 565,  Mean reward: 3.63, Mean Entropy: 0.18500392138957977, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 566,  Mean reward: 7.974683544303797, Mean Entropy: 0.12720414996147156, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 567,  Mean reward: 2.52, Mean Entropy: 0.35952550172805786, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 568,  Mean reward: 7.128787878787879, Mean Entropy: 0.26575323939323425, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 569,  Mean reward: 7.2835820895522385, Mean Entropy: 0.32936975359916687, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 570,  Mean reward: 7.384057971014493, Mean Entropy: 0.3063342571258545, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 571,  Mean reward: 7.7534246575342465, Mean Entropy: 0.11816229671239853, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 572,  Mean reward: 7.981012658227848, Mean Entropy: 0.3063206672668457, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.70s
Iteration: 573,  Mean reward: 1.5277777777777777, Mean Entropy: 0.5661253333091736, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 574,  Mean reward: 4.950980392156863, Mean Entropy: 0.2568776309490204, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 575,  Mean reward: 7.981012658227848, Mean Entropy: 0.0016757349949330091, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 576,  Mean reward: 8.0, Mean Entropy: 0.1670684516429901, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 577,  Mean reward: 7.253623188405797, Mean Entropy: 0.11274544894695282, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 578,  Mean reward: 7.961538461538462, Mean Entropy: 0.009721679612994194, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 579,  Mean reward: 8.0, Mean Entropy: 0.0076017617247998714, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 580,  Mean reward: 8.0, Mean Entropy: 0.17454206943511963, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 581,  Mean reward: 1.8645833333333333, Mean Entropy: 0.2920585572719574, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 582,  Mean reward: 8.0, Mean Entropy: 0.22701847553253174, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 583,  Mean reward: 6.653846153846154, Mean Entropy: 0.33738943934440613, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 584,  Mean reward: 7.121212121212121, Mean Entropy: 0.34216660261154175, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 585,  Mean reward: 7.253623188405797, Mean Entropy: 0.29696130752563477, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 586,  Mean reward: 7.454545454545454, Mean Entropy: 0.3038446307182312, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 587,  Mean reward: 7.507462686567164, Mean Entropy: 0.31731414794921875, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 588,  Mean reward: 7.375, Mean Entropy: 0.09248002618551254, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 589,  Mean reward: 7.948717948717949, Mean Entropy: 0.32523050904273987, complete_episode_count: 78.0, Gather time: 0.69s, Train time: 0.76s
Iteration: 590,  Mean reward: 7.261904761904762, Mean Entropy: 0.1270638108253479, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 591,  Mean reward: 7.9423076923076925, Mean Entropy: 0.017909135669469833, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 592,  Mean reward: 8.0, Mean Entropy: 0.20244906842708588, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 593,  Mean reward: 7.174242424242424, Mean Entropy: 0.35855358839035034, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 594,  Mean reward: 7.635714285714286, Mean Entropy: 0.276029109954834, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 595,  Mean reward: 7.608695652173913, Mean Entropy: 0.2660540044307709, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 596,  Mean reward: 6.977611940298507, Mean Entropy: 0.28021055459976196, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 597,  Mean reward: 6.76984126984127, Mean Entropy: 0.35288071632385254, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 598,  Mean reward: 7.0, Mean Entropy: 0.3580182194709778, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 599,  Mean reward: 6.901515151515151, Mean Entropy: 0.03542499244213104, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 600,  Mean reward: 8.0, Mean Entropy: 0.0021241323556751013, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 8.0, Mean Entropy: 0.001830176799558103, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 602,  Mean reward: 8.0, Mean Entropy: 0.001808444387279451, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 603,  Mean reward: 8.0, Mean Entropy: 0.0016981740482151508, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 604,  Mean reward: 8.0, Mean Entropy: 0.0015875024255365133, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 605,  Mean reward: 8.0, Mean Entropy: 0.001930272439494729, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.002795268315821886, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 607,  Mean reward: 8.0, Mean Entropy: 0.007104109972715378, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 608,  Mean reward: 8.0, Mean Entropy: 0.04074397683143616, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 609,  Mean reward: 7.9423076923076925, Mean Entropy: 0.055112116038799286, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 610,  Mean reward: 7.961538461538462, Mean Entropy: 0.025983860716223717, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 611,  Mean reward: 8.0, Mean Entropy: 0.15332743525505066, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 612,  Mean reward: 7.382352941176471, Mean Entropy: 0.14188802242279053, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 613,  Mean reward: 7.961538461538462, Mean Entropy: 0.06080257147550583, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 614,  Mean reward: 7.961538461538462, Mean Entropy: 0.06470207124948502, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 615,  Mean reward: 7.88, Mean Entropy: 0.026436593383550644, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 616,  Mean reward: 8.0, Mean Entropy: 0.04401835426688194, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 617,  Mean reward: 7.922077922077922, Mean Entropy: 0.009367583319544792, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 618,  Mean reward: 8.0, Mean Entropy: 0.002659653080627322, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 619,  Mean reward: 8.0, Mean Entropy: 0.0044255247339606285, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 620,  Mean reward: 8.0, Mean Entropy: 0.006871383637189865, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 621,  Mean reward: 8.0, Mean Entropy: 0.014294130727648735, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 622,  Mean reward: 7.981012658227848, Mean Entropy: 0.009220937266945839, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 623,  Mean reward: 8.0, Mean Entropy: 0.007301810663193464, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 624,  Mean reward: 8.0, Mean Entropy: 0.01442091166973114, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 625,  Mean reward: 8.0, Mean Entropy: 0.10994386672973633, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 626,  Mean reward: 7.881578947368421, Mean Entropy: 0.04064670205116272, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 627,  Mean reward: 8.0, Mean Entropy: 0.12189090251922607, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 628,  Mean reward: 7.901315789473684, Mean Entropy: 0.15168777108192444, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.94s
Iteration: 629,  Mean reward: 7.608695652173913, Mean Entropy: 0.12675438821315765, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 630,  Mean reward: 7.382352941176471, Mean Entropy: 0.09262527525424957, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 631,  Mean reward: 8.0, Mean Entropy: 0.013496873900294304, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 632,  Mean reward: 7.981012658227848, Mean Entropy: 0.0037981695495545864, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 633,  Mean reward: 8.0, Mean Entropy: 0.0035798929166048765, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 634,  Mean reward: 8.0, Mean Entropy: 0.0048074969090521336, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 635,  Mean reward: 8.0, Mean Entropy: 0.009056756272912025, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 636,  Mean reward: 7.981012658227848, Mean Entropy: 0.019984522834420204, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 637,  Mean reward: 8.0, Mean Entropy: 0.14158277213573456, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 638,  Mean reward: 2.67, Mean Entropy: 0.21258829534053802, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 639,  Mean reward: 8.0, Mean Entropy: 0.019686125218868256, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 640,  Mean reward: 7.962025316455696, Mean Entropy: 0.004933408927172422, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 641,  Mean reward: 8.0, Mean Entropy: 0.003688347525894642, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 642,  Mean reward: 8.0, Mean Entropy: 0.004556916654109955, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 643,  Mean reward: 8.0, Mean Entropy: 0.011294934898614883, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 644,  Mean reward: 8.0, Mean Entropy: 0.05878753587603569, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 645,  Mean reward: 7.901315789473684, Mean Entropy: 0.06222304329276085, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 646,  Mean reward: 7.981012658227848, Mean Entropy: 0.04245966672897339, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 647,  Mean reward: 7.981012658227848, Mean Entropy: 0.024334009736776352, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 648,  Mean reward: 8.0, Mean Entropy: 0.07928640395402908, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 649,  Mean reward: 7.8175675675675675, Mean Entropy: 0.039480436593294144, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 650,  Mean reward: 7.981012658227848, Mean Entropy: 0.02732725813984871, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 651,  Mean reward: 7.981012658227848, Mean Entropy: 0.047717608511447906, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.44it/s]100%|| 1/1 [00:00<00:00,  1.44it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.80it/s]100%|| 1/1 [00:00<00:00,  1.80it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.80it/s]100%|| 1/1 [00:00<00:00,  1.80it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: 8.0
  std over seeds: 0.0
  per seed: [8.000 8.000 8.000]

success_rate.......
  avg over seeds: 1.0
  std over seeds: 0.0
  per seed: [1.000 1.000 1.000]

batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: None
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: v
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
critic: v
node_dim: 7
lstm_on: False
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy(
  (FE): FeatureExtractor(
    (gat): GATv2(7, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (theta8_v): Linear(in_features=24, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with LSTM switched off and GATv2 feature extraction
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.theta8_v.weight        [1, 24]      requires_grad=True
V.theta8_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 6698
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -4.45, Mean Entropy: 0.9169758558273315, complete_episode_count: 40.0, Gather time: 5.75s, Train time: 3.36s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.390243902439025, Mean Entropy: 0.9386364817619324, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.49s
Iteration: 2,  Mean reward: -5.414634146341464, Mean Entropy: 0.9747366905212402, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 3,  Mean reward: -4.904255319148936, Mean Entropy: 0.9602926969528198, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 4,  Mean reward: -3.841463414634146, Mean Entropy: 0.9602903723716736, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 5,  Mean reward: -4.865853658536586, Mean Entropy: 0.9097499847412109, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 6,  Mean reward: -2.4642857142857144, Mean Entropy: 0.9747343063354492, complete_episode_count: 42.0, Gather time: 0.62s, Train time: 1.43s
Iteration: 7,  Mean reward: -5.2439024390243905, Mean Entropy: 0.9458522796630859, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 8,  Mean reward: -5.935897435897436, Mean Entropy: 0.9675084948539734, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 9,  Mean reward: -4.448717948717949, Mean Entropy: 0.9530701041221619, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 10,  Mean reward: -6.523255813953488, Mean Entropy: 0.8664266467094421, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 11,  Mean reward: -1.930232558139535, Mean Entropy: 0.909747302532196, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 12,  Mean reward: -3.6875, Mean Entropy: 0.9891670942306519, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.47s
Iteration: 13,  Mean reward: -4.337209302325581, Mean Entropy: 0.9458433389663696, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 14,  Mean reward: -2.9782608695652173, Mean Entropy: 0.945835292339325, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 15,  Mean reward: -3.011904761904762, Mean Entropy: 1.0035746097564697, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 16,  Mean reward: -3.8222222222222224, Mean Entropy: 0.9169273376464844, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 17,  Mean reward: -4.775, Mean Entropy: 0.9529613256454468, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 18,  Mean reward: -4.75, Mean Entropy: 0.9889024496078491, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 19,  Mean reward: -4.214285714285714, Mean Entropy: 0.9599640369415283, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.42s
Iteration: 20,  Mean reward: -5.325, Mean Entropy: 0.9525498151779175, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.44s
Iteration: 21,  Mean reward: -3.9186046511627906, Mean Entropy: 0.9810950756072998, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 22,  Mean reward: -4.413043478260869, Mean Entropy: 0.9370485544204712, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.50s
Iteration: 23,  Mean reward: -4.777777777777778, Mean Entropy: 0.9928730726242065, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 24,  Mean reward: -5.3522727272727275, Mean Entropy: 0.9384570121765137, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 25,  Mean reward: -3.4431818181818183, Mean Entropy: 0.9874412417411804, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 26,  Mean reward: -2.5833333333333335, Mean Entropy: 0.9130150675773621, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 27,  Mean reward: -3.9125, Mean Entropy: 0.9755325317382812, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 28,  Mean reward: -5.934782608695652, Mean Entropy: 0.9478930234909058, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 29,  Mean reward: -4.136363636363637, Mean Entropy: 0.854508638381958, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.47s
Iteration: 30,  Mean reward: -2.9479166666666665, Mean Entropy: 0.9513236880302429, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 31,  Mean reward: -2.735294117647059, Mean Entropy: 0.9516458511352539, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 32,  Mean reward: -0.7978723404255319, Mean Entropy: 0.90594881772995, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 33,  Mean reward: -3.380434782608696, Mean Entropy: 1.0057185888290405, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 34,  Mean reward: -5.431818181818182, Mean Entropy: 0.9258386492729187, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.48s
Iteration: 35,  Mean reward: -4.6022727272727275, Mean Entropy: 0.9222269058227539, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 36,  Mean reward: -2.0980392156862746, Mean Entropy: 0.8503139019012451, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 37,  Mean reward: -5.695652173913044, Mean Entropy: 0.8641842603683472, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 38,  Mean reward: -4.93, Mean Entropy: 0.8331412076950073, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 39,  Mean reward: -3.83, Mean Entropy: 0.8798609972000122, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 40,  Mean reward: -3.855769230769231, Mean Entropy: 0.850618839263916, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.47s
Iteration: 41,  Mean reward: -5.298076923076923, Mean Entropy: 0.7830228805541992, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 42,  Mean reward: -4.258928571428571, Mean Entropy: 0.8022027015686035, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 43,  Mean reward: -5.068627450980392, Mean Entropy: 0.8312519788742065, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 44,  Mean reward: -2.236842105263158, Mean Entropy: 0.7832066416740417, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 45,  Mean reward: -2.1203703703703702, Mean Entropy: 0.6473492383956909, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 46,  Mean reward: -4.2936507936507935, Mean Entropy: 0.5936750769615173, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 47,  Mean reward: -3.4918032786885247, Mean Entropy: 0.7779025435447693, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 48,  Mean reward: -3.0, Mean Entropy: 0.6951343417167664, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 49,  Mean reward: -3.7265625, Mean Entropy: 0.6141698360443115, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 50,  Mean reward: -2.3306451612903225, Mean Entropy: 0.6121272444725037, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 51,  Mean reward: -4.688524590163935, Mean Entropy: 0.49262914061546326, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 52,  Mean reward: -2.196969696969697, Mean Entropy: 0.4331870973110199, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.73s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 53,  Mean reward: -0.04411764705882353, Mean Entropy: 0.5193661451339722, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 54,  Mean reward: -2.9453125, Mean Entropy: 0.6342255473136902, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 55,  Mean reward: -2.7796610169491527, Mean Entropy: 0.6628730297088623, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 56,  Mean reward: -1.7615384615384615, Mean Entropy: 0.7269104719161987, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.93s
Iteration: 57,  Mean reward: -1.2666666666666666, Mean Entropy: 0.6254159808158875, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 58,  Mean reward: -3.4836065573770494, Mean Entropy: 0.5151849985122681, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 59,  Mean reward: -2.5661764705882355, Mean Entropy: 0.4190632700920105, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 60,  Mean reward: -1.9014084507042253, Mean Entropy: 0.4798247218132019, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 61,  Mean reward: 0.208955223880597, Mean Entropy: 0.8851412534713745, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 62,  Mean reward: -5.296296296296297, Mean Entropy: 0.4141077697277069, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 63,  Mean reward: -3.7928571428571427, Mean Entropy: 0.38854098320007324, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 64,  Mean reward: -2.942857142857143, Mean Entropy: 0.437362402677536, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 65,  Mean reward: -2.130434782608696, Mean Entropy: 0.5056061744689941, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 66,  Mean reward: -2.8333333333333335, Mean Entropy: 0.5730433464050293, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 67,  Mean reward: -3.3951612903225805, Mean Entropy: 0.581012487411499, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 68,  Mean reward: -0.5078125, Mean Entropy: 0.6116011738777161, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 69,  Mean reward: -3.290909090909091, Mean Entropy: 0.9515764117240906, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 70,  Mean reward: -4.416666666666667, Mean Entropy: 0.637061595916748, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 71,  Mean reward: -0.0234375, Mean Entropy: 0.6298706531524658, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 72,  Mean reward: -2.353846153846154, Mean Entropy: 0.6552183032035828, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 73,  Mean reward: -4.627272727272727, Mean Entropy: 0.6299315690994263, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 74,  Mean reward: -3.990740740740741, Mean Entropy: 0.5771458148956299, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 75,  Mean reward: -4.015873015873016, Mean Entropy: 0.5498976707458496, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 76,  Mean reward: -1.0859375, Mean Entropy: 0.5282013416290283, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 77,  Mean reward: -3.746031746031746, Mean Entropy: 0.44804152846336365, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 78,  Mean reward: -2.0606060606060606, Mean Entropy: 0.3405834436416626, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 79,  Mean reward: -2.464788732394366, Mean Entropy: 0.2983904480934143, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 80,  Mean reward: -1.5285714285714285, Mean Entropy: 0.7305619120597839, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 81,  Mean reward: -5.049019607843137, Mean Entropy: 0.2352847158908844, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 82,  Mean reward: -1.4375, Mean Entropy: 0.3016023635864258, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 83,  Mean reward: -1.662162162162162, Mean Entropy: 0.35374271869659424, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 84,  Mean reward: -1.9322033898305084, Mean Entropy: 0.4709147810935974, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 85,  Mean reward: -1.3828125, Mean Entropy: 0.4242408275604248, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 86,  Mean reward: -1.4857142857142858, Mean Entropy: 0.31103357672691345, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 87,  Mean reward: -2.0785714285714287, Mean Entropy: 0.15370014309883118, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 88,  Mean reward: -3.427536231884058, Mean Entropy: 0.1671837568283081, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 89,  Mean reward: -3.584507042253521, Mean Entropy: 0.2534193694591522, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 90,  Mean reward: -3.301470588235294, Mean Entropy: 0.24624404311180115, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 91,  Mean reward: -2.388059701492537, Mean Entropy: 0.16028690338134766, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 92,  Mean reward: -2.0357142857142856, Mean Entropy: 0.25249776244163513, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 93,  Mean reward: -2.342857142857143, Mean Entropy: 0.2789146304130554, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 94,  Mean reward: -1.7384615384615385, Mean Entropy: 0.3085325062274933, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.97s
Iteration: 95,  Mean reward: -1.3985507246376812, Mean Entropy: 0.5611107349395752, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 96,  Mean reward: -3.9836065573770494, Mean Entropy: 0.378537654876709, complete_episode_count: 61.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 97,  Mean reward: -3.120967741935484, Mean Entropy: 0.26259753108024597, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 98,  Mean reward: -3.3676470588235294, Mean Entropy: 0.33817291259765625, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 99,  Mean reward: -0.27611940298507465, Mean Entropy: 0.3410625457763672, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 100,  Mean reward: -1.286764705882353, Mean Entropy: 0.33392828702926636, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -1.7045454545454546, Mean Entropy: 0.3804381489753723, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 102,  Mean reward: -2.775, Mean Entropy: 0.2016206979751587, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 103,  Mean reward: -2.914285714285714, Mean Entropy: 0.3083828389644623, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 104,  Mean reward: -3.5597014925373136, Mean Entropy: 0.313732385635376, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 105,  Mean reward: -3.2028985507246377, Mean Entropy: 0.3246750235557556, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 106,  Mean reward: -1.7536231884057971, Mean Entropy: 0.3717747628688812, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 107,  Mean reward: -1.619047619047619, Mean Entropy: 0.2756913900375366, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 108,  Mean reward: -3.13768115942029, Mean Entropy: 0.3254486918449402, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 109,  Mean reward: -1.1492537313432836, Mean Entropy: 0.3399219512939453, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 110,  Mean reward: -1.3088235294117647, Mean Entropy: 0.3642650842666626, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 111,  Mean reward: -1.108695652173913, Mean Entropy: 0.41686028242111206, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.73s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 112,  Mean reward: 0.29545454545454547, Mean Entropy: 0.34399330615997314, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 113,  Mean reward: -4.21875, Mean Entropy: 0.40315520763397217, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 114,  Mean reward: -3.746153846153846, Mean Entropy: 0.39169785380363464, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 115,  Mean reward: -1.6810344827586208, Mean Entropy: 0.39259788393974304, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 116,  Mean reward: -2.4140625, Mean Entropy: 0.348507821559906, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 117,  Mean reward: -4.007692307692308, Mean Entropy: 0.3202453553676605, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 118,  Mean reward: -1.3308823529411764, Mean Entropy: 0.3291407525539398, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 119,  Mean reward: -1.9621212121212122, Mean Entropy: 0.4095645844936371, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 120,  Mean reward: -3.2333333333333334, Mean Entropy: 0.4081540107727051, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 121,  Mean reward: -3.7803030303030303, Mean Entropy: 0.38742727041244507, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 122,  Mean reward: -3.3461538461538463, Mean Entropy: 0.3310983180999756, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 123,  Mean reward: -3.6307692307692307, Mean Entropy: 0.3106135129928589, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 124,  Mean reward: -1.2, Mean Entropy: 0.29490697383880615, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 125,  Mean reward: -4.336065573770492, Mean Entropy: 0.437654972076416, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 126,  Mean reward: -2.119047619047619, Mean Entropy: 0.3689347207546234, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 127,  Mean reward: -3.015625, Mean Entropy: 0.37203264236450195, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 128,  Mean reward: -4.007692307692308, Mean Entropy: 0.39268749952316284, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 129,  Mean reward: -2.1343283582089554, Mean Entropy: 0.30501464009284973, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 130,  Mean reward: -1.0147058823529411, Mean Entropy: 0.3395213782787323, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 131,  Mean reward: -3.7, Mean Entropy: 0.41626811027526855, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 132,  Mean reward: -3.5245901639344264, Mean Entropy: 0.41033726930618286, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 133,  Mean reward: -1.7615384615384615, Mean Entropy: 0.33209043741226196, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 134,  Mean reward: -3.0238095238095237, Mean Entropy: 0.42059940099716187, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.94s
Iteration: 135,  Mean reward: -3.723076923076923, Mean Entropy: 0.35939204692840576, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 136,  Mean reward: -2.96875, Mean Entropy: 0.3407512307167053, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 137,  Mean reward: -1.6126760563380282, Mean Entropy: 0.28090476989746094, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 138,  Mean reward: -0.20422535211267606, Mean Entropy: 0.33220988512039185, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 139,  Mean reward: -1.9782608695652173, Mean Entropy: 0.33092400431632996, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 140,  Mean reward: -3.2777777777777777, Mean Entropy: 0.40126833319664, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 141,  Mean reward: -5.354838709677419, Mean Entropy: 0.4082622528076172, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 142,  Mean reward: -2.3230769230769233, Mean Entropy: 0.3810570240020752, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 143,  Mean reward: -2.8, Mean Entropy: 0.3179422616958618, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 144,  Mean reward: -5.032258064516129, Mean Entropy: 0.39486145973205566, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 145,  Mean reward: -5.753968253968254, Mean Entropy: 0.3682478368282318, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 146,  Mean reward: -3.640625, Mean Entropy: 0.3149714469909668, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 147,  Mean reward: -3.3059701492537314, Mean Entropy: 0.3135509490966797, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 148,  Mean reward: -2.8260869565217392, Mean Entropy: 0.35553786158561707, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 149,  Mean reward: -4.172413793103448, Mean Entropy: 0.2872371971607208, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 150,  Mean reward: -0.5072463768115942, Mean Entropy: 0.23393957316875458, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 151,  Mean reward: -2.142857142857143, Mean Entropy: 0.3154403269290924, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 152,  Mean reward: -1.378787878787879, Mean Entropy: 0.37164825201034546, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 153,  Mean reward: -1.3768115942028984, Mean Entropy: 0.3450542092323303, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 154,  Mean reward: -0.15625, Mean Entropy: 0.3070187270641327, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 155,  Mean reward: -3.5597014925373136, Mean Entropy: 0.325725257396698, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 156,  Mean reward: -3.5555555555555554, Mean Entropy: 0.32982346415519714, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 157,  Mean reward: -4.201492537313433, Mean Entropy: 0.286704957485199, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 158,  Mean reward: -1.6470588235294117, Mean Entropy: 0.30125120282173157, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 159,  Mean reward: -4.2421875, Mean Entropy: 0.35004398226737976, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 160,  Mean reward: -2.6417910447761193, Mean Entropy: 0.3242011070251465, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 161,  Mean reward: -0.5307692307692308, Mean Entropy: 0.3513093888759613, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 162,  Mean reward: -3.3461538461538463, Mean Entropy: 0.3769453465938568, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 163,  Mean reward: -1.7698412698412698, Mean Entropy: 0.3254067599773407, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.75s
Iteration: 164,  Mean reward: -1.0820895522388059, Mean Entropy: 0.36190199851989746, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 165,  Mean reward: -0.8387096774193549, Mean Entropy: 0.36812782287597656, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 166,  Mean reward: -1.492537313432836, Mean Entropy: 0.19948303699493408, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 167,  Mean reward: -0.6458333333333334, Mean Entropy: 0.2711728811264038, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 168,  Mean reward: -4.8768115942028984, Mean Entropy: 0.34338435530662537, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 169,  Mean reward: -1.671641791044776, Mean Entropy: 0.2682970464229584, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 170,  Mean reward: -3.723076923076923, Mean Entropy: 0.29598578810691833, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 171,  Mean reward: -2.4411764705882355, Mean Entropy: 0.3534955382347107, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 172,  Mean reward: -2.423076923076923, Mean Entropy: 0.34097158908843994, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 173,  Mean reward: -1.4701492537313432, Mean Entropy: 0.23555998504161835, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 174,  Mean reward: -3.514925373134328, Mean Entropy: 0.28468000888824463, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.94s
Iteration: 175,  Mean reward: -1.286764705882353, Mean Entropy: 0.30006998777389526, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 176,  Mean reward: -4.2890625, Mean Entropy: 0.30166059732437134, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 177,  Mean reward: -2.1971830985915495, Mean Entropy: 0.2747262716293335, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 178,  Mean reward: -1.9782608695652173, Mean Entropy: 0.3390541970729828, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 179,  Mean reward: -3.024193548387097, Mean Entropy: 0.4033716320991516, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 180,  Mean reward: -4.71875, Mean Entropy: 0.3839990794658661, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 181,  Mean reward: -4.8671875, Mean Entropy: 0.36397290229797363, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 182,  Mean reward: -4.265625, Mean Entropy: 0.33017364144325256, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 183,  Mean reward: -3.471014492753623, Mean Entropy: 0.3116774559020996, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 184,  Mean reward: -2.261904761904762, Mean Entropy: 0.34169650077819824, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 185,  Mean reward: -0.8188405797101449, Mean Entropy: 0.28561538457870483, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 186,  Mean reward: -2.0671641791044775, Mean Entropy: 0.3146358132362366, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 187,  Mean reward: -4.484126984126984, Mean Entropy: 0.380401074886322, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 188,  Mean reward: -2.8, Mean Entropy: 0.34598779678344727, complete_episode_count: 65.0, Gather time: 0.65s, Train time: 0.73s
Iteration: 189,  Mean reward: -3.28125, Mean Entropy: 0.3240179419517517, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 190,  Mean reward: -2.4692307692307693, Mean Entropy: 0.28443121910095215, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 191,  Mean reward: -2.1911764705882355, Mean Entropy: 0.22286884486675262, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 192,  Mean reward: -3.2214285714285715, Mean Entropy: 0.2973235845565796, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 193,  Mean reward: -2.873015873015873, Mean Entropy: 0.35560473799705505, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 194,  Mean reward: -0.7272727272727273, Mean Entropy: 0.2249051332473755, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 195,  Mean reward: -3.7928571428571427, Mean Entropy: 0.2943689823150635, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 196,  Mean reward: -2.871212121212121, Mean Entropy: 0.34763988852500916, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 197,  Mean reward: -2.6194029850746268, Mean Entropy: 0.2954352796077728, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 198,  Mean reward: -4.21875, Mean Entropy: 0.2781796455383301, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 199,  Mean reward: -4.7734375, Mean Entropy: 0.36790844798088074, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 200,  Mean reward: -1.626984126984127, Mean Entropy: 0.2820860743522644, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.847826086956522, Mean Entropy: 0.3466969132423401, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 202,  Mean reward: -1.8174603174603174, Mean Entropy: 0.3468126058578491, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 203,  Mean reward: -2.4692307692307693, Mean Entropy: 0.2668156623840332, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 204,  Mean reward: -1.1492537313432836, Mean Entropy: 0.28508448600769043, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 205,  Mean reward: -3.5597014925373136, Mean Entropy: 0.34356141090393066, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 206,  Mean reward: -5.532258064516129, Mean Entropy: 0.39972883462905884, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 207,  Mean reward: -0.2230769230769231, Mean Entropy: 0.30544063448905945, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 208,  Mean reward: -1.0147058823529411, Mean Entropy: 0.28481847047805786, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 209,  Mean reward: -3.8358208955223883, Mean Entropy: 0.31083232164382935, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 210,  Mean reward: -3.7803030303030303, Mean Entropy: 0.30528146028518677, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 211,  Mean reward: -1.8307692307692307, Mean Entropy: 0.26444733142852783, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 212,  Mean reward: -2.7183098591549295, Mean Entropy: 0.2738276720046997, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 213,  Mean reward: -1.792857142857143, Mean Entropy: 0.2971774935722351, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 214,  Mean reward: -4.292307692307692, Mean Entropy: 0.29679176211357117, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 215,  Mean reward: -0.8731343283582089, Mean Entropy: 0.28591686487197876, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 216,  Mean reward: -3.4057971014492754, Mean Entropy: 0.2829205393791199, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 217,  Mean reward: -3.3455882352941178, Mean Entropy: 0.32560861110687256, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 218,  Mean reward: -2.65625, Mean Entropy: 0.27108603715896606, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 219,  Mean reward: -4.41044776119403, Mean Entropy: 0.26194337010383606, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 220,  Mean reward: -4.028985507246377, Mean Entropy: 0.29928651452064514, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 221,  Mean reward: -3.4545454545454546, Mean Entropy: 0.3268526494503021, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 222,  Mean reward: -4.297101449275362, Mean Entropy: 0.2768588960170746, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 223,  Mean reward: -2.6865671641791047, Mean Entropy: 0.31823739409446716, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 224,  Mean reward: -2.9453125, Mean Entropy: 0.32305216789245605, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 225,  Mean reward: -1.8970588235294117, Mean Entropy: 0.3198114335536957, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 226,  Mean reward: -3.8805970149253732, Mean Entropy: 0.32108426094055176, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 227,  Mean reward: -0.36153846153846153, Mean Entropy: 0.24148696660995483, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 228,  Mean reward: -1.1521739130434783, Mean Entropy: 0.3294445872306824, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 229,  Mean reward: -3.1594202898550723, Mean Entropy: 0.3073130249977112, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 230,  Mean reward: -3.0615384615384613, Mean Entropy: 0.30038508772850037, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 231,  Mean reward: -3.142857142857143, Mean Entropy: 0.32925403118133545, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 232,  Mean reward: -4.360655737704918, Mean Entropy: 0.23002305626869202, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 233,  Mean reward: -3.2214285714285715, Mean Entropy: 0.25163811445236206, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 234,  Mean reward: -4.586956521739131, Mean Entropy: 0.2795160710811615, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 235,  Mean reward: -2.6417910447761193, Mean Entropy: 0.2372572124004364, complete_episode_count: 67.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 236,  Mean reward: -2.6865671641791047, Mean Entropy: 0.2346254587173462, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 237,  Mean reward: -1.286764705882353, Mean Entropy: 0.2702667713165283, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 238,  Mean reward: -3.449275362318841, Mean Entropy: 0.33847612142562866, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 239,  Mean reward: -2.2183098591549295, Mean Entropy: 0.24708960950374603, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 240,  Mean reward: -2.6641791044776117, Mean Entropy: 0.2663477063179016, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 241,  Mean reward: -2.3857142857142857, Mean Entropy: 0.2424815148115158, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 242,  Mean reward: -4.429577464788732, Mean Entropy: 0.2871554493904114, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 243,  Mean reward: -0.8731343283582089, Mean Entropy: 0.21569733321666718, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 244,  Mean reward: -1.8142857142857143, Mean Entropy: 0.24814260005950928, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 245,  Mean reward: -2.2183098591549295, Mean Entropy: 0.2866329252719879, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 246,  Mean reward: -4.698529411764706, Mean Entropy: 0.3375410735607147, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 247,  Mean reward: -3.5317460317460316, Mean Entropy: 0.2755906581878662, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 248,  Mean reward: -3.0615384615384613, Mean Entropy: 0.2683548629283905, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 249,  Mean reward: -2.044776119402985, Mean Entropy: 0.2784128189086914, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 250,  Mean reward: -1.792857142857143, Mean Entropy: 0.30614542961120605, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 251,  Mean reward: -1.6029411764705883, Mean Entropy: 0.35172972083091736, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 252,  Mean reward: -2.0396825396825395, Mean Entropy: 0.2811022698879242, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 253,  Mean reward: -3.1515151515151514, Mean Entropy: 0.292866051197052, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 254,  Mean reward: -0.65, Mean Entropy: 0.2695004940032959, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 255,  Mean reward: -1.5588235294117647, Mean Entropy: 0.2848505973815918, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 256,  Mean reward: -1.9621212121212122, Mean Entropy: 0.2164396494626999, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 257,  Mean reward: -2.7794117647058822, Mean Entropy: 0.24106425046920776, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 258,  Mean reward: -2.7142857142857144, Mean Entropy: 0.25256410241127014, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 259,  Mean reward: -3.0846153846153848, Mean Entropy: 0.2535334825515747, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 260,  Mean reward: -2.485294117647059, Mean Entropy: 0.3128991723060608, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 261,  Mean reward: -0.9142857142857143, Mean Entropy: 0.30198273062705994, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 262,  Mean reward: -1.9848484848484849, Mean Entropy: 0.22814199328422546, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 263,  Mean reward: -5.110294117647059, Mean Entropy: 0.25086113810539246, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 264,  Mean reward: -1.4420289855072463, Mean Entropy: 0.2718741297721863, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 265,  Mean reward: -1.1940298507462686, Mean Entropy: 0.2428966611623764, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 266,  Mean reward: -3.537313432835821, Mean Entropy: 0.2777535915374756, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 267,  Mean reward: -3.0615384615384613, Mean Entropy: 0.26337307691574097, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 268,  Mean reward: -2.1, Mean Entropy: 0.20501135289669037, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 269,  Mean reward: -2.985074626865672, Mean Entropy: 0.24361686408519745, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 270,  Mean reward: -0.07857142857142857, Mean Entropy: 0.24977105855941772, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 271,  Mean reward: -1.4857142857142858, Mean Entropy: 0.18375632166862488, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 272,  Mean reward: -3.411764705882353, Mean Entropy: 0.23481802642345428, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 273,  Mean reward: -2.2246376811594204, Mean Entropy: 0.25770795345306396, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 274,  Mean reward: -2.917910447761194, Mean Entropy: 0.1936277449131012, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 275,  Mean reward: -2.1, Mean Entropy: 0.2308366745710373, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 276,  Mean reward: -1.4583333333333333, Mean Entropy: 0.2553117275238037, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 277,  Mean reward: -3.6307692307692307, Mean Entropy: 0.2983650863170624, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 278,  Mean reward: -1.0147058823529411, Mean Entropy: 0.27493369579315186, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 279,  Mean reward: -1.875, Mean Entropy: 0.3317250907421112, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 280,  Mean reward: -0.20422535211267606, Mean Entropy: 0.2554636001586914, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 281,  Mean reward: -2.1911764705882355, Mean Entropy: 0.24488453567028046, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 282,  Mean reward: -0.4855072463768116, Mean Entropy: 0.2710554003715515, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 283,  Mean reward: -3.9338235294117645, Mean Entropy: 0.3051183819770813, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 284,  Mean reward: -2.4411764705882355, Mean Entropy: 0.2871565520763397, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 285,  Mean reward: -3.1159420289855073, Mean Entropy: 0.2412302941083908, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 286,  Mean reward: -1.6666666666666667, Mean Entropy: 0.21384358406066895, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 287,  Mean reward: -3.582089552238806, Mean Entropy: 0.2851996421813965, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 288,  Mean reward: -0.38235294117647056, Mean Entropy: 0.2488957941532135, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 289,  Mean reward: -1.4857142857142858, Mean Entropy: 0.2741735875606537, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 290,  Mean reward: -1.5808823529411764, Mean Entropy: 0.26498422026634216, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 291,  Mean reward: -4.08955223880597, Mean Entropy: 0.2673797309398651, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 292,  Mean reward: -3.5955882352941178, Mean Entropy: 0.22471599280834198, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 293,  Mean reward: -1.0810810810810811, Mean Entropy: 0.19533613324165344, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 294,  Mean reward: -2.6285714285714286, Mean Entropy: 0.24466127157211304, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 295,  Mean reward: -3.55, Mean Entropy: 0.30359429121017456, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 296,  Mean reward: -2.6641791044776117, Mean Entropy: 0.2729751765727997, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 297,  Mean reward: -2.08955223880597, Mean Entropy: 0.2075004279613495, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 298,  Mean reward: 0.9246575342465754, Mean Entropy: 0.15331611037254333, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 299,  Mean reward: -2.5579710144927534, Mean Entropy: 0.23665198683738708, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 300,  Mean reward: -3.582089552238806, Mean Entropy: 0.2466084063053131, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -2.7142857142857144, Mean Entropy: 0.1799309104681015, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 302,  Mean reward: -4.205882352941177, Mean Entropy: 0.21502789855003357, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 303,  Mean reward: -3.449275362318841, Mean Entropy: 0.18914207816123962, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 304,  Mean reward: -0.625, Mean Entropy: 0.1902865767478943, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 305,  Mean reward: -4.007246376811594, Mean Entropy: 0.2871558666229248, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 306,  Mean reward: -2.0434782608695654, Mean Entropy: 0.1980658769607544, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 307,  Mean reward: -3.128787878787879, Mean Entropy: 0.2422589659690857, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 308,  Mean reward: -3.6956521739130435, Mean Entropy: 0.2713446617126465, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 309,  Mean reward: -3.911764705882353, Mean Entropy: 0.22716853022575378, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 310,  Mean reward: -4.477611940298507, Mean Entropy: 0.25254181027412415, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 311,  Mean reward: -3.449275362318841, Mean Entropy: 0.24394547939300537, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 312,  Mean reward: -5.456521739130435, Mean Entropy: 0.1940414160490036, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 313,  Mean reward: -1.0492957746478873, Mean Entropy: 0.16200287640094757, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 314,  Mean reward: -2.65, Mean Entropy: 0.15362648665905, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 315,  Mean reward: -2.5, Mean Entropy: 0.1960790604352951, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 316,  Mean reward: -2.5579710144927534, Mean Entropy: 0.17163357138633728, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 317,  Mean reward: -3.1041666666666665, Mean Entropy: 0.1993483006954193, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 318,  Mean reward: -2.8680555555555554, Mean Entropy: 0.2211911976337433, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 319,  Mean reward: -5.598484848484849, Mean Entropy: 0.2212158441543579, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 320,  Mean reward: -0.9142857142857143, Mean Entropy: 0.15904474258422852, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 321,  Mean reward: -4.050724637681159, Mean Entropy: 0.2736067473888397, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 322,  Mean reward: -1.091549295774648, Mean Entropy: 0.24160288274288177, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 323,  Mean reward: -3.2028985507246377, Mean Entropy: 0.25582727789878845, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 324,  Mean reward: -2.0, Mean Entropy: 0.2032238245010376, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 325,  Mean reward: -2.5, Mean Entropy: 0.24896883964538574, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 326,  Mean reward: -1.4202898550724639, Mean Entropy: 0.275002658367157, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 327,  Mean reward: -1.171641791044776, Mean Entropy: 0.18430140614509583, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 328,  Mean reward: -2.2183098591549295, Mean Entropy: 0.2205260992050171, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 329,  Mean reward: -3.3028169014084505, Mean Entropy: 0.22555294632911682, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 330,  Mean reward: -3.0211267605633805, Mean Entropy: 0.23602984845638275, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 331,  Mean reward: -2.342857142857143, Mean Entropy: 0.22683212161064148, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 332,  Mean reward: -4.201492537313433, Mean Entropy: 0.25006556510925293, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 333,  Mean reward: -3.0, Mean Entropy: 0.18661749362945557, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 334,  Mean reward: -1.9930555555555556, Mean Entropy: 0.16062471270561218, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.72s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 335,  Mean reward: 1.3310810810810811, Mean Entropy: 0.1873907893896103, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 336,  Mean reward: -1.4166666666666667, Mean Entropy: 0.26384106278419495, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 337,  Mean reward: -1.9782608695652173, Mean Entropy: 0.32212182879447937, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 338,  Mean reward: -2.9357142857142855, Mean Entropy: 0.31680139899253845, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 339,  Mean reward: -2.265151515151515, Mean Entropy: 0.2843228876590729, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 340,  Mean reward: -0.11029411764705882, Mean Entropy: 0.1812010407447815, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 341,  Mean reward: -2.1549295774647885, Mean Entropy: 0.22620868682861328, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 342,  Mean reward: -0.7887323943661971, Mean Entropy: 0.20843879878520966, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 343,  Mean reward: -1.0704225352112675, Mean Entropy: 0.20243015885353088, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 344,  Mean reward: -3.242857142857143, Mean Entropy: 0.2912459969520569, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 345,  Mean reward: -1.286764705882353, Mean Entropy: 0.23353730142116547, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 346,  Mean reward: -2.7573529411764706, Mean Entropy: 0.2268614023923874, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 347,  Mean reward: -0.3263888888888889, Mean Entropy: 0.19317269325256348, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 348,  Mean reward: -2.73943661971831, Mean Entropy: 0.2180980145931244, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 349,  Mean reward: -3.4057971014492754, Mean Entropy: 0.2394234836101532, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 350,  Mean reward: -1.5071428571428571, Mean Entropy: 0.22038376331329346, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 351,  Mean reward: -3.8358208955223883, Mean Entropy: 0.20253436267375946, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 352,  Mean reward: -3.5597014925373136, Mean Entropy: 0.2045750617980957, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 353,  Mean reward: -1.6126760563380282, Mean Entropy: 0.13442537188529968, complete_episode_count: 71.0, Gather time: 0.63s, Train time: 0.72s
Iteration: 354,  Mean reward: -2.73943661971831, Mean Entropy: 0.1548592895269394, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 355,  Mean reward: -2.9315068493150687, Mean Entropy: 0.23492038249969482, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 356,  Mean reward: -3.507142857142857, Mean Entropy: 0.22956502437591553, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 357,  Mean reward: -2.3125, Mean Entropy: 0.22306250035762787, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 358,  Mean reward: -3.0422535211267605, Mean Entropy: 0.26240411400794983, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 359,  Mean reward: -0.9236111111111112, Mean Entropy: 0.20792198181152344, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 360,  Mean reward: -2.407142857142857, Mean Entropy: 0.18719522655010223, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 361,  Mean reward: -0.5833333333333334, Mean Entropy: 0.19649434089660645, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 362,  Mean reward: -1.5071428571428571, Mean Entropy: 0.23558351397514343, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 363,  Mean reward: -2.044776119402985, Mean Entropy: 0.1773807257413864, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 364,  Mean reward: -2.914285714285714, Mean Entropy: 0.2122335135936737, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 365,  Mean reward: -2.65, Mean Entropy: 0.16375300288200378, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 366,  Mean reward: -1.0136986301369864, Mean Entropy: 0.15914207696914673, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 367,  Mean reward: -5.044117647058823, Mean Entropy: 0.2111395001411438, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 368,  Mean reward: -4.297101449275362, Mean Entropy: 0.2683400511741638, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 369,  Mean reward: -1.2013888888888888, Mean Entropy: 0.16221831738948822, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 370,  Mean reward: -3.7171052631578947, Mean Entropy: 0.2276914119720459, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 371,  Mean reward: -1.9722222222222223, Mean Entropy: 0.1842062920331955, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 372,  Mean reward: -2.5694444444444446, Mean Entropy: 0.203993558883667, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 373,  Mean reward: -2.65, Mean Entropy: 0.2708628177642822, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 374,  Mean reward: -3.2196969696969697, Mean Entropy: 0.1922386884689331, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 375,  Mean reward: -1.6944444444444444, Mean Entropy: 0.23101970553398132, complete_episode_count: 72.0, Gather time: 0.72s, Train time: 0.74s
Iteration: 376,  Mean reward: -0.7887323943661971, Mean Entropy: 0.20854799449443817, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 377,  Mean reward: -2.057142857142857, Mean Entropy: 0.21943366527557373, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 378,  Mean reward: -0.4859154929577465, Mean Entropy: 0.17859308421611786, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 379,  Mean reward: -1.1805555555555556, Mean Entropy: 0.2343875616788864, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 380,  Mean reward: -2.0, Mean Entropy: 0.23821157217025757, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 381,  Mean reward: -1.028169014084507, Mean Entropy: 0.19698664546012878, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 382,  Mean reward: -0.9027777777777778, Mean Entropy: 0.14428679645061493, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 383,  Mean reward: -3.9791666666666665, Mean Entropy: 0.2191038727760315, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 384,  Mean reward: -2.9788732394366195, Mean Entropy: 0.20819906890392303, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 385,  Mean reward: -0.44366197183098594, Mean Entropy: 0.13193020224571228, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 386,  Mean reward: -2.5694444444444446, Mean Entropy: 0.18764108419418335, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 387,  Mean reward: -4.472222222222222, Mean Entropy: 0.29557564854621887, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 388,  Mean reward: -3.9558823529411766, Mean Entropy: 0.29175829887390137, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 389,  Mean reward: -1.2013888888888888, Mean Entropy: 0.12594901025295258, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 390,  Mean reward: -1.7152777777777777, Mean Entropy: 0.17396293580532074, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 391,  Mean reward: -0.18, Mean Entropy: 0.19655142724514008, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 392,  Mean reward: -1.0869565217391304, Mean Entropy: 0.1815512627363205, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 393,  Mean reward: -2.222972972972973, Mean Entropy: 0.20770543813705444, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 394,  Mean reward: -1.852112676056338, Mean Entropy: 0.17833980917930603, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 395,  Mean reward: -4.3428571428571425, Mean Entropy: 0.2085840404033661, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 396,  Mean reward: -1.7152777777777777, Mean Entropy: 0.15687184035778046, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 397,  Mean reward: -2.436619718309859, Mean Entropy: 0.1753554791212082, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 398,  Mean reward: -1.662162162162162, Mean Entropy: 0.20437023043632507, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 399,  Mean reward: -2.3857142857142857, Mean Entropy: 0.17157813906669617, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 400,  Mean reward: -1.2671232876712328, Mean Entropy: 0.16934847831726074, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -2.3424657534246576, Mean Entropy: 0.15827229619026184, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 402,  Mean reward: -3.0933333333333333, Mean Entropy: 0.16999511420726776, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 403,  Mean reward: -1.8356164383561644, Mean Entropy: 0.13776665925979614, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 404,  Mean reward: -1.9324324324324325, Mean Entropy: 0.15681754052639008, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 405,  Mean reward: -2.548611111111111, Mean Entropy: 0.18859615921974182, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 406,  Mean reward: -2.5694444444444446, Mean Entropy: 0.15234723687171936, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 407,  Mean reward: -1.5133333333333334, Mean Entropy: 0.14096109569072723, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 408,  Mean reward: 0.37333333333333335, Mean Entropy: 0.1464107185602188, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 409,  Mean reward: 0.23026315789473684, Mean Entropy: 0.1434280127286911, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 410,  Mean reward: -1.9513888888888888, Mean Entropy: 0.21748201549053192, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 411,  Mean reward: -3.7714285714285714, Mean Entropy: 0.3101029396057129, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 412,  Mean reward: -3.9296875, Mean Entropy: 0.2996984124183655, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.79s
Iteration: 413,  Mean reward: -3.6956521739130435, Mean Entropy: 0.22810742259025574, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 414,  Mean reward: -2.5069444444444446, Mean Entropy: 0.21469804644584656, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 415,  Mean reward: -3.1041666666666665, Mean Entropy: 0.22479383647441864, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 416,  Mean reward: -1.7569444444444444, Mean Entropy: 0.14833205938339233, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 417,  Mean reward: -2.56, Mean Entropy: 0.14692340791225433, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 418,  Mean reward: -2.2708333333333335, Mean Entropy: 0.12204845994710922, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 419,  Mean reward: -0.6013513513513513, Mean Entropy: 0.09264879673719406, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 420,  Mean reward: -5.135135135135135, Mean Entropy: 0.18768779933452606, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 421,  Mean reward: -4.8125, Mean Entropy: 0.2219228744506836, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 422,  Mean reward: 0.2708333333333333, Mean Entropy: 0.12450288236141205, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 423,  Mean reward: -1.1866666666666668, Mean Entropy: 0.09097719192504883, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 424,  Mean reward: -2.52, Mean Entropy: 0.12956100702285767, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 425,  Mean reward: -0.6933333333333334, Mean Entropy: 0.10990705341100693, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 426,  Mean reward: 1.1013513513513513, Mean Entropy: 0.11097242683172226, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 427,  Mean reward: -3.0933333333333333, Mean Entropy: 0.1923520863056183, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 428,  Mean reward: -0.4266666666666667, Mean Entropy: 0.18411104381084442, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 429,  Mean reward: -3.8661971830985915, Mean Entropy: 0.22470617294311523, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 430,  Mean reward: -2.7183098591549295, Mean Entropy: 0.21667882800102234, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 431,  Mean reward: -4.493055555555555, Mean Entropy: 0.22342729568481445, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 432,  Mean reward: -3.574324324324324, Mean Entropy: 0.18031196296215057, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 433,  Mean reward: -1.6418918918918919, Mean Entropy: 0.12945514917373657, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 434,  Mean reward: -2.6, Mean Entropy: 0.1416846215724945, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 435,  Mean reward: -3.184931506849315, Mean Entropy: 0.19370520114898682, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 436,  Mean reward: -2.826388888888889, Mean Entropy: 0.14966443181037903, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 437,  Mean reward: -2.707792207792208, Mean Entropy: 0.1538904905319214, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 438,  Mean reward: -3.304054054054054, Mean Entropy: 0.18927231431007385, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 439,  Mean reward: -2.4210526315789473, Mean Entropy: 0.1468070149421692, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 440,  Mean reward: -3.164383561643836, Mean Entropy: 0.16493716835975647, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 441,  Mean reward: -2.1095890410958904, Mean Entropy: 0.1223524659872055, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 442,  Mean reward: -1.3289473684210527, Mean Entropy: 0.11851885914802551, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 443,  Mean reward: -1.5723684210526316, Mean Entropy: 0.15851706266403198, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 444,  Mean reward: -2.7183098591549295, Mean Entropy: 0.18329846858978271, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 445,  Mean reward: -0.581081081081081, Mean Entropy: 0.12277437746524811, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 446,  Mean reward: -3.304054054054054, Mean Entropy: 0.13195708394050598, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 447,  Mean reward: -2.006666666666667, Mean Entropy: 0.10979419946670532, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 448,  Mean reward: -2.2733333333333334, Mean Entropy: 0.09049644321203232, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 449,  Mean reward: -3.414473684210526, Mean Entropy: 0.1061134785413742, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 450,  Mean reward: -2.448051948051948, Mean Entropy: 0.09811924397945404, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 451,  Mean reward: -1.8924050632911393, Mean Entropy: 0.08850668370723724, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 452,  Mean reward: -2.651898734177215, Mean Entropy: 0.08169857412576675, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 453,  Mean reward: -2.3987341772151898, Mean Entropy: 0.07033902406692505, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 454,  Mean reward: -2.707792207792208, Mean Entropy: 0.06579560041427612, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 455,  Mean reward: -3.7662337662337664, Mean Entropy: 0.06644946336746216, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.80s
Iteration: 456,  Mean reward: -2.5, Mean Entropy: 0.046563781797885895, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 457,  Mean reward: -3.75, Mean Entropy: 0.0345543697476387, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 458,  Mean reward: -1.8924050632911393, Mean Entropy: 0.027952825650572777, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 459,  Mean reward: -1.0, Mean Entropy: 0.04637857526540756, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 460,  Mean reward: -1.8924050632911393, Mean Entropy: 0.07026132196187973, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 461,  Mean reward: -0.879746835443038, Mean Entropy: 0.051855556666851044, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 462,  Mean reward: -1.948051948051948, Mean Entropy: 0.06685738265514374, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 463,  Mean reward: -0.37012987012987014, Mean Entropy: 0.053218111395835876, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 464,  Mean reward: -3.1582278481012658, Mean Entropy: 0.07580059766769409, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 465,  Mean reward: 0.38961038961038963, Mean Entropy: 0.06725290417671204, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 466,  Mean reward: -2.0384615384615383, Mean Entropy: 0.09136175364255905, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 467,  Mean reward: -2.888157894736842, Mean Entropy: 0.07488545775413513, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 468,  Mean reward: -2.651898734177215, Mean Entropy: 0.053188614547252655, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 469,  Mean reward: -4.189873417721519, Mean Entropy: 0.04202647507190704, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 470,  Mean reward: -0.6265822784810127, Mean Entropy: 0.026547834277153015, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 471,  Mean reward: -3.7467532467532467, Mean Entropy: 0.041089579463005066, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 472,  Mean reward: -3.1582278481012658, Mean Entropy: 0.03216010332107544, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 473,  Mean reward: -3.1582278481012658, Mean Entropy: 0.027142297476530075, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 474,  Mean reward: -4.170886075949367, Mean Entropy: 0.028057407587766647, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 475,  Mean reward: -3.576923076923077, Mean Entropy: 0.022923048585653305, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 476,  Mean reward: -2.1455696202531644, Mean Entropy: 0.02006453648209572, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 477,  Mean reward: -3.25, Mean Entropy: 0.02932664379477501, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 478,  Mean reward: -2.9050632911392404, Mean Entropy: 0.02675752341747284, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 479,  Mean reward: -3.5, Mean Entropy: 0.03205105662345886, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 480,  Mean reward: -2.551282051282051, Mean Entropy: 0.023629045113921165, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 481,  Mean reward: -1.7628205128205128, Mean Entropy: 0.02070089429616928, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 482,  Mean reward: -3.6645569620253164, Mean Entropy: 0.038367338478565216, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 483,  Mean reward: -2.9050632911392404, Mean Entropy: 0.01867448166012764, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 484,  Mean reward: -1.8924050632911393, Mean Entropy: 0.0151551878079772, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 485,  Mean reward: -2.8860759493670884, Mean Entropy: 0.02848951146006584, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 486,  Mean reward: -0.5, Mean Entropy: 0.017930585891008377, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 487,  Mean reward: -3.411392405063291, Mean Entropy: 0.02323761396110058, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 488,  Mean reward: -2.75, Mean Entropy: 0.015031198039650917, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 489,  Mean reward: -3.25, Mean Entropy: 0.017563078552484512, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 490,  Mean reward: 0.13291139240506328, Mean Entropy: 0.018389426171779633, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 491,  Mean reward: -1.0, Mean Entropy: 0.044068656861782074, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 492,  Mean reward: -0.5, Mean Entropy: 0.060845427215099335, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 493,  Mean reward: -0.879746835443038, Mean Entropy: 0.038197264075279236, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 494,  Mean reward: 0.25, Mean Entropy: 0.04168701171875, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 495,  Mean reward: -1.5, Mean Entropy: 0.06474585831165314, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 496,  Mean reward: -4.14, Mean Entropy: 0.046946655958890915, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 497,  Mean reward: -2.0384615384615383, Mean Entropy: 0.02337174117565155, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 498,  Mean reward: -2.1455696202531644, Mean Entropy: 0.022319233044981956, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 499,  Mean reward: -2.3987341772151898, Mean Entropy: 0.030842751264572144, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 500,  Mean reward: -2.9050632911392404, Mean Entropy: 0.026772623881697655, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -3.25, Mean Entropy: 0.01630917750298977, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 502,  Mean reward: -2.75, Mean Entropy: 0.01475993450731039, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 503,  Mean reward: 0.0, Mean Entropy: 0.015719003975391388, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 504,  Mean reward: -2.25, Mean Entropy: 0.033825479447841644, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 505,  Mean reward: -2.1455696202531644, Mean Entropy: 0.041493967175483704, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 506,  Mean reward: -3.75, Mean Entropy: 0.02878732606768608, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 507,  Mean reward: -1.8924050632911393, Mean Entropy: 0.016616616398096085, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 508,  Mean reward: -3.6645569620253164, Mean Entropy: 0.02031417191028595, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 509,  Mean reward: -0.8607594936708861, Mean Entropy: 0.015030566602945328, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 510,  Mean reward: -3.25, Mean Entropy: 0.023309599608182907, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 511,  Mean reward: -2.5, Mean Entropy: 0.026950925588607788, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 512,  Mean reward: -4.5, Mean Entropy: 0.023375630378723145, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 513,  Mean reward: -0.879746835443038, Mean Entropy: 0.013174078427255154, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 514,  Mean reward: -2.3797468354430378, Mean Entropy: 0.018033523112535477, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 515,  Mean reward: -0.5, Mean Entropy: 0.031801845878362656, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 516,  Mean reward: -0.75, Mean Entropy: 0.06788133084774017, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 517,  Mean reward: -0.9090909090909091, Mean Entropy: 0.07291149348020554, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 518,  Mean reward: -3.414473684210526, Mean Entropy: 0.07075297087430954, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 519,  Mean reward: -2.2948717948717947, Mean Entropy: 0.044975072145462036, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 520,  Mean reward: -1.75, Mean Entropy: 0.03450242802500725, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 521,  Mean reward: -1.2692307692307692, Mean Entropy: 0.052478186786174774, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 522,  Mean reward: -2.188311688311688, Mean Entropy: 0.0678737610578537, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 523,  Mean reward: -0.5192307692307693, Mean Entropy: 0.059032224118709564, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 524,  Mean reward: -1.639240506329114, Mean Entropy: 0.05952773615717888, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 525,  Mean reward: -2.75, Mean Entropy: 0.042053163051605225, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 526,  Mean reward: -1.5, Mean Entropy: 0.0212932787835598, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 527,  Mean reward: -3.0, Mean Entropy: 0.022319063544273376, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 528,  Mean reward: -3.25, Mean Entropy: 0.01350228488445282, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 529,  Mean reward: -2.651898734177215, Mean Entropy: 0.011645540595054626, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 530,  Mean reward: -1.1329113924050633, Mean Entropy: 0.014717998914420605, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 531,  Mean reward: -1.25, Mean Entropy: 0.029001429677009583, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 532,  Mean reward: -1.75, Mean Entropy: 0.04427383467555046, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 533,  Mean reward: -0.948051948051948, Mean Entropy: 0.0386582612991333, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 534,  Mean reward: -1.8012820512820513, Mean Entropy: 0.043622881174087524, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 535,  Mean reward: -0.37341772151898733, Mean Entropy: 0.04650632292032242, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 536,  Mean reward: -4.677215189873418, Mean Entropy: 0.04043225944042206, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 537,  Mean reward: -1.0, Mean Entropy: 0.028911873698234558, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 538,  Mean reward: -1.25, Mean Entropy: 0.04314175248146057, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 539,  Mean reward: -3.0, Mean Entropy: 0.04229399189352989, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 540,  Mean reward: -3.0641025641025643, Mean Entropy: 0.014759617857635021, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 541,  Mean reward: -2.3987341772151898, Mean Entropy: 0.011087758466601372, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 542,  Mean reward: -3.0, Mean Entropy: 0.02227305993437767, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 543,  Mean reward: -2.0, Mean Entropy: 0.020187746733427048, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 544,  Mean reward: -0.5, Mean Entropy: 0.03375695273280144, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 545,  Mean reward: -3.6835443037974684, Mean Entropy: 0.045457854866981506, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 546,  Mean reward: -2.532051282051282, Mean Entropy: 0.029060041531920433, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 547,  Mean reward: -1.1329113924050633, Mean Entropy: 0.02394036576151848, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 548,  Mean reward: -3.6455696202531644, Mean Entropy: 0.041524484753608704, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 549,  Mean reward: -0.6265822784810127, Mean Entropy: 0.03467421233654022, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 550,  Mean reward: -1.9285714285714286, Mean Entropy: 0.03326258808374405, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 551,  Mean reward: -2.25, Mean Entropy: 0.04229513928294182, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 552,  Mean reward: -0.5, Mean Entropy: 0.04661158472299576, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 553,  Mean reward: -1.9090909090909092, Mean Entropy: 0.03408530727028847, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 554,  Mean reward: -3.9407894736842106, Mean Entropy: 0.04137391597032547, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 555,  Mean reward: -1.5, Mean Entropy: 0.04178234934806824, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 556,  Mean reward: -0.879746835443038, Mean Entropy: 0.042993661016225815, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 557,  Mean reward: -1.3860759493670887, Mean Entropy: 0.06750363111495972, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 558,  Mean reward: -2.207792207792208, Mean Entropy: 0.08135119080543518, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 559,  Mean reward: -1.3860759493670887, Mean Entropy: 0.05147911235690117, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 560,  Mean reward: -2.25, Mean Entropy: 0.05996132642030716, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 561,  Mean reward: -1.639240506329114, Mean Entropy: 0.042168647050857544, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 562,  Mean reward: -1.8012820512820513, Mean Entropy: 0.05260445922613144, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 563,  Mean reward: -2.5, Mean Entropy: 0.05626671761274338, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 564,  Mean reward: -3.0641025641025643, Mean Entropy: 0.02720664069056511, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 565,  Mean reward: 0.0, Mean Entropy: 0.02014891803264618, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 566,  Mean reward: -0.5, Mean Entropy: 0.033289775252342224, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 567,  Mean reward: -1.4050632911392404, Mean Entropy: 0.05285850167274475, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 568,  Mean reward: -1.75, Mean Entropy: 0.040477167814970016, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 569,  Mean reward: -1.0, Mean Entropy: 0.03205487132072449, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 570,  Mean reward: -2.3987341772151898, Mean Entropy: 0.04113899916410446, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 571,  Mean reward: -2.1455696202531644, Mean Entropy: 0.026135213673114777, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 572,  Mean reward: -1.25, Mean Entropy: 0.031191697344183922, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 573,  Mean reward: -4.25, Mean Entropy: 0.047344084829092026, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 574,  Mean reward: -1.25, Mean Entropy: 0.03312145173549652, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 575,  Mean reward: -1.0, Mean Entropy: 0.03390302136540413, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 576,  Mean reward: -1.75, Mean Entropy: 0.03439711779356003, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 577,  Mean reward: -2.3987341772151898, Mean Entropy: 0.02715912088751793, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 578,  Mean reward: -3.1582278481012658, Mean Entropy: 0.023178590461611748, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 579,  Mean reward: -3.5, Mean Entropy: 0.011280614882707596, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 580,  Mean reward: -2.25, Mean Entropy: 0.009447602555155754, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 581,  Mean reward: -0.25, Mean Entropy: 0.015728138387203217, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 582,  Mean reward: -3.0, Mean Entropy: 0.02951955422759056, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 583,  Mean reward: -0.37341772151898733, Mean Entropy: 0.026802223175764084, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 584,  Mean reward: -1.5, Mean Entropy: 0.032606836408376694, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 585,  Mean reward: -1.639240506329114, Mean Entropy: 0.03135079890489578, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 586,  Mean reward: -3.8525641025641026, Mean Entropy: 0.017272960394620895, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 587,  Mean reward: -2.0, Mean Entropy: 0.007426523603498936, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 588,  Mean reward: -1.5, Mean Entropy: 0.009840335696935654, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 589,  Mean reward: -3.0, Mean Entropy: 0.017181245610117912, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 590,  Mean reward: -1.1329113924050633, Mean Entropy: 0.01964833214879036, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 591,  Mean reward: -3.75, Mean Entropy: 0.019570864737033844, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 592,  Mean reward: -1.2692307692307692, Mean Entropy: 0.013685992918908596, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 593,  Mean reward: -2.75, Mean Entropy: 0.01693085953593254, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 594,  Mean reward: -1.0, Mean Entropy: 0.016874954104423523, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 595,  Mean reward: -2.5, Mean Entropy: 0.020831964910030365, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 596,  Mean reward: -3.5, Mean Entropy: 0.012679132632911205, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 597,  Mean reward: -3.5, Mean Entropy: 0.008731808513402939, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 598,  Mean reward: -1.25, Mean Entropy: 0.010239221155643463, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 599,  Mean reward: -2.75, Mean Entropy: 0.016384458169341087, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 600,  Mean reward: -2.25, Mean Entropy: 0.020383577793836594, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.1455696202531644, Mean Entropy: 0.0222769808024168, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 602,  Mean reward: -1.3860759493670887, Mean Entropy: 0.03262607008218765, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 603,  Mean reward: -0.5, Mean Entropy: 0.039619091898202896, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 604,  Mean reward: -1.5, Mean Entropy: 0.032381944358348846, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 605,  Mean reward: -1.5921052631578947, Mean Entropy: 0.0227657463401556, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 606,  Mean reward: -2.75, Mean Entropy: 0.017888803035020828, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 607,  Mean reward: -1.3860759493670887, Mean Entropy: 0.015275010839104652, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 608,  Mean reward: -1.8924050632911393, Mean Entropy: 0.021131187677383423, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.80s
Iteration: 609,  Mean reward: -3.1582278481012658, Mean Entropy: 0.024057690054178238, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 610,  Mean reward: -2.1455696202531644, Mean Entropy: 0.014257843606173992, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.77s
Iteration: 611,  Mean reward: -1.5, Mean Entropy: 0.016901962459087372, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 612,  Mean reward: -1.0, Mean Entropy: 0.03110579214990139, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 613,  Mean reward: -1.4050632911392404, Mean Entropy: 0.0525229386985302, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 614,  Mean reward: -2.727272727272727, Mean Entropy: 0.029237693175673485, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 615,  Mean reward: -2.25, Mean Entropy: 0.014755146577954292, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 616,  Mean reward: -2.5, Mean Entropy: 0.014058905653655529, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 617,  Mean reward: -2.0384615384615383, Mean Entropy: 0.0150529183447361, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 618,  Mean reward: -1.0, Mean Entropy: 0.018555745482444763, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 619,  Mean reward: -2.5, Mean Entropy: 0.01868126355111599, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 620,  Mean reward: -1.75, Mean Entropy: 0.01151058729737997, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 621,  Mean reward: -1.5, Mean Entropy: 0.010926157236099243, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 622,  Mean reward: -1.3860759493670887, Mean Entropy: 0.008525618351995945, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 623,  Mean reward: -3.5, Mean Entropy: 0.008060288615524769, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 624,  Mean reward: -3.0, Mean Entropy: 0.0065703727304935455, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 625,  Mean reward: 0.5, Mean Entropy: 0.004961125552654266, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 626,  Mean reward: -1.5, Mean Entropy: 0.013002276420593262, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 627,  Mean reward: -3.5, Mean Entropy: 0.015377513132989407, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 628,  Mean reward: -3.25, Mean Entropy: 0.006544605828821659, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 629,  Mean reward: -0.5, Mean Entropy: 0.006235239561647177, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 630,  Mean reward: -3.75, Mean Entropy: 0.009901599958539009, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 631,  Mean reward: -2.0, Mean Entropy: 0.010204526595771313, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 632,  Mean reward: -2.5, Mean Entropy: 0.014337490312755108, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 633,  Mean reward: -0.5, Mean Entropy: 0.01686447113752365, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 634,  Mean reward: -2.551282051282051, Mean Entropy: 0.021304376423358917, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 635,  Mean reward: -3.1582278481012658, Mean Entropy: 0.01030373852699995, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 636,  Mean reward: -2.25, Mean Entropy: 0.006220325827598572, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 637,  Mean reward: -2.75, Mean Entropy: 0.007783753331750631, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 638,  Mean reward: -4.25, Mean Entropy: 0.005456435959786177, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 639,  Mean reward: -2.75, Mean Entropy: 0.0032720379531383514, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 640,  Mean reward: -3.0, Mean Entropy: 0.004730270244181156, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 641,  Mean reward: -3.75, Mean Entropy: 0.005093669518828392, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 642,  Mean reward: -2.75, Mean Entropy: 0.0034339288249611855, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 643,  Mean reward: -3.25, Mean Entropy: 0.0034674513153731823, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.98s
Iteration: 644,  Mean reward: -1.5, Mean Entropy: 0.004218899644911289, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 645,  Mean reward: -2.5, Mean Entropy: 0.007654272019863129, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 646,  Mean reward: -3.0, Mean Entropy: 0.008423024788498878, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 647,  Mean reward: -0.6265822784810127, Mean Entropy: 0.00902144517749548, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 648,  Mean reward: -1.5, Mean Entropy: 0.01229042373597622, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 649,  Mean reward: -2.75, Mean Entropy: 0.015097047202289104, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 650,  Mean reward: -1.0, Mean Entropy: 0.017325691878795624, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 651,  Mean reward: -3.9177215189873418, Mean Entropy: 0.020667139440774918, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 652,  Mean reward: -2.1455696202531644, Mean Entropy: 0.01627783291041851, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 653,  Mean reward: 0.0, Mean Entropy: 0.023653939366340637, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 654,  Mean reward: -0.6265822784810127, Mean Entropy: 0.034853048622608185, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 655,  Mean reward: -1.8012820512820513, Mean Entropy: 0.0307377427816391, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 656,  Mean reward: -2.9050632911392404, Mean Entropy: 0.018142124637961388, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 657,  Mean reward: -1.5, Mean Entropy: 0.018140621483325958, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 658,  Mean reward: -1.5, Mean Entropy: 0.02918091043829918, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 659,  Mean reward: -0.8987341772151899, Mean Entropy: 0.03669043630361557, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 660,  Mean reward: -3.487012987012987, Mean Entropy: 0.030739877372980118, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 661,  Mean reward: -2.9240506329113924, Mean Entropy: 0.008464658632874489, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 662,  Mean reward: -1.0, Mean Entropy: 0.005675904452800751, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 663,  Mean reward: -0.37341772151898733, Mean Entropy: 0.012908456847071648, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 664,  Mean reward: -1.25, Mean Entropy: 0.01822410151362419, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 665,  Mean reward: -2.3987341772151898, Mean Entropy: 0.02044101618230343, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 666,  Mean reward: -2.1455696202531644, Mean Entropy: 0.01252406369894743, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 667,  Mean reward: -1.25, Mean Entropy: 0.012948185205459595, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 668,  Mean reward: -2.5, Mean Entropy: 0.011349145323038101, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 669,  Mean reward: -3.25, Mean Entropy: 0.006717953365296125, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 670,  Mean reward: -3.8333333333333335, Mean Entropy: 0.0028052479028701782, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 671,  Mean reward: -2.5, Mean Entropy: 0.001953931525349617, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 672,  Mean reward: -3.411392405063291, Mean Entropy: 0.0024764894042164087, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 673,  Mean reward: -3.0, Mean Entropy: 0.002278601983562112, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 674,  Mean reward: -3.25, Mean Entropy: 0.002232288010418415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 675,  Mean reward: -2.25, Mean Entropy: 0.0027305136900395155, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 676,  Mean reward: -2.75, Mean Entropy: 0.004241605289280415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 677,  Mean reward: -2.75, Mean Entropy: 0.00384983466938138, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 678,  Mean reward: -1.25, Mean Entropy: 0.004068444482982159, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 679,  Mean reward: -3.25, Mean Entropy: 0.008096968755126, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 680,  Mean reward: -2.651898734177215, Mean Entropy: 0.004835131578147411, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 681,  Mean reward: 0.5, Mean Entropy: 0.005673266015946865, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.98s
Iteration: 682,  Mean reward: -1.0, Mean Entropy: 0.011841027066111565, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 683,  Mean reward: -2.5, Mean Entropy: 0.011911597102880478, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 684,  Mean reward: -2.5, Mean Entropy: 0.009068513289093971, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 685,  Mean reward: -2.9050632911392404, Mean Entropy: 0.003682310227304697, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 686,  Mean reward: -1.75, Mean Entropy: 0.004092243034392595, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 687,  Mean reward: 0.0, Mean Entropy: 0.007606146391481161, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 688,  Mean reward: -3.0, Mean Entropy: 0.011947127990424633, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 689,  Mean reward: -1.5, Mean Entropy: 0.013917411677539349, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 690,  Mean reward: -1.8924050632911393, Mean Entropy: 0.02230389043688774, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 691,  Mean reward: -1.3860759493670887, Mean Entropy: 0.013415472581982613, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 692,  Mean reward: -1.0, Mean Entropy: 0.014829415827989578, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.80s
Iteration: 693,  Mean reward: -2.0, Mean Entropy: 0.013894960284233093, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 694,  Mean reward: -1.75, Mean Entropy: 0.00872811209410429, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 695,  Mean reward: -2.75, Mean Entropy: 0.006188616156578064, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 696,  Mean reward: -2.25, Mean Entropy: 0.008079982362687588, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 697,  Mean reward: -2.0, Mean Entropy: 0.012386593036353588, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 698,  Mean reward: -3.0, Mean Entropy: 0.013680955395102501, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 699,  Mean reward: -2.0, Mean Entropy: 0.009395739063620567, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 700,  Mean reward: -1.25, Mean Entropy: 0.016949336975812912, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -3.0, Mean Entropy: 0.02255832403898239, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 702,  Mean reward: -1.75, Mean Entropy: 0.013394668698310852, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 703,  Mean reward: 0.3860759493670886, Mean Entropy: 0.01910516619682312, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 704,  Mean reward: -1.1329113924050633, Mean Entropy: 0.02743227779865265, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 705,  Mean reward: -3.9177215189873418, Mean Entropy: 0.01670059561729431, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 706,  Mean reward: -0.879746835443038, Mean Entropy: 0.008954746648669243, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 707,  Mean reward: -3.0, Mean Entropy: 0.010282205417752266, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 708,  Mean reward: -0.75, Mean Entropy: 0.009624190628528595, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 709,  Mean reward: -1.639240506329114, Mean Entropy: 0.015884464606642723, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 710,  Mean reward: -1.25, Mean Entropy: 0.01618359051644802, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 711,  Mean reward: -1.639240506329114, Mean Entropy: 0.013535633683204651, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 712,  Mean reward: -2.0, Mean Entropy: 0.007609136402606964, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 713,  Mean reward: -3.25, Mean Entropy: 0.00239265663549304, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 714,  Mean reward: -1.5, Mean Entropy: 0.0015619841869920492, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 715,  Mean reward: -0.75, Mean Entropy: 0.0034598372876644135, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 716,  Mean reward: -2.75, Mean Entropy: 0.00561106950044632, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 717,  Mean reward: -1.75, Mean Entropy: 0.006404725369066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 718,  Mean reward: -0.5, Mean Entropy: 0.010392731986939907, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 719,  Mean reward: -2.0, Mean Entropy: 0.011887677013874054, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 720,  Mean reward: -1.25, Mean Entropy: 0.009944910183548927, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 721,  Mean reward: -3.6645569620253164, Mean Entropy: 0.009049257263541222, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 722,  Mean reward: -2.5, Mean Entropy: 0.005033920519053936, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 723,  Mean reward: -1.75, Mean Entropy: 0.005960423033684492, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 724,  Mean reward: 0.0, Mean Entropy: 0.01586293615400791, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 725,  Mean reward: -3.1582278481012658, Mean Entropy: 0.021671123802661896, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 726,  Mean reward: -2.1455696202531644, Mean Entropy: 0.00989132933318615, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 727,  Mean reward: -1.75, Mean Entropy: 0.00805081520229578, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 728,  Mean reward: -1.25, Mean Entropy: 0.009676958434283733, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 729,  Mean reward: -2.5, Mean Entropy: 0.00878521054983139, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.81s
Iteration: 730,  Mean reward: 0.3860759493670886, Mean Entropy: 0.008458520285785198, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 731,  Mean reward: -2.0, Mean Entropy: 0.01191454753279686, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 732,  Mean reward: -2.25, Mean Entropy: 0.0070846108719706535, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 733,  Mean reward: -0.75, Mean Entropy: 0.0061881463043391705, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 734,  Mean reward: -3.0, Mean Entropy: 0.007916024886071682, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 735,  Mean reward: -3.75, Mean Entropy: 0.0037346023600548506, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 736,  Mean reward: -3.25, Mean Entropy: 0.0012044234899803996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 737,  Mean reward: -2.75, Mean Entropy: 0.0010527655249461532, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 738,  Mean reward: 0.0, Mean Entropy: 0.0019032061100006104, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 739,  Mean reward: 0.0, Mean Entropy: 0.004717420320957899, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 740,  Mean reward: -2.5, Mean Entropy: 0.009299447759985924, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 741,  Mean reward: -1.0, Mean Entropy: 0.011176827363669872, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 742,  Mean reward: -0.6265822784810127, Mean Entropy: 0.008506211452186108, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 743,  Mean reward: -3.25, Mean Entropy: 0.01109680812805891, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 744,  Mean reward: -1.5, Mean Entropy: 0.008495564572513103, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 745,  Mean reward: -1.3860759493670887, Mean Entropy: 0.017884977161884308, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 746,  Mean reward: -3.25, Mean Entropy: 0.011031481437385082, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 747,  Mean reward: -3.411392405063291, Mean Entropy: 0.0019130939617753029, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 748,  Mean reward: 0.25, Mean Entropy: 0.0021421690471470356, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.79s
Iteration: 749,  Mean reward: -1.75, Mean Entropy: 0.005627479404211044, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 750,  Mean reward: -0.25, Mean Entropy: 0.0121276481077075, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 751,  Mean reward: -1.8924050632911393, Mean Entropy: 0.021278414875268936, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 752,  Mean reward: -5.0, Mean Entropy: 0.010966329835355282, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 753,  Mean reward: -3.25, Mean Entropy: 0.0039354776963591576, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.71s
Iteration: 754,  Mean reward: -1.5, Mean Entropy: 0.004801336210221052, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 755,  Mean reward: 0.0, Mean Entropy: 0.00920693390071392, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 756,  Mean reward: -1.5, Mean Entropy: 0.02774752676486969, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 757,  Mean reward: -2.5, Mean Entropy: 0.025772619992494583, complete_episode_count: 80.0, Gather time: 0.73s, Train time: 0.72s
Iteration: 758,  Mean reward: -1.75, Mean Entropy: 0.01905619166791439, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 759,  Mean reward: -1.5, Mean Entropy: 0.012839926406741142, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 760,  Mean reward: -2.5, Mean Entropy: 0.011499973945319653, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 761,  Mean reward: -1.75, Mean Entropy: 0.011160289868712425, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 762,  Mean reward: -3.0, Mean Entropy: 0.011715270578861237, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 763,  Mean reward: -1.5, Mean Entropy: 0.005028901621699333, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 764,  Mean reward: -1.25, Mean Entropy: 0.008018521592020988, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 765,  Mean reward: -2.2948717948717947, Mean Entropy: 0.014354636892676353, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 766,  Mean reward: -0.5, Mean Entropy: 0.013507707044482231, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 767,  Mean reward: -1.75, Mean Entropy: 0.01438533328473568, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 768,  Mean reward: -2.25, Mean Entropy: 0.012486963532865047, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 769,  Mean reward: -1.1329113924050633, Mean Entropy: 0.012308958917856216, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 770,  Mean reward: -1.5, Mean Entropy: 0.009770529344677925, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 771,  Mean reward: -2.25, Mean Entropy: 0.004315103869885206, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 772,  Mean reward: -1.0, Mean Entropy: 0.004000198096036911, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 773,  Mean reward: -2.75, Mean Entropy: 0.00614398717880249, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 774,  Mean reward: -1.3860759493670887, Mean Entropy: 0.005458946339786053, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 775,  Mean reward: -0.5, Mean Entropy: 0.009364508092403412, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 776,  Mean reward: -1.0, Mean Entropy: 0.01229681633412838, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 777,  Mean reward: -1.5, Mean Entropy: 0.006293093785643578, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 778,  Mean reward: -2.5, Mean Entropy: 0.005670203827321529, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 779,  Mean reward: -2.75, Mean Entropy: 0.0017528206808492541, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 780,  Mean reward: -1.5, Mean Entropy: 0.0015452567022293806, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 781,  Mean reward: -0.5, Mean Entropy: 0.0029710230883210897, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 782,  Mean reward: -3.5, Mean Entropy: 0.007879073731601238, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 783,  Mean reward: 0.25, Mean Entropy: 0.006624070927500725, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 784,  Mean reward: -4.5, Mean Entropy: 0.007628059014678001, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 785,  Mean reward: -2.0, Mean Entropy: 0.004687273409217596, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 786,  Mean reward: 0.25, Mean Entropy: 0.005474449601024389, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 787,  Mean reward: -3.1582278481012658, Mean Entropy: 0.00773340230807662, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 788,  Mean reward: -3.75, Mean Entropy: 0.0022412273101508617, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 789,  Mean reward: -3.0, Mean Entropy: 0.0008614575490355492, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 790,  Mean reward: 0.75, Mean Entropy: 0.0011720432667061687, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 791,  Mean reward: -0.75, Mean Entropy: 0.004047873895615339, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 792,  Mean reward: -3.411392405063291, Mean Entropy: 0.00775911845266819, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 793,  Mean reward: -2.3987341772151898, Mean Entropy: 0.006945216096937656, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 794,  Mean reward: -1.25, Mean Entropy: 0.00612527085468173, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 795,  Mean reward: -1.75, Mean Entropy: 0.009349535219371319, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 796,  Mean reward: -3.75, Mean Entropy: 0.008828168734908104, complete_episode_count: 80.0, Gather time: 0.75s, Train time: 0.70s
Iteration: 797,  Mean reward: -1.25, Mean Entropy: 0.0054849120788276196, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 798,  Mean reward: -1.8924050632911393, Mean Entropy: 0.008591540157794952, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 799,  Mean reward: -3.25, Mean Entropy: 0.005423250608146191, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 800,  Mean reward: -3.392405063291139, Mean Entropy: 0.0010457574389874935, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -1.0, Mean Entropy: 0.0013632572954520583, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 802,  Mean reward: -0.5, Mean Entropy: 0.004378222394734621, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 803,  Mean reward: -0.5, Mean Entropy: 0.013527430593967438, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 804,  Mean reward: -1.75, Mean Entropy: 0.021768808364868164, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 805,  Mean reward: -4.25, Mean Entropy: 0.00975013803690672, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 806,  Mean reward: -1.25, Mean Entropy: 0.00472600944340229, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 807,  Mean reward: -3.0, Mean Entropy: 0.00869668647646904, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 808,  Mean reward: -2.75, Mean Entropy: 0.008180969394743443, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 809,  Mean reward: -1.25, Mean Entropy: 0.008297097869217396, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 810,  Mean reward: -2.5, Mean Entropy: 0.011643683537840843, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 811,  Mean reward: -1.0, Mean Entropy: 0.018099263310432434, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 812,  Mean reward: -2.75, Mean Entropy: 0.02287609688937664, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 813,  Mean reward: -1.3860759493670887, Mean Entropy: 0.020004257559776306, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 814,  Mean reward: -2.5, Mean Entropy: 0.011690525338053703, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 815,  Mean reward: -3.0, Mean Entropy: 0.0024695429019629955, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 816,  Mean reward: -3.25, Mean Entropy: 0.0006727647269144654, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 817,  Mean reward: -1.5, Mean Entropy: 0.000843584188260138, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 818,  Mean reward: -2.5, Mean Entropy: 0.0013566960114985704, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 819,  Mean reward: -0.5, Mean Entropy: 0.002576233586296439, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 820,  Mean reward: -0.75, Mean Entropy: 0.005890670232474804, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 821,  Mean reward: -4.0, Mean Entropy: 0.006954885553568602, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 822,  Mean reward: -1.639240506329114, Mean Entropy: 0.005701872520148754, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 823,  Mean reward: -1.75, Mean Entropy: 0.006566055119037628, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 824,  Mean reward: -1.0, Mean Entropy: 0.008752608671784401, complete_episode_count: 80.0, Gather time: 0.70s, Train time: 0.77s
Iteration: 825,  Mean reward: -2.5, Mean Entropy: 0.008279631845653057, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 826,  Mean reward: -2.75, Mean Entropy: 0.004651907831430435, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 827,  Mean reward: -1.25, Mean Entropy: 0.006423039361834526, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 828,  Mean reward: -2.75, Mean Entropy: 0.006753157824277878, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 829,  Mean reward: -2.5, Mean Entropy: 0.0062119802460074425, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 830,  Mean reward: -3.0, Mean Entropy: 0.006375753320753574, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 831,  Mean reward: -3.0, Mean Entropy: 0.0037211996968835592, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 832,  Mean reward: -2.25, Mean Entropy: 0.004836500622332096, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 833,  Mean reward: -1.75, Mean Entropy: 0.006968598812818527, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.94s
Iteration: 834,  Mean reward: -3.1582278481012658, Mean Entropy: 0.0078801354393363, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 835,  Mean reward: -4.25, Mean Entropy: 0.0025756664108484983, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.74s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -4.372093023255814, Mean Entropy: 0.9891788363456726, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.46s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.036585365853658, Mean Entropy: 0.9675179719924927, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.44s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -4.304878048780488, Mean Entropy: 0.9169759750366211, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -3.8095238095238093, Mean Entropy: 0.9386366605758667, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.42s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 4,  Mean reward: -3.2564102564102564, Mean Entropy: 0.9819554090499878, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 5,  Mean reward: -5.607142857142857, Mean Entropy: 0.9458470344543457, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.40s
Iteration: 6,  Mean reward: -3.3536585365853657, Mean Entropy: 0.9313995242118835, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 7,  Mean reward: -5.878048780487805, Mean Entropy: 0.95306396484375, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.52s
Iteration: 8,  Mean reward: -5.390243902439025, Mean Entropy: 0.9747252464294434, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.44s
Iteration: 9,  Mean reward: -5.255813953488372, Mean Entropy: 0.9025272130966187, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 10,  Mean reward: -4.275, Mean Entropy: 0.9458533525466919, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 11,  Mean reward: -3.8289473684210527, Mean Entropy: 0.9891773462295532, complete_episode_count: 38.0, Gather time: 0.50s, Train time: 1.44s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 12,  Mean reward: -3.0, Mean Entropy: 0.9169692993164062, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 13,  Mean reward: -2.25, Mean Entropy: 0.9458544254302979, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 14,  Mean reward: -4.5813953488372094, Mean Entropy: 0.9747345447540283, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 15,  Mean reward: -4.144736842105263, Mean Entropy: 0.9097527861595154, complete_episode_count: 38.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 16,  Mean reward: -4.597560975609756, Mean Entropy: 0.9169694781303406, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 17,  Mean reward: -4.595744680851064, Mean Entropy: 0.9169694185256958, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 18,  Mean reward: -4.0, Mean Entropy: 0.9314086437225342, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.40s
Iteration: 19,  Mean reward: -4.773809523809524, Mean Entropy: 0.9386247396469116, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 20,  Mean reward: -5.965116279069767, Mean Entropy: 0.9674962759017944, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 21,  Mean reward: -4.2439024390243905, Mean Entropy: 0.9025167226791382, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 22,  Mean reward: -3.4125, Mean Entropy: 0.9169591665267944, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 23,  Mean reward: -2.6818181818181817, Mean Entropy: 0.9602880477905273, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 24,  Mean reward: -7.205128205128205, Mean Entropy: 0.9313918948173523, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 25,  Mean reward: -3.2195121951219514, Mean Entropy: 0.888049304485321, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 26,  Mean reward: -4.55, Mean Entropy: 0.9385439157485962, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 27,  Mean reward: -3.686046511627907, Mean Entropy: 0.9096896052360535, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 28,  Mean reward: -4.406976744186046, Mean Entropy: 0.989041805267334, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 29,  Mean reward: -5.8076923076923075, Mean Entropy: 0.8808287382125854, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 30,  Mean reward: -5.5777777777777775, Mean Entropy: 0.9602107405662537, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 31,  Mean reward: -4.925, Mean Entropy: 0.9384907484054565, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 32,  Mean reward: -4.954545454545454, Mean Entropy: 0.9745628237724304, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 33,  Mean reward: -5.5777777777777775, Mean Entropy: 0.9383411407470703, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 34,  Mean reward: -1.4878048780487805, Mean Entropy: 0.9526782035827637, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 35,  Mean reward: -5.2, Mean Entropy: 0.9239400625228882, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 36,  Mean reward: -4.573170731707317, Mean Entropy: 0.9312872886657715, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.49s
Iteration: 37,  Mean reward: -4.543478260869565, Mean Entropy: 0.9240419268608093, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.50s
Iteration: 38,  Mean reward: -7.5256410256410255, Mean Entropy: 0.9672527313232422, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 39,  Mean reward: -4.988095238095238, Mean Entropy: 0.9384399652481079, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.47s
Iteration: 40,  Mean reward: -3.347826086956522, Mean Entropy: 1.0104858875274658, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 41,  Mean reward: -4.329545454545454, Mean Entropy: 0.9311354160308838, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 42,  Mean reward: -4.012195121951219, Mean Entropy: 0.9167366027832031, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.50s
Iteration: 43,  Mean reward: -6.093023255813954, Mean Entropy: 0.9095353484153748, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.44s
Iteration: 44,  Mean reward: -4.6022727272727275, Mean Entropy: 0.9816348552703857, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.44s
Iteration: 45,  Mean reward: -6.9021739130434785, Mean Entropy: 0.9526834487915039, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 46,  Mean reward: -3.3095238095238093, Mean Entropy: 0.9381499290466309, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 47,  Mean reward: -5.146341463414634, Mean Entropy: 0.9956812858581543, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.44s
Iteration: 48,  Mean reward: -4.522222222222222, Mean Entropy: 0.974087119102478, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 49,  Mean reward: -5.174418604651163, Mean Entropy: 0.9814149737358093, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 50,  Mean reward: -3.097560975609756, Mean Entropy: 0.9668313264846802, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 51,  Mean reward: -4.7, Mean Entropy: 0.9592574834823608, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 52,  Mean reward: -3.465909090909091, Mean Entropy: 0.872257649898529, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 53,  Mean reward: -4.182926829268292, Mean Entropy: 0.9574823379516602, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 54,  Mean reward: -4.890243902439025, Mean Entropy: 0.9576244354248047, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.47s
Iteration: 55,  Mean reward: -4.686046511627907, Mean Entropy: 0.9880273342132568, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 56,  Mean reward: -4.2560975609756095, Mean Entropy: 0.9595643877983093, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 57,  Mean reward: -4.0777777777777775, Mean Entropy: 0.9011658430099487, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 58,  Mean reward: -2.9651162790697674, Mean Entropy: 0.9429842233657837, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.40s
Iteration: 59,  Mean reward: -6.318181818181818, Mean Entropy: 0.9988822937011719, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 60,  Mean reward: -6.341463414634147, Mean Entropy: 0.9144884347915649, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 61,  Mean reward: -2.3260869565217392, Mean Entropy: 1.0139684677124023, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.40s
Iteration: 62,  Mean reward: -5.214285714285714, Mean Entropy: 0.8787054419517517, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 63,  Mean reward: -4.8522727272727275, Mean Entropy: 0.9002170562744141, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.44s
Iteration: 64,  Mean reward: -3.8863636363636362, Mean Entropy: 0.9623197913169861, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 65,  Mean reward: -5.439024390243903, Mean Entropy: 0.8966759443283081, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.63s
Iteration: 66,  Mean reward: -4.975, Mean Entropy: 0.8761051893234253, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 67,  Mean reward: -3.024390243902439, Mean Entropy: 0.932697057723999, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 68,  Mean reward: -5.511363636363637, Mean Entropy: 0.9109596014022827, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.52s
Iteration: 69,  Mean reward: -3.784090909090909, Mean Entropy: 0.936773955821991, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 70,  Mean reward: -5.813953488372093, Mean Entropy: 0.9346094131469727, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 71,  Mean reward: -7.035714285714286, Mean Entropy: 0.9246274828910828, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 72,  Mean reward: -3.2625, Mean Entropy: 1.002364158630371, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 73,  Mean reward: -6.154761904761905, Mean Entropy: 0.9444283246994019, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 74,  Mean reward: -6.097560975609756, Mean Entropy: 0.8330604434013367, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 75,  Mean reward: -6.865853658536586, Mean Entropy: 0.937865138053894, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 76,  Mean reward: -2.7065217391304346, Mean Entropy: 0.929472804069519, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 77,  Mean reward: -4.573170731707317, Mean Entropy: 0.9197707176208496, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 78,  Mean reward: -3.5657894736842106, Mean Entropy: 0.9592446088790894, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.45s
Iteration: 79,  Mean reward: -3.619565217391304, Mean Entropy: 0.895072877407074, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 80,  Mean reward: -6.095744680851064, Mean Entropy: 0.8736857175827026, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 81,  Mean reward: -4.344444444444444, Mean Entropy: 0.8953465223312378, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.47s
Iteration: 82,  Mean reward: -4.088888888888889, Mean Entropy: 0.9076272249221802, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 83,  Mean reward: -5.095744680851064, Mean Entropy: 0.8260997533798218, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 84,  Mean reward: -5.730769230769231, Mean Entropy: 0.7496808767318726, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 85,  Mean reward: -3.3229166666666665, Mean Entropy: 0.8127528429031372, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 86,  Mean reward: -1.4361702127659575, Mean Entropy: 0.9651205539703369, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 87,  Mean reward: -4.391304347826087, Mean Entropy: 0.8331488966941833, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.45s
Iteration: 88,  Mean reward: -1.7456140350877194, Mean Entropy: 0.8758527636528015, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 89,  Mean reward: -1.6442307692307692, Mean Entropy: 0.9710370898246765, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 90,  Mean reward: -4.184782608695652, Mean Entropy: 0.8119809031486511, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 91,  Mean reward: -2.63, Mean Entropy: 0.6872615814208984, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 92,  Mean reward: -2.6944444444444446, Mean Entropy: 0.5634526610374451, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 93,  Mean reward: -3.19672131147541, Mean Entropy: 0.627051591873169, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 94,  Mean reward: -4.7727272727272725, Mean Entropy: 0.569750189781189, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 95,  Mean reward: -2.6818181818181817, Mean Entropy: 0.5274287462234497, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.79s
Iteration: 96,  Mean reward: -2.2083333333333335, Mean Entropy: 0.735099732875824, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 97,  Mean reward: -2.107843137254902, Mean Entropy: 0.6939897537231445, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 98,  Mean reward: -4.19, Mean Entropy: 0.5778873562812805, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.67s
Iteration: 99,  Mean reward: -2.3114754098360657, Mean Entropy: 0.6539823412895203, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.82s
Iteration: 100,  Mean reward: -4.851851851851852, Mean Entropy: 0.7526431083679199, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.44s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -3.087719298245614, Mean Entropy: 0.6308633685112, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 102,  Mean reward: -2.0245901639344264, Mean Entropy: 0.6875004768371582, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 103,  Mean reward: -2.727272727272727, Mean Entropy: 0.706359326839447, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 104,  Mean reward: -3.02, Mean Entropy: 0.6757665872573853, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.46s
Iteration: 105,  Mean reward: -1.7, Mean Entropy: 0.5439406037330627, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 106,  Mean reward: -3.1491228070175437, Mean Entropy: 0.5579938888549805, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.48s
Iteration: 107,  Mean reward: -3.4047619047619047, Mean Entropy: 0.5653848648071289, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 108,  Mean reward: -4.758928571428571, Mean Entropy: 0.5489428639411926, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 109,  Mean reward: -1.0952380952380953, Mean Entropy: 0.5252971649169922, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 110,  Mean reward: -2.5762711864406778, Mean Entropy: 0.49739402532577515, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 111,  Mean reward: -2.6615384615384614, Mean Entropy: 0.4467470049858093, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 112,  Mean reward: -2.08955223880597, Mean Entropy: 0.4824807643890381, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 113,  Mean reward: -6.169354838709677, Mean Entropy: 0.47014760971069336, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 114,  Mean reward: -3.7049180327868854, Mean Entropy: 0.4278861880302429, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 115,  Mean reward: -2.2196969696969697, Mean Entropy: 0.5265607833862305, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 116,  Mean reward: -3.319672131147541, Mean Entropy: 0.4066774249076843, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 117,  Mean reward: -2.507936507936508, Mean Entropy: 0.4831712245941162, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.82s
Iteration: 118,  Mean reward: -2.515873015873016, Mean Entropy: 0.5424510836601257, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 119,  Mean reward: -1.9538461538461538, Mean Entropy: 0.5897810459136963, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 120,  Mean reward: -4.453703703703703, Mean Entropy: 0.4644831418991089, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 121,  Mean reward: -1.8692307692307693, Mean Entropy: 0.49137675762176514, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 122,  Mean reward: -4.225, Mean Entropy: 0.4693937301635742, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 123,  Mean reward: -3.373015873015873, Mean Entropy: 0.5062785148620605, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.75s
Iteration: 124,  Mean reward: -2.336206896551724, Mean Entropy: 0.37853217124938965, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 125,  Mean reward: -3.1076923076923078, Mean Entropy: 0.5369398593902588, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 126,  Mean reward: -5.754237288135593, Mean Entropy: 0.4606863856315613, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 127,  Mean reward: -1.5, Mean Entropy: 0.5996516942977905, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 128,  Mean reward: -4.883333333333334, Mean Entropy: 0.5366486310958862, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 129,  Mean reward: -4.025862068965517, Mean Entropy: 0.4857425093650818, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 130,  Mean reward: -2.185483870967742, Mean Entropy: 0.5198168754577637, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 131,  Mean reward: -3.675, Mean Entropy: 0.4037996232509613, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.66s
Iteration: 132,  Mean reward: -3.6311475409836067, Mean Entropy: 0.46150898933410645, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 133,  Mean reward: -1.7734375, Mean Entropy: 0.5292081832885742, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 134,  Mean reward: -2.406779661016949, Mean Entropy: 0.4932931363582611, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 135,  Mean reward: -4.0701754385964914, Mean Entropy: 0.502210795879364, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 136,  Mean reward: -2.9322033898305087, Mean Entropy: 0.6149746775627136, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 137,  Mean reward: -3.8482142857142856, Mean Entropy: 0.5359596014022827, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 138,  Mean reward: -2.9836065573770494, Mean Entropy: 0.5723006725311279, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 139,  Mean reward: -4.781818181818182, Mean Entropy: 0.6757203936576843, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 140,  Mean reward: -2.4464285714285716, Mean Entropy: 0.6374282836914062, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 141,  Mean reward: -2.560344827586207, Mean Entropy: 0.6058279275894165, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 142,  Mean reward: -1.853448275862069, Mean Entropy: 0.542322039604187, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 143,  Mean reward: -2.7301587301587302, Mean Entropy: 0.5271189212799072, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 144,  Mean reward: -1.1779661016949152, Mean Entropy: 0.5687746405601501, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 145,  Mean reward: -1.8064516129032258, Mean Entropy: 0.48100751638412476, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 146,  Mean reward: -5.455357142857143, Mean Entropy: 0.5400314331054688, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.41s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 147,  Mean reward: -0.9910714285714286, Mean Entropy: 0.5927937030792236, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 148,  Mean reward: -2.103448275862069, Mean Entropy: 0.5524295568466187, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 149,  Mean reward: -3.8771929824561404, Mean Entropy: 0.42191776633262634, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 150,  Mean reward: -5.25, Mean Entropy: 0.49807009100914, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 151,  Mean reward: -4.590163934426229, Mean Entropy: 0.444124698638916, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 152,  Mean reward: -3.6311475409836067, Mean Entropy: 0.3768320083618164, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 153,  Mean reward: -4.596774193548387, Mean Entropy: 0.37010693550109863, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 154,  Mean reward: -1.4375, Mean Entropy: 0.33948835730552673, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 155,  Mean reward: -1.7868852459016393, Mean Entropy: 0.3326588273048401, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 156,  Mean reward: -0.9307692307692308, Mean Entropy: 0.29837220907211304, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 157,  Mean reward: -0.9761904761904762, Mean Entropy: 0.40734121203422546, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 158,  Mean reward: -1.2076923076923076, Mean Entropy: 0.461334228515625, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 159,  Mean reward: -3.789473684210526, Mean Entropy: 0.41533660888671875, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 160,  Mean reward: -1.9918032786885247, Mean Entropy: 0.4387916922569275, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 161,  Mean reward: -2.6120689655172415, Mean Entropy: 0.3930310904979706, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 162,  Mean reward: -2.6796875, Mean Entropy: 0.36353832483291626, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 163,  Mean reward: -2.238095238095238, Mean Entropy: 0.3091670274734497, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 164,  Mean reward: -1.9672131147540983, Mean Entropy: 0.2884830832481384, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 165,  Mean reward: -3.8833333333333333, Mean Entropy: 0.23964092135429382, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.60s
Iteration: 166,  Mean reward: -2.1048387096774195, Mean Entropy: 0.31202518939971924, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 167,  Mean reward: -4.075, Mean Entropy: 0.38577958941459656, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 168,  Mean reward: -3.096774193548387, Mean Entropy: 0.35872703790664673, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 169,  Mean reward: -3.75, Mean Entropy: 0.3324947655200958, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 170,  Mean reward: -4.127118644067797, Mean Entropy: 0.2705594301223755, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 171,  Mean reward: -1.984375, Mean Entropy: 0.2622326612472534, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 172,  Mean reward: -1.7272727272727273, Mean Entropy: 0.24699155986309052, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 173,  Mean reward: -4.258333333333334, Mean Entropy: 0.10506884753704071, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 174,  Mean reward: -4.3125, Mean Entropy: 0.0892476886510849, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 175,  Mean reward: -2.8230769230769233, Mean Entropy: 0.10095548629760742, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 176,  Mean reward: -1.2153846153846153, Mean Entropy: 0.10102027654647827, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 177,  Mean reward: -2.8, Mean Entropy: 0.15564021468162537, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 178,  Mean reward: -2.6796875, Mean Entropy: 0.17537164688110352, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 179,  Mean reward: -6.308333333333334, Mean Entropy: 0.10935169458389282, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 180,  Mean reward: -4.782258064516129, Mean Entropy: 0.11603598296642303, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 181,  Mean reward: -2.390625, Mean Entropy: 0.12418560683727264, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 182,  Mean reward: -0.12121212121212122, Mean Entropy: 0.13676689565181732, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 183,  Mean reward: -4.663934426229508, Mean Entropy: 0.18018803000450134, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 184,  Mean reward: -2.261904761904762, Mean Entropy: 0.13113325834274292, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 185,  Mean reward: -1.8492063492063493, Mean Entropy: 0.14404511451721191, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 186,  Mean reward: -0.6417910447761194, Mean Entropy: 0.18495284020900726, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 187,  Mean reward: -3.0847457627118646, Mean Entropy: 0.13227270543575287, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 188,  Mean reward: -0.4453125, Mean Entropy: 0.10092981904745102, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 189,  Mean reward: -4.311475409836065, Mean Entropy: 0.07925289124250412, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 190,  Mean reward: -2.65625, Mean Entropy: 0.08906197547912598, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 191,  Mean reward: -2.3671875, Mean Entropy: 0.09042049944400787, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 192,  Mean reward: -3.0, Mean Entropy: 0.1322241872549057, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 193,  Mean reward: -3.4193548387096775, Mean Entropy: 0.1800304353237152, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 194,  Mean reward: -0.515625, Mean Entropy: 0.22615006566047668, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 195,  Mean reward: -3.238095238095238, Mean Entropy: 0.2564637064933777, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.76s
Iteration: 196,  Mean reward: -3.566666666666667, Mean Entropy: 0.1278105229139328, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 197,  Mean reward: -4.925, Mean Entropy: 0.12161040306091309, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 198,  Mean reward: -1.71875, Mean Entropy: 0.14096161723136902, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 199,  Mean reward: -3.183333333333333, Mean Entropy: 0.19155564904212952, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 200,  Mean reward: -0.4453125, Mean Entropy: 0.3155324459075928, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -3.3421052631578947, Mean Entropy: 0.30447492003440857, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 202,  Mean reward: -1.8629032258064515, Mean Entropy: 0.21942561864852905, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 203,  Mean reward: -2.129032258064516, Mean Entropy: 0.22005698084831238, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 204,  Mean reward: -2.0546875, Mean Entropy: 0.19545704126358032, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 205,  Mean reward: -1.1171875, Mean Entropy: 0.18587827682495117, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 206,  Mean reward: -2.0546875, Mean Entropy: 0.22522108256816864, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 207,  Mean reward: -2.4516129032258065, Mean Entropy: 0.13583993911743164, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 208,  Mean reward: -3.1904761904761907, Mean Entropy: 0.08540167659521103, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 209,  Mean reward: -2.5317460317460316, Mean Entropy: 0.061561089009046555, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 210,  Mean reward: -3.3046875, Mean Entropy: 0.06489548087120056, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 211,  Mean reward: -0.75, Mean Entropy: 0.07212909311056137, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 212,  Mean reward: -4.709677419354839, Mean Entropy: 0.09350895881652832, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 213,  Mean reward: -3.7661290322580645, Mean Entropy: 0.08150341361761093, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 214,  Mean reward: -3.1076923076923078, Mean Entropy: 0.05778723210096359, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 215,  Mean reward: -3.5555555555555554, Mean Entropy: 0.04978296160697937, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 216,  Mean reward: -2.5153846153846153, Mean Entropy: 0.055335771292448044, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 217,  Mean reward: -3.443548387096774, Mean Entropy: 0.06776601076126099, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 218,  Mean reward: -3.6311475409836067, Mean Entropy: 0.09255167841911316, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 219,  Mean reward: -4.2421875, Mean Entropy: 0.08666925132274628, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 220,  Mean reward: -1.8492063492063493, Mean Entropy: 0.09064966440200806, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 221,  Mean reward: -0.9769230769230769, Mean Entropy: 0.1125991940498352, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 222,  Mean reward: -2.6031746031746033, Mean Entropy: 0.14746591448783875, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.78s
Iteration: 223,  Mean reward: -1.4296875, Mean Entropy: 0.1260974109172821, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 224,  Mean reward: -4.688524590163935, Mean Entropy: 0.09017187356948853, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 225,  Mean reward: -2.725806451612903, Mean Entropy: 0.0783257782459259, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 226,  Mean reward: -4.286885245901639, Mean Entropy: 0.09788915514945984, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 227,  Mean reward: -2.6796875, Mean Entropy: 0.08702638745307922, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 228,  Mean reward: -2.5793650793650795, Mean Entropy: 0.10033729672431946, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 229,  Mean reward: -2.1048387096774195, Mean Entropy: 0.09609868377447128, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 230,  Mean reward: -1.2384615384615385, Mean Entropy: 0.1066582053899765, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 231,  Mean reward: -0.6461538461538462, Mean Entropy: 0.12951219081878662, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 232,  Mean reward: -4.411290322580645, Mean Entropy: 0.07722707837820053, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 233,  Mean reward: -1.6953125, Mean Entropy: 0.055806126445531845, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 234,  Mean reward: -3.6311475409836067, Mean Entropy: 0.051183782517910004, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 235,  Mean reward: -2.078125, Mean Entropy: 0.045776575803756714, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 236,  Mean reward: -0.44696969696969696, Mean Entropy: 0.07945503294467926, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 237,  Mean reward: -3.0390625, Mean Entropy: 0.0927489846944809, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 238,  Mean reward: -1.8968253968253967, Mean Entropy: 0.10977219045162201, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 239,  Mean reward: -1.626984126984127, Mean Entropy: 0.09408009052276611, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 240,  Mean reward: -0.2923076923076923, Mean Entropy: 0.08420765399932861, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 241,  Mean reward: -1.9206349206349207, Mean Entropy: 0.09112988412380219, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 242,  Mean reward: -1.876923076923077, Mean Entropy: 0.09695323556661606, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 243,  Mean reward: -0.9076923076923077, Mean Entropy: 0.1042327731847763, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 244,  Mean reward: -2.403225806451613, Mean Entropy: 0.07688595354557037, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 245,  Mean reward: -2.0546875, Mean Entropy: 0.06859441846609116, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 246,  Mean reward: -4.531746031746032, Mean Entropy: 0.05807720124721527, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 247,  Mean reward: -1.3828125, Mean Entropy: 0.046571116894483566, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 248,  Mean reward: -3.5317460317460316, Mean Entropy: 0.06103923171758652, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 249,  Mean reward: -4.064516129032258, Mean Entropy: 0.04855741187930107, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 250,  Mean reward: -5.016393442622951, Mean Entropy: 0.05022796243429184, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 251,  Mean reward: -3.6311475409836067, Mean Entropy: 0.060070037841796875, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 252,  Mean reward: -0.4696969696969697, Mean Entropy: 0.08277086913585663, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 253,  Mean reward: -1.2846153846153847, Mean Entropy: 0.11684240400791168, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 254,  Mean reward: -2.8225806451612905, Mean Entropy: 0.061942096799612045, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 255,  Mean reward: -1.71875, Mean Entropy: 0.054000452160835266, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 256,  Mean reward: -3.261904761904762, Mean Entropy: 0.03642556816339493, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 257,  Mean reward: -4.311475409836065, Mean Entropy: 0.030652089044451714, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 258,  Mean reward: -6.060344827586207, Mean Entropy: 0.023135576397180557, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.41s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 259,  Mean reward: 0.3208955223880597, Mean Entropy: 0.04922475665807724, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 260,  Mean reward: -2.75, Mean Entropy: 0.10685235261917114, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 261,  Mean reward: -2.261904761904762, Mean Entropy: 0.15688621997833252, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 262,  Mean reward: -0.8046875, Mean Entropy: 0.16714975237846375, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 263,  Mean reward: 0.007936507936507936, Mean Entropy: 0.19064724445343018, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 264,  Mean reward: -3.3524590163934427, Mean Entropy: 0.22164809703826904, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 265,  Mean reward: -6.087719298245614, Mean Entropy: 0.06772258132696152, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 266,  Mean reward: -4.008196721311475, Mean Entropy: 0.06921135634183884, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 267,  Mean reward: -2.078125, Mean Entropy: 0.13659262657165527, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 268,  Mean reward: -2.5166666666666666, Mean Entropy: 0.1753305196762085, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 269,  Mean reward: -3.925, Mean Entropy: 0.12354867905378342, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 270,  Mean reward: -2.3688524590163933, Mean Entropy: 0.10392645001411438, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 271,  Mean reward: -2.2857142857142856, Mean Entropy: 0.09429308772087097, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 272,  Mean reward: -3.443548387096774, Mean Entropy: 0.09994569420814514, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.76s
Iteration: 273,  Mean reward: -1.3095238095238095, Mean Entropy: 0.09321727603673935, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 274,  Mean reward: -3.0245901639344264, Mean Entropy: 0.05749265477061272, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 275,  Mean reward: -0.78125, Mean Entropy: 0.05447785183787346, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 276,  Mean reward: -3.9, Mean Entropy: 0.05160999298095703, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 277,  Mean reward: -3.1451612903225805, Mean Entropy: 0.054766617715358734, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 278,  Mean reward: -4.925, Mean Entropy: 0.07603274285793304, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 279,  Mean reward: -1.765625, Mean Entropy: 0.06573017686605453, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 280,  Mean reward: -4.925, Mean Entropy: 0.040268830955028534, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 281,  Mean reward: -3.096774193548387, Mean Entropy: 0.0398453027009964, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 282,  Mean reward: -4.008196721311475, Mean Entropy: 0.04365258663892746, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.62s
Iteration: 283,  Mean reward: -2.078125, Mean Entropy: 0.06386901438236237, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 284,  Mean reward: -2.2142857142857144, Mean Entropy: 0.08714070916175842, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 285,  Mean reward: -2.0546875, Mean Entropy: 0.09729818999767303, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 286,  Mean reward: 0.6194029850746269, Mean Entropy: 0.0911572203040123, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 287,  Mean reward: -1.5923076923076922, Mean Entropy: 0.0934186801314354, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 288,  Mean reward: -0.9538461538461539, Mean Entropy: 0.08071292191743851, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 289,  Mean reward: -3.6557377049180326, Mean Entropy: 0.057163305580616, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 290,  Mean reward: -0.6, Mean Entropy: 0.058575328439474106, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 291,  Mean reward: -2.238095238095238, Mean Entropy: 0.053252846002578735, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 292,  Mean reward: -2.774193548387097, Mean Entropy: 0.04893316328525543, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 293,  Mean reward: -2.078125, Mean Entropy: 0.043042611330747604, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 294,  Mean reward: 0.6194029850746269, Mean Entropy: 0.049956630915403366, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 295,  Mean reward: -3.680327868852459, Mean Entropy: 0.043322138488292694, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 296,  Mean reward: -2.8968253968253967, Mean Entropy: 0.03974561393260956, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 297,  Mean reward: -3.7903225806451615, Mean Entropy: 0.029341157525777817, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 298,  Mean reward: -6.576271186440678, Mean Entropy: 0.01809350959956646, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 299,  Mean reward: -4.639344262295082, Mean Entropy: 0.016015958040952682, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 300,  Mean reward: 0.0, Mean Entropy: 0.026114558801054955, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -3.096774193548387, Mean Entropy: 0.04943249747157097, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 302,  Mean reward: -2.2142857142857144, Mean Entropy: 0.07025803625583649, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 303,  Mean reward: -1.5461538461538462, Mean Entropy: 0.10622835159301758, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.69s
Iteration: 304,  Mean reward: -3.096774193548387, Mean Entropy: 0.1104099377989769, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 305,  Mean reward: -2.261904761904762, Mean Entropy: 0.08835999667644501, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 306,  Mean reward: -3.0, Mean Entropy: 0.06668779253959656, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 307,  Mean reward: -4.008196721311475, Mean Entropy: 0.035097550600767136, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 308,  Mean reward: -2.4140625, Mean Entropy: 0.028887934982776642, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 309,  Mean reward: -0.9076923076923077, Mean Entropy: 0.025669991970062256, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 310,  Mean reward: -2.6031746031746033, Mean Entropy: 0.034142713993787766, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 311,  Mean reward: -4.112903225806452, Mean Entropy: 0.024112436920404434, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 312,  Mean reward: -3.096774193548387, Mean Entropy: 0.02162056975066662, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 313,  Mean reward: -3.7661290322580645, Mean Entropy: 0.0226423442363739, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.75s
Iteration: 314,  Mean reward: -3.6311475409836067, Mean Entropy: 0.025897331535816193, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 315,  Mean reward: -3.278688524590164, Mean Entropy: 0.022067947313189507, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 316,  Mean reward: -0.6, Mean Entropy: 0.032913655042648315, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 317,  Mean reward: -4.541666666666667, Mean Entropy: 0.032431796193122864, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 318,  Mean reward: -3.7049180327868854, Mean Entropy: 0.023983849212527275, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 319,  Mean reward: -2.0546875, Mean Entropy: 0.026539232581853867, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 320,  Mean reward: -2.261904761904762, Mean Entropy: 0.033206745982170105, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 321,  Mean reward: -1.5461538461538462, Mean Entropy: 0.046499624848365784, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 322,  Mean reward: -3.096774193548387, Mean Entropy: 0.04571991413831711, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 323,  Mean reward: -1.40625, Mean Entropy: 0.04651261866092682, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 324,  Mean reward: -4.311475409836065, Mean Entropy: 0.0250849612057209, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 325,  Mean reward: -2.261904761904762, Mean Entropy: 0.02319684438407421, complete_episode_count: 63.0, Gather time: 0.69s, Train time: 0.71s
Iteration: 326,  Mean reward: -3.443548387096774, Mean Entropy: 0.018197545781731606, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 327,  Mean reward: -3.9836065573770494, Mean Entropy: 0.021546639502048492, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 328,  Mean reward: -5.368852459016393, Mean Entropy: 0.012823808006942272, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 329,  Mean reward: -1.121212121212121, Mean Entropy: 0.013822379522025585, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 330,  Mean reward: -2.238095238095238, Mean Entropy: 0.03182707354426384, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 331,  Mean reward: 0.29850746268656714, Mean Entropy: 0.07889241725206375, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 332,  Mean reward: -2.261904761904762, Mean Entropy: 0.12130118906497955, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 333,  Mean reward: -6.815789473684211, Mean Entropy: 0.037029098719358444, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 334,  Mean reward: -2.078125, Mean Entropy: 0.021015610545873642, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 335,  Mean reward: -5.822033898305085, Mean Entropy: 0.013379029929637909, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 336,  Mean reward: -0.34328358208955223, Mean Entropy: 0.028427496552467346, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 337,  Mean reward: -3.096774193548387, Mean Entropy: 0.0580194815993309, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 338,  Mean reward: -5.1440677966101696, Mean Entropy: 0.05462440475821495, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 339,  Mean reward: -4.008196721311475, Mean Entropy: 0.06167309731245041, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 340,  Mean reward: -2.078125, Mean Entropy: 0.07677818834781647, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 341,  Mean reward: -0.46875, Mean Entropy: 0.10341645777225494, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 342,  Mean reward: -0.36153846153846153, Mean Entropy: 0.12639334797859192, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 343,  Mean reward: -3.6311475409836067, Mean Entropy: 0.08767469227313995, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 344,  Mean reward: -1.626984126984127, Mean Entropy: 0.0664713978767395, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 345,  Mean reward: -1.7890625, Mean Entropy: 0.06683538854122162, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 346,  Mean reward: -2.4516129032258065, Mean Entropy: 0.04303805157542229, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 347,  Mean reward: -1.40625, Mean Entropy: 0.05986347422003746, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 348,  Mean reward: -3.467741935483871, Mean Entropy: 0.03317053243517876, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 349,  Mean reward: -5.741379310344827, Mean Entropy: 0.004896300844848156, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 350,  Mean reward: -5.11864406779661, Mean Entropy: 0.007173316087573767, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 351,  Mean reward: -1.2615384615384615, Mean Entropy: 0.03936870023608208, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 352,  Mean reward: -2.4516129032258065, Mean Entropy: 0.1162436231970787, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 353,  Mean reward: -2.3095238095238093, Mean Entropy: 0.13804906606674194, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 354,  Mean reward: -2.5, Mean Entropy: 0.12605157494544983, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 355,  Mean reward: -0.9538461538461539, Mean Entropy: 0.1327352523803711, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 356,  Mean reward: -0.8387096774193549, Mean Entropy: 0.15084245800971985, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 357,  Mean reward: -2.9444444444444446, Mean Entropy: 0.12651056051254272, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 358,  Mean reward: -1.0081967213114753, Mean Entropy: 0.14910882711410522, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 359,  Mean reward: -4.258333333333334, Mean Entropy: 0.09891077876091003, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 360,  Mean reward: -2.7983870967741935, Mean Entropy: 0.08026725798845291, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 361,  Mean reward: -4.258333333333334, Mean Entropy: 0.02905253879725933, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 362,  Mean reward: -0.9307692307692308, Mean Entropy: 0.02643394283950329, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 363,  Mean reward: -0.49242424242424243, Mean Entropy: 0.054571639746427536, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 364,  Mean reward: -3.5161290322580645, Mean Entropy: 0.046632617712020874, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 365,  Mean reward: -3.278688524590164, Mean Entropy: 0.01828032173216343, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 366,  Mean reward: -3.814516129032258, Mean Entropy: 0.005751396995037794, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 367,  Mean reward: -2.390625, Mean Entropy: 0.006041094660758972, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 368,  Mean reward: -1.7421875, Mean Entropy: 0.01718267612159252, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 369,  Mean reward: -1.0703125, Mean Entropy: 0.029539812356233597, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 370,  Mean reward: -4.541666666666667, Mean Entropy: 0.016816793009638786, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 371,  Mean reward: -3.30327868852459, Mean Entropy: 0.017142698168754578, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 372,  Mean reward: -2.5793650793650795, Mean Entropy: 0.011659286916255951, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.76s
Iteration: 373,  Mean reward: -2.4274193548387095, Mean Entropy: 0.011635775677859783, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 374,  Mean reward: 0.06153846153846154, Mean Entropy: 0.03733598068356514, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 375,  Mean reward: -5.258333333333334, Mean Entropy: 0.02061181142926216, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 376,  Mean reward: -1.8968253968253967, Mean Entropy: 0.02943289279937744, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 377,  Mean reward: -2.9754098360655736, Mean Entropy: 0.03247065469622612, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 378,  Mean reward: -2.1015625, Mean Entropy: 0.028416737914085388, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 379,  Mean reward: -0.9444444444444444, Mean Entropy: 0.03350875526666641, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 380,  Mean reward: -0.3153846153846154, Mean Entropy: 0.03573925420641899, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 381,  Mean reward: -3.467741935483871, Mean Entropy: 0.015709009021520615, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 382,  Mean reward: 0.29850746268656714, Mean Entropy: 0.02946109138429165, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 383,  Mean reward: -2.1048387096774195, Mean Entropy: 0.038951657712459564, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 384,  Mean reward: -2.1015625, Mean Entropy: 0.04009201377630234, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 385,  Mean reward: -3.7419354838709675, Mean Entropy: 0.020944584161043167, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 386,  Mean reward: -1.5923076923076922, Mean Entropy: 0.031016051769256592, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 387,  Mean reward: -3.278688524590164, Mean Entropy: 0.03257060423493385, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 388,  Mean reward: -3.120967741935484, Mean Entropy: 0.026224713772535324, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 389,  Mean reward: -3.7903225806451615, Mean Entropy: 0.0242098830640316, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 390,  Mean reward: -4.9, Mean Entropy: 0.007552619557827711, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 391,  Mean reward: -2.5793650793650795, Mean Entropy: 0.011234450154006481, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.77s
Iteration: 392,  Mean reward: -2.238095238095238, Mean Entropy: 0.019324693828821182, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 393,  Mean reward: -3.120967741935484, Mean Entropy: 0.01781993731856346, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 394,  Mean reward: -4.336065573770492, Mean Entropy: 0.012957965955138206, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 395,  Mean reward: -4.336065573770492, Mean Entropy: 0.009530610404908657, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 396,  Mean reward: -3.120967741935484, Mean Entropy: 0.009827729314565659, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 397,  Mean reward: -3.6031746031746033, Mean Entropy: 0.015764152631163597, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 398,  Mean reward: -4.9, Mean Entropy: 0.012808850035071373, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 399,  Mean reward: -2.4140625, Mean Entropy: 0.019797950983047485, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 400,  Mean reward: -0.6230769230769231, Mean Entropy: 0.03939786180853844, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -0.6461538461538462, Mean Entropy: 0.05336497351527214, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 402,  Mean reward: -2.9206349206349205, Mean Entropy: 0.03588404878973961, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 403,  Mean reward: -3.467741935483871, Mean Entropy: 0.021975573152303696, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 404,  Mean reward: -5.258333333333334, Mean Entropy: 0.013296853750944138, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 405,  Mean reward: -3.261904761904762, Mean Entropy: 0.004455900285393, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 406,  Mean reward: -2.5793650793650795, Mean Entropy: 0.004877103492617607, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 407,  Mean reward: -3.6311475409836067, Mean Entropy: 0.005843248218297958, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 408,  Mean reward: -4.137096774193548, Mean Entropy: 0.004346757661551237, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 409,  Mean reward: -1.0703125, Mean Entropy: 0.009413829073309898, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 410,  Mean reward: -2.261904761904762, Mean Entropy: 0.015035228803753853, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 411,  Mean reward: -3.9836065573770494, Mean Entropy: 0.011597193777561188, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 412,  Mean reward: -2.5793650793650795, Mean Entropy: 0.01683272235095501, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 413,  Mean reward: -2.4140625, Mean Entropy: 0.013937791809439659, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 414,  Mean reward: -2.5793650793650795, Mean Entropy: 0.024590251967310905, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 415,  Mean reward: 0.5970149253731343, Mean Entropy: 0.039166465401649475, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 416,  Mean reward: -4.336065573770492, Mean Entropy: 0.031765714287757874, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 417,  Mean reward: -2.9508196721311477, Mean Entropy: 0.03088293969631195, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 418,  Mean reward: -2.9508196721311477, Mean Entropy: 0.03371644765138626, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 419,  Mean reward: -1.40625, Mean Entropy: 0.04432056099176407, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 420,  Mean reward: -3.9836065573770494, Mean Entropy: 0.026455901563167572, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.45s
Iteration: 421,  Mean reward: -0.9307692307692308, Mean Entropy: 0.024197470396757126, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 422,  Mean reward: -4.057377049180328, Mean Entropy: 0.016950467601418495, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 423,  Mean reward: -1.8968253968253967, Mean Entropy: 0.016299493610858917, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 424,  Mean reward: -1.9206349206349207, Mean Entropy: 0.024575792253017426, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 425,  Mean reward: -3.3278688524590163, Mean Entropy: 0.026712913066148758, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 426,  Mean reward: -3.261904761904762, Mean Entropy: 0.030093183740973473, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 427,  Mean reward: -0.6, Mean Entropy: 0.03289560601115227, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 428,  Mean reward: -2.2857142857142856, Mean Entropy: 0.03641523793339729, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 429,  Mean reward: -1.3095238095238095, Mean Entropy: 0.030826061964035034, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 430,  Mean reward: -1.0703125, Mean Entropy: 0.027760950848460197, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 431,  Mean reward: -3.467741935483871, Mean Entropy: 0.025254471227526665, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 432,  Mean reward: -1.9206349206349207, Mean Entropy: 0.02490963600575924, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 433,  Mean reward: -0.14393939393939395, Mean Entropy: 0.04017374664545059, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 434,  Mean reward: -3.3278688524590163, Mean Entropy: 0.026511453092098236, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.47s
Iteration: 435,  Mean reward: -3.680327868852459, Mean Entropy: 0.0020040092058479786, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 436,  Mean reward: -3.9836065573770494, Mean Entropy: 0.0011652662651613355, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 437,  Mean reward: -2.4140625, Mean Entropy: 0.002928365720435977, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 438,  Mean reward: -2.5793650793650795, Mean Entropy: 0.009525420144200325, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.97s
Iteration: 439,  Mean reward: -3.6311475409836067, Mean Entropy: 0.012380971573293209, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 440,  Mean reward: -0.4696969696969697, Mean Entropy: 0.03245231881737709, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 441,  Mean reward: -1.0703125, Mean Entropy: 0.05129927024245262, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 442,  Mean reward: -3.1451612903225805, Mean Entropy: 0.043800290673971176, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 443,  Mean reward: -4.008196721311475, Mean Entropy: 0.01501423679292202, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 444,  Mean reward: -1.765625, Mean Entropy: 0.011949688196182251, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 445,  Mean reward: -1.8968253968253967, Mean Entropy: 0.01662403531372547, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 446,  Mean reward: -2.75, Mean Entropy: 0.020771600306034088, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 447,  Mean reward: -0.14393939393939395, Mean Entropy: 0.03867633268237114, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 448,  Mean reward: -2.9206349206349205, Mean Entropy: 0.04765310138463974, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.75s
Iteration: 449,  Mean reward: -3.9836065573770494, Mean Entropy: 0.026320070028305054, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 450,  Mean reward: -1.0703125, Mean Entropy: 0.026718877255916595, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 451,  Mean reward: -1.8968253968253967, Mean Entropy: 0.03110337071120739, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 452,  Mean reward: -1.9206349206349207, Mean Entropy: 0.03062218241393566, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 453,  Mean reward: -0.9307692307692308, Mean Entropy: 0.041829030960798264, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 454,  Mean reward: -2.9508196721311477, Mean Entropy: 0.04773823916912079, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 455,  Mean reward: -4.183333333333334, Mean Entropy: 0.029400262981653214, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 456,  Mean reward: -3.467741935483871, Mean Entropy: 0.03063150867819786, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 457,  Mean reward: -2.8225806451612905, Mean Entropy: 0.027650397270917892, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 458,  Mean reward: -2.475806451612903, Mean Entropy: 0.022430434823036194, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 459,  Mean reward: -3.261904761904762, Mean Entropy: 0.015131516382098198, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 460,  Mean reward: -2.261904761904762, Mean Entropy: 0.013744833879172802, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 461,  Mean reward: -4.336065573770492, Mean Entropy: 0.007600001059472561, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.46s
Iteration: 462,  Mean reward: -2.75, Mean Entropy: 0.009988664649426937, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 463,  Mean reward: -1.5923076923076922, Mean Entropy: 0.015004630200564861, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 464,  Mean reward: -3.096774193548387, Mean Entropy: 0.02121804654598236, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 465,  Mean reward: -3.9836065573770494, Mean Entropy: 0.015210108831524849, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 466,  Mean reward: -4.183333333333334, Mean Entropy: 0.008175761438906193, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 467,  Mean reward: -4.688524590163935, Mean Entropy: 0.002258797874674201, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.46s
Iteration: 468,  Mean reward: -2.238095238095238, Mean Entropy: 0.008526147343218327, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 469,  Mean reward: -3.7903225806451615, Mean Entropy: 0.020213453099131584, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 470,  Mean reward: -2.75, Mean Entropy: 0.03360236436128616, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 471,  Mean reward: -2.261904761904762, Mean Entropy: 0.052840471267700195, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 472,  Mean reward: -2.078125, Mean Entropy: 0.06616722792387009, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 473,  Mean reward: -4.415254237288136, Mean Entropy: 0.05323425680398941, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 474,  Mean reward: -3.9836065573770494, Mean Entropy: 0.026923533529043198, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 475,  Mean reward: -1.7421875, Mean Entropy: 0.05578155443072319, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.77s
Iteration: 476,  Mean reward: -3.3524590163934427, Mean Entropy: 0.07687676697969437, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.43s
Iteration: 477,  Mean reward: -2.125, Mean Entropy: 0.07356239855289459, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 478,  Mean reward: -2.5416666666666665, Mean Entropy: 0.05019111558794975, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.66s
Iteration: 479,  Mean reward: -1.9682539682539681, Mean Entropy: 0.039782896637916565, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 480,  Mean reward: -1.7421875, Mean Entropy: 0.03608772158622742, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 481,  Mean reward: -4.688524590163935, Mean Entropy: 0.017733320593833923, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 482,  Mean reward: -0.9307692307692308, Mean Entropy: 0.016551002860069275, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 483,  Mean reward: -6.086206896551724, Mean Entropy: 0.007076755166053772, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 484,  Mean reward: -1.7421875, Mean Entropy: 0.009292609989643097, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 485,  Mean reward: -4.008196721311475, Mean Entropy: 0.013628413900732994, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 486,  Mean reward: -4.483870967741935, Mean Entropy: 0.007577728014439344, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 487,  Mean reward: -1.8968253968253967, Mean Entropy: 0.012051033787429333, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 488,  Mean reward: -1.7421875, Mean Entropy: 0.02572700008749962, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 489,  Mean reward: -1.09375, Mean Entropy: 0.05331394821405411, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 490,  Mean reward: -3.467741935483871, Mean Entropy: 0.041474759578704834, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 491,  Mean reward: -1.453125, Mean Entropy: 0.02796219289302826, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 492,  Mean reward: -2.774193548387097, Mean Entropy: 0.03160228952765465, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 493,  Mean reward: -1.5555555555555556, Mean Entropy: 0.027778465300798416, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 494,  Mean reward: -1.09375, Mean Entropy: 0.03381900489330292, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 495,  Mean reward: -3.261904761904762, Mean Entropy: 0.03442859649658203, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 496,  Mean reward: 0.29850746268656714, Mean Entropy: 0.04358915984630585, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 497,  Mean reward: -3.5161290322580645, Mean Entropy: 0.028047483414411545, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 498,  Mean reward: -4.336065573770492, Mean Entropy: 0.013094249181449413, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 499,  Mean reward: -3.120967741935484, Mean Entropy: 0.007874995470046997, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 500,  Mean reward: -2.9508196721311477, Mean Entropy: 0.0073422156274318695, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -2.056451612903226, Mean Entropy: 0.011013071984052658, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 502,  Mean reward: -0.16666666666666666, Mean Entropy: 0.022902553901076317, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 503,  Mean reward: -3.261904761904762, Mean Entropy: 0.02443743869662285, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.78s
Iteration: 504,  Mean reward: -1.7421875, Mean Entropy: 0.03203096240758896, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 505,  Mean reward: -2.078125, Mean Entropy: 0.031154796481132507, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 506,  Mean reward: -1.40625, Mean Entropy: 0.026169028133153915, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 507,  Mean reward: -1.0703125, Mean Entropy: 0.03086639940738678, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 508,  Mean reward: -1.9206349206349207, Mean Entropy: 0.028757616877555847, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 509,  Mean reward: -1.8968253968253967, Mean Entropy: 0.025007884949445724, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 510,  Mean reward: -3.096774193548387, Mean Entropy: 0.013305215165019035, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 511,  Mean reward: -3.096774193548387, Mean Entropy: 0.011231588199734688, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 512,  Mean reward: -5.093220338983051, Mean Entropy: 0.001376778120175004, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 513,  Mean reward: -3.6311475409836067, Mean Entropy: 0.0009756173240020871, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 514,  Mean reward: -4.541666666666667, Mean Entropy: 0.0004250304773449898, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 515,  Mean reward: -4.336065573770492, Mean Entropy: 0.0011129971826449037, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 516,  Mean reward: -2.9262295081967213, Mean Entropy: 0.0037083765491843224, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 517,  Mean reward: -3.9836065573770494, Mean Entropy: 0.007689394056797028, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.73s
Iteration: 518,  Mean reward: -1.2380952380952381, Mean Entropy: 0.022732874378561974, complete_episode_count: 63.0, Gather time: 0.75s, Train time: 0.76s
Iteration: 519,  Mean reward: -1.9206349206349207, Mean Entropy: 0.03681690990924835, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 520,  Mean reward: -5.169491525423729, Mean Entropy: 0.007828829810023308, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 521,  Mean reward: -1.2615384615384615, Mean Entropy: 0.0036794659681618214, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 522,  Mean reward: -3.096774193548387, Mean Entropy: 0.005096234381198883, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 523,  Mean reward: -2.6031746031746033, Mean Entropy: 0.005421078763902187, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 524,  Mean reward: -3.096774193548387, Mean Entropy: 0.01126028224825859, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 525,  Mean reward: -1.7421875, Mean Entropy: 0.019003329798579216, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 526,  Mean reward: -0.9307692307692308, Mean Entropy: 0.07120101153850555, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 527,  Mean reward: -4.208333333333333, Mean Entropy: 0.08135358989238739, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 528,  Mean reward: -2.825, Mean Entropy: 0.055798619985580444, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.44s
Iteration: 529,  Mean reward: -2.8968253968253967, Mean Entropy: 0.04094892367720604, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 530,  Mean reward: -2.5555555555555554, Mean Entropy: 0.024543438106775284, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 531,  Mean reward: -1.7421875, Mean Entropy: 0.027708541601896286, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 532,  Mean reward: -2.7983870967741935, Mean Entropy: 0.02772429585456848, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 533,  Mean reward: -3.096774193548387, Mean Entropy: 0.014294548891484737, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 534,  Mean reward: -1.9444444444444444, Mean Entropy: 0.022945623844861984, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 535,  Mean reward: -5.95, Mean Entropy: 0.011614328250288963, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 536,  Mean reward: -1.2615384615384615, Mean Entropy: 0.007828544825315475, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 537,  Mean reward: -0.44696969696969696, Mean Entropy: 0.025753185153007507, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 538,  Mean reward: -2.2142857142857144, Mean Entropy: 0.05875725671648979, complete_episode_count: 63.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 539,  Mean reward: -4.435483870967742, Mean Entropy: 0.02106703631579876, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 540,  Mean reward: -2.1048387096774195, Mean Entropy: 0.013875181786715984, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.76s
Iteration: 541,  Mean reward: -4.541666666666667, Mean Entropy: 0.012486923485994339, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.45s
Iteration: 542,  Mean reward: -2.6031746031746033, Mean Entropy: 0.00974707119166851, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 543,  Mean reward: -3.096774193548387, Mean Entropy: 0.017590392380952835, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 544,  Mean reward: -4.483870967741935, Mean Entropy: 0.00701864343136549, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.44s
Iteration: 545,  Mean reward: -3.9836065573770494, Mean Entropy: 0.004684143699705601, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 546,  Mean reward: -2.8968253968253967, Mean Entropy: 0.0044130124151706696, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 547,  Mean reward: -3.238095238095238, Mean Entropy: 0.005843835882842541, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 548,  Mean reward: -1.8968253968253967, Mean Entropy: 0.008222147822380066, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 549,  Mean reward: -1.40625, Mean Entropy: 0.030558045953512192, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 550,  Mean reward: -2.078125, Mean Entropy: 0.029664374887943268, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 551,  Mean reward: -2.0806451612903225, Mean Entropy: 0.026057740673422813, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 552,  Mean reward: -2.261904761904762, Mean Entropy: 0.05204638093709946, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.78s
Iteration: 553,  Mean reward: -2.9921875, Mean Entropy: 0.04027968645095825, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 554,  Mean reward: -1.7045454545454546, Mean Entropy: 0.057228922843933105, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.76s
Iteration: 555,  Mean reward: -1.5461538461538462, Mean Entropy: 0.12666040658950806, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 556,  Mean reward: -2.4932432432432434, Mean Entropy: 0.12451314926147461, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 557,  Mean reward: -1.3716216216216217, Mean Entropy: 0.029477860778570175, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.76s
Iteration: 558,  Mean reward: -0.75, Mean Entropy: 0.021114163100719452, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 559,  Mean reward: -2.25, Mean Entropy: 0.02685016766190529, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 560,  Mean reward: -3.1582278481012658, Mean Entropy: 0.178565114736557, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 561,  Mean reward: -2.4527027027027026, Mean Entropy: 0.05520709976553917, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 562,  Mean reward: -3.1582278481012658, Mean Entropy: 0.07900963723659515, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 563,  Mean reward: -3.3205128205128207, Mean Entropy: 0.17755991220474243, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 564,  Mean reward: -4.344594594594595, Mean Entropy: 0.07593794167041779, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 565,  Mean reward: -1.8205128205128205, Mean Entropy: 0.006043429486453533, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 566,  Mean reward: -1.0, Mean Entropy: 0.0032780549954622984, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 567,  Mean reward: -1.1329113924050633, Mean Entropy: 0.0042407456785440445, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 568,  Mean reward: -2.75, Mean Entropy: 0.0025022246409207582, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 569,  Mean reward: -1.75, Mean Entropy: 0.010033709928393364, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 570,  Mean reward: -2.25, Mean Entropy: 0.002121465280652046, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 571,  Mean reward: -2.5, Mean Entropy: 0.035863593220710754, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 572,  Mean reward: -0.24358974358974358, Mean Entropy: 0.005634593777358532, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 573,  Mean reward: -2.0, Mean Entropy: 0.0016697859391570091, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 574,  Mean reward: -0.5, Mean Entropy: 0.003001886187121272, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 575,  Mean reward: -0.5, Mean Entropy: 0.003197341924533248, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 576,  Mean reward: -2.5, Mean Entropy: 0.003670824458822608, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 577,  Mean reward: -0.5, Mean Entropy: 0.004062784370034933, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 578,  Mean reward: -4.0, Mean Entropy: 0.02723017707467079, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 579,  Mean reward: -3.9367088607594938, Mean Entropy: 0.1899334043264389, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 580,  Mean reward: -1.2876712328767124, Mean Entropy: 0.011678001843392849, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 581,  Mean reward: -1.0, Mean Entropy: 0.0015064552426338196, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 582,  Mean reward: -2.25, Mean Entropy: 0.0031864296179264784, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 583,  Mean reward: -1.25, Mean Entropy: 0.009588261134922504, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 584,  Mean reward: -1.5, Mean Entropy: 0.006768939085304737, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 585,  Mean reward: -2.5, Mean Entropy: 0.0037563047371804714, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 586,  Mean reward: -2.75, Mean Entropy: 0.005856303032487631, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 587,  Mean reward: -2.5, Mean Entropy: 0.009629330597817898, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 588,  Mean reward: -2.75, Mean Entropy: 0.008118666708469391, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 589,  Mean reward: -3.0, Mean Entropy: 0.02446409873664379, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 590,  Mean reward: -2.0, Mean Entropy: 0.01980215311050415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 591,  Mean reward: -3.0, Mean Entropy: 0.014448179863393307, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 592,  Mean reward: -2.25, Mean Entropy: 0.026310786604881287, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 593,  Mean reward: -1.639240506329114, Mean Entropy: 0.003300898242741823, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 594,  Mean reward: -1.0, Mean Entropy: 0.003998937085270882, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.00s
Iteration: 595,  Mean reward: -3.0, Mean Entropy: 0.0037573769222944975, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 596,  Mean reward: -3.0, Mean Entropy: 0.01029728539288044, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 597,  Mean reward: -2.1265822784810124, Mean Entropy: 0.028420573100447655, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 598,  Mean reward: -1.3860759493670887, Mean Entropy: 0.012532006949186325, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 599,  Mean reward: -2.0, Mean Entropy: 0.0034008510410785675, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 600,  Mean reward: -1.75, Mean Entropy: 0.01054068561643362, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -3.5, Mean Entropy: 0.005983380135148764, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 602,  Mean reward: -2.25, Mean Entropy: 0.011750220321118832, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 603,  Mean reward: -2.25, Mean Entropy: 0.006183314602822065, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 604,  Mean reward: -1.0, Mean Entropy: 0.003504151478409767, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 605,  Mean reward: -2.5, Mean Entropy: 0.004563495516777039, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 606,  Mean reward: -1.75, Mean Entropy: 0.005466166418045759, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 607,  Mean reward: -3.25, Mean Entropy: 0.00928521528840065, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 608,  Mean reward: -3.6645569620253164, Mean Entropy: 0.011597877368330956, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 609,  Mean reward: -1.75, Mean Entropy: 0.01617082767188549, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 610,  Mean reward: -4.25, Mean Entropy: 0.007002008613198996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 611,  Mean reward: -3.0, Mean Entropy: 0.011856618337333202, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 612,  Mean reward: -2.25, Mean Entropy: 0.006590002682060003, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 613,  Mean reward: -2.25, Mean Entropy: 0.004574059974402189, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 614,  Mean reward: -0.25, Mean Entropy: 0.004645241424441338, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 615,  Mean reward: -1.5, Mean Entropy: 0.005179096013307571, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 616,  Mean reward: -1.25, Mean Entropy: 0.008373534306883812, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 617,  Mean reward: -1.25, Mean Entropy: 0.0050442833453416824, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 618,  Mean reward: -1.75, Mean Entropy: 0.004605122841894627, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 619,  Mean reward: -1.75, Mean Entropy: 0.004407329484820366, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 620,  Mean reward: -0.5, Mean Entropy: 0.004972346127033234, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 621,  Mean reward: -2.75, Mean Entropy: 0.003745556576177478, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 622,  Mean reward: -1.75, Mean Entropy: 0.00758608803153038, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 623,  Mean reward: -0.879746835443038, Mean Entropy: 0.0046881865710020065, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 624,  Mean reward: -0.25, Mean Entropy: 0.005662519484758377, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 625,  Mean reward: -1.75, Mean Entropy: 0.01630556583404541, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 626,  Mean reward: -3.0641025641025643, Mean Entropy: 0.0814906656742096, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 627,  Mean reward: -2.448051948051948, Mean Entropy: 0.03208435699343681, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 628,  Mean reward: -3.25, Mean Entropy: 0.01796504110097885, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 629,  Mean reward: -2.5, Mean Entropy: 0.015500861220061779, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 630,  Mean reward: -3.25, Mean Entropy: 0.017758170142769814, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 631,  Mean reward: -1.75, Mean Entropy: 0.01356368325650692, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 632,  Mean reward: 0.8734177215189873, Mean Entropy: 0.004186032805591822, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 633,  Mean reward: -2.75, Mean Entropy: 0.05900384113192558, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 634,  Mean reward: -3.2662337662337664, Mean Entropy: 0.08613180369138718, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 635,  Mean reward: -1.4675324675324675, Mean Entropy: 0.011175855994224548, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.75s
Iteration: 636,  Mean reward: -2.25, Mean Entropy: 0.004134347662329674, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 637,  Mean reward: -2.0384615384615383, Mean Entropy: 0.00330749130807817, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 638,  Mean reward: -3.5, Mean Entropy: 0.004738324321806431, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 639,  Mean reward: -1.3860759493670887, Mean Entropy: 0.004256143234670162, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 640,  Mean reward: -2.25, Mean Entropy: 0.004585742950439453, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 641,  Mean reward: 0.3860759493670886, Mean Entropy: 0.0033405935391783714, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 642,  Mean reward: -2.0, Mean Entropy: 0.029574302956461906, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 643,  Mean reward: -0.5, Mean Entropy: 0.04712706804275513, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 644,  Mean reward: -2.651898734177215, Mean Entropy: 0.03879354149103165, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 645,  Mean reward: -2.0384615384615383, Mean Entropy: 0.01405640970915556, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 646,  Mean reward: -0.6265822784810127, Mean Entropy: 0.004951573442667723, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 647,  Mean reward: -2.3987341772151898, Mean Entropy: 0.019460028037428856, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 648,  Mean reward: -2.3987341772151898, Mean Entropy: 0.019283873960375786, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 649,  Mean reward: -0.75, Mean Entropy: 0.0058189742267131805, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 650,  Mean reward: -2.0, Mean Entropy: 0.03401949256658554, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 651,  Mean reward: -2.0, Mean Entropy: 0.10451357811689377, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 652,  Mean reward: -2.006666666666667, Mean Entropy: 0.01149480976164341, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 653,  Mean reward: -0.5, Mean Entropy: 0.005458023399114609, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 654,  Mean reward: -1.8924050632911393, Mean Entropy: 0.0034562405198812485, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 655,  Mean reward: -2.0, Mean Entropy: 0.01026153564453125, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 656,  Mean reward: -1.5, Mean Entropy: 0.008994556963443756, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 657,  Mean reward: 0.75, Mean Entropy: 0.004439130891114473, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 658,  Mean reward: -1.75, Mean Entropy: 0.1400114893913269, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 659,  Mean reward: -2.3857142857142857, Mean Entropy: 0.14564841985702515, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 660,  Mean reward: -1.9565217391304348, Mean Entropy: 0.021764233708381653, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 661,  Mean reward: -1.1329113924050633, Mean Entropy: 0.005876353941857815, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 662,  Mean reward: -1.5, Mean Entropy: 0.01321130059659481, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 663,  Mean reward: -3.9177215189873418, Mean Entropy: 0.05985329672694206, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 664,  Mean reward: -3.0641025641025643, Mean Entropy: 0.005447823088616133, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 665,  Mean reward: -1.4050632911392404, Mean Entropy: 0.004311187192797661, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 666,  Mean reward: -2.0, Mean Entropy: 0.002047215588390827, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 667,  Mean reward: -2.1455696202531644, Mean Entropy: 0.004470938816666603, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 668,  Mean reward: -1.75, Mean Entropy: 0.016958121210336685, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 669,  Mean reward: -3.75, Mean Entropy: 0.06994390487670898, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 670,  Mean reward: -2.1455696202531644, Mean Entropy: 0.008295499719679356, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 671,  Mean reward: -1.75, Mean Entropy: 0.01304105669260025, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 672,  Mean reward: -1.25, Mean Entropy: 0.00720943883061409, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 673,  Mean reward: -2.5, Mean Entropy: 0.007428785320371389, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 674,  Mean reward: -1.75, Mean Entropy: 0.010123489424586296, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 675,  Mean reward: -0.75, Mean Entropy: 0.011537199839949608, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 676,  Mean reward: -1.75, Mean Entropy: 0.013564914464950562, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 677,  Mean reward: -0.6265822784810127, Mean Entropy: 0.007887085899710655, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.77s
Iteration: 678,  Mean reward: -2.25, Mean Entropy: 0.006543372757732868, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 679,  Mean reward: -1.25, Mean Entropy: 0.007688834331929684, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 680,  Mean reward: -1.0, Mean Entropy: 0.008146950975060463, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 681,  Mean reward: -0.6265822784810127, Mean Entropy: 0.0066714040003716946, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 682,  Mean reward: -2.25, Mean Entropy: 0.00827519129961729, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 683,  Mean reward: -2.25, Mean Entropy: 0.0045328582637012005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 684,  Mean reward: -1.5, Mean Entropy: 0.004469050094485283, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 685,  Mean reward: -2.25, Mean Entropy: 0.005130711942911148, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 686,  Mean reward: -3.5, Mean Entropy: 0.007294772192835808, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 687,  Mean reward: -5.25, Mean Entropy: 0.009096534922719002, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 688,  Mean reward: -1.5, Mean Entropy: 0.0049158912152051926, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 689,  Mean reward: -2.5, Mean Entropy: 0.01463034562766552, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 690,  Mean reward: -1.3860759493670887, Mean Entropy: 0.03050091490149498, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 691,  Mean reward: -0.743421052631579, Mean Entropy: 0.04841560870409012, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 692,  Mean reward: -0.7564102564102564, Mean Entropy: 0.06487488746643066, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 693,  Mean reward: -1.1866666666666668, Mean Entropy: 0.017218738794326782, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 694,  Mean reward: -2.75, Mean Entropy: 0.020425092428922653, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 695,  Mean reward: -0.75, Mean Entropy: 0.00932441558688879, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 696,  Mean reward: -1.5, Mean Entropy: 0.005033618770539761, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 697,  Mean reward: -2.9050632911392404, Mean Entropy: 0.00501193618401885, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 698,  Mean reward: -1.25, Mean Entropy: 0.004236944019794464, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 699,  Mean reward: -1.5, Mean Entropy: 0.006616436876356602, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 700,  Mean reward: -2.75, Mean Entropy: 0.006390376947820187, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -2.5, Mean Entropy: 0.0039650858379900455, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 702,  Mean reward: -1.75, Mean Entropy: 0.003956357948482037, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 703,  Mean reward: -2.25, Mean Entropy: 0.009753449819982052, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 704,  Mean reward: -1.0128205128205128, Mean Entropy: 0.006237804424017668, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 705,  Mean reward: -1.0, Mean Entropy: 0.007913556881248951, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 706,  Mean reward: -1.7820512820512822, Mean Entropy: 0.005417196080088615, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 707,  Mean reward: -2.75, Mean Entropy: 0.004289290867745876, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 708,  Mean reward: -2.0, Mean Entropy: 0.0027516509871929884, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 709,  Mean reward: -1.75, Mean Entropy: 0.0020241625607013702, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 710,  Mean reward: -3.0, Mean Entropy: 0.005143235437572002, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 711,  Mean reward: -3.0, Mean Entropy: 0.007506559602916241, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 712,  Mean reward: -2.5, Mean Entropy: 0.0036833230406045914, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 713,  Mean reward: -2.0, Mean Entropy: 0.0024789031594991684, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 714,  Mean reward: -2.25, Mean Entropy: 0.003970886580646038, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 715,  Mean reward: -1.75, Mean Entropy: 0.005343140568584204, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 716,  Mean reward: -2.5, Mean Entropy: 0.0042753806337714195, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 717,  Mean reward: -3.5, Mean Entropy: 0.0060097407549619675, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 718,  Mean reward: -1.1329113924050633, Mean Entropy: 0.003324424847960472, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 719,  Mean reward: -2.5, Mean Entropy: 0.001960786059498787, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 720,  Mean reward: -3.0, Mean Entropy: 0.0021762342657893896, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 721,  Mean reward: -1.0, Mean Entropy: 0.0008199067669920623, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 722,  Mean reward: -1.25, Mean Entropy: 0.0003225585678592324, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 723,  Mean reward: 0.5, Mean Entropy: 0.000619641796220094, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 724,  Mean reward: -2.25, Mean Entropy: 0.001137523795478046, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.78s
Iteration: 725,  Mean reward: -1.0, Mean Entropy: 0.007305902894586325, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 726,  Mean reward: -2.0, Mean Entropy: 0.03889629244804382, complete_episode_count: 80.0, Gather time: 0.68s, Train time: 0.72s
Iteration: 727,  Mean reward: -3.6645569620253164, Mean Entropy: 0.017451636493206024, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 728,  Mean reward: -1.0, Mean Entropy: 0.0070877717807888985, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 729,  Mean reward: -1.1329113924050633, Mean Entropy: 0.0033938251435756683, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 730,  Mean reward: -1.0, Mean Entropy: 0.002471587620675564, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 731,  Mean reward: -1.75, Mean Entropy: 0.0017228926299139857, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 732,  Mean reward: -0.75, Mean Entropy: 0.005014899652451277, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 733,  Mean reward: -1.5, Mean Entropy: 0.0035743163898587227, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 734,  Mean reward: -4.5, Mean Entropy: 0.02715815231204033, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 735,  Mean reward: -1.5, Mean Entropy: 0.004048015922307968, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 736,  Mean reward: -1.0, Mean Entropy: 0.0050879959017038345, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 737,  Mean reward: -2.5, Mean Entropy: 0.01744748279452324, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 738,  Mean reward: -2.0, Mean Entropy: 0.00981202907860279, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 739,  Mean reward: -2.0, Mean Entropy: 0.008201884105801582, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 740,  Mean reward: -3.5, Mean Entropy: 0.009284411557018757, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 741,  Mean reward: -1.5, Mean Entropy: 0.006935163401067257, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.77s
Iteration: 742,  Mean reward: -0.25, Mean Entropy: 0.004218487534672022, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 743,  Mean reward: -2.5, Mean Entropy: 0.005872506648302078, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 744,  Mean reward: -1.8924050632911393, Mean Entropy: 0.023338526487350464, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 745,  Mean reward: 1.25, Mean Entropy: 0.0038363307248800993, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 746,  Mean reward: -1.0, Mean Entropy: 0.0018517873249948025, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 747,  Mean reward: -2.0, Mean Entropy: 0.001860703807324171, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 748,  Mean reward: -1.5, Mean Entropy: 0.00305168773047626, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 749,  Mean reward: -2.75, Mean Entropy: 0.0025663613341748714, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 750,  Mean reward: -0.75, Mean Entropy: 0.0017368565313518047, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 751,  Mean reward: 0.0, Mean Entropy: 0.0017877895152196288, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 752,  Mean reward: -1.25, Mean Entropy: 0.0015218786429613829, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 753,  Mean reward: -0.75, Mean Entropy: 0.0022229696623981, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 754,  Mean reward: -2.0, Mean Entropy: 0.0020102497655898333, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 755,  Mean reward: -1.25, Mean Entropy: 0.0031902785412967205, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 756,  Mean reward: -1.25, Mean Entropy: 0.02009604312479496, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 757,  Mean reward: -2.25, Mean Entropy: 0.03159221634268761, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 758,  Mean reward: -3.25, Mean Entropy: 0.006198449991643429, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 759,  Mean reward: -3.5, Mean Entropy: 0.003506449516862631, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 760,  Mean reward: -1.5, Mean Entropy: 0.002909381641075015, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 761,  Mean reward: -0.25, Mean Entropy: 0.0023815638851374388, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 762,  Mean reward: -2.0, Mean Entropy: 0.008770270273089409, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 763,  Mean reward: -2.0, Mean Entropy: 0.006918009370565414, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 764,  Mean reward: -1.75, Mean Entropy: 0.007148614153265953, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 765,  Mean reward: -0.25, Mean Entropy: 0.004965797532349825, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 766,  Mean reward: -0.75, Mean Entropy: 0.0020444486290216446, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 767,  Mean reward: -0.5, Mean Entropy: 0.0017388764536008239, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 768,  Mean reward: -2.75, Mean Entropy: 0.0007080297800712287, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 769,  Mean reward: 0.0, Mean Entropy: 0.001146640395745635, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 770,  Mean reward: -2.75, Mean Entropy: 0.0075923725962638855, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 771,  Mean reward: -1.0, Mean Entropy: 0.0045335376635193825, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 772,  Mean reward: -0.5, Mean Entropy: 0.003928317688405514, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 773,  Mean reward: -2.5, Mean Entropy: 0.00685781380161643, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.80s
Iteration: 774,  Mean reward: -1.25, Mean Entropy: 0.005657130852341652, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 775,  Mean reward: -0.75, Mean Entropy: 0.005394315347075462, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 776,  Mean reward: -3.5, Mean Entropy: 0.011392561718821526, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 777,  Mean reward: -1.25, Mean Entropy: 0.013305450789630413, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 778,  Mean reward: -1.75, Mean Entropy: 0.010748540982604027, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 779,  Mean reward: -2.25, Mean Entropy: 0.01084909401834011, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 780,  Mean reward: -2.632911392405063, Mean Entropy: 0.009387937374413013, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 781,  Mean reward: -0.7564102564102564, Mean Entropy: 0.008296916261315346, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 782,  Mean reward: -2.5, Mean Entropy: 0.006754159927368164, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 783,  Mean reward: -3.5, Mean Entropy: 0.006889799144119024, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 784,  Mean reward: -2.25, Mean Entropy: 0.00890296045690775, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 785,  Mean reward: -2.25, Mean Entropy: 0.005865042097866535, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 786,  Mean reward: -1.75, Mean Entropy: 0.004080931190401316, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 787,  Mean reward: -3.0, Mean Entropy: 0.004368422087281942, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 788,  Mean reward: -3.25, Mean Entropy: 0.005289576482027769, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 789,  Mean reward: -3.0, Mean Entropy: 0.00632947264239192, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.76s
Iteration: 790,  Mean reward: -2.75, Mean Entropy: 0.004497296176850796, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 791,  Mean reward: -2.5, Mean Entropy: 0.004606256261467934, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 792,  Mean reward: -2.5, Mean Entropy: 0.003619491122663021, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 793,  Mean reward: -2.3987341772151898, Mean Entropy: 0.006738664116710424, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 794,  Mean reward: -0.75, Mean Entropy: 0.007551164831966162, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 795,  Mean reward: -1.25, Mean Entropy: 0.005381081253290176, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 796,  Mean reward: -1.25, Mean Entropy: 0.006285229232162237, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 797,  Mean reward: -2.651898734177215, Mean Entropy: 0.005345018580555916, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 798,  Mean reward: -1.5, Mean Entropy: 0.003017003647983074, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 799,  Mean reward: -3.25, Mean Entropy: 0.003741365624591708, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 800,  Mean reward: -1.5, Mean Entropy: 0.0032728994265198708, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -3.25, Mean Entropy: 0.005589778535068035, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 802,  Mean reward: -1.25, Mean Entropy: 0.007565548177808523, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 803,  Mean reward: -3.25, Mean Entropy: 0.00521945022046566, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 804,  Mean reward: -1.5, Mean Entropy: 0.003119350876659155, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 805,  Mean reward: -1.25, Mean Entropy: 0.00571620836853981, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 806,  Mean reward: -2.25, Mean Entropy: 0.007010688539594412, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 807,  Mean reward: -1.25, Mean Entropy: 0.006696109659969807, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 808,  Mean reward: -0.75, Mean Entropy: 0.006412782706320286, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 809,  Mean reward: -1.0, Mean Entropy: 0.004447707906365395, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 810,  Mean reward: -0.5, Mean Entropy: 0.004085049964487553, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 811,  Mean reward: -1.25, Mean Entropy: 0.004678219091147184, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 812,  Mean reward: -1.75, Mean Entropy: 0.005177844315767288, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 813,  Mean reward: -0.75, Mean Entropy: 0.0056366948410868645, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 814,  Mean reward: -1.5, Mean Entropy: 0.003968900069594383, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 815,  Mean reward: -2.5, Mean Entropy: 0.004700076300650835, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 816,  Mean reward: -1.75, Mean Entropy: 0.003687718417495489, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 817,  Mean reward: -2.75, Mean Entropy: 0.002701960504055023, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 818,  Mean reward: -2.25, Mean Entropy: 0.0016936089377850294, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 819,  Mean reward: -0.75, Mean Entropy: 0.0013081543147563934, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 820,  Mean reward: -2.75, Mean Entropy: 0.002758555579930544, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 821,  Mean reward: -3.0, Mean Entropy: 0.004548181779682636, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 822,  Mean reward: -1.25, Mean Entropy: 0.006875996943563223, complete_episode_count: 80.0, Gather time: 0.72s, Train time: 0.76s
Iteration: 823,  Mean reward: -1.5, Mean Entropy: 0.004359752871096134, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 824,  Mean reward: -2.5, Mean Entropy: 0.0035533837508410215, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 825,  Mean reward: -2.5, Mean Entropy: 0.005045763216912746, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 826,  Mean reward: -2.25, Mean Entropy: 0.0037846965715289116, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 827,  Mean reward: -1.0, Mean Entropy: 0.0027742735110223293, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 828,  Mean reward: -1.0, Mean Entropy: 0.004713085014373064, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 829,  Mean reward: -0.5, Mean Entropy: 0.005561425816267729, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 830,  Mean reward: -1.0, Mean Entropy: 0.005121389403939247, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 831,  Mean reward: -1.75, Mean Entropy: 0.0051696281880140305, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 832,  Mean reward: -1.25, Mean Entropy: 0.007845999673008919, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 833,  Mean reward: -4.75, Mean Entropy: 0.004242187365889549, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 834,  Mean reward: -3.25, Mean Entropy: 0.0065933093428611755, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 835,  Mean reward: -3.0, Mean Entropy: 0.012906862422823906, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 836,  Mean reward: -1.5, Mean Entropy: 0.016350029036402702, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 837,  Mean reward: -1.25, Mean Entropy: 0.007831471972167492, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 838,  Mean reward: -0.25, Mean Entropy: 0.010687468573451042, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 839,  Mean reward: -2.651898734177215, Mean Entropy: 0.011327876709401608, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 840,  Mean reward: -1.25, Mean Entropy: 0.012880705296993256, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 841,  Mean reward: -2.0, Mean Entropy: 0.006790977902710438, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 842,  Mean reward: -3.25, Mean Entropy: 0.00948295183479786, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 843,  Mean reward: -2.75, Mean Entropy: 0.008469272404909134, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 844,  Mean reward: -4.0, Mean Entropy: 0.010516537353396416, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 845,  Mean reward: -1.1329113924050633, Mean Entropy: 0.007797665894031525, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 846,  Mean reward: -1.5, Mean Entropy: 0.004824778996407986, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 847,  Mean reward: -4.0, Mean Entropy: 0.01403026282787323, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 848,  Mean reward: -2.0, Mean Entropy: 0.007820786908268929, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 849,  Mean reward: -2.5, Mean Entropy: 0.0161339920014143, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 850,  Mean reward: -2.25, Mean Entropy: 0.017061371356248856, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 851,  Mean reward: -3.25, Mean Entropy: 0.0139401163905859, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 852,  Mean reward: -3.0, Mean Entropy: 0.013149181380867958, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 853,  Mean reward: -3.25, Mean Entropy: 0.014154225587844849, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 854,  Mean reward: 0.2692307692307692, Mean Entropy: 0.014584447257220745, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 855,  Mean reward: -1.1329113924050633, Mean Entropy: 0.01150876097381115, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 856,  Mean reward: -2.25, Mean Entropy: 0.0030831354670226574, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 857,  Mean reward: -2.5, Mean Entropy: 0.008905290625989437, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 858,  Mean reward: -0.37341772151898733, Mean Entropy: 0.01834876462817192, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 859,  Mean reward: -3.411392405063291, Mean Entropy: 0.0058304984122514725, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.80s
Iteration: 860,  Mean reward: -1.75, Mean Entropy: 0.008203002624213696, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 861,  Mean reward: -3.0, Mean Entropy: 0.006180086638778448, complete_episode_count: 80.0, Gather time: 0.75s, Train time: 0.73s
Iteration: 862,  Mean reward: -2.5, Mean Entropy: 0.005107087083160877, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.79s
Iteration: 863,  Mean reward: -3.0, Mean Entropy: 0.006697069853544235, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 864,  Mean reward: -2.3987341772151898, Mean Entropy: 0.007639673538506031, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 865,  Mean reward: -2.0, Mean Entropy: 0.007261741906404495, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 866,  Mean reward: -2.0, Mean Entropy: 0.006792150437831879, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 867,  Mean reward: -3.0, Mean Entropy: 0.005446608178317547, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 868,  Mean reward: -2.5, Mean Entropy: 0.0058821155689656734, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 869,  Mean reward: -1.25, Mean Entropy: 0.006104395724833012, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 870,  Mean reward: -2.5, Mean Entropy: 0.005252675618976355, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 871,  Mean reward: -3.5, Mean Entropy: 0.01191069558262825, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 872,  Mean reward: -1.75, Mean Entropy: 0.009930258616805077, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 873,  Mean reward: -3.25, Mean Entropy: 0.007214011158794165, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 874,  Mean reward: -1.0, Mean Entropy: 0.007965427823364735, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 875,  Mean reward: -4.443037974683544, Mean Entropy: 0.005813829600811005, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 876,  Mean reward: -3.75, Mean Entropy: 0.009323852136731148, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 877,  Mean reward: -4.5, Mean Entropy: 0.004848054610192776, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 878,  Mean reward: -2.75, Mean Entropy: 0.00890682265162468, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 879,  Mean reward: -2.25, Mean Entropy: 0.005386893637478352, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.75s
Iteration: 880,  Mean reward: -2.25, Mean Entropy: 0.005349274724721909, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 881,  Mean reward: -3.25, Mean Entropy: 0.005293739028275013, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 882,  Mean reward: -3.5, Mean Entropy: 0.0057387943379580975, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 883,  Mean reward: -2.1455696202531644, Mean Entropy: 0.005227784626185894, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 884,  Mean reward: -2.0, Mean Entropy: 0.002846234943717718, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 885,  Mean reward: -2.1455696202531644, Mean Entropy: 0.004047234542667866, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 886,  Mean reward: -2.25, Mean Entropy: 0.003247972112149, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 887,  Mean reward: 1.0, Mean Entropy: 0.0034563615918159485, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 888,  Mean reward: -3.25, Mean Entropy: 0.00216928543522954, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 889,  Mean reward: -2.5, Mean Entropy: 0.003518868237733841, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 890,  Mean reward: -1.5, Mean Entropy: 0.004775797016918659, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 891,  Mean reward: -2.651898734177215, Mean Entropy: 0.006407462060451508, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 892,  Mean reward: -2.3987341772151898, Mean Entropy: 0.0074266958981752396, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 893,  Mean reward: -1.5, Mean Entropy: 0.0046116081066429615, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 894,  Mean reward: -3.75, Mean Entropy: 0.0026527284644544125, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 895,  Mean reward: -0.25, Mean Entropy: 0.004634553100913763, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 896,  Mean reward: -3.0, Mean Entropy: 0.005103450734168291, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 897,  Mean reward: -2.0, Mean Entropy: 0.005647639743983746, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 898,  Mean reward: -3.75, Mean Entropy: 0.00478788185864687, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 899,  Mean reward: -2.25, Mean Entropy: 0.005226563662290573, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 900,  Mean reward: -1.5, Mean Entropy: 0.004017408937215805, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -2.0, Mean Entropy: 0.0028952723369002342, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 902,  Mean reward: -0.25, Mean Entropy: 0.00420968234539032, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 903,  Mean reward: -1.0, Mean Entropy: 0.00329332472756505, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 904,  Mean reward: -2.5, Mean Entropy: 0.001922272378578782, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 905,  Mean reward: -4.0, Mean Entropy: 0.001545187085866928, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 906,  Mean reward: -1.75, Mean Entropy: 0.0057591283693909645, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 907,  Mean reward: -2.25, Mean Entropy: 0.004907389171421528, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 908,  Mean reward: -2.5, Mean Entropy: 0.0030684350058436394, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 909,  Mean reward: -3.25, Mean Entropy: 0.0021743662655353546, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 910,  Mean reward: -2.75, Mean Entropy: 0.0032728181686252356, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 911,  Mean reward: -2.25, Mean Entropy: 0.0035160700790584087, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 912,  Mean reward: -2.5, Mean Entropy: 0.003104237373918295, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 913,  Mean reward: -2.75, Mean Entropy: 0.0012215862516313791, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 914,  Mean reward: -1.25, Mean Entropy: 0.004288146272301674, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 915,  Mean reward: -3.0, Mean Entropy: 0.004301601089537144, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 916,  Mean reward: -3.5, Mean Entropy: 0.0009166877716779709, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 917,  Mean reward: -0.25, Mean Entropy: 0.006600619293749332, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 918,  Mean reward: -2.25, Mean Entropy: 0.007403900846838951, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 919,  Mean reward: -3.411392405063291, Mean Entropy: 0.0037114559672772884, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 920,  Mean reward: -3.0, Mean Entropy: 0.0038732110988348722, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 921,  Mean reward: -0.25, Mean Entropy: 0.006293000187724829, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 922,  Mean reward: -1.3860759493670887, Mean Entropy: 0.00769939785823226, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 923,  Mean reward: -3.25, Mean Entropy: 0.05527007579803467, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 924,  Mean reward: 0.13291139240506328, Mean Entropy: 0.006022744812071323, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 925,  Mean reward: -0.879746835443038, Mean Entropy: 0.008180197328329086, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 926,  Mean reward: -3.5, Mean Entropy: 0.004794464912265539, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 927,  Mean reward: -2.0, Mean Entropy: 0.006306617986410856, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 928,  Mean reward: -1.75, Mean Entropy: 0.008465444669127464, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 929,  Mean reward: 0.75, Mean Entropy: 0.010446618311107159, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 930,  Mean reward: -1.0, Mean Entropy: 0.004755921196192503, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 931,  Mean reward: -3.0, Mean Entropy: 0.022397853434085846, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 932,  Mean reward: -3.0, Mean Entropy: 0.0021090698428452015, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 933,  Mean reward: -3.0, Mean Entropy: 0.0047418284229934216, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 934,  Mean reward: -2.25, Mean Entropy: 0.008349137380719185, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 935,  Mean reward: -2.25, Mean Entropy: 0.00889962911605835, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 936,  Mean reward: -2.0, Mean Entropy: 0.00960095226764679, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 937,  Mean reward: -4.0, Mean Entropy: 0.009792457334697247, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 938,  Mean reward: -1.5, Mean Entropy: 0.01067127101123333, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 939,  Mean reward: -1.0, Mean Entropy: 0.014635106548666954, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 940,  Mean reward: -1.1139240506329113, Mean Entropy: 0.009286870248615742, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 941,  Mean reward: -1.75, Mean Entropy: 0.006911349482834339, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 942,  Mean reward: -3.0, Mean Entropy: 0.003485636319965124, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 943,  Mean reward: -1.0, Mean Entropy: 0.005593661218881607, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 944,  Mean reward: -2.75, Mean Entropy: 0.00797581672668457, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 945,  Mean reward: -2.0384615384615383, Mean Entropy: 0.010197319090366364, complete_episode_count: 78.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 946,  Mean reward: -1.5, Mean Entropy: 0.008776921778917313, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 947,  Mean reward: -2.75, Mean Entropy: 0.007642218377441168, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 948,  Mean reward: -1.75, Mean Entropy: 0.007866118103265762, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 949,  Mean reward: -0.25, Mean Entropy: 0.007895378395915031, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 950,  Mean reward: -2.75, Mean Entropy: 0.00679387990385294, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 951,  Mean reward: -3.25, Mean Entropy: 0.004223702475428581, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 952,  Mean reward: -3.0, Mean Entropy: 0.0039181821048259735, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 953,  Mean reward: -2.0, Mean Entropy: 0.005806039087474346, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 954,  Mean reward: -3.0, Mean Entropy: 0.004309630952775478, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 955,  Mean reward: -1.0, Mean Entropy: 0.005042196251451969, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 956,  Mean reward: -1.75, Mean Entropy: 0.005902188830077648, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 957,  Mean reward: -3.5, Mean Entropy: 0.0029746515210717916, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 958,  Mean reward: -3.25, Mean Entropy: 0.0033070999197661877, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 959,  Mean reward: -1.75, Mean Entropy: 0.003220721147954464, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 960,  Mean reward: -1.5, Mean Entropy: 0.002915731631219387, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 961,  Mean reward: -4.5, Mean Entropy: 0.0013606862630695105, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 962,  Mean reward: -0.75, Mean Entropy: 0.0026499598752707243, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 963,  Mean reward: -0.5, Mean Entropy: 0.004212677478790283, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 964,  Mean reward: -1.5, Mean Entropy: 0.0031235895585268736, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 965,  Mean reward: 0.13291139240506328, Mean Entropy: 0.004367274697870016, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 966,  Mean reward: -2.0, Mean Entropy: 0.00265494454652071, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 967,  Mean reward: -3.75, Mean Entropy: 0.00019882011110894382, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 968,  Mean reward: -1.0, Mean Entropy: 0.0018925529439002275, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 969,  Mean reward: -1.0, Mean Entropy: 0.0032364986836910248, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 970,  Mean reward: -2.0, Mean Entropy: 0.0025513023138046265, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 971,  Mean reward: -2.1455696202531644, Mean Entropy: 0.0015366384759545326, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 972,  Mean reward: -4.0, Mean Entropy: 0.0010804430348798633, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 973,  Mean reward: -4.0, Mean Entropy: 0.0009020182187668979, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 974,  Mean reward: -0.5, Mean Entropy: 0.001817311393097043, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 975,  Mean reward: -2.9050632911392404, Mean Entropy: 0.0012801006669178605, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 976,  Mean reward: -2.25, Mean Entropy: 0.0012773070484399796, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 977,  Mean reward: -1.5, Mean Entropy: 0.0017120509874075651, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 978,  Mean reward: -3.5, Mean Entropy: 0.0012397418031468987, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 979,  Mean reward: -3.25, Mean Entropy: 0.0009721657261252403, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 980,  Mean reward: -1.25, Mean Entropy: 0.0018542142352089286, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 981,  Mean reward: -0.5, Mean Entropy: 0.00344479875639081, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 982,  Mean reward: -1.25, Mean Entropy: 0.0014009055448696017, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 983,  Mean reward: -2.25, Mean Entropy: 0.0008513634093105793, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 984,  Mean reward: -2.75, Mean Entropy: 0.00016520409553777426, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 985,  Mean reward: -2.25, Mean Entropy: 0.0009329614113084972, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 986,  Mean reward: -0.5, Mean Entropy: 0.0012529311934486032, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.82s
Iteration: 987,  Mean reward: -2.25, Mean Entropy: 0.0010046502575278282, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 988,  Mean reward: -1.25, Mean Entropy: 0.000930332113057375, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 989,  Mean reward: -2.5, Mean Entropy: 0.0031155087053775787, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 990,  Mean reward: -2.25, Mean Entropy: 0.0011130725033581257, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 991,  Mean reward: -2.0, Mean Entropy: 0.0006265711272135377, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 992,  Mean reward: -1.25, Mean Entropy: 0.0007246403838507831, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 993,  Mean reward: -4.25, Mean Entropy: 0.0002983084414154291, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 994,  Mean reward: -1.5, Mean Entropy: 0.0008169411448761821, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 995,  Mean reward: -3.5, Mean Entropy: 0.0007720842259004712, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 996,  Mean reward: -2.0, Mean Entropy: 0.0012799528194591403, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 997,  Mean reward: -1.75, Mean Entropy: 0.0013446612283587456, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 998,  Mean reward: -2.75, Mean Entropy: 0.0016710496274754405, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 999,  Mean reward: -2.5, Mean Entropy: 0.0016344356117770076, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1000,  Mean reward: -1.75, Mean Entropy: 0.0016322605079039931, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: -1.5, Mean Entropy: 0.0013402638724073768, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1002,  Mean reward: -2.75, Mean Entropy: 0.001281156437471509, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1003,  Mean reward: -2.75, Mean Entropy: 0.001592079410329461, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1004,  Mean reward: -4.0, Mean Entropy: 0.0006874228711239994, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1005,  Mean reward: -2.5, Mean Entropy: 0.0012245146790519357, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1006,  Mean reward: 0.0, Mean Entropy: 0.001822782913222909, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1007,  Mean reward: -2.75, Mean Entropy: 0.0012070511002093554, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1008,  Mean reward: -1.5, Mean Entropy: 0.0005672037368640304, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1009,  Mean reward: -3.0, Mean Entropy: 0.0009125623619183898, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1010,  Mean reward: -2.75, Mean Entropy: 0.0009489373187534511, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.73s
Iteration: 1011,  Mean reward: 0.0, Mean Entropy: 0.0016605069395154715, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1012,  Mean reward: -0.75, Mean Entropy: 0.0020617530681192875, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 1013,  Mean reward: -0.5, Mean Entropy: 0.0011799264466390014, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1014,  Mean reward: -1.75, Mean Entropy: 0.0012657608604058623, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1015,  Mean reward: -2.25, Mean Entropy: 0.000855941092595458, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1016,  Mean reward: -2.25, Mean Entropy: 0.0010589610319584608, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1017,  Mean reward: -3.0, Mean Entropy: 0.0009818411199375987, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1018,  Mean reward: -2.5, Mean Entropy: 0.001309949904680252, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1019,  Mean reward: -1.75, Mean Entropy: 0.0017412251327186823, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1020,  Mean reward: 0.0, Mean Entropy: 0.002567622810602188, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1021,  Mean reward: -1.5, Mean Entropy: 0.001348232850432396, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1022,  Mean reward: -1.0, Mean Entropy: 0.001140143838711083, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1023,  Mean reward: -2.75, Mean Entropy: 0.00019625439017545432, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1024,  Mean reward: -0.75, Mean Entropy: 0.0010559726506471634, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1025,  Mean reward: -1.5, Mean Entropy: 0.0020880866795778275, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1026,  Mean reward: -0.879746835443038, Mean Entropy: 0.001471286523155868, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1027,  Mean reward: -1.25, Mean Entropy: 0.0012495402479544282, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1028,  Mean reward: -3.25, Mean Entropy: 0.0009848927147686481, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1029,  Mean reward: -2.0, Mean Entropy: 0.00130052852910012, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1030,  Mean reward: -3.25, Mean Entropy: 0.001623004674911499, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1031,  Mean reward: -4.0, Mean Entropy: 0.0010528583079576492, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1032,  Mean reward: 0.75, Mean Entropy: 0.00589333102107048, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1033,  Mean reward: -2.3987341772151898, Mean Entropy: 0.007803012616932392, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1034,  Mean reward: -2.0, Mean Entropy: 0.003819068893790245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1035,  Mean reward: -0.75, Mean Entropy: 0.005672249011695385, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1036,  Mean reward: -3.5, Mean Entropy: 0.0038439463824033737, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1037,  Mean reward: -1.5, Mean Entropy: 0.0038729007355868816, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1038,  Mean reward: -2.75, Mean Entropy: 0.003142495406791568, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1039,  Mean reward: -2.25, Mean Entropy: 0.0038026224356144667, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1040,  Mean reward: -1.0, Mean Entropy: 0.006450214888900518, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1041,  Mean reward: -0.75, Mean Entropy: 0.004251704551279545, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1042,  Mean reward: -2.5, Mean Entropy: 0.0010646814480423927, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1043,  Mean reward: -1.75, Mean Entropy: 0.0025315266102552414, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1044,  Mean reward: -1.75, Mean Entropy: 0.003876256523653865, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1045,  Mean reward: -1.25, Mean Entropy: 0.004015328362584114, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1046,  Mean reward: -3.5, Mean Entropy: 0.0016542410012334585, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1047,  Mean reward: -0.5, Mean Entropy: 0.0037457856815308332, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1048,  Mean reward: -1.75, Mean Entropy: 0.005129803903400898, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1049,  Mean reward: -0.5, Mean Entropy: 0.00726456381380558, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1050,  Mean reward: -2.25, Mean Entropy: 0.004869045224040747, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 1051,  Mean reward: -1.0, Mean Entropy: 0.003964615520089865, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1052,  Mean reward: -2.0, Mean Entropy: 0.002787088742479682, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1053,  Mean reward: -2.0, Mean Entropy: 0.002109975554049015, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1054,  Mean reward: -1.5, Mean Entropy: 0.0022182574030011892, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1055,  Mean reward: -2.5, Mean Entropy: 0.0022314172238111496, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1056,  Mean reward: -0.5, Mean Entropy: 0.004124846775084734, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.76s
Iteration: 1057,  Mean reward: -1.639240506329114, Mean Entropy: 0.00646212138235569, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1058,  Mean reward: -1.5, Mean Entropy: 0.00522434338927269, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1059,  Mean reward: -2.5, Mean Entropy: 0.00438547320663929, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1060,  Mean reward: -2.5, Mean Entropy: 0.0055216774344444275, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1061,  Mean reward: -1.25, Mean Entropy: 0.0067976913414895535, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1062,  Mean reward: -3.25, Mean Entropy: 0.0037928640376776457, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1063,  Mean reward: -2.5, Mean Entropy: 0.0033839400857686996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1064,  Mean reward: -1.75, Mean Entropy: 0.004508942365646362, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1065,  Mean reward: -4.0, Mean Entropy: 0.002833339385688305, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1066,  Mean reward: 0.0, Mean Entropy: 0.003819994628429413, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1067,  Mean reward: -2.0, Mean Entropy: 0.003180336905643344, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1068,  Mean reward: 0.75, Mean Entropy: 0.0030947551131248474, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1069,  Mean reward: -2.5, Mean Entropy: 0.002443866804242134, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1070,  Mean reward: -0.5, Mean Entropy: 0.0022342165466398, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1071,  Mean reward: 0.25, Mean Entropy: 0.0030403134878724813, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1072,  Mean reward: -3.0, Mean Entropy: 0.0026061534881591797, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1073,  Mean reward: -1.5, Mean Entropy: 0.0010766666382551193, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1074,  Mean reward: -0.75, Mean Entropy: 0.003247631713747978, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1075,  Mean reward: -1.75, Mean Entropy: 0.00251584779471159, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1076,  Mean reward: -0.5, Mean Entropy: 0.0036093266680836678, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1077,  Mean reward: -1.75, Mean Entropy: 0.0027170181274414062, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1078,  Mean reward: -3.0, Mean Entropy: 3.1965315429260954e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1079,  Mean reward: -2.75, Mean Entropy: 0.0038282210007309914, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1080,  Mean reward: -2.5, Mean Entropy: 0.006662530358880758, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1081,  Mean reward: -2.0, Mean Entropy: 0.008816712535917759, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1082,  Mean reward: -0.5, Mean Entropy: 0.00816771574318409, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1083,  Mean reward: -2.0, Mean Entropy: 0.0050709424540400505, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1084,  Mean reward: -2.0, Mean Entropy: 0.002907202346250415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 1085,  Mean reward: -2.0, Mean Entropy: 0.004349599592387676, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 1086,  Mean reward: -1.0, Mean Entropy: 0.005773920565843582, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1087,  Mean reward: -3.0, Mean Entropy: 0.0037932812701910734, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1088,  Mean reward: 0.13291139240506328, Mean Entropy: 0.0065803565084934235, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 1089,  Mean reward: -3.5, Mean Entropy: 0.003299292176961899, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1090,  Mean reward: -3.75, Mean Entropy: 0.0024088257923722267, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1091,  Mean reward: -2.5, Mean Entropy: 0.003301546908915043, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1092,  Mean reward: -4.5, Mean Entropy: 0.002539644483476877, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1093,  Mean reward: -0.5, Mean Entropy: 0.0037214725743979216, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1094,  Mean reward: -2.0, Mean Entropy: 0.0031951989512890577, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1095,  Mean reward: -0.37341772151898733, Mean Entropy: 0.005713192280381918, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1096,  Mean reward: -1.75, Mean Entropy: 0.003080076305195689, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1097,  Mean reward: -1.0, Mean Entropy: 0.0022971048019826412, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1098,  Mean reward: -2.5, Mean Entropy: 0.0009505011839792132, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1099,  Mean reward: -2.5, Mean Entropy: 0.0029182271100580692, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1100,  Mean reward: -1.25, Mean Entropy: 0.0035726958885788918, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: -1.75, Mean Entropy: 0.004121634177863598, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1102,  Mean reward: -1.75, Mean Entropy: 0.003137382445856929, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1103,  Mean reward: -3.75, Mean Entropy: 0.0019131982699036598, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.74s
Iteration: 1104,  Mean reward: -3.25, Mean Entropy: 0.002452028449624777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1105,  Mean reward: -3.75, Mean Entropy: 0.003816889598965645, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1106,  Mean reward: -3.25, Mean Entropy: 0.0039434609934687614, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1107,  Mean reward: -0.879746835443038, Mean Entropy: 0.006700764410197735, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1108,  Mean reward: -2.1455696202531644, Mean Entropy: 0.004260072484612465, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1109,  Mean reward: -3.0, Mean Entropy: 0.002694261260330677, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1110,  Mean reward: -1.0, Mean Entropy: 0.0031163040548563004, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1111,  Mean reward: -1.5, Mean Entropy: 0.003882140852510929, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.74s
Iteration: 1112,  Mean reward: -1.75, Mean Entropy: 0.003204895183444023, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1113,  Mean reward: -1.25, Mean Entropy: 0.0041632382199168205, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1114,  Mean reward: -1.75, Mean Entropy: 0.0034672203473746777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1115,  Mean reward: -2.25, Mean Entropy: 0.002846688497811556, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1116,  Mean reward: -1.25, Mean Entropy: 0.0033990086521953344, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1117,  Mean reward: -3.25, Mean Entropy: 0.0027110069058835506, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1118,  Mean reward: -3.75, Mean Entropy: 0.002475402783602476, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1119,  Mean reward: -3.25, Mean Entropy: 0.0034610561560839415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1120,  Mean reward: -0.25, Mean Entropy: 0.004789265803992748, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1121,  Mean reward: -2.75, Mean Entropy: 0.004834449850022793, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1122,  Mean reward: -2.25, Mean Entropy: 0.003132469719275832, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 1123,  Mean reward: -4.75, Mean Entropy: 0.0018158862367272377, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1124,  Mean reward: -4.0, Mean Entropy: 0.0023681786842644215, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1125,  Mean reward: -1.0, Mean Entropy: 0.007505377288907766, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1126,  Mean reward: -0.5, Mean Entropy: 0.006641842424869537, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 1127,  Mean reward: -3.0, Mean Entropy: 0.002208305522799492, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1128,  Mean reward: -2.75, Mean Entropy: 0.002242804504930973, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 1129,  Mean reward: -1.25, Mean Entropy: 0.0032457206398248672, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1130,  Mean reward: -1.8924050632911393, Mean Entropy: 0.004089437425136566, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1131,  Mean reward: -2.0, Mean Entropy: 0.0024932322558015585, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1132,  Mean reward: -2.5, Mean Entropy: 0.0031506463419646025, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1133,  Mean reward: -0.75, Mean Entropy: 0.0038175745867192745, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1134,  Mean reward: -1.5, Mean Entropy: 0.003913436084985733, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1135,  Mean reward: -1.5, Mean Entropy: 0.002964492654427886, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1136,  Mean reward: -3.0, Mean Entropy: 0.0026626447215676308, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1137,  Mean reward: -1.75, Mean Entropy: 0.0027038492262363434, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1138,  Mean reward: -2.5, Mean Entropy: 0.0028109285049140453, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1139,  Mean reward: -0.5, Mean Entropy: 0.005078761838376522, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1140,  Mean reward: -1.5, Mean Entropy: 0.0034143421798944473, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1141,  Mean reward: -1.0, Mean Entropy: 0.002165159909054637, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.78s
Iteration: 1142,  Mean reward: -1.75, Mean Entropy: 0.0020590282510966063, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1143,  Mean reward: -0.5, Mean Entropy: 0.00219223415479064, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1144,  Mean reward: -1.5, Mean Entropy: 0.0028068237006664276, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 1145,  Mean reward: -3.5, Mean Entropy: 0.0018803765997290611, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1146,  Mean reward: -2.5, Mean Entropy: 0.0025205123238265514, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1147,  Mean reward: -3.0, Mean Entropy: 0.003453540615737438, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1148,  Mean reward: -2.5, Mean Entropy: 0.003623108845204115, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1149,  Mean reward: -3.0, Mean Entropy: 0.0033913087099790573, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1150,  Mean reward: -2.5, Mean Entropy: 0.00394089799374342, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1151,  Mean reward: 0.5, Mean Entropy: 0.004804852418601513, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1152,  Mean reward: -0.879746835443038, Mean Entropy: 0.0045061251148581505, complete_episode_count: 79.0, Gather time: 0.69s, Train time: 0.70s
Iteration: 1153,  Mean reward: -2.25, Mean Entropy: 0.0014622677117586136, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1154,  Mean reward: -2.0, Mean Entropy: 0.0014244094491004944, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1155,  Mean reward: -1.75, Mean Entropy: 0.0024675584863871336, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1156,  Mean reward: 0.0, Mean Entropy: 0.0042056189849972725, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1157,  Mean reward: -2.25, Mean Entropy: 0.0031159725040197372, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.80s
Iteration: 1158,  Mean reward: -1.0, Mean Entropy: 0.0020847776904702187, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1159,  Mean reward: -1.75, Mean Entropy: 0.0023617451079189777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.74s
Iteration: 1160,  Mean reward: -3.5, Mean Entropy: 0.004308685660362244, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1161,  Mean reward: -1.5, Mean Entropy: 0.0035798780154436827, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1162,  Mean reward: -3.25, Mean Entropy: 0.002513410057872534, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1163,  Mean reward: -1.75, Mean Entropy: 0.0030441477429121733, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1164,  Mean reward: -1.25, Mean Entropy: 0.0034125978127121925, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 1165,  Mean reward: -4.0, Mean Entropy: 0.0028983294032514095, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1166,  Mean reward: -0.75, Mean Entropy: 0.004398712422698736, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1167,  Mean reward: -3.0, Mean Entropy: 0.002856557723134756, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1168,  Mean reward: -3.25, Mean Entropy: 0.002701869932934642, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1169,  Mean reward: -2.0, Mean Entropy: 0.003723763395100832, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1170,  Mean reward: 0.0, Mean Entropy: 0.003734589321538806, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1171,  Mean reward: -4.75, Mean Entropy: 0.001987077761441469, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.75s
Iteration: 1172,  Mean reward: -1.0, Mean Entropy: 0.0028169886209070683, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1173,  Mean reward: -1.5, Mean Entropy: 0.0020784027874469757, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.77s
Iteration: 1174,  Mean reward: -3.0, Mean Entropy: 0.0026427595876157284, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.76s
Iteration: 1175,  Mean reward: -1.75, Mean Entropy: 0.00307173072360456, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1176,  Mean reward: -3.75, Mean Entropy: 0.00279981829226017, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1177,  Mean reward: -2.0, Mean Entropy: 0.0036581314634531736, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1178,  Mean reward: -0.6265822784810127, Mean Entropy: 0.0042437342926859856, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1179,  Mean reward: -2.5, Mean Entropy: 0.003036707639694214, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1180,  Mean reward: -0.5, Mean Entropy: 0.0017845313996076584, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1181,  Mean reward: -2.25, Mean Entropy: 0.001987602561712265, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.78s
Iteration: 1182,  Mean reward: -2.75, Mean Entropy: 0.002388036111369729, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1183,  Mean reward: -1.75, Mean Entropy: 0.0026593978982418776, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1184,  Mean reward: -2.5, Mean Entropy: 0.0017734002321958542, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1185,  Mean reward: -2.0, Mean Entropy: 0.001510152593255043, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1186,  Mean reward: -1.75, Mean Entropy: 0.001956529449671507, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1187,  Mean reward: -1.75, Mean Entropy: 0.0018549322849139571, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1188,  Mean reward: -1.5, Mean Entropy: 0.0018773706397041678, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1189,  Mean reward: -2.25, Mean Entropy: 0.0016776202246546745, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1190,  Mean reward: -0.75, Mean Entropy: 0.002083355560898781, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1191,  Mean reward: -3.25, Mean Entropy: 0.0016425217036157846, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1192,  Mean reward: -2.5, Mean Entropy: 0.002258959226310253, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 1193,  Mean reward: -1.75, Mean Entropy: 0.0017836636397987604, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1194,  Mean reward: -3.5, Mean Entropy: 0.0015864865854382515, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1195,  Mean reward: -2.0, Mean Entropy: 0.0026209212373942137, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.75s
Iteration: 1196,  Mean reward: -2.0, Mean Entropy: 0.002310733776539564, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1197,  Mean reward: -4.25, Mean Entropy: 0.002355447504669428, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1198,  Mean reward: -0.5, Mean Entropy: 0.00221317820250988, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1199,  Mean reward: -0.5, Mean Entropy: 0.0022612009197473526, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1200,  Mean reward: -2.0, Mean Entropy: 0.0016826465725898743, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: -3.5, Mean Entropy: 8.48158379085362e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1202,  Mean reward: -1.5, Mean Entropy: 0.001620501629076898, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 1203,  Mean reward: -2.25, Mean Entropy: 0.002380542689934373, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1204,  Mean reward: -3.25, Mean Entropy: 0.002524664858356118, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1205,  Mean reward: -2.75, Mean Entropy: 0.0025025135837495327, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1206,  Mean reward: -0.5, Mean Entropy: 0.002655288903042674, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1207,  Mean reward: -2.0, Mean Entropy: 0.0026553075294941664, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1208,  Mean reward: -2.25, Mean Entropy: 0.0015290621668100357, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1209,  Mean reward: -2.5, Mean Entropy: 0.0013771008234471083, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1210,  Mean reward: -2.75, Mean Entropy: 0.001379111665301025, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1211,  Mean reward: -1.5, Mean Entropy: 0.0017044572159647942, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1212,  Mean reward: -4.0, Mean Entropy: 0.0006613497389480472, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1213,  Mean reward: -3.0, Mean Entropy: 0.0014132240321487188, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1214,  Mean reward: -1.25, Mean Entropy: 0.002560480497777462, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1215,  Mean reward: -2.0, Mean Entropy: 0.002953227609395981, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1216,  Mean reward: -2.0, Mean Entropy: 0.0014530196785926819, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1217,  Mean reward: -4.0, Mean Entropy: 0.00013880922051612288, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1218,  Mean reward: -2.5, Mean Entropy: 0.0020992406643927097, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1219,  Mean reward: -1.5, Mean Entropy: 0.0031057395972311497, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1220,  Mean reward: -0.75, Mean Entropy: 0.0021610516123473644, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1221,  Mean reward: -2.75, Mean Entropy: 0.0021592257544398308, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1222,  Mean reward: -1.0, Mean Entropy: 0.0017064264975488186, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1223,  Mean reward: -1.25, Mean Entropy: 0.00224918220192194, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1224,  Mean reward: -0.75, Mean Entropy: 0.0029727944638580084, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1225,  Mean reward: -2.25, Mean Entropy: 0.00257315207272768, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1226,  Mean reward: -1.5, Mean Entropy: 0.0017806298565119505, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1227,  Mean reward: -3.0, Mean Entropy: 0.0020925006829202175, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1228,  Mean reward: -3.0, Mean Entropy: 0.0021643126383423805, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1229,  Mean reward: -2.0, Mean Entropy: 0.0020421657245606184, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1230,  Mean reward: -3.5, Mean Entropy: 0.0015613747527822852, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1231,  Mean reward: -0.75, Mean Entropy: 0.0017495739739388227, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1232,  Mean reward: -3.75, Mean Entropy: 0.0010299005080014467, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1233,  Mean reward: -2.25, Mean Entropy: 0.0010554369073361158, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1234,  Mean reward: -1.75, Mean Entropy: 0.0010327608324587345, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1235,  Mean reward: -1.5, Mean Entropy: 0.001205067615956068, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1236,  Mean reward: -2.0, Mean Entropy: 0.0013114968314766884, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1237,  Mean reward: -3.9177215189873418, Mean Entropy: 0.0010829841485247016, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1238,  Mean reward: -2.0, Mean Entropy: 0.0017558946274220943, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1239,  Mean reward: -2.75, Mean Entropy: 0.0013313924428075552, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1240,  Mean reward: -3.25, Mean Entropy: 0.0011569495545700192, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 1241,  Mean reward: -3.0, Mean Entropy: 0.000917131663300097, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1242,  Mean reward: -2.5, Mean Entropy: 0.0009696536581031978, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1243,  Mean reward: -2.25, Mean Entropy: 0.0006777651142328978, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1244,  Mean reward: -3.0, Mean Entropy: 0.0008605727925896645, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1245,  Mean reward: -2.5, Mean Entropy: 0.0009427600307390094, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.559523809523809, Mean Entropy: 0.8953148722648621, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.36s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.309523809523809, Mean Entropy: 0.8736538887023926, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 2,  Mean reward: -5.5476190476190474, Mean Entropy: 1.0252724885940552, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 3,  Mean reward: -5.880952380952381, Mean Entropy: 0.9602603316307068, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 4,  Mean reward: -5.423076923076923, Mean Entropy: 0.909702718257904, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.38s
Iteration: 5,  Mean reward: -5.089743589743589, Mean Entropy: 0.9386321902275085, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.37s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 6,  Mean reward: -4.1375, Mean Entropy: 0.9458569288253784, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.39s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 7,  Mean reward: -3.048780487804878, Mean Entropy: 0.9097528457641602, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 8,  Mean reward: -3.0609756097560976, Mean Entropy: 0.9314001798629761, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 9,  Mean reward: -5.686046511627907, Mean Entropy: 0.9530584216117859, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 10,  Mean reward: -5.738095238095238, Mean Entropy: 0.8952929973602295, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 11,  Mean reward: -3.25, Mean Entropy: 0.9602760076522827, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.40s
Iteration: 12,  Mean reward: -4.069767441860465, Mean Entropy: 1.0180375576019287, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 13,  Mean reward: -5.378048780487805, Mean Entropy: 0.9891564249992371, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 14,  Mean reward: -5.5, Mean Entropy: 0.9313956499099731, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 15,  Mean reward: -3.960526315789474, Mean Entropy: 0.9963803291320801, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.39s
Iteration: 16,  Mean reward: -6.166666666666667, Mean Entropy: 0.9241799116134644, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.36s
Iteration: 17,  Mean reward: -3.922222222222222, Mean Entropy: 0.9169590473175049, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.36s
Iteration: 18,  Mean reward: -7.804878048780488, Mean Entropy: 0.9314026832580566, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.38s
Iteration: 19,  Mean reward: -5.2125, Mean Entropy: 0.8953032493591309, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.40s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 20,  Mean reward: -2.5681818181818183, Mean Entropy: 0.9386206865310669, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 21,  Mean reward: -3.9523809523809526, Mean Entropy: 0.989161491394043, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.40s
Iteration: 22,  Mean reward: -3.802325581395349, Mean Entropy: 0.93862384557724, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.36s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 23,  Mean reward: -1.1428571428571428, Mean Entropy: 1.0108140707015991, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 24,  Mean reward: -7.5625, Mean Entropy: 0.9241782426834106, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 25,  Mean reward: -4.463414634146342, Mean Entropy: 0.9819401502609253, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.36s
Iteration: 26,  Mean reward: -5.5625, Mean Entropy: 0.9458454847335815, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.38s
Iteration: 27,  Mean reward: -3.0476190476190474, Mean Entropy: 0.9025212526321411, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.37s
Iteration: 28,  Mean reward: -3.9743589743589745, Mean Entropy: 0.9241747856140137, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.61s
Iteration: 29,  Mean reward: -4.8125, Mean Entropy: 0.9458245635032654, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 30,  Mean reward: -5.8, Mean Entropy: 0.9602343440055847, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.36s
Iteration: 31,  Mean reward: -5.154761904761905, Mean Entropy: 0.9458028078079224, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 32,  Mean reward: -6.184782608695652, Mean Entropy: 0.9024836421012878, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 33,  Mean reward: -5.488888888888889, Mean Entropy: 0.9457402229309082, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 34,  Mean reward: -5.2560975609756095, Mean Entropy: 0.9601365327835083, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.40s
Iteration: 35,  Mean reward: -6.023809523809524, Mean Entropy: 0.9529021978378296, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 36,  Mean reward: -5.273809523809524, Mean Entropy: 0.9095711708068848, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.40s
Iteration: 37,  Mean reward: -3.5543478260869565, Mean Entropy: 0.9167904853820801, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 38,  Mean reward: -4.102564102564102, Mean Entropy: 0.9383326768875122, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 39,  Mean reward: -5.186046511627907, Mean Entropy: 0.9742525815963745, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 40,  Mean reward: -3.642857142857143, Mean Entropy: 0.9379075765609741, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 41,  Mean reward: -3.7261904761904763, Mean Entropy: 0.9147243499755859, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.42s
Iteration: 42,  Mean reward: -3.5697674418604652, Mean Entropy: 0.9710737466812134, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 43,  Mean reward: -2.6842105263157894, Mean Entropy: 0.9426085352897644, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.40s
Iteration: 44,  Mean reward: -4.662790697674419, Mean Entropy: 0.93939608335495, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 45,  Mean reward: -3.3255813953488373, Mean Entropy: 0.8985252380371094, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 46,  Mean reward: -2.616279069767442, Mean Entropy: 0.9119062423706055, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.41s
Iteration: 47,  Mean reward: -3.2777777777777777, Mean Entropy: 0.9602896571159363, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 48,  Mean reward: -4.1521739130434785, Mean Entropy: 0.9764968752861023, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 49,  Mean reward: -2.6808510638297873, Mean Entropy: 0.9104890823364258, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.43s
Iteration: 50,  Mean reward: -2.09375, Mean Entropy: 0.9377163648605347, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.36s
Iteration: 51,  Mean reward: -5.01063829787234, Mean Entropy: 0.9918292760848999, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 52,  Mean reward: -1.846938775510204, Mean Entropy: 0.9202370643615723, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 53,  Mean reward: -2.24468085106383, Mean Entropy: 0.8954634070396423, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.38s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 54,  Mean reward: -1.1203703703703705, Mean Entropy: 0.9981975555419922, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 55,  Mean reward: -4.232558139534884, Mean Entropy: 0.8474151492118835, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.36s
Iteration: 56,  Mean reward: -2.5982142857142856, Mean Entropy: 0.6831234693527222, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 57,  Mean reward: -3.2109375, Mean Entropy: 0.7893766164779663, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 58,  Mean reward: -3.980769230769231, Mean Entropy: 0.7259346842765808, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 59,  Mean reward: -2.1153846153846154, Mean Entropy: 0.8977184295654297, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.41s
Iteration: 60,  Mean reward: -4.19, Mean Entropy: 0.5307278037071228, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 61,  Mean reward: -3.0245901639344264, Mean Entropy: 0.5236323475837708, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 62,  Mean reward: -1.9047619047619047, Mean Entropy: 0.6972333788871765, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 63,  Mean reward: -2.9642857142857144, Mean Entropy: 0.626739501953125, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 64,  Mean reward: -2.211864406779661, Mean Entropy: 0.8226849436759949, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 65,  Mean reward: -2.9285714285714284, Mean Entropy: 0.4993123710155487, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 66,  Mean reward: -3.0846153846153848, Mean Entropy: 0.23085644841194153, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 67,  Mean reward: -2.0357142857142856, Mean Entropy: 0.21789954602718353, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 68,  Mean reward: -0.6041666666666666, Mean Entropy: 0.8078228235244751, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 69,  Mean reward: -5.346938775510204, Mean Entropy: 0.31602421402931213, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 70,  Mean reward: -1.55, Mean Entropy: 0.33435389399528503, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 71,  Mean reward: -3.0294117647058822, Mean Entropy: 0.3056986927986145, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 72,  Mean reward: -1.9366197183098592, Mean Entropy: 0.34674009680747986, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 73,  Mean reward: -1.1376811594202898, Mean Entropy: 0.6449122428894043, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 74,  Mean reward: -3.272727272727273, Mean Entropy: 0.5015251040458679, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 75,  Mean reward: -3.2583333333333333, Mean Entropy: 0.320008248090744, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 76,  Mean reward: -0.8283582089552238, Mean Entropy: 0.6635745763778687, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 77,  Mean reward: -2.5545454545454547, Mean Entropy: 0.8976799249649048, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 78,  Mean reward: -4.678571428571429, Mean Entropy: 0.7893414497375488, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.39s
Iteration: 79,  Mean reward: -2.0294117647058822, Mean Entropy: 0.7165141105651855, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 80,  Mean reward: -1.2962962962962963, Mean Entropy: 0.3929242789745331, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 81,  Mean reward: -2.566666666666667, Mean Entropy: 0.518609881401062, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 82,  Mean reward: -4.318181818181818, Mean Entropy: 0.11623980104923248, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 83,  Mean reward: -3.739130434782609, Mean Entropy: 0.18862563371658325, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 84,  Mean reward: -2.463235294117647, Mean Entropy: 0.21154457330703735, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 85,  Mean reward: -2.869565217391304, Mean Entropy: 0.45697087049484253, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 86,  Mean reward: -4.089285714285714, Mean Entropy: 0.15330779552459717, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 87,  Mean reward: -3.0615384615384613, Mean Entropy: 0.15341266989707947, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.69s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 88,  Mean reward: -0.11029411764705882, Mean Entropy: 0.2423771321773529, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 89,  Mean reward: -1.1557377049180328, Mean Entropy: 0.6401904821395874, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 90,  Mean reward: -1.9230769230769231, Mean Entropy: 0.6727573275566101, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.40s
Iteration: 91,  Mean reward: -2.8157894736842106, Mean Entropy: 0.1180030107498169, complete_episode_count: 57.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 92,  Mean reward: -2.5681818181818183, Mean Entropy: 0.10715079307556152, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 93,  Mean reward: -3.0846153846153848, Mean Entropy: 0.12730327248573303, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 94,  Mean reward: -3.0074626865671643, Mean Entropy: 0.13216841220855713, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 95,  Mean reward: -3.0846153846153848, Mean Entropy: 0.14601507782936096, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 96,  Mean reward: -2.463235294117647, Mean Entropy: 0.1383192390203476, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 97,  Mean reward: -2.388059701492537, Mean Entropy: 0.13446685671806335, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 98,  Mean reward: 0.050724637681159424, Mean Entropy: 0.17680001258850098, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 99,  Mean reward: -2.6641791044776117, Mean Entropy: 0.3446807265281677, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 100,  Mean reward: -1.6953125, Mean Entropy: 0.4194577634334564, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -2.7131147540983607, Mean Entropy: 0.5436145067214966, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 102,  Mean reward: -3.172727272727273, Mean Entropy: 0.6089960336685181, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 103,  Mean reward: -2.418181818181818, Mean Entropy: 0.3912599980831146, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 104,  Mean reward: -2.566666666666667, Mean Entropy: 0.45996904373168945, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 105,  Mean reward: -2.0526315789473686, Mean Entropy: 0.31260502338409424, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 106,  Mean reward: -3.6779661016949152, Mean Entropy: 0.49131259322166443, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 107,  Mean reward: -0.9508196721311475, Mean Entropy: 0.5447362661361694, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 108,  Mean reward: -4.185185185185185, Mean Entropy: 0.33307453989982605, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 109,  Mean reward: -2.111111111111111, Mean Entropy: 0.44642505049705505, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 110,  Mean reward: -3.5245901639344264, Mean Entropy: 0.27019616961479187, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 111,  Mean reward: -1.6363636363636365, Mean Entropy: 0.2974134385585785, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 112,  Mean reward: -2.296875, Mean Entropy: 0.33430713415145874, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 113,  Mean reward: -3.782258064516129, Mean Entropy: 0.35462769865989685, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 114,  Mean reward: -4.190476190476191, Mean Entropy: 0.3891480565071106, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 115,  Mean reward: -3.5254237288135593, Mean Entropy: 0.42441338300704956, complete_episode_count: 59.0, Gather time: 0.50s, Train time: 1.35s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 116,  Mean reward: 0.2777777777777778, Mean Entropy: 0.4977515637874603, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 117,  Mean reward: -1.4661016949152543, Mean Entropy: 0.3407505750656128, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 118,  Mean reward: -0.828125, Mean Entropy: 0.6608752012252808, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 119,  Mean reward: -3.827272727272727, Mean Entropy: 0.5079392194747925, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.38s
Iteration: 120,  Mean reward: -0.38596491228070173, Mean Entropy: 0.6369678378105164, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 121,  Mean reward: -3.1416666666666666, Mean Entropy: 0.6208786368370056, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 122,  Mean reward: -1.5666666666666667, Mean Entropy: 0.4760158061981201, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 123,  Mean reward: -3.75, Mean Entropy: 0.34518298506736755, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 124,  Mean reward: -3.0615384615384613, Mean Entropy: 0.3364717960357666, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 125,  Mean reward: -1.4765625, Mean Entropy: 0.40938591957092285, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 126,  Mean reward: -3.457627118644068, Mean Entropy: 0.6458122134208679, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 127,  Mean reward: -3.508771929824561, Mean Entropy: 0.5918518900871277, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 128,  Mean reward: -1.1944444444444444, Mean Entropy: 0.6110671758651733, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 129,  Mean reward: -2.8727272727272726, Mean Entropy: 0.5881873965263367, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 130,  Mean reward: -1.9609375, Mean Entropy: 0.5809307098388672, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 131,  Mean reward: -0.3387096774193548, Mean Entropy: 0.6539051532745361, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 132,  Mean reward: -2.9134615384615383, Mean Entropy: 0.7546846270561218, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.39s
Iteration: 133,  Mean reward: -3.490566037735849, Mean Entropy: 0.26937419176101685, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.57s
Iteration: 134,  Mean reward: -1.9426229508196722, Mean Entropy: 0.3767784833908081, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 135,  Mean reward: -4.84375, Mean Entropy: 0.4377292990684509, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 136,  Mean reward: -3.120967741935484, Mean Entropy: 0.3437323570251465, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 137,  Mean reward: -3.8015873015873014, Mean Entropy: 0.34396836161613464, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 138,  Mean reward: -1.765625, Mean Entropy: 0.45230162143707275, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 139,  Mean reward: -1.0403225806451613, Mean Entropy: 0.600525438785553, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 140,  Mean reward: -1.8307692307692307, Mean Entropy: 0.3045799732208252, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 141,  Mean reward: -0.7045454545454546, Mean Entropy: 0.5545498132705688, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 142,  Mean reward: -4.3828125, Mean Entropy: 0.5945958495140076, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 143,  Mean reward: -3.379032258064516, Mean Entropy: 0.26035743951797485, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 144,  Mean reward: -1.71875, Mean Entropy: 0.36745625734329224, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 145,  Mean reward: -3.8333333333333335, Mean Entropy: 0.4656044840812683, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 146,  Mean reward: -2.376923076923077, Mean Entropy: 0.33510062098503113, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 147,  Mean reward: -5.685483870967742, Mean Entropy: 0.37004756927490234, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 148,  Mean reward: -2.0223880597014925, Mean Entropy: 0.3150191009044647, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 149,  Mean reward: -1.946969696969697, Mean Entropy: 0.2811760902404785, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 150,  Mean reward: -0.6928571428571428, Mean Entropy: 0.2821357846260071, complete_episode_count: 70.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 151,  Mean reward: -3.242857142857143, Mean Entropy: 0.3773036003112793, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 152,  Mean reward: -1.4420289855072463, Mean Entropy: 0.4538770318031311, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 153,  Mean reward: -4.915384615384616, Mean Entropy: 0.30977433919906616, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 154,  Mean reward: -3.639705882352941, Mean Entropy: 0.2723812460899353, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 155,  Mean reward: -4.028985507246377, Mean Entropy: 0.2432253360748291, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 156,  Mean reward: -2.125, Mean Entropy: 0.25610941648483276, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 157,  Mean reward: -0.8405797101449275, Mean Entropy: 0.3727080225944519, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 158,  Mean reward: -0.1956521739130435, Mean Entropy: 0.4586619734764099, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 159,  Mean reward: -1.5625, Mean Entropy: 0.4203503131866455, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 160,  Mean reward: -3.2950819672131146, Mean Entropy: 0.36738234758377075, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 161,  Mean reward: -3.096774193548387, Mean Entropy: 0.2967873811721802, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 162,  Mean reward: -1.0147058823529411, Mean Entropy: 0.3533155918121338, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 163,  Mean reward: -2.537878787878788, Mean Entropy: 0.3786867558956146, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 164,  Mean reward: -3.283582089552239, Mean Entropy: 0.44326886534690857, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 165,  Mean reward: -4.468253968253968, Mean Entropy: 0.4097755551338196, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 166,  Mean reward: -1.4202898550724639, Mean Entropy: 0.4452894330024719, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 167,  Mean reward: -4.892307692307693, Mean Entropy: 0.2774578034877777, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 168,  Mean reward: -1.412162162162162, Mean Entropy: 0.2489698827266693, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 169,  Mean reward: -2.25, Mean Entropy: 0.3761748969554901, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 170,  Mean reward: -3.0, Mean Entropy: 0.4685779809951782, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 171,  Mean reward: -0.9841269841269841, Mean Entropy: 0.5192276835441589, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 172,  Mean reward: -4.701612903225806, Mean Entropy: 0.424934446811676, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 173,  Mean reward: -3.6538461538461537, Mean Entropy: 0.2031613439321518, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.92s
Iteration: 174,  Mean reward: -0.7753623188405797, Mean Entropy: 0.18399202823638916, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 175,  Mean reward: -0.027777777777777776, Mean Entropy: 0.3799150586128235, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 176,  Mean reward: -2.9375, Mean Entropy: 0.5169463157653809, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 177,  Mean reward: -3.808333333333333, Mean Entropy: 0.44142019748687744, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 178,  Mean reward: -2.5238095238095237, Mean Entropy: 0.516527533531189, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 179,  Mean reward: -3.5234375, Mean Entropy: 0.5888773798942566, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 180,  Mean reward: -0.25396825396825395, Mean Entropy: 0.5999273657798767, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 181,  Mean reward: -3.7454545454545456, Mean Entropy: 0.4913111925125122, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 182,  Mean reward: -4.1875, Mean Entropy: 0.5555750727653503, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 183,  Mean reward: -2.5258620689655173, Mean Entropy: 0.6898412704467773, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 184,  Mean reward: -0.8833333333333333, Mean Entropy: 0.5879358053207397, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 185,  Mean reward: -2.1440677966101696, Mean Entropy: 0.6531665325164795, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 186,  Mean reward: -1.6517857142857142, Mean Entropy: 0.6644562482833862, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.36s
Iteration: 187,  Mean reward: -3.1810344827586206, Mean Entropy: 0.5925571918487549, complete_episode_count: 58.0, Gather time: 0.50s, Train time: 1.37s
Iteration: 188,  Mean reward: -2.443548387096774, Mean Entropy: 0.5966570377349854, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 189,  Mean reward: -4.707692307692308, Mean Entropy: 0.5855857133865356, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 190,  Mean reward: -1.4836065573770492, Mean Entropy: 0.5143812894821167, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 191,  Mean reward: -4.330645161290323, Mean Entropy: 0.5203812122344971, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 192,  Mean reward: -4.046875, Mean Entropy: 0.2751544713973999, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 193,  Mean reward: -3.323943661971831, Mean Entropy: 0.23752932250499725, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 194,  Mean reward: -2.869565217391304, Mean Entropy: 0.2927912175655365, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 195,  Mean reward: -2.4788732394366195, Mean Entropy: 0.42755159735679626, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 196,  Mean reward: -1.9318181818181819, Mean Entropy: 0.48942384123802185, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 197,  Mean reward: -4.016129032258065, Mean Entropy: 0.38017237186431885, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 198,  Mean reward: -1.3943661971830985, Mean Entropy: 0.38751745223999023, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 199,  Mean reward: -1.5833333333333333, Mean Entropy: 0.4162521958351135, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 200,  Mean reward: -2.5522388059701493, Mean Entropy: 0.2780577540397644, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.69s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.463235294117647, Mean Entropy: 0.25205743312835693, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 202,  Mean reward: -3.757575757575758, Mean Entropy: 0.23854827880859375, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 203,  Mean reward: -1.6884057971014492, Mean Entropy: 0.2596242129802704, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 204,  Mean reward: -1.6408450704225352, Mean Entropy: 0.3038133382797241, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 205,  Mean reward: -0.5070422535211268, Mean Entropy: 0.33555924892425537, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 206,  Mean reward: -0.05, Mean Entropy: 0.5358279347419739, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 207,  Mean reward: -4.579365079365079, Mean Entropy: 0.5035145878791809, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 208,  Mean reward: -2.957627118644068, Mean Entropy: 0.26541146636009216, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 209,  Mean reward: -3.3676470588235294, Mean Entropy: 0.2902584969997406, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 210,  Mean reward: -2.985074626865672, Mean Entropy: 0.236546128988266, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 211,  Mean reward: -2.2681159420289854, Mean Entropy: 0.2519342601299286, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 212,  Mean reward: -2.962686567164179, Mean Entropy: 0.3019583821296692, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 213,  Mean reward: -3.3524590163934427, Mean Entropy: 0.302893728017807, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 214,  Mean reward: -2.1384615384615384, Mean Entropy: 0.35572612285614014, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 215,  Mean reward: -3.346774193548387, Mean Entropy: 0.30986496806144714, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 216,  Mean reward: -3.370967741935484, Mean Entropy: 0.31594276428222656, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 217,  Mean reward: -3.717391304347826, Mean Entropy: 0.2846824824810028, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 218,  Mean reward: -4.17910447761194, Mean Entropy: 0.2472369372844696, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 219,  Mean reward: -3.760869565217391, Mean Entropy: 0.20626364648342133, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 220,  Mean reward: -0.6013513513513513, Mean Entropy: 0.18195681273937225, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 221,  Mean reward: -3.563380281690141, Mean Entropy: 0.27344757318496704, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 222,  Mean reward: -3.5714285714285716, Mean Entropy: 0.25447165966033936, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 223,  Mean reward: -1.6338028169014085, Mean Entropy: 0.22535400092601776, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 224,  Mean reward: -4.572463768115942, Mean Entropy: 0.2571747899055481, complete_episode_count: 69.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 225,  Mean reward: 0.1875, Mean Entropy: 0.21128249168395996, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 226,  Mean reward: -4.387096774193548, Mean Entropy: 0.25131356716156006, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 227,  Mean reward: 0.22857142857142856, Mean Entropy: 0.25913649797439575, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 228,  Mean reward: -2.5579710144927534, Mean Entropy: 0.3382438123226166, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 229,  Mean reward: -2.5681818181818183, Mean Entropy: 0.348155677318573, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 230,  Mean reward: -1.6818181818181819, Mean Entropy: 0.3294270634651184, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 231,  Mean reward: -3.4153846153846152, Mean Entropy: 0.31371167302131653, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 232,  Mean reward: -2.1911764705882355, Mean Entropy: 0.34593045711517334, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 233,  Mean reward: -2.242424242424242, Mean Entropy: 0.2572447657585144, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 234,  Mean reward: -1.4701492537313432, Mean Entropy: 0.3244210481643677, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 235,  Mean reward: -3.265151515151515, Mean Entropy: 0.3195435404777527, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 236,  Mean reward: -2.869565217391304, Mean Entropy: 0.27285894751548767, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 237,  Mean reward: -3.389705882352941, Mean Entropy: 0.2936239242553711, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 238,  Mean reward: -2.7573529411764706, Mean Entropy: 0.2789996862411499, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 239,  Mean reward: -2.404109589041096, Mean Entropy: 0.2554742991924286, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 240,  Mean reward: -3.2196969696969697, Mean Entropy: 0.2903553247451782, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 241,  Mean reward: -3.2642857142857142, Mean Entropy: 0.24618899822235107, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 242,  Mean reward: -0.13013698630136986, Mean Entropy: 0.22757649421691895, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 243,  Mean reward: 0.1875, Mean Entropy: 0.3730418384075165, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 244,  Mean reward: -2.871212121212121, Mean Entropy: 0.32612499594688416, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 245,  Mean reward: -3.0615384615384613, Mean Entropy: 0.3030105531215668, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 246,  Mean reward: -3.323529411764706, Mean Entropy: 0.23112909495830536, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 247,  Mean reward: -2.2681159420289854, Mean Entropy: 0.20929215848445892, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 248,  Mean reward: -3.3028169014084505, Mean Entropy: 0.19462035596370697, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 249,  Mean reward: -3.458904109589041, Mean Entropy: 0.18667817115783691, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 250,  Mean reward: -1.2671232876712328, Mean Entropy: 0.19331137835979462, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 251,  Mean reward: -3.1041666666666665, Mean Entropy: 0.2577570676803589, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 252,  Mean reward: -2.2708333333333335, Mean Entropy: 0.30549338459968567, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 253,  Mean reward: -2.6214285714285714, Mean Entropy: 0.29045143723487854, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 254,  Mean reward: -3.578125, Mean Entropy: 0.2358398735523224, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 255,  Mean reward: -1.0136986301369864, Mean Entropy: 0.25530725717544556, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 256,  Mean reward: -4.105633802816901, Mean Entropy: 0.24482089281082153, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 257,  Mean reward: -4.236111111111111, Mean Entropy: 0.2111826390028, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 258,  Mean reward: -1.2876712328767124, Mean Entropy: 0.1785384863615036, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 259,  Mean reward: -4.035714285714286, Mean Entropy: 0.2550942301750183, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 260,  Mean reward: -0.9027777777777778, Mean Entropy: 0.1599532961845398, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 261,  Mean reward: 0.5, Mean Entropy: 0.267544150352478, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 262,  Mean reward: -2.2916666666666665, Mean Entropy: 0.3544570803642273, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 263,  Mean reward: 0.6086956521739131, Mean Entropy: 0.37688371539115906, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 264,  Mean reward: -1.7651515151515151, Mean Entropy: 0.3452562093734741, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 265,  Mean reward: -1.8142857142857143, Mean Entropy: 0.32974034547805786, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 266,  Mean reward: -2.176056338028169, Mean Entropy: 0.39648541808128357, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 267,  Mean reward: -0.8880597014925373, Mean Entropy: 0.26970112323760986, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 268,  Mean reward: -1.957142857142857, Mean Entropy: 0.37959998846054077, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 269,  Mean reward: -2.3285714285714287, Mean Entropy: 0.303032785654068, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 270,  Mean reward: -3.0073529411764706, Mean Entropy: 0.26450034976005554, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 271,  Mean reward: -0.05714285714285714, Mean Entropy: 0.2873661518096924, complete_episode_count: 70.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 272,  Mean reward: -2.0597014925373136, Mean Entropy: 0.28383350372314453, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 273,  Mean reward: -2.08955223880597, Mean Entropy: 0.264503538608551, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 274,  Mean reward: -3.4545454545454546, Mean Entropy: 0.29725775122642517, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 275,  Mean reward: -1.286764705882353, Mean Entropy: 0.26300501823425293, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 276,  Mean reward: -1.2878787878787878, Mean Entropy: 0.2615707516670227, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 277,  Mean reward: -1.0704225352112675, Mean Entropy: 0.2922937572002411, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 278,  Mean reward: -1.5633802816901408, Mean Entropy: 0.3521577715873718, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 279,  Mean reward: -3.621212121212121, Mean Entropy: 0.2617626488208771, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 280,  Mean reward: -4.326086956521739, Mean Entropy: 0.22361716628074646, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 281,  Mean reward: -3.887323943661972, Mean Entropy: 0.19659224152565002, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 282,  Mean reward: -2.176056338028169, Mean Entropy: 0.16260650753974915, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 283,  Mean reward: -3.760869565217391, Mean Entropy: 0.18996505439281464, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 284,  Mean reward: -1.662162162162162, Mean Entropy: 0.18320676684379578, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 285,  Mean reward: -0.96, Mean Entropy: 0.33632463216781616, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 286,  Mean reward: -2.2318840579710146, Mean Entropy: 0.32718849182128906, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 287,  Mean reward: -2.044776119402985, Mean Entropy: 0.22460848093032837, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 288,  Mean reward: -2.342857142857143, Mean Entropy: 0.29054492712020874, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 289,  Mean reward: -1.4513888888888888, Mean Entropy: 0.24324719607830048, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 290,  Mean reward: -1.6283783783783783, Mean Entropy: 0.14835509657859802, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 291,  Mean reward: -2.2733333333333334, Mean Entropy: 0.16782641410827637, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 292,  Mean reward: -3.324324324324324, Mean Entropy: 0.17503409087657928, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 293,  Mean reward: -1.0492957746478873, Mean Entropy: 0.13909204304218292, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 294,  Mean reward: -1.2266666666666666, Mean Entropy: 0.19908666610717773, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 295,  Mean reward: -1.78, Mean Entropy: 0.30925270915031433, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 296,  Mean reward: -0.31756756756756754, Mean Entropy: 0.3202853202819824, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 297,  Mean reward: -1.3028169014084507, Mean Entropy: 0.34339866042137146, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 298,  Mean reward: -1.4444444444444444, Mean Entropy: 0.3981599509716034, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 299,  Mean reward: -3.0454545454545454, Mean Entropy: 0.2746034562587738, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 300,  Mean reward: -1.9722222222222223, Mean Entropy: 0.20309743285179138, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.68s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -1.7714285714285714, Mean Entropy: 0.2150355577468872, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 302,  Mean reward: -1.1388888888888888, Mean Entropy: 0.1731177419424057, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 303,  Mean reward: -1.6824324324324325, Mean Entropy: 0.18493831157684326, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 304,  Mean reward: 0.25, Mean Entropy: 0.20975035429000854, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 305,  Mean reward: -2.026666666666667, Mean Entropy: 0.27834969758987427, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 306,  Mean reward: -3.111111111111111, Mean Entropy: 0.21152779459953308, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 307,  Mean reward: -2.28, Mean Entropy: 0.17517194151878357, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 308,  Mean reward: -0.9246575342465754, Mean Entropy: 0.13923302292823792, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 309,  Mean reward: -1.3896103896103895, Mean Entropy: 0.21879741549491882, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 310,  Mean reward: -0.17123287671232876, Mean Entropy: 0.19118809700012207, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 311,  Mean reward: -0.6363636363636364, Mean Entropy: 0.39254236221313477, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 312,  Mean reward: -0.9285714285714286, Mean Entropy: 0.39329344034194946, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 313,  Mean reward: -3.1857142857142855, Mean Entropy: 0.2550974488258362, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 314,  Mean reward: -3.0135135135135136, Mean Entropy: 0.22626829147338867, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 315,  Mean reward: -4.243421052631579, Mean Entropy: 0.16055095195770264, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 316,  Mean reward: -2.026666666666667, Mean Entropy: 0.18189504742622375, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 317,  Mean reward: -2.6447368421052633, Mean Entropy: 0.07306864857673645, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 318,  Mean reward: -1.6493506493506493, Mean Entropy: 0.1288282871246338, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 319,  Mean reward: -1.9675324675324675, Mean Entropy: 0.22655557096004486, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 320,  Mean reward: -1.523076923076923, Mean Entropy: 0.17867611348628998, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 321,  Mean reward: -1.1044776119402986, Mean Entropy: 0.14541026949882507, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 322,  Mean reward: -4.601351351351352, Mean Entropy: 0.17851059138774872, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 323,  Mean reward: -1.948051948051948, Mean Entropy: 0.1828726977109909, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 324,  Mean reward: -1.4733333333333334, Mean Entropy: 0.17132042348384857, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 325,  Mean reward: -2.97972972972973, Mean Entropy: 0.2028268277645111, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 326,  Mean reward: -2.222972972972973, Mean Entropy: 0.12315907329320908, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 327,  Mean reward: -1.948051948051948, Mean Entropy: 0.1185368001461029, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 328,  Mean reward: -2.9675324675324677, Mean Entropy: 0.0673259049654007, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 329,  Mean reward: -1.8924050632911393, Mean Entropy: 0.07598244398832321, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 330,  Mean reward: -2.4675324675324677, Mean Entropy: 0.10336457192897797, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 331,  Mean reward: -1.5723684210526316, Mean Entropy: 0.11769130825996399, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 332,  Mean reward: -2.8333333333333335, Mean Entropy: 0.13215389847755432, complete_episode_count: 75.0, Gather time: 0.75s, Train time: 0.70s
Iteration: 333,  Mean reward: 0.10666666666666667, Mean Entropy: 0.11019308865070343, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 334,  Mean reward: -2.54, Mean Entropy: 0.1453402042388916, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 335,  Mean reward: -1.8918918918918919, Mean Entropy: 0.21764838695526123, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 336,  Mean reward: -2.2681159420289854, Mean Entropy: 0.06416743248701096, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 337,  Mean reward: -2.25, Mean Entropy: 0.08636268228292465, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 338,  Mean reward: -3.1582278481012658, Mean Entropy: 0.03615501523017883, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 339,  Mean reward: -2.651898734177215, Mean Entropy: 0.03716563433408737, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 340,  Mean reward: -1.4050632911392404, Mean Entropy: 0.10727258771657944, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 341,  Mean reward: -2.7662337662337664, Mean Entropy: 0.055499639362096786, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 342,  Mean reward: -2.4177215189873418, Mean Entropy: 0.049131542444229126, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 343,  Mean reward: -0.5, Mean Entropy: 0.05419892817735672, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 344,  Mean reward: -4.025974025974026, Mean Entropy: 0.057652659714221954, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 345,  Mean reward: -2.670886075949367, Mean Entropy: 0.13966889679431915, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 346,  Mean reward: -2.227272727272727, Mean Entropy: 0.037289973348379135, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 347,  Mean reward: -4.424050632911392, Mean Entropy: 0.03905663639307022, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 348,  Mean reward: -1.1329113924050633, Mean Entropy: 0.06741902977228165, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 349,  Mean reward: -2.2948717948717947, Mean Entropy: 0.04642060399055481, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 350,  Mean reward: -1.4090909090909092, Mean Entropy: 0.054129548370838165, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 351,  Mean reward: -1.4050632911392404, Mean Entropy: 0.16007627546787262, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 352,  Mean reward: -2.1095890410958904, Mean Entropy: 0.11001859605312347, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 353,  Mean reward: -0.7948717948717948, Mean Entropy: 0.06447947025299072, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 354,  Mean reward: -1.7820512820512822, Mean Entropy: 0.07320977747440338, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 355,  Mean reward: -3.7857142857142856, Mean Entropy: 0.0616178922355175, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 356,  Mean reward: -2.0384615384615383, Mean Entropy: 0.04660623520612717, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 357,  Mean reward: 0.03205128205128205, Mean Entropy: 0.07466492056846619, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 358,  Mean reward: -2.138157894736842, Mean Entropy: 0.0845690369606018, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 359,  Mean reward: -2.3141025641025643, Mean Entropy: 0.07820141315460205, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 360,  Mean reward: -1.7820512820512822, Mean Entropy: 0.041201941668987274, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 361,  Mean reward: -2.632911392405063, Mean Entropy: 0.05652442201972008, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 362,  Mean reward: -2.9050632911392404, Mean Entropy: 0.0677437037229538, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 363,  Mean reward: -0.7564102564102564, Mean Entropy: 0.03691892698407173, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 364,  Mean reward: 0.0, Mean Entropy: 0.07969575375318527, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 365,  Mean reward: -0.38961038961038963, Mean Entropy: 0.11316296458244324, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 366,  Mean reward: -2.7467532467532467, Mean Entropy: 0.12274999916553497, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 367,  Mean reward: -2.727272727272727, Mean Entropy: 0.15233266353607178, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 368,  Mean reward: -1.6688311688311688, Mean Entropy: 0.054691895842552185, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 369,  Mean reward: -0.37012987012987014, Mean Entropy: 0.1324707716703415, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.90s
Iteration: 370,  Mean reward: -2.1095890410958904, Mean Entropy: 0.14184483885765076, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 371,  Mean reward: -4.890625, Mean Entropy: 0.23192881047725677, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 372,  Mean reward: -2.6575342465753424, Mean Entropy: 0.037448450922966, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 373,  Mean reward: -1.1329113924050633, Mean Entropy: 0.04845850169658661, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 374,  Mean reward: -2.168831168831169, Mean Entropy: 0.18823252618312836, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 375,  Mean reward: -3.582089552238806, Mean Entropy: 0.1204511746764183, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 376,  Mean reward: -3.7419354838709675, Mean Entropy: 0.08737437427043915, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 377,  Mean reward: -0.4090909090909091, Mean Entropy: 0.05662159249186516, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 378,  Mean reward: -0.5, Mean Entropy: 0.1118348091840744, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 379,  Mean reward: -1.0855263157894737, Mean Entropy: 0.08505770564079285, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 380,  Mean reward: -2.722972972972973, Mean Entropy: 0.15042203664779663, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 381,  Mean reward: -3.0533333333333332, Mean Entropy: 0.21636657416820526, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 382,  Mean reward: -1.7686567164179106, Mean Entropy: 0.08782070875167847, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 383,  Mean reward: -2.487012987012987, Mean Entropy: 0.0821022316813469, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 384,  Mean reward: -2.707792207792208, Mean Entropy: 0.04064074531197548, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 385,  Mean reward: -1.1329113924050633, Mean Entropy: 0.04439019411802292, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 386,  Mean reward: -1.25, Mean Entropy: 0.19249950349330902, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 387,  Mean reward: 0.18571428571428572, Mean Entropy: 0.12497223913669586, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 388,  Mean reward: -3.7903225806451615, Mean Entropy: 0.2085946798324585, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.73s
Iteration: 389,  Mean reward: -1.8970588235294117, Mean Entropy: 0.14756467938423157, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 390,  Mean reward: -1.3918918918918919, Mean Entropy: 0.11833067238330841, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.69s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 391,  Mean reward: 0.8896103896103896, Mean Entropy: 0.0761728584766388, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 392,  Mean reward: -2.948051948051948, Mean Entropy: 0.17586462199687958, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 393,  Mean reward: -3.9166666666666665, Mean Entropy: 0.05392615124583244, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 394,  Mean reward: -3.25, Mean Entropy: 0.049306124448776245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 395,  Mean reward: -2.0, Mean Entropy: 0.0538598969578743, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 396,  Mean reward: -1.3860759493670887, Mean Entropy: 0.0639512687921524, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 397,  Mean reward: -2.585526315789474, Mean Entropy: 0.07140692323446274, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 398,  Mean reward: -1.4533333333333334, Mean Entropy: 0.04998227208852768, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 399,  Mean reward: -2.3141025641025643, Mean Entropy: 0.06287812441587448, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 400,  Mean reward: -2.632911392405063, Mean Entropy: 0.09493786096572876, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.69s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -2.0384615384615383, Mean Entropy: 0.05126090347766876, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 402,  Mean reward: -3.6776315789473686, Mean Entropy: 0.21401600539684296, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 403,  Mean reward: -2.0347222222222223, Mean Entropy: 0.027262037619948387, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 404,  Mean reward: -1.1329113924050633, Mean Entropy: 0.03266286104917526, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 405,  Mean reward: -3.25, Mean Entropy: 0.07624296844005585, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 406,  Mean reward: -0.25, Mean Entropy: 0.08323842287063599, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 407,  Mean reward: -2.0, Mean Entropy: 0.07409834861755371, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 408,  Mean reward: -2.3141025641025643, Mean Entropy: 0.0397803820669651, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 409,  Mean reward: -0.879746835443038, Mean Entropy: 0.06212452054023743, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 410,  Mean reward: 0.3860759493670886, Mean Entropy: 0.10465057939291, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 411,  Mean reward: -1.2692307692307692, Mean Entropy: 0.1478218138217926, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 412,  Mean reward: -1.0128205128205128, Mean Entropy: 0.1853296011686325, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 413,  Mean reward: -2.533333333333333, Mean Entropy: 0.2595157325267792, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 414,  Mean reward: -4.144927536231884, Mean Entropy: 0.24117712676525116, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 415,  Mean reward: -2.873134328358209, Mean Entropy: 0.1369142383337021, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 416,  Mean reward: -2.753846153846154, Mean Entropy: 0.08451084792613983, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 417,  Mean reward: -2.9932432432432434, Mean Entropy: 0.15969952940940857, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 418,  Mean reward: -1.9324324324324325, Mean Entropy: 0.064241923391819, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 419,  Mean reward: -1.1329113924050633, Mean Entropy: 0.10044972598552704, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 420,  Mean reward: -2.26, Mean Entropy: 0.14947596192359924, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 421,  Mean reward: -4.717105263157895, Mean Entropy: 0.06885732710361481, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 422,  Mean reward: -2.0576923076923075, Mean Entropy: 0.07098796963691711, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 423,  Mean reward: -2.651898734177215, Mean Entropy: 0.014494247734546661, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 424,  Mean reward: -1.5, Mean Entropy: 0.020433759316802025, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 425,  Mean reward: -2.0, Mean Entropy: 0.05341022461652756, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 426,  Mean reward: -1.7820512820512822, Mean Entropy: 0.12461435794830322, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 427,  Mean reward: -1.3896103896103895, Mean Entropy: 0.25666025280952454, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 428,  Mean reward: -2.0869565217391304, Mean Entropy: 0.1841070055961609, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 429,  Mean reward: -1.9375, Mean Entropy: 0.10466771572828293, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 430,  Mean reward: -3.732876712328767, Mean Entropy: 0.19804352521896362, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 431,  Mean reward: -3.3923076923076922, Mean Entropy: 0.14565469324588776, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 432,  Mean reward: -1.9782608695652173, Mean Entropy: 0.17439810931682587, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 433,  Mean reward: -1.3513513513513513, Mean Entropy: 0.09960891306400299, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 434,  Mean reward: -1.6582278481012658, Mean Entropy: 0.07033099979162216, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 435,  Mean reward: -1.1688311688311688, Mean Entropy: 0.0775713324546814, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 436,  Mean reward: -0.75, Mean Entropy: 0.11559512466192245, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 437,  Mean reward: -2.7733333333333334, Mean Entropy: 0.10131704062223434, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 438,  Mean reward: -2.448051948051948, Mean Entropy: 0.0867149755358696, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 439,  Mean reward: -2.019230769230769, Mean Entropy: 0.07183864712715149, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 440,  Mean reward: -3.0, Mean Entropy: 0.04103783890604973, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 441,  Mean reward: -1.7820512820512822, Mean Entropy: 0.03517825901508331, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 442,  Mean reward: -1.639240506329114, Mean Entropy: 0.22776538133621216, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 443,  Mean reward: -2.609375, Mean Entropy: 0.07378500699996948, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 444,  Mean reward: -2.1015625, Mean Entropy: 0.07892218232154846, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 445,  Mean reward: -3.0483870967741935, Mean Entropy: 0.1429758071899414, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 446,  Mean reward: -2.6641791044776117, Mean Entropy: 0.08001058548688889, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 447,  Mean reward: -2.3987341772151898, Mean Entropy: 0.10998411476612091, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 448,  Mean reward: -1.9113924050632911, Mean Entropy: 0.15844273567199707, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 449,  Mean reward: -2.7635135135135136, Mean Entropy: 0.12868307530879974, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 450,  Mean reward: -1.7820512820512822, Mean Entropy: 0.024044005200266838, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 451,  Mean reward: -3.75, Mean Entropy: 0.016202040016651154, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 452,  Mean reward: -2.4177215189873418, Mean Entropy: 0.014735423028469086, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 453,  Mean reward: -2.1455696202531644, Mean Entropy: 0.021112624555826187, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 454,  Mean reward: -1.5, Mean Entropy: 0.15472599864006042, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 455,  Mean reward: -2.0163934426229506, Mean Entropy: 0.053472913801670074, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 456,  Mean reward: -2.5, Mean Entropy: 0.0993615984916687, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 457,  Mean reward: -2.727272727272727, Mean Entropy: 0.03987973555922508, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 458,  Mean reward: -2.8076923076923075, Mean Entropy: 0.02578619122505188, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 459,  Mean reward: -2.551282051282051, Mean Entropy: 0.015396211296319962, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 460,  Mean reward: -0.75, Mean Entropy: 0.14190876483917236, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 461,  Mean reward: -2.9050632911392404, Mean Entropy: 0.08372940868139267, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 462,  Mean reward: -2.826923076923077, Mean Entropy: 0.08725972473621368, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 463,  Mean reward: -0.12025316455696203, Mean Entropy: 0.05096698924899101, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 464,  Mean reward: -1.4155844155844155, Mean Entropy: 0.06102675199508667, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 465,  Mean reward: -0.5, Mean Entropy: 0.22436439990997314, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 466,  Mean reward: -0.5743243243243243, Mean Entropy: 0.21324726939201355, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 467,  Mean reward: -2.4461538461538463, Mean Entropy: 0.150650292634964, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 468,  Mean reward: -2.551282051282051, Mean Entropy: 0.038707438856363297, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 469,  Mean reward: -3.0, Mean Entropy: 0.04198713228106499, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 470,  Mean reward: -2.1455696202531644, Mean Entropy: 0.053281158208847046, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 471,  Mean reward: -2.25, Mean Entropy: 0.03561191260814667, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 472,  Mean reward: -1.8924050632911393, Mean Entropy: 0.04057595878839493, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 473,  Mean reward: -1.639240506329114, Mean Entropy: 0.17976701259613037, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 474,  Mean reward: -3.0074626865671643, Mean Entropy: 0.0500153973698616, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 475,  Mean reward: -0.7727272727272727, Mean Entropy: 0.05563678964972496, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 476,  Mean reward: -3.959016393442623, Mean Entropy: 0.11340027302503586, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 477,  Mean reward: -4.435483870967742, Mean Entropy: 0.09441447257995605, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 478,  Mean reward: -1.0757575757575757, Mean Entropy: 0.04667344316840172, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 479,  Mean reward: -3.096774193548387, Mean Entropy: 0.07554657757282257, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 480,  Mean reward: -1.9444444444444444, Mean Entropy: 0.05212455987930298, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 481,  Mean reward: -2.261904761904762, Mean Entropy: 0.040864862501621246, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 482,  Mean reward: -2.5793650793650795, Mean Entropy: 0.04083827883005142, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 483,  Mean reward: -2.0806451612903225, Mean Entropy: 0.057450246065855026, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 484,  Mean reward: -2.9206349206349205, Mean Entropy: 0.07617056369781494, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 485,  Mean reward: -2.9921875, Mean Entropy: 0.13885454833507538, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 486,  Mean reward: -2.4692307692307693, Mean Entropy: 0.14321404695510864, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.90s
Iteration: 487,  Mean reward: -2.9166666666666665, Mean Entropy: 0.14368826150894165, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 488,  Mean reward: -2.8680555555555554, Mean Entropy: 0.03380030393600464, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 489,  Mean reward: -0.12025316455696203, Mean Entropy: 0.030464842915534973, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 490,  Mean reward: -1.639240506329114, Mean Entropy: 0.05788736790418625, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 491,  Mean reward: -1.1688311688311688, Mean Entropy: 0.049496881663799286, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 492,  Mean reward: -2.9050632911392404, Mean Entropy: 0.029512476176023483, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 493,  Mean reward: -2.25, Mean Entropy: 0.02081756852567196, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 494,  Mean reward: 0.6392405063291139, Mean Entropy: 0.025428414344787598, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 495,  Mean reward: -0.8860759493670886, Mean Entropy: 0.03363903611898422, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 496,  Mean reward: -1.6688311688311688, Mean Entropy: 0.07473131269216537, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 497,  Mean reward: -4.673333333333333, Mean Entropy: 0.14391648769378662, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 498,  Mean reward: -1.74, Mean Entropy: 0.03827235847711563, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 499,  Mean reward: -2.9050632911392404, Mean Entropy: 0.031376417726278305, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 500,  Mean reward: -0.5, Mean Entropy: 0.04497629404067993, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -2.207792207792208, Mean Entropy: 0.06304843723773956, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 502,  Mean reward: -0.5, Mean Entropy: 0.029605884104967117, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 503,  Mean reward: -2.207792207792208, Mean Entropy: 0.031567662954330444, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 504,  Mean reward: -1.8924050632911393, Mean Entropy: 0.029369108378887177, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 505,  Mean reward: -2.0, Mean Entropy: 0.01769620180130005, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 506,  Mean reward: -3.0, Mean Entropy: 0.016453880816698074, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 507,  Mean reward: -1.544871794871795, Mean Entropy: 0.008509148843586445, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 508,  Mean reward: -3.411392405063291, Mean Entropy: 0.03064938448369503, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 509,  Mean reward: -3.75, Mean Entropy: 0.009415525011718273, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 510,  Mean reward: -2.5, Mean Entropy: 0.00984027050435543, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 511,  Mean reward: -2.5, Mean Entropy: 0.02082134038209915, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 512,  Mean reward: -1.1329113924050633, Mean Entropy: 0.044092755764722824, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 513,  Mean reward: -0.879746835443038, Mean Entropy: 0.12018077075481415, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 514,  Mean reward: -3.2837837837837838, Mean Entropy: 0.20526862144470215, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 515,  Mean reward: -2.265151515151515, Mean Entropy: 0.0899309366941452, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 516,  Mean reward: -1.876923076923077, Mean Entropy: 0.16463004052639008, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 517,  Mean reward: -2.41044776119403, Mean Entropy: 0.10871168971061707, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 518,  Mean reward: -2.388059701492537, Mean Entropy: 0.14042958617210388, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 519,  Mean reward: -2.423076923076923, Mean Entropy: 0.14403721690177917, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 520,  Mean reward: -3.537313432835821, Mean Entropy: 0.16620032489299774, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 521,  Mean reward: -5.901515151515151, Mean Entropy: 0.15370935201644897, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 522,  Mean reward: -1.6590909090909092, Mean Entropy: 0.09734748303890228, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 523,  Mean reward: -2.801470588235294, Mean Entropy: 0.02148466370999813, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 524,  Mean reward: -2.0, Mean Entropy: 0.08147537708282471, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 525,  Mean reward: -2.3141025641025643, Mean Entropy: 0.019495656713843346, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 526,  Mean reward: -1.5, Mean Entropy: 0.028439363464713097, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 527,  Mean reward: -1.3860759493670887, Mean Entropy: 0.020161651074886322, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 528,  Mean reward: -1.75, Mean Entropy: 0.036867327988147736, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 529,  Mean reward: -2.75, Mean Entropy: 0.03121374174952507, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 530,  Mean reward: -1.8924050632911393, Mean Entropy: 0.03343667834997177, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 531,  Mean reward: -0.75, Mean Entropy: 0.024398308247327805, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 532,  Mean reward: -3.411392405063291, Mean Entropy: 0.037124957889318466, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 533,  Mean reward: -1.0, Mean Entropy: 0.17866449058055878, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 534,  Mean reward: -1.1015625, Mean Entropy: 0.035632457584142685, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 535,  Mean reward: -4.566666666666666, Mean Entropy: 0.09484517574310303, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 536,  Mean reward: -2.6796875, Mean Entropy: 0.12474336475133896, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.74s
Iteration: 537,  Mean reward: -0.07575757575757576, Mean Entropy: 0.12330691516399384, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 538,  Mean reward: -1.7318840579710144, Mean Entropy: 0.08366172760725021, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 539,  Mean reward: -1.5461538461538462, Mean Entropy: 0.09057881683111191, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 540,  Mean reward: -1.3529411764705883, Mean Entropy: 0.08339540660381317, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 541,  Mean reward: -1.5692307692307692, Mean Entropy: 0.11420407146215439, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 542,  Mean reward: -2.5793650793650795, Mean Entropy: 0.09156283736228943, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 543,  Mean reward: -2.6796875, Mean Entropy: 0.09153309464454651, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 544,  Mean reward: -1.873015873015873, Mean Entropy: 0.1022331565618515, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 545,  Mean reward: -2.044776119402985, Mean Entropy: 0.10128109157085419, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 546,  Mean reward: -2.4461538461538463, Mean Entropy: 0.11230983585119247, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 547,  Mean reward: -1.6590909090909092, Mean Entropy: 0.12257091701030731, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 548,  Mean reward: -4.6692307692307695, Mean Entropy: 0.09597201645374298, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 549,  Mean reward: -1.6590909090909092, Mean Entropy: 0.1139521598815918, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 550,  Mean reward: -3.90625, Mean Entropy: 0.1182875782251358, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 551,  Mean reward: -1.0075757575757576, Mean Entropy: 0.11582398414611816, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 552,  Mean reward: -2.893939393939394, Mean Entropy: 0.14323753118515015, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 553,  Mean reward: -4.1953125, Mean Entropy: 0.12444446980953217, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 554,  Mean reward: -1.6470588235294117, Mean Entropy: 0.09690394252538681, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 555,  Mean reward: -4.53125, Mean Entropy: 0.14557257294654846, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 556,  Mean reward: -2.96875, Mean Entropy: 0.14599594473838806, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 557,  Mean reward: -4.801587301587301, Mean Entropy: 0.15676340460777283, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 558,  Mean reward: -1.625, Mean Entropy: 0.13186855614185333, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 559,  Mean reward: -1.6666666666666667, Mean Entropy: 0.11508165299892426, complete_episode_count: 69.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 560,  Mean reward: -2.893939393939394, Mean Entropy: 0.14815925061702728, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 561,  Mean reward: -3.477272727272727, Mean Entropy: 0.14722633361816406, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 562,  Mean reward: -2.8, Mean Entropy: 0.14745032787322998, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 563,  Mean reward: -1.792857142857143, Mean Entropy: 0.1747289001941681, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 564,  Mean reward: -1.5588235294117647, Mean Entropy: 0.16192331910133362, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 565,  Mean reward: -2.6417910447761193, Mean Entropy: 0.1613595187664032, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.90s
Iteration: 566,  Mean reward: -2.0671641791044775, Mean Entropy: 0.22071753442287445, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 567,  Mean reward: 0.9647887323943662, Mean Entropy: 0.19198215007781982, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 568,  Mean reward: -4.166666666666667, Mean Entropy: 0.2619691789150238, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 569,  Mean reward: -1.55, Mean Entropy: 0.18798232078552246, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 570,  Mean reward: -1.0147058823529411, Mean Entropy: 0.2135981321334839, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 571,  Mean reward: -2.713235294117647, Mean Entropy: 0.2049446552991867, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 572,  Mean reward: -3.7, Mean Entropy: 0.18892961740493774, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 573,  Mean reward: -1.6338028169014085, Mean Entropy: 0.13919591903686523, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 574,  Mean reward: -2.3857142857142857, Mean Entropy: 0.17910979688167572, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 575,  Mean reward: -0.5507246376811594, Mean Entropy: 0.15825006365776062, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 576,  Mean reward: -3.063380281690141, Mean Entropy: 0.22958645224571228, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 577,  Mean reward: -1.4328358208955223, Mean Entropy: 0.1719447374343872, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 578,  Mean reward: -0.21232876712328766, Mean Entropy: 0.15964196622371674, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 579,  Mean reward: -2.2, Mean Entropy: 0.024659618735313416, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 580,  Mean reward: -2.4140625, Mean Entropy: 0.037566155195236206, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 581,  Mean reward: -0.7954545454545454, Mean Entropy: 0.04393766447901726, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 582,  Mean reward: -4.663934426229508, Mean Entropy: 0.10559295117855072, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 583,  Mean reward: -5.119047619047619, Mean Entropy: 0.11777680367231369, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 584,  Mean reward: -1.7045454545454546, Mean Entropy: 0.07866702973842621, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 585,  Mean reward: 0.3208955223880597, Mean Entropy: 0.03744950145483017, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 586,  Mean reward: -1.2384615384615385, Mean Entropy: 0.028116784989833832, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 587,  Mean reward: -4.183333333333334, Mean Entropy: 0.07857482135295868, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 588,  Mean reward: -4.064516129032258, Mean Entropy: 0.14139476418495178, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 589,  Mean reward: -4.777777777777778, Mean Entropy: 0.14949791133403778, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 590,  Mean reward: -1.7318840579710144, Mean Entropy: 0.0992414653301239, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 591,  Mean reward: -1.8076923076923077, Mean Entropy: 0.06671630591154099, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 592,  Mean reward: -3.328125, Mean Entropy: 0.07034572213888168, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 593,  Mean reward: -2.287878787878788, Mean Entropy: 0.1253720223903656, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 594,  Mean reward: -2.4692307692307693, Mean Entropy: 0.13847020268440247, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 595,  Mean reward: -1.4202898550724639, Mean Entropy: 0.16466888785362244, complete_episode_count: 69.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 596,  Mean reward: -4.455223880597015, Mean Entropy: 0.16959193348884583, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 597,  Mean reward: -2.0785714285714287, Mean Entropy: 0.16623668372631073, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 598,  Mean reward: -1.4642857142857142, Mean Entropy: 0.14417597651481628, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 599,  Mean reward: -1.3287671232876712, Mean Entropy: 0.17197877168655396, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 600,  Mean reward: -1.8134328358208955, Mean Entropy: 0.15300506353378296, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.6285714285714286, Mean Entropy: 0.18081703782081604, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 602,  Mean reward: -3.537313432835821, Mean Entropy: 0.14993160963058472, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 603,  Mean reward: -0.2608695652173913, Mean Entropy: 0.13165932893753052, complete_episode_count: 69.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 604,  Mean reward: -2.801470588235294, Mean Entropy: 0.1329558938741684, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 605,  Mean reward: -0.7676056338028169, Mean Entropy: 0.16442985832691193, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 606,  Mean reward: -0.8819444444444444, Mean Entropy: 0.17964255809783936, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 607,  Mean reward: -1.5071428571428571, Mean Entropy: 0.14280185103416443, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 608,  Mean reward: -1.9285714285714286, Mean Entropy: 0.2087375521659851, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 609,  Mean reward: -2.3424657534246576, Mean Entropy: 0.17391738295555115, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 610,  Mean reward: -2.4411764705882355, Mean Entropy: 0.03583439439535141, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 611,  Mean reward: -3.0725806451612905, Mean Entropy: 0.06547914445400238, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 612,  Mean reward: -0.8692307692307693, Mean Entropy: 0.14844928681850433, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 613,  Mean reward: -2.485294117647059, Mean Entropy: 0.1665857881307602, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 614,  Mean reward: -2.548611111111111, Mean Entropy: 0.13070033490657806, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 615,  Mean reward: -0.879746835443038, Mean Entropy: 0.14467063546180725, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 616,  Mean reward: -2.3835616438356166, Mean Entropy: 0.131153404712677, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 617,  Mean reward: -1.0657894736842106, Mean Entropy: 0.1312416046857834, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 618,  Mean reward: -2.727272727272727, Mean Entropy: 0.07224835455417633, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 619,  Mean reward: -2.5705128205128207, Mean Entropy: 0.12513060867786407, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 620,  Mean reward: -2.987012987012987, Mean Entropy: 0.06909451633691788, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 621,  Mean reward: -1.2066666666666668, Mean Entropy: 0.09353108704090118, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 622,  Mean reward: -3.1904761904761907, Mean Entropy: 0.04689698666334152, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 623,  Mean reward: -4.806451612903226, Mean Entropy: 0.09280741214752197, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 624,  Mean reward: -1.2164179104477613, Mean Entropy: 0.03368420526385307, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 625,  Mean reward: -0.9076923076923077, Mean Entropy: 0.023962296545505524, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 626,  Mean reward: -0.6, Mean Entropy: 0.026725102216005325, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 627,  Mean reward: -4.541666666666667, Mean Entropy: 0.03654878959059715, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 628,  Mean reward: -2.078125, Mean Entropy: 0.025611869990825653, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 629,  Mean reward: -5.11864406779661, Mean Entropy: 0.025255193933844566, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 630,  Mean reward: -3.443548387096774, Mean Entropy: 0.02506444603204727, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 631,  Mean reward: -2.9206349206349205, Mean Entropy: 0.018252063542604446, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 632,  Mean reward: -4.360655737704918, Mean Entropy: 0.02242596261203289, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 633,  Mean reward: -4.008196721311475, Mean Entropy: 0.016490107402205467, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 634,  Mean reward: -1.4296875, Mean Entropy: 0.009068464860320091, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 635,  Mean reward: -1.7421875, Mean Entropy: 0.013364268466830254, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 636,  Mean reward: -1.8968253968253967, Mean Entropy: 0.035057127475738525, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 637,  Mean reward: -1.765625, Mean Entropy: 0.025106851011514664, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 638,  Mean reward: -2.622950819672131, Mean Entropy: 0.044017694890499115, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 639,  Mean reward: -1.40625, Mean Entropy: 0.02634337544441223, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 640,  Mean reward: -4.713114754098361, Mean Entropy: 0.014185996726155281, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 641,  Mean reward: -1.40625, Mean Entropy: 0.014925017021596432, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 642,  Mean reward: -1.5923076923076922, Mean Entropy: 0.016821768134832382, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 643,  Mean reward: -3.278688524590164, Mean Entropy: 0.03334620222449303, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 644,  Mean reward: -3.814516129032258, Mean Entropy: 0.033504974097013474, complete_episode_count: 62.0, Gather time: 0.70s, Train time: 1.38s
Iteration: 645,  Mean reward: -2.0546875, Mean Entropy: 0.03298372030258179, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 646,  Mean reward: -0.2923076923076923, Mean Entropy: 0.10839661210775375, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 647,  Mean reward: -1.6229508196721312, Mean Entropy: 0.09787261486053467, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 648,  Mean reward: -2.680327868852459, Mean Entropy: 0.10235840082168579, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 649,  Mean reward: -1.09375, Mean Entropy: 0.08697615563869476, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 650,  Mean reward: -1.1774193548387097, Mean Entropy: 0.178400456905365, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 651,  Mean reward: -1.036764705882353, Mean Entropy: 0.1682981252670288, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 652,  Mean reward: -0.7133333333333334, Mean Entropy: 0.261709600687027, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 653,  Mean reward: -1.7571428571428571, Mean Entropy: 0.34117215871810913, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 654,  Mean reward: -1.8986486486486487, Mean Entropy: 0.09434972703456879, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 655,  Mean reward: -1.8924050632911393, Mean Entropy: 0.11415442079305649, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 656,  Mean reward: -3.0833333333333335, Mean Entropy: 0.03666754066944122, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 657,  Mean reward: -1.75, Mean Entropy: 0.061339110136032104, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 658,  Mean reward: -2.9050632911392404, Mean Entropy: 0.06628365814685822, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 659,  Mean reward: -2.0448717948717947, Mean Entropy: 0.01307757943868637, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 660,  Mean reward: 0.25, Mean Entropy: 0.03728955239057541, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 661,  Mean reward: -2.5, Mean Entropy: 0.03444621339440346, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 662,  Mean reward: -1.0192307692307692, Mean Entropy: 0.05480574816465378, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 663,  Mean reward: -3.430379746835443, Mean Entropy: 0.04413635656237602, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 664,  Mean reward: -1.639240506329114, Mean Entropy: 0.022141102701425552, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 665,  Mean reward: -3.9240506329113924, Mean Entropy: 0.002835081657394767, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 666,  Mean reward: -3.0, Mean Entropy: 0.17428642511367798, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 667,  Mean reward: -3.7222222222222223, Mean Entropy: 0.19286052882671356, complete_episode_count: 72.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 668,  Mean reward: -3.2054794520547945, Mean Entropy: 0.07971084117889404, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 669,  Mean reward: -2.9078947368421053, Mean Entropy: 0.029247939586639404, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 670,  Mean reward: -3.430379746835443, Mean Entropy: 0.07120902836322784, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 671,  Mean reward: -3.3205128205128207, Mean Entropy: 0.007907690480351448, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 672,  Mean reward: -2.0, Mean Entropy: 0.0038343840278685093, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 673,  Mean reward: -1.25, Mean Entropy: 0.008482150733470917, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 674,  Mean reward: -1.0, Mean Entropy: 0.083036407828331, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 675,  Mean reward: -2.301282051282051, Mean Entropy: 0.061174772679805756, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 676,  Mean reward: -3.5, Mean Entropy: 0.03921077400445938, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 677,  Mean reward: -0.879746835443038, Mean Entropy: 0.07042448967695236, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 678,  Mean reward: -1.8012820512820513, Mean Entropy: 0.02666141465306282, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 679,  Mean reward: -2.5, Mean Entropy: 0.024059724062681198, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 680,  Mean reward: -1.75, Mean Entropy: 0.04668273776769638, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 681,  Mean reward: -1.639240506329114, Mean Entropy: 0.12986309826374054, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 682,  Mean reward: -0.3924050632911392, Mean Entropy: 0.25247520208358765, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 683,  Mean reward: -3.691780821917808, Mean Entropy: 0.24876122176647186, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 684,  Mean reward: -2.54, Mean Entropy: 0.11686667054891586, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 685,  Mean reward: -4.365384615384615, Mean Entropy: 0.08909255266189575, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 686,  Mean reward: -2.8076923076923075, Mean Entropy: 0.123715840280056, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 687,  Mean reward: -1.639240506329114, Mean Entropy: 0.0942184329032898, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 688,  Mean reward: -1.1883116883116882, Mean Entropy: 0.3377148509025574, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 689,  Mean reward: -0.9657534246575342, Mean Entropy: 0.2762843370437622, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 690,  Mean reward: -1.5136986301369864, Mean Entropy: 0.3175245523452759, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 691,  Mean reward: -2.4662162162162162, Mean Entropy: 0.27854424715042114, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 692,  Mean reward: -1.1216216216216217, Mean Entropy: 0.2703198194503784, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 693,  Mean reward: -0.2945205479452055, Mean Entropy: 0.2596394419670105, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 694,  Mean reward: -2.6315789473684212, Mean Entropy: 0.08294138312339783, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 695,  Mean reward: -3.301282051282051, Mean Entropy: 0.1192011833190918, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 696,  Mean reward: -1.9350649350649352, Mean Entropy: 0.1001134142279625, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 697,  Mean reward: -1.3860759493670887, Mean Entropy: 0.17919103801250458, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 698,  Mean reward: -3.076923076923077, Mean Entropy: 0.25087878108024597, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 699,  Mean reward: -2.651315789473684, Mean Entropy: 0.17993932962417603, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 700,  Mean reward: -2.207792207792208, Mean Entropy: 0.1634291112422943, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: 0.01282051282051282, Mean Entropy: 0.21407561004161835, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 702,  Mean reward: -0.24342105263157895, Mean Entropy: 0.2765713334083557, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 703,  Mean reward: -0.7285714285714285, Mean Entropy: 0.33536195755004883, complete_episode_count: 70.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 704,  Mean reward: -1.2602739726027397, Mean Entropy: 0.27511894702911377, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 705,  Mean reward: -0.5675675675675675, Mean Entropy: 0.2078825682401657, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 706,  Mean reward: -1.2884615384615385, Mean Entropy: 0.13120582699775696, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 707,  Mean reward: 0.1232876712328767, Mean Entropy: 0.1080816388130188, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 708,  Mean reward: -2.532051282051282, Mean Entropy: 0.01815956085920334, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 709,  Mean reward: -2.651898734177215, Mean Entropy: 0.0194680318236351, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 710,  Mean reward: -3.75, Mean Entropy: 0.003512426046654582, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 711,  Mean reward: -2.25, Mean Entropy: 0.004808667581528425, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 712,  Mean reward: -3.1582278481012658, Mean Entropy: 0.005546189844608307, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 713,  Mean reward: -2.25, Mean Entropy: 0.006213533692061901, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 714,  Mean reward: -0.75, Mean Entropy: 0.016706448048353195, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 715,  Mean reward: -2.5, Mean Entropy: 0.022277621552348137, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 716,  Mean reward: -3.392405063291139, Mean Entropy: 0.008702198043465614, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 717,  Mean reward: -3.25, Mean Entropy: 0.0027619213797152042, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 718,  Mean reward: -1.0, Mean Entropy: 0.004271442070603371, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 719,  Mean reward: -2.5, Mean Entropy: 0.010127766989171505, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 720,  Mean reward: 0.0, Mean Entropy: 0.0492248497903347, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 721,  Mean reward: -1.9113924050632911, Mean Entropy: 0.032733019441366196, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 722,  Mean reward: -3.0, Mean Entropy: 0.02027072012424469, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 723,  Mean reward: -1.0, Mean Entropy: 0.044151246547698975, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 724,  Mean reward: -1.75, Mean Entropy: 0.024206548929214478, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 725,  Mean reward: -2.3987341772151898, Mean Entropy: 0.0260881669819355, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 726,  Mean reward: -3.25, Mean Entropy: 0.01432054117321968, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 727,  Mean reward: -2.3987341772151898, Mean Entropy: 0.006429172120988369, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 728,  Mean reward: -3.75, Mean Entropy: 0.005459447856992483, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 729,  Mean reward: -2.0, Mean Entropy: 0.002728039864450693, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 730,  Mean reward: -1.25, Mean Entropy: 0.006776782684028149, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 731,  Mean reward: -2.1455696202531644, Mean Entropy: 0.005615153815597296, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 732,  Mean reward: -2.25, Mean Entropy: 0.016449637711048126, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 733,  Mean reward: -1.7820512820512822, Mean Entropy: 0.01731596514582634, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 734,  Mean reward: -0.75, Mean Entropy: 0.034152619540691376, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 735,  Mean reward: -2.8076923076923075, Mean Entropy: 0.01834159344434738, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 736,  Mean reward: -3.0, Mean Entropy: 0.008856400847434998, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 737,  Mean reward: -2.5, Mean Entropy: 0.005282487720251083, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 738,  Mean reward: -1.25, Mean Entropy: 0.004211781546473503, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 739,  Mean reward: -1.25, Mean Entropy: 0.012006568722426891, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 740,  Mean reward: -3.25, Mean Entropy: 0.0063140904530882835, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 741,  Mean reward: -3.1582278481012658, Mean Entropy: 0.0032630094792693853, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 742,  Mean reward: -2.0, Mean Entropy: 0.004109654575586319, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 743,  Mean reward: -3.0, Mean Entropy: 0.0027235483285039663, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 744,  Mean reward: -0.75, Mean Entropy: 0.010462647303938866, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 745,  Mean reward: -2.1455696202531644, Mean Entropy: 0.01961982622742653, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 746,  Mean reward: -1.25, Mean Entropy: 0.06322610378265381, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 747,  Mean reward: -2.0448717948717947, Mean Entropy: 0.04729606956243515, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 748,  Mean reward: -0.48717948717948717, Mean Entropy: 0.04465224966406822, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 749,  Mean reward: -0.25, Mean Entropy: 0.040090322494506836, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 750,  Mean reward: -3.430379746835443, Mean Entropy: 0.027975162491202354, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 751,  Mean reward: -1.0, Mean Entropy: 0.046678271144628525, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 752,  Mean reward: -0.5, Mean Entropy: 0.026318427175283432, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 753,  Mean reward: -0.5, Mean Entropy: 0.032368458807468414, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 754,  Mean reward: -1.0, Mean Entropy: 0.027826733887195587, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 755,  Mean reward: -0.75, Mean Entropy: 0.026682592928409576, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 756,  Mean reward: -1.25, Mean Entropy: 0.02313764952123165, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 757,  Mean reward: -4.430379746835443, Mean Entropy: 0.013435503467917442, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 758,  Mean reward: -1.75, Mean Entropy: 0.007307989057153463, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 759,  Mean reward: -0.75, Mean Entropy: 0.011160159483551979, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 760,  Mean reward: -1.25, Mean Entropy: 0.027903061360120773, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 761,  Mean reward: -1.75, Mean Entropy: 0.02576502226293087, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 762,  Mean reward: -1.639240506329114, Mean Entropy: 0.02044563554227352, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 763,  Mean reward: -0.75, Mean Entropy: 0.013176584616303444, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 764,  Mean reward: -0.5, Mean Entropy: 0.017428923398256302, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 765,  Mean reward: -2.0, Mean Entropy: 0.012690852396190166, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 766,  Mean reward: -2.75, Mean Entropy: 0.00709256436675787, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 767,  Mean reward: -2.75, Mean Entropy: 0.0046439580619335175, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 768,  Mean reward: -1.0, Mean Entropy: 0.003761275438591838, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 769,  Mean reward: -1.25, Mean Entropy: 0.005860473029315472, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 770,  Mean reward: -4.25, Mean Entropy: 0.0039221919141709805, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 771,  Mean reward: -2.0, Mean Entropy: 0.0030886956956237555, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 772,  Mean reward: -1.5, Mean Entropy: 0.004160548560321331, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 773,  Mean reward: -0.75, Mean Entropy: 0.011686067096889019, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 774,  Mean reward: -2.0, Mean Entropy: 0.01384245790541172, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 775,  Mean reward: -2.6582278481012658, Mean Entropy: 0.010241039097309113, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 776,  Mean reward: -1.639240506329114, Mean Entropy: 0.012031114660203457, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 777,  Mean reward: -0.75, Mean Entropy: 0.02142724022269249, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 778,  Mean reward: -2.301282051282051, Mean Entropy: 0.015730416402220726, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 779,  Mean reward: -1.5, Mean Entropy: 0.00970807857811451, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 780,  Mean reward: -3.75, Mean Entropy: 0.004112638533115387, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 781,  Mean reward: -2.0, Mean Entropy: 0.0029376125894486904, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 782,  Mean reward: -0.75, Mean Entropy: 0.004311400931328535, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 783,  Mean reward: -4.0, Mean Entropy: 0.004045090638101101, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 784,  Mean reward: -0.25, Mean Entropy: 0.0035027991980314255, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 785,  Mean reward: -4.0, Mean Entropy: 0.0025891230907291174, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 786,  Mean reward: 0.5, Mean Entropy: 0.0038769699167460203, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 787,  Mean reward: -2.75, Mean Entropy: 0.004377586301416159, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 788,  Mean reward: -1.0, Mean Entropy: 0.0074952272698283195, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 789,  Mean reward: -1.25, Mean Entropy: 0.011423705145716667, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 790,  Mean reward: -1.5, Mean Entropy: 0.014680298045277596, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 791,  Mean reward: -3.25, Mean Entropy: 0.010966189205646515, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 792,  Mean reward: -1.5, Mean Entropy: 0.011020774953067303, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 793,  Mean reward: -0.75, Mean Entropy: 0.01492262165993452, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 794,  Mean reward: 1.75, Mean Entropy: 0.018446166068315506, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 795,  Mean reward: -1.139240506329114, Mean Entropy: 0.01645829528570175, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 796,  Mean reward: -3.25, Mean Entropy: 0.0125120859593153, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 797,  Mean reward: -0.8987341772151899, Mean Entropy: 0.02791188657283783, complete_episode_count: 79.0, Gather time: 0.72s, Train time: 0.68s
Iteration: 798,  Mean reward: -1.639240506329114, Mean Entropy: 0.031378328800201416, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 799,  Mean reward: -0.8607594936708861, Mean Entropy: 0.021870478987693787, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 800,  Mean reward: -3.0, Mean Entropy: 0.01971849612891674, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: 0.25, Mean Entropy: 0.027841828763484955, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 802,  Mean reward: -1.0, Mean Entropy: 0.0215330570936203, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 803,  Mean reward: -1.6582278481012658, Mean Entropy: 0.013037875294685364, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 804,  Mean reward: -4.5, Mean Entropy: 0.008496584370732307, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 805,  Mean reward: -2.0, Mean Entropy: 0.010504079982638359, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 806,  Mean reward: -1.5, Mean Entropy: 0.009978675283491611, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 807,  Mean reward: -3.0, Mean Entropy: 0.008936071768403053, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 808,  Mean reward: -2.0, Mean Entropy: 0.008079389110207558, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 809,  Mean reward: -1.639240506329114, Mean Entropy: 0.005125008523464203, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 810,  Mean reward: -2.911392405063291, Mean Entropy: 0.006121603772044182, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 811,  Mean reward: -2.1455696202531644, Mean Entropy: 0.009371213614940643, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 812,  Mean reward: -3.0, Mean Entropy: 0.0057416497729718685, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 813,  Mean reward: -2.25, Mean Entropy: 0.006017087958753109, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 814,  Mean reward: -3.25, Mean Entropy: 0.0007135211490094662, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 815,  Mean reward: -2.5, Mean Entropy: 0.000717742252163589, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 816,  Mean reward: -0.75, Mean Entropy: 0.00460997736081481, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 817,  Mean reward: -0.75, Mean Entropy: 0.008730590343475342, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 818,  Mean reward: -0.37341772151898733, Mean Entropy: 0.009502647444605827, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 819,  Mean reward: -1.75, Mean Entropy: 0.005508051253855228, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 820,  Mean reward: -1.1329113924050633, Mean Entropy: 0.002868073992431164, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 821,  Mean reward: -0.75, Mean Entropy: 0.002062747720628977, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 822,  Mean reward: -3.25, Mean Entropy: 0.0015492960810661316, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 823,  Mean reward: -1.639240506329114, Mean Entropy: 0.001737296930514276, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 824,  Mean reward: 0.5, Mean Entropy: 0.0030702659860253334, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 825,  Mean reward: -1.75, Mean Entropy: 0.0034749172627925873, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 826,  Mean reward: -2.75, Mean Entropy: 0.0026876660995185375, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 827,  Mean reward: -0.5, Mean Entropy: 0.0030870435293763876, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 828,  Mean reward: -1.75, Mean Entropy: 0.0028107936959713697, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 829,  Mean reward: 0.5, Mean Entropy: 0.003639819100499153, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 830,  Mean reward: -1.75, Mean Entropy: 0.004102693870663643, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 831,  Mean reward: -2.25, Mean Entropy: 0.0031215455383062363, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 832,  Mean reward: -1.25, Mean Entropy: 0.0027011947240680456, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 833,  Mean reward: -1.25, Mean Entropy: 0.002610979601740837, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 834,  Mean reward: -1.75, Mean Entropy: 0.0026528513990342617, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 835,  Mean reward: -3.25, Mean Entropy: 0.0016871526604518294, complete_episode_count: 80.0, Gather time: 0.74s, Train time: 0.70s
Iteration: 836,  Mean reward: -2.5, Mean Entropy: 0.0012015437241643667, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 837,  Mean reward: -1.5, Mean Entropy: 0.0010512748267501593, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 838,  Mean reward: -3.0, Mean Entropy: 0.0009346194565296173, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 839,  Mean reward: -3.75, Mean Entropy: 0.000901270832400769, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 840,  Mean reward: -1.5, Mean Entropy: 0.0006022300804033875, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 841,  Mean reward: -2.0, Mean Entropy: 0.0010596424108371139, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 842,  Mean reward: -3.25, Mean Entropy: 0.0006799632683396339, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 843,  Mean reward: -2.25, Mean Entropy: 0.0008507230086252093, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 844,  Mean reward: -3.5, Mean Entropy: 0.0008462418336421251, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 845,  Mean reward: -1.25, Mean Entropy: 0.0007475944003090262, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 846,  Mean reward: -1.0, Mean Entropy: 0.0015396224334836006, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 847,  Mean reward: -2.75, Mean Entropy: 0.0015828016912564635, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 848,  Mean reward: -2.25, Mean Entropy: 0.0012114312266930938, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 849,  Mean reward: -1.75, Mean Entropy: 0.0013111790176481009, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 850,  Mean reward: -2.25, Mean Entropy: 0.0015003120061010122, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 851,  Mean reward: -1.75, Mean Entropy: 0.0016058774199336767, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 852,  Mean reward: -3.5, Mean Entropy: 0.001017916714772582, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 853,  Mean reward: -2.5, Mean Entropy: 0.0009490855736657977, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 854,  Mean reward: -2.0, Mean Entropy: 0.0009964643977582455, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 855,  Mean reward: -1.75, Mean Entropy: 0.0013235555961728096, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 856,  Mean reward: -0.5, Mean Entropy: 0.0040539326146245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 857,  Mean reward: -2.0, Mean Entropy: 0.005250214599072933, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 858,  Mean reward: -3.25, Mean Entropy: 0.005275479517877102, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 859,  Mean reward: -4.0, Mean Entropy: 0.0016018155729398131, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 860,  Mean reward: -1.5, Mean Entropy: 0.0030142851173877716, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 861,  Mean reward: -3.25, Mean Entropy: 0.0007128118886612356, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 862,  Mean reward: -1.75, Mean Entropy: 0.0011436459608376026, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 863,  Mean reward: -0.75, Mean Entropy: 0.005682923831045628, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 864,  Mean reward: -2.0, Mean Entropy: 0.0037543680518865585, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 865,  Mean reward: -2.0, Mean Entropy: 0.009066466242074966, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 866,  Mean reward: -3.0, Mean Entropy: 0.08646418899297714, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 867,  Mean reward: -1.1866666666666668, Mean Entropy: 0.005838792771100998, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.73s
Iteration: 868,  Mean reward: -1.75, Mean Entropy: 0.0028688970487564802, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 869,  Mean reward: -0.75, Mean Entropy: 0.00809359923005104, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 870,  Mean reward: -4.25, Mean Entropy: 0.0032611526548862457, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 871,  Mean reward: -4.5, Mean Entropy: 0.0011799620697274804, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 872,  Mean reward: -2.0, Mean Entropy: 0.04570695757865906, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.93s
Iteration: 873,  Mean reward: -2.826923076923077, Mean Entropy: 0.019999148324131966, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 874,  Mean reward: -1.1329113924050633, Mean Entropy: 0.026755884289741516, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 875,  Mean reward: -3.75, Mean Entropy: 0.003179457737132907, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 876,  Mean reward: -3.25, Mean Entropy: 0.08080936968326569, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 877,  Mean reward: -0.12025316455696203, Mean Entropy: 0.09484840929508209, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 878,  Mean reward: -3.151315789473684, Mean Entropy: 0.011498680338263512, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 879,  Mean reward: -1.75, Mean Entropy: 0.015003620646893978, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 880,  Mean reward: -1.3860759493670887, Mean Entropy: 0.014285571873188019, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 881,  Mean reward: -1.25, Mean Entropy: 0.008371111936867237, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 882,  Mean reward: -3.0, Mean Entropy: 0.016218004748225212, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 883,  Mean reward: -3.25, Mean Entropy: 0.09030282497406006, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 884,  Mean reward: -1.5256410256410255, Mean Entropy: 0.07014819979667664, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 885,  Mean reward: -3.33974358974359, Mean Entropy: 0.09593728184700012, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 886,  Mean reward: -4.045454545454546, Mean Entropy: 0.051097773015499115, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 887,  Mean reward: -3.2467532467532467, Mean Entropy: 0.027995243668556213, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 888,  Mean reward: 0.0, Mean Entropy: 0.012005304917693138, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 889,  Mean reward: -2.5, Mean Entropy: 0.004629136063158512, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 890,  Mean reward: -1.5, Mean Entropy: 0.006243817508220673, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 891,  Mean reward: -3.5, Mean Entropy: 0.01133166067302227, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 892,  Mean reward: -2.5, Mean Entropy: 0.016740841791033745, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 893,  Mean reward: -1.639240506329114, Mean Entropy: 0.012690248899161816, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 894,  Mean reward: -1.75, Mean Entropy: 0.011247302405536175, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 895,  Mean reward: -3.9177215189873418, Mean Entropy: 0.011832594871520996, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 896,  Mean reward: -3.1582278481012658, Mean Entropy: 0.01663227565586567, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 897,  Mean reward: -1.75, Mean Entropy: 0.010222828015685081, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 898,  Mean reward: -2.9050632911392404, Mean Entropy: 0.01080949418246746, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 899,  Mean reward: 0.25, Mean Entropy: 0.00641550961881876, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 900,  Mean reward: -0.5, Mean Entropy: 0.010446088388562202, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -1.25, Mean Entropy: 0.011319855228066444, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 902,  Mean reward: -2.0, Mean Entropy: 0.011864256113767624, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 903,  Mean reward: 0.25, Mean Entropy: 0.014869347214698792, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 904,  Mean reward: -2.75, Mean Entropy: 0.017046455293893814, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 905,  Mean reward: -3.411392405063291, Mean Entropy: 0.008188330568373203, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 906,  Mean reward: -0.75, Mean Entropy: 0.008728031069040298, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 907,  Mean reward: -2.75, Mean Entropy: 0.006968071684241295, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 908,  Mean reward: -2.5, Mean Entropy: 0.008371077477931976, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 909,  Mean reward: -2.75, Mean Entropy: 0.011687295511364937, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 910,  Mean reward: -2.5, Mean Entropy: 0.010560408234596252, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 911,  Mean reward: -4.0, Mean Entropy: 0.009806139394640923, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 912,  Mean reward: -0.75, Mean Entropy: 0.01088348776102066, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 913,  Mean reward: 0.5, Mean Entropy: 0.011008530855178833, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 914,  Mean reward: -3.25, Mean Entropy: 0.015466327778995037, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 915,  Mean reward: -3.5, Mean Entropy: 0.010655163787305355, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 916,  Mean reward: -2.25, Mean Entropy: 0.011539509519934654, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 917,  Mean reward: -1.5, Mean Entropy: 0.012107478454709053, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 918,  Mean reward: -2.3987341772151898, Mean Entropy: 0.008294424042105675, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 919,  Mean reward: -3.25, Mean Entropy: 0.005569647066295147, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 920,  Mean reward: -1.0, Mean Entropy: 0.017700817435979843, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 921,  Mean reward: -3.1582278481012658, Mean Entropy: 0.03098343312740326, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 922,  Mean reward: -3.25, Mean Entropy: 0.03341172635555267, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 923,  Mean reward: -1.25, Mean Entropy: 0.028373047709465027, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 924,  Mean reward: -1.5256410256410255, Mean Entropy: 0.024456672370433807, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 925,  Mean reward: -2.75, Mean Entropy: 0.02723681926727295, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 926,  Mean reward: -3.5961538461538463, Mean Entropy: 0.025189226493239403, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 927,  Mean reward: -2.75, Mean Entropy: 0.03675314038991928, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 928,  Mean reward: 0.11392405063291139, Mean Entropy: 0.01499521266669035, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 929,  Mean reward: -1.5, Mean Entropy: 0.015072355978190899, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 930,  Mean reward: 0.0, Mean Entropy: 0.01617824099957943, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 931,  Mean reward: -2.551282051282051, Mean Entropy: 0.026706606149673462, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 932,  Mean reward: -2.25, Mean Entropy: 0.021380797028541565, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 933,  Mean reward: -2.3987341772151898, Mean Entropy: 0.028478523716330528, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 934,  Mean reward: -1.639240506329114, Mean Entropy: 0.04821792244911194, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 935,  Mean reward: -0.12025316455696203, Mean Entropy: 0.04555462300777435, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 936,  Mean reward: -2.0, Mean Entropy: 0.021774347871541977, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 937,  Mean reward: -2.0, Mean Entropy: 0.015152720734477043, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 938,  Mean reward: -3.5, Mean Entropy: 0.009093426167964935, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 939,  Mean reward: -1.5, Mean Entropy: 0.008592592552304268, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 940,  Mean reward: -1.0, Mean Entropy: 0.01110153179615736, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 941,  Mean reward: -2.25, Mean Entropy: 0.013148434460163116, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 942,  Mean reward: -0.75, Mean Entropy: 0.021360374987125397, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 943,  Mean reward: -3.75, Mean Entropy: 0.013399243354797363, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 944,  Mean reward: -3.6645569620253164, Mean Entropy: 0.007668453734368086, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 945,  Mean reward: -3.0, Mean Entropy: 0.002720785094425082, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 946,  Mean reward: -1.5, Mean Entropy: 0.006230408325791359, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 947,  Mean reward: -3.75, Mean Entropy: 0.004267171956598759, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 948,  Mean reward: -2.0, Mean Entropy: 0.005989133380353451, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.93s
Iteration: 949,  Mean reward: -2.0, Mean Entropy: 0.007201475091278553, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 950,  Mean reward: -2.75, Mean Entropy: 0.008322995156049728, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 951,  Mean reward: -1.25, Mean Entropy: 0.014374121092259884, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 952,  Mean reward: -2.9050632911392404, Mean Entropy: 0.011129218153655529, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 953,  Mean reward: -3.0, Mean Entropy: 0.017995532602071762, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 954,  Mean reward: -1.0, Mean Entropy: 0.018355123698711395, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 955,  Mean reward: -1.5, Mean Entropy: 0.024105042219161987, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 956,  Mean reward: -1.5, Mean Entropy: 0.02775556407868862, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 957,  Mean reward: -1.0, Mean Entropy: 0.036625318229198456, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 958,  Mean reward: -2.25, Mean Entropy: 0.029007811099290848, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 959,  Mean reward: -3.1582278481012658, Mean Entropy: 0.015449278056621552, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 960,  Mean reward: -2.5, Mean Entropy: 0.013168244622647762, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 961,  Mean reward: -2.0, Mean Entropy: 0.011173026636242867, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 962,  Mean reward: -2.25, Mean Entropy: 0.010882323607802391, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 963,  Mean reward: -2.5, Mean Entropy: 0.013941816985607147, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 964,  Mean reward: -0.24358974358974358, Mean Entropy: 0.04150111600756645, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 965,  Mean reward: -1.0192307692307692, Mean Entropy: 0.06341882050037384, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 966,  Mean reward: -1.2692307692307692, Mean Entropy: 0.08884173631668091, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 967,  Mean reward: -1.6455696202531647, Mean Entropy: 0.09873228520154953, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 968,  Mean reward: -1.8355263157894737, Mean Entropy: 0.12031947076320648, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 969,  Mean reward: -3.2337662337662336, Mean Entropy: 0.1563141793012619, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 970,  Mean reward: -3.08974358974359, Mean Entropy: 0.14626717567443848, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 971,  Mean reward: -2.348684210526316, Mean Entropy: 0.08604705333709717, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 972,  Mean reward: -0.75, Mean Entropy: 0.06639590859413147, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 973,  Mean reward: -0.879746835443038, Mean Entropy: 0.10565084218978882, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 974,  Mean reward: -3.1582278481012658, Mean Entropy: 0.12415634095668793, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 975,  Mean reward: -2.0448717948717947, Mean Entropy: 0.08909352868795395, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 976,  Mean reward: -1.18, Mean Entropy: 0.1331993043422699, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 977,  Mean reward: -2.301282051282051, Mean Entropy: 0.02900448814034462, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 978,  Mean reward: -0.12025316455696203, Mean Entropy: 0.02593277394771576, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 979,  Mean reward: -1.5, Mean Entropy: 0.02298189327120781, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 980,  Mean reward: -1.1329113924050633, Mean Entropy: 0.03806677088141441, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 981,  Mean reward: -2.2012987012987013, Mean Entropy: 0.037325866520404816, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 982,  Mean reward: -3.25, Mean Entropy: 0.026595134288072586, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 983,  Mean reward: -2.651898734177215, Mean Entropy: 0.013446549884974957, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 984,  Mean reward: -3.1392405063291138, Mean Entropy: 0.00901569053530693, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 985,  Mean reward: -1.25, Mean Entropy: 0.012749407440423965, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 986,  Mean reward: -1.25, Mean Entropy: 0.0155112249776721, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 987,  Mean reward: -2.25, Mean Entropy: 0.029818732291460037, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 988,  Mean reward: 0.25, Mean Entropy: 0.04993307590484619, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 989,  Mean reward: -0.75, Mean Entropy: 0.052348144352436066, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 990,  Mean reward: 0.01282051282051282, Mean Entropy: 0.06051911041140556, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 991,  Mean reward: -3.411392405063291, Mean Entropy: 0.02935871295630932, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 992,  Mean reward: -2.0, Mean Entropy: 0.030745062977075577, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 993,  Mean reward: -1.0, Mean Entropy: 0.02448093704879284, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 994,  Mean reward: -1.6455696202531647, Mean Entropy: 0.01697290875017643, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 995,  Mean reward: -1.8924050632911393, Mean Entropy: 0.008426758460700512, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 996,  Mean reward: -2.75, Mean Entropy: 0.008062561973929405, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 997,  Mean reward: -0.5, Mean Entropy: 0.008517967537045479, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 998,  Mean reward: -1.639240506329114, Mean Entropy: 0.005455474369227886, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 999,  Mean reward: -2.5, Mean Entropy: 0.004494663793593645, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1000,  Mean reward: -3.1582278481012658, Mean Entropy: 0.0032809264957904816, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: -0.75, Mean Entropy: 0.002440420677885413, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1002,  Mean reward: -3.75, Mean Entropy: 0.0020861260127276182, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1003,  Mean reward: -1.25, Mean Entropy: 0.0022022284101694822, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1004,  Mean reward: -1.25, Mean Entropy: 0.0022579659707844257, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1005,  Mean reward: -1.75, Mean Entropy: 0.003247868036851287, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1006,  Mean reward: -3.1582278481012658, Mean Entropy: 0.001638885005377233, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1007,  Mean reward: -4.5, Mean Entropy: 0.0008414788171648979, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1008,  Mean reward: -1.25, Mean Entropy: 0.0015038184355944395, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1009,  Mean reward: -2.25, Mean Entropy: 0.0003633237211033702, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1010,  Mean reward: 0.25, Mean Entropy: 0.002113697584718466, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1011,  Mean reward: -1.75, Mean Entropy: 0.002044024644419551, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1012,  Mean reward: -2.0, Mean Entropy: 0.0019132680026814342, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1013,  Mean reward: -0.75, Mean Entropy: 0.0041814944706857204, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1014,  Mean reward: -2.25, Mean Entropy: 0.003745192429050803, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1015,  Mean reward: -1.25, Mean Entropy: 0.004509930498898029, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1016,  Mean reward: 0.25, Mean Entropy: 0.010984208434820175, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1017,  Mean reward: -1.8924050632911393, Mean Entropy: 0.013714591041207314, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1018,  Mean reward: -2.5, Mean Entropy: 0.04102649167180061, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1019,  Mean reward: -3.0641025641025643, Mean Entropy: 0.10376586765050888, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1020,  Mean reward: -2.7467532467532467, Mean Entropy: 0.17782896757125854, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1021,  Mean reward: 1.8513513513513513, Mean Entropy: 0.0854162871837616, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1022,  Mean reward: -2.176056338028169, Mean Entropy: 0.07675221562385559, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1023,  Mean reward: -1.5723684210526316, Mean Entropy: 0.12916673719882965, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1024,  Mean reward: -1.5921052631578947, Mean Entropy: 0.07124820351600647, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 1025,  Mean reward: -0.9090909090909091, Mean Entropy: 0.048857979476451874, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1026,  Mean reward: -2.651898734177215, Mean Entropy: 0.1936464011669159, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1027,  Mean reward: -3.2635135135135136, Mean Entropy: 0.23753754794597626, complete_episode_count: 74.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1028,  Mean reward: -1.875, Mean Entropy: 0.11415794491767883, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1029,  Mean reward: -1.6688311688311688, Mean Entropy: 0.0215641837567091, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1030,  Mean reward: -4.424050632911392, Mean Entropy: 0.014007746241986752, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1031,  Mean reward: -2.5, Mean Entropy: 0.005810785572975874, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1032,  Mean reward: -2.0, Mean Entropy: 0.007603259291499853, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1033,  Mean reward: -3.25, Mean Entropy: 0.007723379880189896, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1034,  Mean reward: -1.0, Mean Entropy: 0.006026633083820343, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1035,  Mean reward: -3.25, Mean Entropy: 0.007962960749864578, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1036,  Mean reward: -2.5, Mean Entropy: 0.008209951221942902, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1037,  Mean reward: -0.75, Mean Entropy: 0.00527157261967659, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1038,  Mean reward: -4.75, Mean Entropy: 0.00635374104604125, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1039,  Mean reward: -2.0, Mean Entropy: 0.007362111937254667, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1040,  Mean reward: -1.25, Mean Entropy: 0.005056486465036869, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1041,  Mean reward: -2.5, Mean Entropy: 0.008008332923054695, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1042,  Mean reward: -4.5, Mean Entropy: 0.004353113006800413, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1043,  Mean reward: -0.8607594936708861, Mean Entropy: 0.004514411091804504, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1044,  Mean reward: -1.639240506329114, Mean Entropy: 0.002150067128241062, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1045,  Mean reward: -3.0, Mean Entropy: 0.00533319590613246, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1046,  Mean reward: -3.75, Mean Entropy: 0.00457149650901556, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1047,  Mean reward: -2.75, Mean Entropy: 0.0033908882178366184, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1048,  Mean reward: -0.25, Mean Entropy: 0.002584329340606928, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1049,  Mean reward: -2.75, Mean Entropy: 0.004063760861754417, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1050,  Mean reward: -2.0, Mean Entropy: 0.005681504961103201, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1051,  Mean reward: -2.75, Mean Entropy: 0.004866837523877621, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1052,  Mean reward: 0.0, Mean Entropy: 0.008355843834578991, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1053,  Mean reward: -2.0, Mean Entropy: 0.009400670416653156, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1054,  Mean reward: -3.25, Mean Entropy: 0.006510256789624691, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1055,  Mean reward: -3.392405063291139, Mean Entropy: 0.0021591305267065763, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1056,  Mean reward: -1.0, Mean Entropy: 0.001762985484674573, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1057,  Mean reward: -2.5, Mean Entropy: 0.0024163657799363136, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1058,  Mean reward: -1.75, Mean Entropy: 0.004848170559853315, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1059,  Mean reward: -1.25, Mean Entropy: 0.012776315212249756, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1060,  Mean reward: -0.879746835443038, Mean Entropy: 0.01815837249159813, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1061,  Mean reward: -1.3860759493670887, Mean Entropy: 0.015080554410815239, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1062,  Mean reward: -2.5, Mean Entropy: 0.00877013336867094, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 1063,  Mean reward: -0.5, Mean Entropy: 0.009174789302051067, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1064,  Mean reward: -2.75, Mean Entropy: 0.009599074721336365, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1065,  Mean reward: -1.0, Mean Entropy: 0.009583884850144386, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1066,  Mean reward: -4.0, Mean Entropy: 0.01103177946060896, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1067,  Mean reward: -2.25, Mean Entropy: 0.01152607798576355, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1068,  Mean reward: -2.3987341772151898, Mean Entropy: 0.0043055107817053795, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1069,  Mean reward: -3.25, Mean Entropy: 0.001630835235118866, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1070,  Mean reward: -2.25, Mean Entropy: 0.0025564366951584816, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1071,  Mean reward: -1.75, Mean Entropy: 0.0013702432624995708, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1072,  Mean reward: -1.5, Mean Entropy: 0.012666280381381512, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1073,  Mean reward: -1.1329113924050633, Mean Entropy: 0.019221093505620956, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1074,  Mean reward: -1.75, Mean Entropy: 0.025264576077461243, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1075,  Mean reward: -0.5, Mean Entropy: 0.02662324719130993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1076,  Mean reward: -2.75, Mean Entropy: 0.0263584665954113, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1077,  Mean reward: -4.089743589743589, Mean Entropy: 0.04462072253227234, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1078,  Mean reward: -1.0, Mean Entropy: 0.057096850126981735, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1079,  Mean reward: -1.3860759493670887, Mean Entropy: 0.0721818283200264, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1080,  Mean reward: -0.12025316455696203, Mean Entropy: 0.05177708715200424, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1081,  Mean reward: -2.25, Mean Entropy: 0.03575463593006134, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1082,  Mean reward: -0.75, Mean Entropy: 0.03454630821943283, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1083,  Mean reward: -1.0, Mean Entropy: 0.03512721508741379, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1084,  Mean reward: -1.5256410256410255, Mean Entropy: 0.04607706889510155, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1085,  Mean reward: -1.639240506329114, Mean Entropy: 0.05740218609571457, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1086,  Mean reward: -1.3860759493670887, Mean Entropy: 0.07033280283212662, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1087,  Mean reward: -2.1645569620253164, Mean Entropy: 0.07683828473091125, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1088,  Mean reward: -0.4807692307692308, Mean Entropy: 0.05866041034460068, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1089,  Mean reward: -0.75, Mean Entropy: 0.046742748469114304, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1090,  Mean reward: -1.7820512820512822, Mean Entropy: 0.03388143703341484, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1091,  Mean reward: 0.00641025641025641, Mean Entropy: 0.03247590363025665, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1092,  Mean reward: -1.0128205128205128, Mean Entropy: 0.027143364772200584, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1093,  Mean reward: -0.6265822784810127, Mean Entropy: 0.022731101140379906, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1094,  Mean reward: -2.0, Mean Entropy: 0.02689904347062111, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1095,  Mean reward: -0.26282051282051283, Mean Entropy: 0.019382916390895844, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1096,  Mean reward: 0.25, Mean Entropy: 0.016864951699972153, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1097,  Mean reward: -2.25, Mean Entropy: 0.013407009653747082, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1098,  Mean reward: -3.5, Mean Entropy: 0.012455841526389122, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1099,  Mean reward: -0.879746835443038, Mean Entropy: 0.009776337072253227, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1100,  Mean reward: -2.0, Mean Entropy: 0.0051323720254004, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: 0.0, Mean Entropy: 0.008141446858644485, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1102,  Mean reward: -1.25, Mean Entropy: 0.009139273315668106, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1103,  Mean reward: -1.8924050632911393, Mean Entropy: 0.006089876405894756, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1104,  Mean reward: -2.5, Mean Entropy: 0.0039882175624370575, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1105,  Mean reward: -1.75, Mean Entropy: 0.002794303698465228, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1106,  Mean reward: -1.25, Mean Entropy: 0.00307583250105381, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1107,  Mean reward: -1.75, Mean Entropy: 0.003982951864600182, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1108,  Mean reward: -0.75, Mean Entropy: 0.0056955330073833466, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1109,  Mean reward: -2.0, Mean Entropy: 0.006550959777086973, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1110,  Mean reward: -1.25, Mean Entropy: 0.007018923293799162, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1111,  Mean reward: -1.5, Mean Entropy: 0.006751225795596838, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1112,  Mean reward: -4.5, Mean Entropy: 0.004753374494612217, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1113,  Mean reward: -2.651898734177215, Mean Entropy: 0.008148816414177418, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1114,  Mean reward: -3.6645569620253164, Mean Entropy: 0.008028339594602585, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1115,  Mean reward: -2.0, Mean Entropy: 0.016563959419727325, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1116,  Mean reward: -1.8924050632911393, Mean Entropy: 0.0409107506275177, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1117,  Mean reward: -1.3860759493670887, Mean Entropy: 0.027123011648654938, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1118,  Mean reward: -2.0, Mean Entropy: 0.01713293418288231, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1119,  Mean reward: -3.0, Mean Entropy: 0.013938769698143005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.72s
Iteration: 1120,  Mean reward: -1.5, Mean Entropy: 0.01358766108751297, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1121,  Mean reward: -2.911392405063291, Mean Entropy: 0.023645907640457153, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1122,  Mean reward: -1.75, Mean Entropy: 0.024886399507522583, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1123,  Mean reward: -1.25, Mean Entropy: 0.027074642479419708, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1124,  Mean reward: -1.5, Mean Entropy: 0.02486674301326275, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1125,  Mean reward: -1.75, Mean Entropy: 0.024066610261797905, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1126,  Mean reward: -2.1455696202531644, Mean Entropy: 0.03719586879014969, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1127,  Mean reward: -1.3860759493670887, Mean Entropy: 0.054639868438243866, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1128,  Mean reward: -2.0576923076923075, Mean Entropy: 0.0731242224574089, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1129,  Mean reward: -2.2948717948717947, Mean Entropy: 0.08072328567504883, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1130,  Mean reward: -1.9285714285714286, Mean Entropy: 0.05872838944196701, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1131,  Mean reward: -1.544871794871795, Mean Entropy: 0.06621718406677246, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1132,  Mean reward: -1.7820512820512822, Mean Entropy: 0.06018200144171715, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1133,  Mean reward: -3.2467532467532467, Mean Entropy: 0.03694572299718857, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1134,  Mean reward: -2.3987341772151898, Mean Entropy: 0.03480073809623718, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1135,  Mean reward: -2.5, Mean Entropy: 0.0333971343934536, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1136,  Mean reward: -2.651898734177215, Mean Entropy: 0.05021342635154724, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1137,  Mean reward: -1.8924050632911393, Mean Entropy: 0.07733078300952911, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1138,  Mean reward: -2.3684210526315788, Mean Entropy: 0.07669005542993546, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 1139,  Mean reward: -2.4177215189873418, Mean Entropy: 0.030435416847467422, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1140,  Mean reward: -4.326923076923077, Mean Entropy: 6.887890049256384e-05, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1141,  Mean reward: -3.25, Mean Entropy: 8.064070243563037e-06, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1142,  Mean reward: -1.5, Mean Entropy: 0.0001105125920730643, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1143,  Mean reward: -1.0, Mean Entropy: 0.006965033710002899, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1144,  Mean reward: -0.5, Mean Entropy: 0.05392811819911003, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1145,  Mean reward: -1.8924050632911393, Mean Entropy: 0.03825763612985611, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1146,  Mean reward: 0.5, Mean Entropy: 0.0324399471282959, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1147,  Mean reward: -2.1455696202531644, Mean Entropy: 0.034345708787441254, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1148,  Mean reward: -1.8924050632911393, Mean Entropy: 0.04536505043506622, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1149,  Mean reward: -3.5064935064935066, Mean Entropy: 0.05407276749610901, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1150,  Mean reward: -3.9177215189873418, Mean Entropy: 0.06800815463066101, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1151,  Mean reward: -2.5, Mean Entropy: 0.07127026468515396, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1152,  Mean reward: -2.8076923076923075, Mean Entropy: 0.05799947306513786, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1153,  Mean reward: -1.8924050632911393, Mean Entropy: 0.04199131578207016, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1154,  Mean reward: -1.639240506329114, Mean Entropy: 0.04656326025724411, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1155,  Mean reward: -3.3205128205128207, Mean Entropy: 0.06047917157411575, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1156,  Mean reward: 0.14935064935064934, Mean Entropy: 0.053021129220724106, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1157,  Mean reward: -2.25, Mean Entropy: 0.034458328038454056, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1158,  Mean reward: -3.6645569620253164, Mean Entropy: 0.01913132518529892, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1159,  Mean reward: -3.5, Mean Entropy: 0.012619837187230587, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1160,  Mean reward: -0.5, Mean Entropy: 0.012894568964838982, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1161,  Mean reward: -2.551282051282051, Mean Entropy: 0.011653831228613853, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1162,  Mean reward: -1.5, Mean Entropy: 0.013887646608054638, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1163,  Mean reward: -0.75, Mean Entropy: 0.028009530156850815, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1164,  Mean reward: -1.0, Mean Entropy: 0.03075394034385681, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1165,  Mean reward: -2.9675324675324677, Mean Entropy: 0.024156248196959496, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1166,  Mean reward: -2.8076923076923075, Mean Entropy: 0.01781655102968216, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1167,  Mean reward: -5.75, Mean Entropy: 0.010465066879987717, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1168,  Mean reward: -1.8924050632911393, Mean Entropy: 0.018734123557806015, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1169,  Mean reward: -1.5, Mean Entropy: 0.01835673674941063, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1170,  Mean reward: -1.25, Mean Entropy: 0.021536987274885178, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1171,  Mean reward: -2.3987341772151898, Mean Entropy: 0.03766363859176636, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1172,  Mean reward: -3.5, Mean Entropy: 0.03526509925723076, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1173,  Mean reward: -3.0, Mean Entropy: 0.031471699476242065, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1174,  Mean reward: -0.12025316455696203, Mean Entropy: 0.02374844253063202, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1175,  Mean reward: -1.0128205128205128, Mean Entropy: 0.03191179782152176, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1176,  Mean reward: -2.1265822784810124, Mean Entropy: 0.047791894525289536, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.84s
Iteration: 1177,  Mean reward: -2.25, Mean Entropy: 0.040673598647117615, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1178,  Mean reward: -2.0, Mean Entropy: 0.022138938307762146, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1179,  Mean reward: -1.0, Mean Entropy: 0.026081323623657227, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1180,  Mean reward: -2.75, Mean Entropy: 0.0324283130466938, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1181,  Mean reward: -2.25, Mean Entropy: 0.03479886054992676, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1182,  Mean reward: -1.5256410256410255, Mean Entropy: 0.043966539204120636, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1183,  Mean reward: -2.651898734177215, Mean Entropy: 0.043069832026958466, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1184,  Mean reward: -3.9177215189873418, Mean Entropy: 0.01763252541422844, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1185,  Mean reward: -4.0, Mean Entropy: 0.030836567282676697, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1186,  Mean reward: -2.2948717948717947, Mean Entropy: 0.031402844935655594, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1187,  Mean reward: -1.544871794871795, Mean Entropy: 0.017925409600138664, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1188,  Mean reward: -2.5, Mean Entropy: 0.012279637157917023, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1189,  Mean reward: -2.75, Mean Entropy: 0.012598119676113129, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1190,  Mean reward: -2.0, Mean Entropy: 0.010582350194454193, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1191,  Mean reward: -2.188311688311688, Mean Entropy: 0.0065206680446863174, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1192,  Mean reward: -2.75, Mean Entropy: 0.012277588248252869, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1193,  Mean reward: -2.25, Mean Entropy: 0.021023189648985863, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1194,  Mean reward: -1.75, Mean Entropy: 0.02430744096636772, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1195,  Mean reward: -3.411392405063291, Mean Entropy: 0.027430932968854904, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1196,  Mean reward: -0.75, Mean Entropy: 0.027218332514166832, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1197,  Mean reward: -1.5, Mean Entropy: 0.029247306287288666, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1198,  Mean reward: -1.3860759493670887, Mean Entropy: 0.023322831839323044, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1199,  Mean reward: -2.2948717948717947, Mean Entropy: 0.015538400039076805, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1200,  Mean reward: -2.1455696202531644, Mean Entropy: 0.012700675055384636, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: -2.0, Mean Entropy: 0.008816535584628582, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1202,  Mean reward: -3.0, Mean Entropy: 0.00960075668990612, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1203,  Mean reward: -3.0, Mean Entropy: 0.006426946725696325, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1204,  Mean reward: -2.0, Mean Entropy: 0.007111358921974897, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1205,  Mean reward: -2.25, Mean Entropy: 0.009426448494195938, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1206,  Mean reward: -2.75, Mean Entropy: 0.009640375152230263, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1207,  Mean reward: -1.0, Mean Entropy: 0.008112379349768162, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1208,  Mean reward: -0.879746835443038, Mean Entropy: 0.00845367182046175, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1209,  Mean reward: -3.0, Mean Entropy: 0.008288823068141937, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1210,  Mean reward: -2.75, Mean Entropy: 0.006050420459359884, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1211,  Mean reward: -0.25, Mean Entropy: 0.006732320412993431, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1212,  Mean reward: 0.0, Mean Entropy: 0.013786779716610909, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1213,  Mean reward: -2.75, Mean Entropy: 0.012700185179710388, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1214,  Mean reward: -1.5, Mean Entropy: 0.014856718480587006, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 1215,  Mean reward: -1.7820512820512822, Mean Entropy: 0.011504722759127617, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1216,  Mean reward: -4.5, Mean Entropy: 0.009905113838613033, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1217,  Mean reward: -2.25, Mean Entropy: 0.0060426159761846066, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.65s
Iteration: 1218,  Mean reward: -1.25, Mean Entropy: 0.00647817924618721, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1219,  Mean reward: -2.75, Mean Entropy: 0.007159039378166199, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1220,  Mean reward: -1.0, Mean Entropy: 0.008093531243503094, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1221,  Mean reward: 0.0, Mean Entropy: 0.0093994764611125, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1222,  Mean reward: -1.5, Mean Entropy: 0.013609535992145538, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1223,  Mean reward: -2.0, Mean Entropy: 0.016867734491825104, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1224,  Mean reward: -2.0, Mean Entropy: 0.014878936111927032, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1225,  Mean reward: -3.5, Mean Entropy: 0.010019080713391304, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1226,  Mean reward: -1.5, Mean Entropy: 0.008816744200885296, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1227,  Mean reward: -3.0, Mean Entropy: 0.005729212425649166, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1228,  Mean reward: -1.25, Mean Entropy: 0.00612148642539978, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1229,  Mean reward: -1.75, Mean Entropy: 0.007772255688905716, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1230,  Mean reward: -2.0, Mean Entropy: 0.007692668121308088, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1231,  Mean reward: -3.0, Mean Entropy: 0.005849529057741165, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1232,  Mean reward: -3.0, Mean Entropy: 0.0009977410081773996, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1233,  Mean reward: -3.5, Mean Entropy: 7.527628622483462e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1234,  Mean reward: -1.5, Mean Entropy: 0.00026380392955616117, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1235,  Mean reward: -2.75, Mean Entropy: 0.0023731845431029797, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1236,  Mean reward: -0.5, Mean Entropy: 0.003953958861529827, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1237,  Mean reward: -2.75, Mean Entropy: 0.009870655834674835, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1238,  Mean reward: -1.0, Mean Entropy: 0.006531659513711929, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1239,  Mean reward: -3.6645569620253164, Mean Entropy: 0.009396817535161972, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1240,  Mean reward: -3.25, Mean Entropy: 0.0003283681289758533, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1241,  Mean reward: -0.5, Mean Entropy: 0.0006498792208731174, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1242,  Mean reward: -2.75, Mean Entropy: 0.0020864270627498627, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1243,  Mean reward: -2.5, Mean Entropy: 0.0023608410265296698, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1244,  Mean reward: -2.5, Mean Entropy: 0.0034878437872976065, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1245,  Mean reward: -2.0, Mean Entropy: 0.005317916627973318, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1246,  Mean reward: -2.0, Mean Entropy: 0.005918845534324646, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 1247,  Mean reward: -3.75, Mean Entropy: 0.002878418890759349, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1248,  Mean reward: -2.75, Mean Entropy: 0.0008897585212253034, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1249,  Mean reward: -2.75, Mean Entropy: 0.0005763382068835199, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1250,  Mean reward: -0.5, Mean Entropy: 0.0026759332977235317, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1251,  Mean reward: -2.75, Mean Entropy: 0.004294365178793669, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1252,  Mean reward: -3.0, Mean Entropy: 0.0040947264060378075, complete_episode_count: 80.0, Gather time: 0.68s, Train time: 0.69s
Iteration: 1253,  Mean reward: -0.5, Mean Entropy: 0.0048647671937942505, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1254,  Mean reward: -1.75, Mean Entropy: 0.0059063620865345, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1255,  Mean reward: -2.1455696202531644, Mean Entropy: 0.0029468717984855175, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1256,  Mean reward: -2.0, Mean Entropy: 0.0022967541590332985, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1257,  Mean reward: -1.639240506329114, Mean Entropy: 0.0041906097903847694, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1258,  Mean reward: -1.5, Mean Entropy: 0.006945830769836903, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1259,  Mean reward: -1.8924050632911393, Mean Entropy: 0.013118887320160866, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1260,  Mean reward: -1.75, Mean Entropy: 0.016810651868581772, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1261,  Mean reward: -2.25, Mean Entropy: 0.018251698464155197, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1262,  Mean reward: -1.75, Mean Entropy: 0.018933426588773727, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1263,  Mean reward: -1.25, Mean Entropy: 0.019482307136058807, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1264,  Mean reward: -2.5, Mean Entropy: 0.016751661896705627, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1265,  Mean reward: -2.5, Mean Entropy: 0.01891355775296688, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1266,  Mean reward: -2.826923076923077, Mean Entropy: 0.020298026502132416, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1267,  Mean reward: -1.639240506329114, Mean Entropy: 0.025983169674873352, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1268,  Mean reward: -2.3987341772151898, Mean Entropy: 0.021089471876621246, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1269,  Mean reward: -2.5, Mean Entropy: 0.011669899336993694, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1270,  Mean reward: -3.25, Mean Entropy: 0.00446988083422184, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1271,  Mean reward: 0.5, Mean Entropy: 0.006413568742573261, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1272,  Mean reward: -1.75, Mean Entropy: 0.00791519321501255, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1273,  Mean reward: -0.25, Mean Entropy: 0.015593240968883038, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1274,  Mean reward: -0.75, Mean Entropy: 0.015493854880332947, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1275,  Mean reward: -2.25, Mean Entropy: 0.018190953880548477, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1276,  Mean reward: -2.0, Mean Entropy: 0.016376452520489693, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1277,  Mean reward: -2.8076923076923075, Mean Entropy: 0.03870686888694763, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1278,  Mean reward: -2.0384615384615383, Mean Entropy: 0.040520790964365005, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1279,  Mean reward: -2.532051282051282, Mean Entropy: 0.05870512127876282, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1280,  Mean reward: -0.6265822784810127, Mean Entropy: 0.05825096368789673, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1281,  Mean reward: -2.9240506329113924, Mean Entropy: 0.0656602680683136, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1282,  Mean reward: -1.4050632911392404, Mean Entropy: 0.040585607290267944, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1283,  Mean reward: -1.0, Mean Entropy: 0.029784653335809708, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 1284,  Mean reward: -2.1265822784810124, Mean Entropy: 0.024992352351546288, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1285,  Mean reward: -3.5, Mean Entropy: 0.022110454738140106, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1286,  Mean reward: -2.25, Mean Entropy: 0.02034863829612732, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1287,  Mean reward: 0.0, Mean Entropy: 0.02270352840423584, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1288,  Mean reward: -2.25, Mean Entropy: 0.022650746628642082, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1289,  Mean reward: -2.551282051282051, Mean Entropy: 0.01814393512904644, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1290,  Mean reward: -0.6265822784810127, Mean Entropy: 0.025419920682907104, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1291,  Mean reward: -1.7884615384615385, Mean Entropy: 0.07013626396656036, complete_episode_count: 78.0, Gather time: 0.64s, Train time: 0.67s
Iteration: 1292,  Mean reward: -0.6265822784810127, Mean Entropy: 0.05629752203822136, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1293,  Mean reward: -0.37341772151898733, Mean Entropy: 0.06097252666950226, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1294,  Mean reward: -1.6688311688311688, Mean Entropy: 0.04532979428768158, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1295,  Mean reward: -2.3987341772151898, Mean Entropy: 0.06954258680343628, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1296,  Mean reward: 0.01282051282051282, Mean Entropy: 0.10774438828229904, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1297,  Mean reward: -1.0320512820512822, Mean Entropy: 0.07476790994405746, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1298,  Mean reward: -2.0576923076923075, Mean Entropy: 0.05652560293674469, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1299,  Mean reward: -0.25, Mean Entropy: 0.06558340787887573, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1300,  Mean reward: -2.8461538461538463, Mean Entropy: 0.06989061087369919, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.67s
rec seq len 2
actor lr 0.0005
Iteration: 1301,  Mean reward: -0.25, Mean Entropy: 0.04229503124952316, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1302,  Mean reward: -1.8924050632911393, Mean Entropy: 0.025171827524900436, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1303,  Mean reward: -2.9050632911392404, Mean Entropy: 0.03169901669025421, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1304,  Mean reward: -3.0641025641025643, Mean Entropy: 0.04620707780122757, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1305,  Mean reward: -2.651898734177215, Mean Entropy: 0.029631804674863815, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1306,  Mean reward: -2.25, Mean Entropy: 0.028383266180753708, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1307,  Mean reward: -1.5, Mean Entropy: 0.02787572145462036, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1308,  Mean reward: -1.544871794871795, Mean Entropy: 0.03539709374308586, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1309,  Mean reward: -0.25, Mean Entropy: 0.02446441911160946, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1310,  Mean reward: -1.8924050632911393, Mean Entropy: 0.018790028989315033, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1311,  Mean reward: -1.5, Mean Entropy: 0.013385292142629623, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1312,  Mean reward: -1.75, Mean Entropy: 0.012444382533431053, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1313,  Mean reward: -2.0, Mean Entropy: 0.012349164113402367, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1314,  Mean reward: -2.25, Mean Entropy: 0.012172361835837364, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1315,  Mean reward: 0.01282051282051282, Mean Entropy: 0.01041340921074152, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1316,  Mean reward: 0.0, Mean Entropy: 0.012363678775727749, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1317,  Mean reward: -4.170886075949367, Mean Entropy: 0.011282059364020824, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1318,  Mean reward: -2.25, Mean Entropy: 0.008065931499004364, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1319,  Mean reward: -3.0, Mean Entropy: 0.007766016758978367, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1320,  Mean reward: -3.0, Mean Entropy: 0.008413439616560936, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1321,  Mean reward: -0.5, Mean Entropy: 0.007206002250313759, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1322,  Mean reward: -1.5, Mean Entropy: 0.007382327690720558, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1323,  Mean reward: -2.25, Mean Entropy: 0.007471776567399502, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1324,  Mean reward: -1.0, Mean Entropy: 0.007722466718405485, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1325,  Mean reward: -0.5, Mean Entropy: 0.008571891114115715, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1326,  Mean reward: -1.5, Mean Entropy: 0.008674963377416134, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1327,  Mean reward: -2.0, Mean Entropy: 0.008762327022850513, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1328,  Mean reward: -1.25, Mean Entropy: 0.007538105361163616, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 1329,  Mean reward: -2.75, Mean Entropy: 0.007499624043703079, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1330,  Mean reward: -1.25, Mean Entropy: 0.007660518400371075, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1331,  Mean reward: -1.8924050632911393, Mean Entropy: 0.014018643647432327, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1332,  Mean reward: 0.5, Mean Entropy: 0.016783742234110832, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1333,  Mean reward: -1.0, Mean Entropy: 0.017129123210906982, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1334,  Mean reward: -2.75, Mean Entropy: 0.018181754276156425, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1335,  Mean reward: -2.25, Mean Entropy: 0.01862896792590618, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1336,  Mean reward: -0.6265822784810127, Mean Entropy: 0.02835334837436676, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1337,  Mean reward: -2.0, Mean Entropy: 0.034802600741386414, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1338,  Mean reward: -1.25, Mean Entropy: 0.03704221546649933, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1339,  Mean reward: -1.3860759493670887, Mean Entropy: 0.05196092650294304, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1340,  Mean reward: -1.8924050632911393, Mean Entropy: 0.0831713154911995, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1341,  Mean reward: -3.487012987012987, Mean Entropy: 0.08125320076942444, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1342,  Mean reward: -3.3205128205128207, Mean Entropy: 0.07956500351428986, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1343,  Mean reward: -1.544871794871795, Mean Entropy: 0.04614846035838127, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1344,  Mean reward: -3.5, Mean Entropy: 0.023163719102740288, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1345,  Mean reward: -3.411392405063291, Mean Entropy: 0.043231233954429626, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1346,  Mean reward: -0.37341772151898733, Mean Entropy: 0.10961854457855225, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1347,  Mean reward: -2.098684210526316, Mean Entropy: 0.07854875177145004, complete_episode_count: 76.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1348,  Mean reward: -2.1455696202531644, Mean Entropy: 0.09698836505413055, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1349,  Mean reward: -3.4935064935064934, Mean Entropy: 0.06651826202869415, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1350,  Mean reward: -2.3141025641025643, Mean Entropy: 0.03411705791950226, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1351,  Mean reward: 0.6392405063291139, Mean Entropy: 0.036979638040065765, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1352,  Mean reward: -0.25, Mean Entropy: 0.0434180423617363, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1353,  Mean reward: -1.7820512820512822, Mean Entropy: 0.03759859874844551, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1354,  Mean reward: -2.651898734177215, Mean Entropy: 0.061044853180646896, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1355,  Mean reward: -1.3860759493670887, Mean Entropy: 0.08826249092817307, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1356,  Mean reward: -4.089743589743589, Mean Entropy: 0.10115208476781845, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1357,  Mean reward: -2.9675324675324677, Mean Entropy: 0.10705213248729706, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1358,  Mean reward: -1.875, Mean Entropy: 0.1897437870502472, complete_episode_count: 76.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1359,  Mean reward: -1.5342465753424657, Mean Entropy: 0.1846979260444641, complete_episode_count: 73.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1360,  Mean reward: -2.098684210526316, Mean Entropy: 0.23711663484573364, complete_episode_count: 76.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1361,  Mean reward: -4.785714285714286, Mean Entropy: 0.15874743461608887, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1362,  Mean reward: -2.227272727272727, Mean Entropy: 0.10992803424596786, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1363,  Mean reward: -1.9285714285714286, Mean Entropy: 0.10803394764661789, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1364,  Mean reward: -3.7532467532467533, Mean Entropy: 0.09262790530920029, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1365,  Mean reward: -0.6298701298701299, Mean Entropy: 0.09834461659193039, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1366,  Mean reward: -3.8333333333333335, Mean Entropy: 0.0698583796620369, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 1367,  Mean reward: -0.12025316455696203, Mean Entropy: 0.05877389758825302, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1368,  Mean reward: -1.2692307692307692, Mean Entropy: 0.09586767852306366, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1369,  Mean reward: -2.8133333333333335, Mean Entropy: 0.0980411171913147, complete_episode_count: 75.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1370,  Mean reward: -3.487012987012987, Mean Entropy: 0.23846594989299774, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1371,  Mean reward: -2.9642857142857144, Mean Entropy: 0.1727636754512787, complete_episode_count: 70.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1372,  Mean reward: -4.17910447761194, Mean Entropy: 0.33749350905418396, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1373,  Mean reward: -3.2432432432432434, Mean Entropy: 0.21463149785995483, complete_episode_count: 74.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1374,  Mean reward: -2.7183098591549295, Mean Entropy: 0.0542854480445385, complete_episode_count: 71.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1375,  Mean reward: -0.5, Mean Entropy: 0.03227522224187851, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1376,  Mean reward: -2.1455696202531644, Mean Entropy: 0.03426855802536011, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1377,  Mean reward: -1.5256410256410255, Mean Entropy: 0.03486210107803345, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1378,  Mean reward: -0.75, Mean Entropy: 0.0737672820687294, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1379,  Mean reward: -1.9675324675324675, Mean Entropy: 0.0626288503408432, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1380,  Mean reward: -2.3987341772151898, Mean Entropy: 0.032790862023830414, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1381,  Mean reward: 0.0, Mean Entropy: 0.027809184044599533, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1382,  Mean reward: -0.5, Mean Entropy: 0.027479862794280052, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1383,  Mean reward: -2.75, Mean Entropy: 0.025418473407626152, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.72s
Iteration: 1384,  Mean reward: -3.25, Mean Entropy: 0.02282623015344143, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1385,  Mean reward: -0.75, Mean Entropy: 0.024367650970816612, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1386,  Mean reward: 1.75, Mean Entropy: 0.026833249256014824, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1387,  Mean reward: -3.8333333333333335, Mean Entropy: 0.045543134212493896, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1388,  Mean reward: -2.0, Mean Entropy: 0.03620224446058273, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1389,  Mean reward: -0.75, Mean Entropy: 0.04126747325062752, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1390,  Mean reward: -2.3987341772151898, Mean Entropy: 0.03856734186410904, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1391,  Mean reward: -0.7564102564102564, Mean Entropy: 0.0313548818230629, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1392,  Mean reward: -1.3860759493670887, Mean Entropy: 0.03217495232820511, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1393,  Mean reward: -2.9050632911392404, Mean Entropy: 0.030632197856903076, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1394,  Mean reward: -0.22435897435897437, Mean Entropy: 0.02862405776977539, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1395,  Mean reward: -1.75, Mean Entropy: 0.030524183064699173, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1396,  Mean reward: -3.392405063291139, Mean Entropy: 0.026825418695807457, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1397,  Mean reward: -2.75, Mean Entropy: 0.030294403433799744, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1398,  Mean reward: 1.0, Mean Entropy: 0.020154891535639763, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1399,  Mean reward: -1.75, Mean Entropy: 0.026886261999607086, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1400,  Mean reward: -1.75, Mean Entropy: 0.02509179338812828, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 1401,  Mean reward: -1.5, Mean Entropy: 0.02418457716703415, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1402,  Mean reward: -2.0, Mean Entropy: 0.023223018273711205, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1403,  Mean reward: -1.75, Mean Entropy: 0.02554452233016491, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1404,  Mean reward: -2.0, Mean Entropy: 0.020397132262587547, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 1405,  Mean reward: -1.0, Mean Entropy: 0.019267618656158447, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1406,  Mean reward: -0.879746835443038, Mean Entropy: 0.01993088237941265, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1407,  Mean reward: -0.6265822784810127, Mean Entropy: 0.018796902149915695, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.72s
Iteration: 1408,  Mean reward: -2.25, Mean Entropy: 0.017992815002799034, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1409,  Mean reward: -1.5, Mean Entropy: 0.01464272290468216, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1410,  Mean reward: -4.0, Mean Entropy: 0.02388175204396248, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1411,  Mean reward: -1.8924050632911393, Mean Entropy: 0.02177785336971283, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1412,  Mean reward: -3.75, Mean Entropy: 0.02410672977566719, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1413,  Mean reward: -2.5, Mean Entropy: 0.041668057441711426, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.70s
Iteration: 1414,  Mean reward: -1.7820512820512822, Mean Entropy: 0.012151378206908703, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1415,  Mean reward: -2.0, Mean Entropy: 0.010807914659380913, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1416,  Mean reward: -0.75, Mean Entropy: 0.010491400957107544, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1417,  Mean reward: -2.0, Mean Entropy: 0.012088675051927567, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1418,  Mean reward: -1.639240506329114, Mean Entropy: 0.012365072965621948, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1419,  Mean reward: -0.879746835443038, Mean Entropy: 0.014158321544528008, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1420,  Mean reward: -1.75, Mean Entropy: 0.016712363809347153, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1421,  Mean reward: -1.25, Mean Entropy: 0.015170617029070854, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1422,  Mean reward: -1.3860759493670887, Mean Entropy: 0.017118707299232483, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1423,  Mean reward: -3.0, Mean Entropy: 0.02187380939722061, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1424,  Mean reward: -1.8924050632911393, Mean Entropy: 0.030498381704092026, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1425,  Mean reward: -1.75, Mean Entropy: 0.016293752938508987, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1426,  Mean reward: -1.5, Mean Entropy: 0.015259159728884697, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1427,  Mean reward: -3.0, Mean Entropy: 0.016235332936048508, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1428,  Mean reward: -1.75, Mean Entropy: 0.015879249200224876, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1429,  Mean reward: -1.25, Mean Entropy: 0.015147073194384575, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1430,  Mean reward: -1.25, Mean Entropy: 0.016468659043312073, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1431,  Mean reward: -1.639240506329114, Mean Entropy: 0.02031511254608631, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1432,  Mean reward: -1.8924050632911393, Mean Entropy: 0.014732341282069683, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1433,  Mean reward: -2.1455696202531644, Mean Entropy: 0.013381901197135448, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1434,  Mean reward: -3.75, Mean Entropy: 0.012637315317988396, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1435,  Mean reward: -1.8924050632911393, Mean Entropy: 0.011514183133840561, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1436,  Mean reward: -2.0, Mean Entropy: 0.01770983636379242, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1437,  Mean reward: -1.25, Mean Entropy: 0.0319170318543911, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1438,  Mean reward: -1.25, Mean Entropy: 0.0391988568007946, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1439,  Mean reward: -2.9240506329113924, Mean Entropy: 0.04130525141954422, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1440,  Mean reward: -0.37012987012987014, Mean Entropy: 0.03518395125865936, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1441,  Mean reward: -1.25, Mean Entropy: 0.049649037420749664, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1442,  Mean reward: -0.75, Mean Entropy: 0.040116626769304276, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 1443,  Mean reward: -3.1392405063291138, Mean Entropy: 0.07764311134815216, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1444,  Mean reward: -0.6298701298701299, Mean Entropy: 0.08959044516086578, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1445,  Mean reward: -2.8266666666666667, Mean Entropy: 0.07231345772743225, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1446,  Mean reward: -3.487012987012987, Mean Entropy: 0.14759129285812378, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1447,  Mean reward: -3.6153846153846154, Mean Entropy: 0.026012307032942772, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1448,  Mean reward: -1.1329113924050633, Mean Entropy: 0.01726679503917694, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1449,  Mean reward: -0.25, Mean Entropy: 0.01003468781709671, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1450,  Mean reward: -1.0, Mean Entropy: 0.009908461943268776, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1451,  Mean reward: -0.5, Mean Entropy: 0.011643871665000916, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1452,  Mean reward: -3.25, Mean Entropy: 0.011832120828330517, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1453,  Mean reward: -3.5, Mean Entropy: 0.005308486521244049, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1454,  Mean reward: -4.0, Mean Entropy: 0.001511467038653791, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1455,  Mean reward: -2.0, Mean Entropy: 0.0012616061139851809, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1456,  Mean reward: -3.25, Mean Entropy: 0.0009295436320826411, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1457,  Mean reward: -2.25, Mean Entropy: 0.0005755551974289119, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1458,  Mean reward: -3.0, Mean Entropy: 0.00083016452845186, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1459,  Mean reward: 1.25, Mean Entropy: 0.010478293523192406, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1460,  Mean reward: -2.0, Mean Entropy: 0.04425051063299179, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1461,  Mean reward: -2.0, Mean Entropy: 0.056535910815000534, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1462,  Mean reward: -2.625, Mean Entropy: 0.035810887813568115, complete_episode_count: 76.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1463,  Mean reward: -3.576923076923077, Mean Entropy: 0.04242467135190964, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1464,  Mean reward: -0.5, Mean Entropy: 0.07619185745716095, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1465,  Mean reward: -0.879746835443038, Mean Entropy: 0.07217539846897125, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1466,  Mean reward: -0.12025316455696203, Mean Entropy: 0.0658717006444931, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1467,  Mean reward: -1.3860759493670887, Mean Entropy: 0.06539286673069, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1468,  Mean reward: -0.4807692307692308, Mean Entropy: 0.03565848246216774, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1469,  Mean reward: -3.1582278481012658, Mean Entropy: 0.04814128205180168, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1470,  Mean reward: -0.8896103896103896, Mean Entropy: 0.034502606838941574, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1471,  Mean reward: -3.301282051282051, Mean Entropy: 0.050534188747406006, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1472,  Mean reward: -2.911392405063291, Mean Entropy: 0.07655046880245209, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1473,  Mean reward: -2.3987341772151898, Mean Entropy: 0.05849603936076164, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.71s
Iteration: 1474,  Mean reward: -1.9350649350649352, Mean Entropy: 0.053069017827510834, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 1475,  Mean reward: -1.1329113924050633, Mean Entropy: 0.1489035189151764, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1476,  Mean reward: -0.6493506493506493, Mean Entropy: 0.20529624819755554, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1477,  Mean reward: -1.9870129870129871, Mean Entropy: 0.1998814046382904, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1478,  Mean reward: -1.5256410256410255, Mean Entropy: 0.21780216693878174, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1479,  Mean reward: -1.2333333333333334, Mean Entropy: 0.3151954412460327, complete_episode_count: 75.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1480,  Mean reward: -1.4726027397260273, Mean Entropy: 0.2850796580314636, complete_episode_count: 73.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 1481,  Mean reward: -1.9733333333333334, Mean Entropy: 0.15057909488677979, complete_episode_count: 75.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1482,  Mean reward: -2.1455696202531644, Mean Entropy: 0.14621688425540924, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1483,  Mean reward: -3.4315068493150687, Mean Entropy: 0.1418328881263733, complete_episode_count: 73.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1484,  Mean reward: -4.4868421052631575, Mean Entropy: 0.06238984316587448, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1485,  Mean reward: -2.9050632911392404, Mean Entropy: 0.027669530361890793, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1486,  Mean reward: -1.75, Mean Entropy: 0.0219745896756649, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1487,  Mean reward: -1.0, Mean Entropy: 0.01972079835832119, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1488,  Mean reward: -2.0, Mean Entropy: 0.03315650671720505, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.71s
Iteration: 1489,  Mean reward: -2.651898734177215, Mean Entropy: 0.07122888416051865, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1490,  Mean reward: -2.5064935064935066, Mean Entropy: 0.13598453998565674, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.67s
Iteration: 1491,  Mean reward: -2.3424657534246576, Mean Entropy: 0.10823997110128403, complete_episode_count: 73.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1492,  Mean reward: -2.3333333333333335, Mean Entropy: 0.1618226021528244, complete_episode_count: 75.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1493,  Mean reward: -1.1597222222222223, Mean Entropy: 0.17117244005203247, complete_episode_count: 72.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1494,  Mean reward: 0.11392405063291139, Mean Entropy: 0.07102640718221664, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1495,  Mean reward: -1.2884615384615385, Mean Entropy: 0.031254127621650696, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1496,  Mean reward: -2.551282051282051, Mean Entropy: 0.02373882196843624, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1497,  Mean reward: -2.0, Mean Entropy: 0.021419823169708252, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1498,  Mean reward: -4.0, Mean Entropy: 0.022999927401542664, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1499,  Mean reward: 0.5, Mean Entropy: 0.03116776794195175, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1500,  Mean reward: -4.949367088607595, Mean Entropy: 0.05291995033621788, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.68s
rec seq len 2
actor lr 0.0005
Iteration: 1501,  Mean reward: -2.5, Mean Entropy: 0.056168656796216965, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1502,  Mean reward: -2.1948051948051948, Mean Entropy: 0.12673133611679077, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.69s
Iteration: 1503,  Mean reward: -2.688311688311688, Mean Entropy: 0.11029820144176483, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.68s
Iteration: 1504,  Mean reward: -3.512987012987013, Mean Entropy: 0.11410272121429443, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1505,  Mean reward: -2.9050632911392404, Mean Entropy: 0.06225252151489258, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.70s
Iteration: 1506,  Mean reward: -2.5, Mean Entropy: 0.050860315561294556, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1507,  Mean reward: -1.5064102564102564, Mean Entropy: 0.031068414449691772, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1508,  Mean reward: -1.25, Mean Entropy: 0.03927057236433029, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1509,  Mean reward: -1.3860759493670887, Mean Entropy: 0.06693261861801147, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.66s
Iteration: 1510,  Mean reward: -0.9935897435897436, Mean Entropy: 0.0935644805431366, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1511,  Mean reward: -3.430379746835443, Mean Entropy: 0.11083364486694336, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1512,  Mean reward: -2.207792207792208, Mean Entropy: 0.11786298453807831, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.70s
Iteration: 1513,  Mean reward: -2.1948051948051948, Mean Entropy: 0.147391214966774, complete_episode_count: 77.0, Gather time: 0.51s, Train time: 0.67s
Iteration: 1514,  Mean reward: -3.171232876712329, Mean Entropy: 0.134374737739563, complete_episode_count: 73.0, Gather time: 0.51s, Train time: 0.66s
Iteration: 1515,  Mean reward: -1.0394736842105263, Mean Entropy: 0.13546010851860046, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.69s
Iteration: 1516,  Mean reward: -3.6645569620253164, Mean Entropy: 0.07279103994369507, complete_episode_count: 79.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1517,  Mean reward: -2.54, Mean Entropy: 0.0427788570523262, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.73s
Iteration: 1518,  Mean reward: -2.75, Mean Entropy: 0.050860416144132614, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.87s
Iteration: 1519,  Mean reward: -2.25, Mean Entropy: 0.05155478045344353, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1520,  Mean reward: -0.7756410256410257, Mean Entropy: 0.04596347361803055, complete_episode_count: 78.0, Gather time: 0.51s, Train time: 0.68s
Iteration: 1521,  Mean reward: -2.5, Mean Entropy: 0.03619871661067009, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.69s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.48it/s]100%|| 1/1 [00:00<00:00,  1.47it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.4 0.6 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.83it/s]100%|| 1/1 [00:00<00:00,  1.83it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.77it/s]100%|| 1/1 [00:00<00:00,  1.77it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 3 action_probs [0.0 0.0 0.0 0.6 0.4 0.0 0.0 0.0] r -1.5 s_ (3, 1)
  s:             (3, 1) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 2)
  s:             (4, 2) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 3 steps, Captured: True Reward: -13.5


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.50 +/- 0.50
   Lengths :[3 2]
Average return: -2.75 +/- 10.75
   Returns :[-13.5 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.75
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type None
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_None/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: -2.25
  std over seeds: 0.3535533905932738
  per seed: [-2.000 -2.000 -2.750]

success_rate.......
  avg over seeds: 0.5
  std over seeds: 0.0
  per seed: [0.500 0.500 0.500]

batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: FE
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: v
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
critic: v
node_dim: 7
lstm_on: True
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy_FE_LSTM(
  (FE): FeatureExtractor_LSTM(
    (lstm): LSTM(7, 24)
    (gat): GATv2(24, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (theta8_v): Linear(in_features=24, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with LSTM (on node features) + GATv2 feature extraction
------------------------------------------
FE.lstm.weight_ih_l0     [96, 7]      requires_grad=True
FE.lstm.weight_hh_l0     [96, 24]     requires_grad=True
FE.lstm.bias_ih_l0       [96]         requires_grad=True
FE.lstm.bias_hh_l0       [96]         requires_grad=True
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.theta8_v.weight        [1, 24]      requires_grad=True
V.theta8_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 10682
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.304878048780488, Mean Entropy: 1.039720892906189, complete_episode_count: 41.0, Gather time: 5.70s, Train time: 3.79s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.288888888888889, Mean Entropy: 0.8953151106834412, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.87s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -2.775, Mean Entropy: 0.8953151106834412, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.88s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -2.0729166666666665, Mean Entropy: 0.9675178527832031, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 4,  Mean reward: -4.226190476190476, Mean Entropy: 0.9097556471824646, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.87s
Iteration: 5,  Mean reward: -3.325, Mean Entropy: 0.9963988661766052, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 6,  Mean reward: -6.5, Mean Entropy: 0.9097554683685303, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 7,  Mean reward: -3.9782608695652173, Mean Entropy: 0.9458567500114441, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 8,  Mean reward: -5.953488372093023, Mean Entropy: 0.9819576144218445, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 9,  Mean reward: -5.3375, Mean Entropy: 0.8664330244064331, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.87s
Iteration: 10,  Mean reward: -3.7875, Mean Entropy: 0.9314152598381042, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.86s
Iteration: 11,  Mean reward: -5.585365853658536, Mean Entropy: 0.8736521601676941, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 12,  Mean reward: -4.2125, Mean Entropy: 0.9314101934432983, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 13,  Mean reward: -3.892857142857143, Mean Entropy: 0.9530684351921082, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 14,  Mean reward: -4.159090909090909, Mean Entropy: 0.9097476005554199, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 15,  Mean reward: -3.6744186046511627, Mean Entropy: 0.931410551071167, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 16,  Mean reward: -5.590909090909091, Mean Entropy: 0.9458532333374023, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 17,  Mean reward: -4.059523809523809, Mean Entropy: 0.9241913557052612, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.86s
Iteration: 18,  Mean reward: -3.2625, Mean Entropy: 0.9891732931137085, complete_episode_count: 40.0, Gather time: 0.61s, Train time: 1.84s
Iteration: 19,  Mean reward: -4.27906976744186, Mean Entropy: 0.9241937398910522, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 20,  Mean reward: -4.25, Mean Entropy: 1.0108336210250854, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 21,  Mean reward: -4.0, Mean Entropy: 0.9169718623161316, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 22,  Mean reward: -5.592105263157895, Mean Entropy: 0.9458538293838501, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 23,  Mean reward: -4.023809523809524, Mean Entropy: 0.9314133524894714, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 24,  Mean reward: -3.3048780487804876, Mean Entropy: 0.9675148725509644, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 2.04s
Iteration: 25,  Mean reward: -2.1136363636363638, Mean Entropy: 0.9602957963943481, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.85s
Iteration: 26,  Mean reward: -2.772727272727273, Mean Entropy: 0.981958270072937, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.85s
Iteration: 27,  Mean reward: -3.125, Mean Entropy: 0.9675177335739136, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 28,  Mean reward: -1.9069767441860466, Mean Entropy: 0.9458571672439575, complete_episode_count: 43.0, Gather time: 0.64s, Train time: 1.86s
Iteration: 29,  Mean reward: -2.7023809523809526, Mean Entropy: 0.9241962432861328, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 30,  Mean reward: -7.2926829268292686, Mean Entropy: 0.9602975249290466, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 31,  Mean reward: -5.75, Mean Entropy: 0.9458568096160889, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.85s
Iteration: 32,  Mean reward: -5.621951219512195, Mean Entropy: 0.9458555579185486, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 33,  Mean reward: -1.8522727272727273, Mean Entropy: 0.9530752897262573, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 34,  Mean reward: -6.0227272727272725, Mean Entropy: 0.960293173789978, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 35,  Mean reward: -5.317073170731708, Mean Entropy: 0.9241846799850464, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 36,  Mean reward: -5.464285714285714, Mean Entropy: 0.8953008651733398, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 37,  Mean reward: -5.715909090909091, Mean Entropy: 1.018040418624878, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 38,  Mean reward: -4.088888888888889, Mean Entropy: 0.9241796731948853, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 39,  Mean reward: -4.392857142857143, Mean Entropy: 0.9674939513206482, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 40,  Mean reward: -8.0, Mean Entropy: 0.9241650104522705, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 41,  Mean reward: -4.107142857142857, Mean Entropy: 0.9097358584403992, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 42,  Mean reward: -2.0113636363636362, Mean Entropy: 0.924184262752533, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 43,  Mean reward: -6.232558139534884, Mean Entropy: 0.9241780042648315, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.98s
Iteration: 44,  Mean reward: -4.454545454545454, Mean Entropy: 0.9458039999008179, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 45,  Mean reward: -4.886363636363637, Mean Entropy: 0.945803165435791, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 46,  Mean reward: -3.5875, Mean Entropy: 0.8880382776260376, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 47,  Mean reward: -5.607142857142857, Mean Entropy: 0.9312633275985718, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 48,  Mean reward: -2.872093023255814, Mean Entropy: 0.9384779930114746, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 49,  Mean reward: -5.3076923076923075, Mean Entropy: 0.8951689600944519, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 50,  Mean reward: -4.182926829268292, Mean Entropy: 0.9817090034484863, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 51,  Mean reward: -5.127906976744186, Mean Entropy: 0.9527968168258667, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 52,  Mean reward: -4.9875, Mean Entropy: 0.8876480460166931, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.87s
Iteration: 53,  Mean reward: -5.7, Mean Entropy: 0.9523939490318298, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.85s
Iteration: 54,  Mean reward: -3.6511627906976742, Mean Entropy: 0.8806214332580566, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 55,  Mean reward: -2.880434782608696, Mean Entropy: 0.9527919292449951, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 56,  Mean reward: -7.904761904761905, Mean Entropy: 0.9601191282272339, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.82s
Iteration: 57,  Mean reward: -4.7555555555555555, Mean Entropy: 0.9668041467666626, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 58,  Mean reward: -5.3625, Mean Entropy: 0.879881739616394, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 59,  Mean reward: -6.722222222222222, Mean Entropy: 0.9426085352897644, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 2.01s
Iteration: 60,  Mean reward: -6.592105263157895, Mean Entropy: 0.9065841436386108, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 61,  Mean reward: -1.644736842105263, Mean Entropy: 0.9373223185539246, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 62,  Mean reward: -4.086956521739131, Mean Entropy: 0.988000750541687, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 63,  Mean reward: -5.654761904761905, Mean Entropy: 0.9940878748893738, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 64,  Mean reward: -4.375, Mean Entropy: 0.9719306230545044, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 65,  Mean reward: -6.829545454545454, Mean Entropy: 0.9694256782531738, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 66,  Mean reward: -4.406976744186046, Mean Entropy: 0.9134189486503601, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 67,  Mean reward: -3.9390243902439024, Mean Entropy: 0.9505982398986816, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 68,  Mean reward: -2.977272727272727, Mean Entropy: 0.9351266622543335, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 69,  Mean reward: -5.475609756097561, Mean Entropy: 0.9061380624771118, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 70,  Mean reward: -5.579545454545454, Mean Entropy: 0.9747587442398071, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 71,  Mean reward: 0.09574468085106383, Mean Entropy: 0.9132488965988159, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.81s
Iteration: 72,  Mean reward: -5.020833333333333, Mean Entropy: 0.9426302909851074, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 73,  Mean reward: -2.8, Mean Entropy: 0.9414383769035339, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 74,  Mean reward: -6.6976744186046515, Mean Entropy: 0.9684826135635376, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 75,  Mean reward: -2.7954545454545454, Mean Entropy: 0.9198169112205505, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 76,  Mean reward: -5.011627906976744, Mean Entropy: 0.9329404830932617, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 77,  Mean reward: -4.934782608695652, Mean Entropy: 0.9314505457878113, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 78,  Mean reward: -4.690476190476191, Mean Entropy: 0.9776440858840942, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 79,  Mean reward: -3.3043478260869565, Mean Entropy: 0.9252270460128784, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 80,  Mean reward: -3.4642857142857144, Mean Entropy: 0.9178783297538757, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 81,  Mean reward: -2.3555555555555556, Mean Entropy: 0.9590627551078796, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.97s
Iteration: 82,  Mean reward: -2.686046511627907, Mean Entropy: 0.9303786158561707, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 83,  Mean reward: -5.127906976744186, Mean Entropy: 0.9163808822631836, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 84,  Mean reward: -5.682926829268292, Mean Entropy: 0.8760719299316406, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 85,  Mean reward: -3.0795454545454546, Mean Entropy: 0.9248785972595215, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 86,  Mean reward: -0.58, Mean Entropy: 0.9592862129211426, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 87,  Mean reward: -3.073170731707317, Mean Entropy: 0.9494132399559021, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 88,  Mean reward: -4.95, Mean Entropy: 0.9716553688049316, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 89,  Mean reward: -4.0, Mean Entropy: 0.8494505882263184, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 90,  Mean reward: -5.955555555555556, Mean Entropy: 0.9400012493133545, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 91,  Mean reward: -4.928571428571429, Mean Entropy: 0.876582145690918, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 92,  Mean reward: -3.5543478260869565, Mean Entropy: 0.8844811916351318, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 93,  Mean reward: -2.5434782608695654, Mean Entropy: 0.8727242350578308, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 2.04s
Iteration: 94,  Mean reward: -5.92, Mean Entropy: 0.8940824270248413, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 95,  Mean reward: -3.5096153846153846, Mean Entropy: 0.9038354158401489, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 96,  Mean reward: -2.5576923076923075, Mean Entropy: 0.9462348818778992, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 97,  Mean reward: -5.021739130434782, Mean Entropy: 0.9137003421783447, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 98,  Mean reward: -4.982142857142857, Mean Entropy: 0.7997632026672363, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 99,  Mean reward: -2.963636363636364, Mean Entropy: 0.8303301334381104, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 100,  Mean reward: -3.7450980392156863, Mean Entropy: 0.7483236193656921, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.85s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -1.316326530612245, Mean Entropy: 0.8915205001831055, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 102,  Mean reward: -2.6702127659574466, Mean Entropy: 0.9930152893066406, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 103,  Mean reward: -3.9318181818181817, Mean Entropy: 0.9637243747711182, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 104,  Mean reward: -6.531914893617022, Mean Entropy: 0.8791601657867432, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 105,  Mean reward: 0.5816326530612245, Mean Entropy: 0.9607278108596802, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.80s
Iteration: 106,  Mean reward: -1.923913043478261, Mean Entropy: 0.9449635744094849, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 107,  Mean reward: -6.440476190476191, Mean Entropy: 0.9601389765739441, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 108,  Mean reward: -3.1511627906976742, Mean Entropy: 0.8868367075920105, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 109,  Mean reward: -4.6022727272727275, Mean Entropy: 0.9418338537216187, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 110,  Mean reward: -5.2125, Mean Entropy: 0.9307343363761902, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 111,  Mean reward: -6.5, Mean Entropy: 0.9597182273864746, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 112,  Mean reward: -2.977272727272727, Mean Entropy: 0.8869268894195557, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 113,  Mean reward: -2.9, Mean Entropy: 0.9717055559158325, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.85s
Iteration: 114,  Mean reward: -5.066666666666666, Mean Entropy: 0.9362947940826416, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 115,  Mean reward: -5.534883720930233, Mean Entropy: 0.9120405912399292, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 116,  Mean reward: -1.25, Mean Entropy: 0.9505890607833862, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 117,  Mean reward: -3.425, Mean Entropy: 0.9604589939117432, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 118,  Mean reward: -3.8, Mean Entropy: 0.9186420440673828, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 119,  Mean reward: -1.5795454545454546, Mean Entropy: 0.833166778087616, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.84s
Iteration: 120,  Mean reward: -2.9183673469387754, Mean Entropy: 1.0054123401641846, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 121,  Mean reward: -1.5222222222222221, Mean Entropy: 0.8962999582290649, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 122,  Mean reward: -2.9583333333333335, Mean Entropy: 0.8515108823776245, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.82s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 123,  Mean reward: 1.6018518518518519, Mean Entropy: 0.8921222686767578, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.81s
Iteration: 124,  Mean reward: 0.24468085106382978, Mean Entropy: 0.8305054903030396, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 125,  Mean reward: -0.8673469387755102, Mean Entropy: 0.8311927318572998, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 126,  Mean reward: 0.723404255319149, Mean Entropy: 0.6483076810836792, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 127,  Mean reward: 1.6454545454545455, Mean Entropy: 0.5579611659049988, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 2.02s
Iteration: 128,  Mean reward: 0.96, Mean Entropy: 0.7259048223495483, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 129,  Mean reward: 0.6415094339622641, Mean Entropy: 0.6910972595214844, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 130,  Mean reward: 1.009433962264151, Mean Entropy: 0.7711575031280518, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 131,  Mean reward: 1.0454545454545454, Mean Entropy: 0.6362497806549072, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 132,  Mean reward: 0.0, Mean Entropy: 0.7027823328971863, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.82s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 133,  Mean reward: 4.177966101694915, Mean Entropy: 0.6411473751068115, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.81s
Iteration: 134,  Mean reward: 0.5408163265306123, Mean Entropy: 0.6726486086845398, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 135,  Mean reward: 4.454545454545454, Mean Entropy: 0.7685858607292175, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.85s
Iteration: 136,  Mean reward: 0.5833333333333334, Mean Entropy: 0.639674961566925, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 137,  Mean reward: 0.33653846153846156, Mean Entropy: 0.6479526162147522, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 138,  Mean reward: 3.375, Mean Entropy: 0.6716746687889099, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 139,  Mean reward: 4.154545454545454, Mean Entropy: 0.7899165153503418, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 140,  Mean reward: 2.3518518518518516, Mean Entropy: 0.5651044845581055, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 141,  Mean reward: -0.63, Mean Entropy: 0.7240239381790161, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 142,  Mean reward: 2.9272727272727272, Mean Entropy: 0.7027280330657959, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 143,  Mean reward: 1.9313725490196079, Mean Entropy: 0.7492713928222656, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 144,  Mean reward: 0.06363636363636363, Mean Entropy: 0.7183281779289246, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 145,  Mean reward: 2.861111111111111, Mean Entropy: 0.6486167907714844, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.85s
Iteration: 146,  Mean reward: 0.5686274509803921, Mean Entropy: 0.7583236694335938, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 147,  Mean reward: 3.0, Mean Entropy: 0.673328161239624, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 148,  Mean reward: 2.15625, Mean Entropy: 0.6321580410003662, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 149,  Mean reward: 1.0377358490566038, Mean Entropy: 0.7767254710197449, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 150,  Mean reward: 0.5333333333333333, Mean Entropy: 0.7198535799980164, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.83s
Iteration: 151,  Mean reward: 1.2962962962962963, Mean Entropy: 0.6950652003288269, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 152,  Mean reward: 1.1296296296296295, Mean Entropy: 0.7424119710922241, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 153,  Mean reward: 1.7589285714285714, Mean Entropy: 0.7164897918701172, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 154,  Mean reward: 1.4, Mean Entropy: 0.7435393333435059, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 155,  Mean reward: -0.69, Mean Entropy: 0.6444680690765381, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 156,  Mean reward: 1.7053571428571428, Mean Entropy: 0.6652676463127136, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 157,  Mean reward: 0.7314814814814815, Mean Entropy: 0.6912155151367188, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 158,  Mean reward: 2.287037037037037, Mean Entropy: 0.7204834818840027, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 159,  Mean reward: -0.9897959183673469, Mean Entropy: 0.7253653407096863, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 2.06s
Iteration: 160,  Mean reward: 1.7222222222222223, Mean Entropy: 0.6358686685562134, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 161,  Mean reward: 3.3333333333333335, Mean Entropy: 0.6159157156944275, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 162,  Mean reward: 1.3113207547169812, Mean Entropy: 0.7946235537528992, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 163,  Mean reward: -0.91, Mean Entropy: 0.7905687093734741, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 164,  Mean reward: 3.0172413793103448, Mean Entropy: 0.6338330507278442, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 165,  Mean reward: 2.4607843137254903, Mean Entropy: 0.710159420967102, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 166,  Mean reward: 1.2075471698113207, Mean Entropy: 0.7835458517074585, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 167,  Mean reward: 2.8947368421052633, Mean Entropy: 0.7666985392570496, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.80s
Iteration: 168,  Mean reward: 1.8518518518518519, Mean Entropy: 0.6796317100524902, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 169,  Mean reward: 1.3421052631578947, Mean Entropy: 0.6604112982749939, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 170,  Mean reward: 2.982142857142857, Mean Entropy: 0.5487686991691589, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.91s
Iteration: 171,  Mean reward: 2.543103448275862, Mean Entropy: 0.7683578729629517, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 172,  Mean reward: 2.9272727272727272, Mean Entropy: 0.6541334390640259, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 173,  Mean reward: 1.45, Mean Entropy: 0.7519489526748657, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.85s
Iteration: 174,  Mean reward: -1.2395833333333333, Mean Entropy: 0.7854413986206055, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 175,  Mean reward: 2.7589285714285716, Mean Entropy: 0.710910975933075, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 176,  Mean reward: 3.2636363636363637, Mean Entropy: 0.7328342795372009, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 177,  Mean reward: 1.6896551724137931, Mean Entropy: 0.7698376178741455, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.83s
Iteration: 178,  Mean reward: 0.009433962264150943, Mean Entropy: 0.7139636874198914, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 179,  Mean reward: 0.05, Mean Entropy: 0.7264337539672852, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 180,  Mean reward: 2.2314814814814814, Mean Entropy: 0.7398373484611511, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 181,  Mean reward: -1.4454545454545455, Mean Entropy: 0.7170721888542175, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 182,  Mean reward: 0.6176470588235294, Mean Entropy: 0.7648904323577881, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 183,  Mean reward: 1.5784313725490196, Mean Entropy: 0.71751469373703, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.96s
Iteration: 184,  Mean reward: -0.82, Mean Entropy: 0.7659286260604858, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 185,  Mean reward: 2.9444444444444446, Mean Entropy: 0.6044413447380066, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 186,  Mean reward: 1.9, Mean Entropy: 0.6997293829917908, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 187,  Mean reward: 1.6403508771929824, Mean Entropy: 0.6264464855194092, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 188,  Mean reward: 2.2037037037037037, Mean Entropy: 0.6814677715301514, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 189,  Mean reward: 1.8839285714285714, Mean Entropy: 0.6242667436599731, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 190,  Mean reward: 3.8515625, Mean Entropy: 0.5619334578514099, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 191,  Mean reward: 3.0576923076923075, Mean Entropy: 0.6964116096496582, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 192,  Mean reward: 1.7692307692307692, Mean Entropy: 0.6624883413314819, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 2.06s
Iteration: 193,  Mean reward: 2.1818181818181817, Mean Entropy: 0.6302956342697144, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 194,  Mean reward: 2.805084745762712, Mean Entropy: 0.6643891334533691, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 195,  Mean reward: 0.2037037037037037, Mean Entropy: 0.7429531812667847, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 196,  Mean reward: 1.8596491228070176, Mean Entropy: 0.681135356426239, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.90s
Iteration: 197,  Mean reward: 1.7692307692307692, Mean Entropy: 0.8281912803649902, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 198,  Mean reward: 3.152542372881356, Mean Entropy: 0.5902774333953857, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 199,  Mean reward: 1.75, Mean Entropy: 0.6955480575561523, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 200,  Mean reward: 1.8448275862068966, Mean Entropy: 0.6571372747421265, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 2.3181818181818183, Mean Entropy: 0.6064778566360474, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 202,  Mean reward: 2.5344827586206895, Mean Entropy: 0.6423127055168152, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 203,  Mean reward: 2.107142857142857, Mean Entropy: 0.667027473449707, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 204,  Mean reward: 1.875, Mean Entropy: 0.6794114112854004, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 205,  Mean reward: 3.307017543859649, Mean Entropy: 0.5986450910568237, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 206,  Mean reward: 0.28431372549019607, Mean Entropy: 0.7647315859794617, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 207,  Mean reward: 1.8166666666666667, Mean Entropy: 0.6378246545791626, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.82s
Iteration: 208,  Mean reward: 1.0480769230769231, Mean Entropy: 0.727780282497406, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 209,  Mean reward: 2.1363636363636362, Mean Entropy: 0.6286195516586304, complete_episode_count: 55.0, Gather time: 0.57s, Train time: 1.80s
Iteration: 210,  Mean reward: 2.6964285714285716, Mean Entropy: 0.7238442301750183, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 211,  Mean reward: 1.1090909090909091, Mean Entropy: 0.6712538003921509, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 212,  Mean reward: 1.65, Mean Entropy: 0.6096328496932983, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 213,  Mean reward: 1.6339285714285714, Mean Entropy: 0.7977005839347839, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 214,  Mean reward: 2.0272727272727273, Mean Entropy: 0.7380844354629517, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 215,  Mean reward: 1.7181818181818183, Mean Entropy: 0.7293294668197632, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 216,  Mean reward: 3.075, Mean Entropy: 0.6032994985580444, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 217,  Mean reward: -1.5517241379310345, Mean Entropy: 0.6908289194107056, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 218,  Mean reward: 0.1574074074074074, Mean Entropy: 0.7130312919616699, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 219,  Mean reward: 1.6018518518518519, Mean Entropy: 0.7047135233879089, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 220,  Mean reward: 2.509259259259259, Mean Entropy: 0.7829947471618652, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 221,  Mean reward: 0.5370370370370371, Mean Entropy: 0.7882565259933472, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 222,  Mean reward: -0.01020408163265306, Mean Entropy: 0.7380994558334351, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 223,  Mean reward: 1.6481481481481481, Mean Entropy: 0.7492756843566895, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 224,  Mean reward: 2.324074074074074, Mean Entropy: 0.8080930113792419, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 2.03s
Iteration: 225,  Mean reward: 1.7358490566037736, Mean Entropy: 0.6734652519226074, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 226,  Mean reward: 3.0185185185185186, Mean Entropy: 0.6988086700439453, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 227,  Mean reward: 3.0, Mean Entropy: 0.6586703658103943, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 228,  Mean reward: 0.6666666666666666, Mean Entropy: 0.8424162864685059, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 229,  Mean reward: 1.9339622641509433, Mean Entropy: 0.7070374488830566, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 230,  Mean reward: 1.5363636363636364, Mean Entropy: 0.7227418422698975, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 231,  Mean reward: 1.7982456140350878, Mean Entropy: 0.7091912627220154, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 232,  Mean reward: 1.7407407407407407, Mean Entropy: 0.7110270857810974, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 233,  Mean reward: 1.4056603773584906, Mean Entropy: 0.7719391584396362, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 234,  Mean reward: 0.8362068965517241, Mean Entropy: 0.6374865174293518, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 235,  Mean reward: 3.3, Mean Entropy: 0.7286523580551147, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 236,  Mean reward: 2.611111111111111, Mean Entropy: 0.7203488349914551, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 237,  Mean reward: 2.466666666666667, Mean Entropy: 0.6034883260726929, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 238,  Mean reward: 1.0660377358490567, Mean Entropy: 0.8270155787467957, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 239,  Mean reward: 1.0740740740740742, Mean Entropy: 0.800604522228241, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 240,  Mean reward: 2.196078431372549, Mean Entropy: 0.617143988609314, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 241,  Mean reward: 2.4722222222222223, Mean Entropy: 0.6264339685440063, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 242,  Mean reward: 0.7307692307692307, Mean Entropy: 0.7976260185241699, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 243,  Mean reward: 1.8333333333333333, Mean Entropy: 0.875860333442688, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 244,  Mean reward: 3.0, Mean Entropy: 0.7218387126922607, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 245,  Mean reward: 1.4339622641509433, Mean Entropy: 0.6048290729522705, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 246,  Mean reward: 1.9196428571428572, Mean Entropy: 0.7056500911712646, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 247,  Mean reward: 2.1545454545454548, Mean Entropy: 0.6759635210037231, complete_episode_count: 55.0, Gather time: 0.70s, Train time: 1.83s
Iteration: 248,  Mean reward: 1.1517857142857142, Mean Entropy: 0.7693423628807068, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 249,  Mean reward: 2.26271186440678, Mean Entropy: 0.6888805627822876, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 250,  Mean reward: 0.696078431372549, Mean Entropy: 0.7398265600204468, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 251,  Mean reward: 1.8679245283018868, Mean Entropy: 0.6263041496276855, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 252,  Mean reward: 1.528301886792453, Mean Entropy: 0.7335318326950073, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 253,  Mean reward: 2.7457627118644066, Mean Entropy: 0.7337433695793152, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 254,  Mean reward: 1.8245614035087718, Mean Entropy: 0.6029300689697266, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 255,  Mean reward: 1.6491228070175439, Mean Entropy: 0.7680554389953613, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 256,  Mean reward: 1.7222222222222223, Mean Entropy: 0.7002218961715698, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 2.03s
Iteration: 257,  Mean reward: 1.8363636363636364, Mean Entropy: 0.6769377589225769, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 258,  Mean reward: -0.009259259259259259, Mean Entropy: 0.8254714012145996, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 259,  Mean reward: 2.5689655172413794, Mean Entropy: 0.6057125926017761, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.92s
Iteration: 260,  Mean reward: -1.5166666666666666, Mean Entropy: 0.6846463084220886, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 261,  Mean reward: 2.4537037037037037, Mean Entropy: 0.6675841808319092, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 262,  Mean reward: 2.240740740740741, Mean Entropy: 0.7049443125724792, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 263,  Mean reward: 2.30188679245283, Mean Entropy: 0.6809548139572144, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 264,  Mean reward: 0.8679245283018868, Mean Entropy: 0.6971842646598816, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 265,  Mean reward: 3.956896551724138, Mean Entropy: 0.5814347267150879, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 266,  Mean reward: -0.49019607843137253, Mean Entropy: 0.6882745623588562, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 267,  Mean reward: 0.37037037037037035, Mean Entropy: 0.6806144714355469, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 268,  Mean reward: 1.0454545454545454, Mean Entropy: 0.7164150476455688, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 269,  Mean reward: 0.7857142857142857, Mean Entropy: 0.773505449295044, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 270,  Mean reward: 2.6636363636363636, Mean Entropy: 0.6829547882080078, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 271,  Mean reward: 1.6568627450980393, Mean Entropy: 0.6956756114959717, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 272,  Mean reward: 0.8942307692307693, Mean Entropy: 0.7111873030662537, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 273,  Mean reward: 1.3962264150943395, Mean Entropy: 0.6229321956634521, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 274,  Mean reward: 2.75, Mean Entropy: 0.5892156958580017, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 275,  Mean reward: 1.2450980392156863, Mean Entropy: 0.6527439951896667, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 276,  Mean reward: 1.8773584905660377, Mean Entropy: 0.7247454524040222, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 277,  Mean reward: 2.1140350877192984, Mean Entropy: 0.6906436681747437, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 278,  Mean reward: 2.824561403508772, Mean Entropy: 0.6911889910697937, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 279,  Mean reward: 0.9454545454545454, Mean Entropy: 0.7327592372894287, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 280,  Mean reward: 3.6, Mean Entropy: 0.7249770164489746, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 281,  Mean reward: -3.172727272727273, Mean Entropy: 0.6399306058883667, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 282,  Mean reward: 1.3981481481481481, Mean Entropy: 0.7723228335380554, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 283,  Mean reward: 2.3125, Mean Entropy: 0.666263997554779, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 284,  Mean reward: -0.038461538461538464, Mean Entropy: 0.701335608959198, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 285,  Mean reward: 2.1538461538461537, Mean Entropy: 0.7644847631454468, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 286,  Mean reward: 1.7924528301886793, Mean Entropy: 0.6853207349777222, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 287,  Mean reward: 2.1, Mean Entropy: 0.6896704435348511, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 288,  Mean reward: 2.688679245283019, Mean Entropy: 0.6246811151504517, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 2.04s
Iteration: 289,  Mean reward: 1.146551724137931, Mean Entropy: 0.6400978565216064, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 290,  Mean reward: 1.3571428571428572, Mean Entropy: 0.7545045614242554, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 291,  Mean reward: 3.0283018867924527, Mean Entropy: 0.5630286335945129, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 292,  Mean reward: 1.0980392156862746, Mean Entropy: 0.7145165801048279, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 293,  Mean reward: 3.6862745098039214, Mean Entropy: 0.5528419017791748, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 294,  Mean reward: 2.9270833333333335, Mean Entropy: 0.7641869783401489, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 295,  Mean reward: 1.5, Mean Entropy: 0.691230058670044, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 296,  Mean reward: 2.7254901960784315, Mean Entropy: 0.6487802267074585, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.82s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 297,  Mean reward: 4.888888888888889, Mean Entropy: 0.5278699994087219, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.87s
Iteration: 298,  Mean reward: 3.54, Mean Entropy: 0.625844419002533, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 299,  Mean reward: 2.78, Mean Entropy: 0.68694007396698, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 300,  Mean reward: 4.618181818181818, Mean Entropy: 0.5828649401664734, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.84s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -0.3723404255319149, Mean Entropy: 0.8164024353027344, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 302,  Mean reward: 3.6203703703703702, Mean Entropy: 0.5711370706558228, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 303,  Mean reward: 0.18478260869565216, Mean Entropy: 0.6928668022155762, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 304,  Mean reward: 2.7037037037037037, Mean Entropy: 0.6388832330703735, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 305,  Mean reward: 4.127272727272727, Mean Entropy: 0.5559592247009277, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 306,  Mean reward: 1.5816326530612246, Mean Entropy: 0.7190251350402832, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 307,  Mean reward: 3.4019607843137254, Mean Entropy: 0.7313954830169678, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 308,  Mean reward: 0.9769230769230769, Mean Entropy: 0.49729427695274353, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 309,  Mean reward: 3.2346938775510203, Mean Entropy: 0.6696450114250183, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 310,  Mean reward: 2.04, Mean Entropy: 0.6603926420211792, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 311,  Mean reward: 3.827272727272727, Mean Entropy: 0.5165692567825317, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 312,  Mean reward: 3.009259259259259, Mean Entropy: 0.5382241010665894, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 313,  Mean reward: 4.12037037037037, Mean Entropy: 0.5932866334915161, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 314,  Mean reward: 2.520408163265306, Mean Entropy: 0.6934468150138855, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 315,  Mean reward: 1.4270833333333333, Mean Entropy: 0.7055627703666687, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 316,  Mean reward: 3.3660714285714284, Mean Entropy: 0.6574342250823975, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 317,  Mean reward: 1.287037037037037, Mean Entropy: 0.6279838681221008, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 318,  Mean reward: 2.0272727272727273, Mean Entropy: 0.6423368453979492, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 319,  Mean reward: 1.6140350877192982, Mean Entropy: 0.6639270782470703, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 320,  Mean reward: 1.875, Mean Entropy: 0.7002366781234741, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 321,  Mean reward: 3.912280701754386, Mean Entropy: 0.6712724566459656, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 2.05s
Iteration: 322,  Mean reward: -0.2978723404255319, Mean Entropy: 0.8083925247192383, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 323,  Mean reward: 1.5686274509803921, Mean Entropy: 0.6429224014282227, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.92s
Iteration: 324,  Mean reward: 2.663793103448276, Mean Entropy: 0.6932982206344604, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 325,  Mean reward: 2.0089285714285716, Mean Entropy: 0.664451539516449, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 326,  Mean reward: 2.0277777777777777, Mean Entropy: 0.7445495128631592, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 327,  Mean reward: -0.625, Mean Entropy: 0.7657384276390076, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 328,  Mean reward: 0.7870370370370371, Mean Entropy: 0.6640365123748779, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 329,  Mean reward: 1.5943396226415094, Mean Entropy: 0.6572680473327637, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 330,  Mean reward: 1.2142857142857142, Mean Entropy: 0.7082297205924988, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 331,  Mean reward: 2.6166666666666667, Mean Entropy: 0.5705946683883667, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 332,  Mean reward: 0.67, Mean Entropy: 0.6498050093650818, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 333,  Mean reward: 1.8076923076923077, Mean Entropy: 0.6209790706634521, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 334,  Mean reward: 1.3703703703703705, Mean Entropy: 0.6573262214660645, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 335,  Mean reward: 3.933333333333333, Mean Entropy: 0.6360863447189331, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 336,  Mean reward: 2.1545454545454548, Mean Entropy: 0.7512922286987305, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 337,  Mean reward: 0.6224489795918368, Mean Entropy: 0.7732025384902954, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 338,  Mean reward: 2.278688524590164, Mean Entropy: 0.6722879409790039, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 339,  Mean reward: 2.425925925925926, Mean Entropy: 0.6864418983459473, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 340,  Mean reward: 2.1538461538461537, Mean Entropy: 0.628888726234436, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 341,  Mean reward: 3.6031746031746033, Mean Entropy: 0.4991617202758789, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 342,  Mean reward: 1.8584905660377358, Mean Entropy: 0.6430409550666809, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 343,  Mean reward: 3.2672413793103448, Mean Entropy: 0.592625617980957, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 344,  Mean reward: 1.2232142857142858, Mean Entropy: 0.6933671832084656, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 345,  Mean reward: 3.6724137931034484, Mean Entropy: 0.678777813911438, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 346,  Mean reward: 2.138888888888889, Mean Entropy: 0.657221794128418, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 347,  Mean reward: 1.0471698113207548, Mean Entropy: 0.7150077819824219, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 348,  Mean reward: 2.2685185185185186, Mean Entropy: 0.7299126982688904, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 349,  Mean reward: 2.2, Mean Entropy: 0.6567987203598022, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 350,  Mean reward: 2.0849056603773586, Mean Entropy: 0.7957865595817566, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 351,  Mean reward: 4.481132075471698, Mean Entropy: 0.7299904823303223, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 352,  Mean reward: 0.5446428571428571, Mean Entropy: 0.6571844816207886, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 353,  Mean reward: 2.2844827586206895, Mean Entropy: 0.5918793678283691, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 354,  Mean reward: 2.2450980392156863, Mean Entropy: 0.7072490453720093, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 2.03s
Iteration: 355,  Mean reward: 2.673076923076923, Mean Entropy: 0.7077302932739258, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 356,  Mean reward: 1.9537037037037037, Mean Entropy: 0.6864404678344727, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 357,  Mean reward: 1.2857142857142858, Mean Entropy: 0.7002390623092651, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 358,  Mean reward: 1.5740740740740742, Mean Entropy: 0.6855361461639404, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 359,  Mean reward: 2.951923076923077, Mean Entropy: 0.6466629505157471, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 360,  Mean reward: 2.669642857142857, Mean Entropy: 0.6497442722320557, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 361,  Mean reward: 2.490740740740741, Mean Entropy: 0.6890265345573425, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 362,  Mean reward: 3.3214285714285716, Mean Entropy: 0.6697506904602051, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.81s
Iteration: 363,  Mean reward: 2.701923076923077, Mean Entropy: 0.612902045249939, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 364,  Mean reward: 2.7641509433962264, Mean Entropy: 0.6138097643852234, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 365,  Mean reward: 3.142857142857143, Mean Entropy: 0.6005502939224243, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 366,  Mean reward: 4.086206896551724, Mean Entropy: 0.5541872382164001, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 367,  Mean reward: 4.443396226415095, Mean Entropy: 0.5133447647094727, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 368,  Mean reward: 3.9727272727272727, Mean Entropy: 0.5657148361206055, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 369,  Mean reward: 3.9272727272727272, Mean Entropy: 0.6143965721130371, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 370,  Mean reward: 4.453703703703703, Mean Entropy: 0.4961056113243103, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 371,  Mean reward: -1.9375, Mean Entropy: 0.5171757340431213, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.91s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 372,  Mean reward: 5.201754385964913, Mean Entropy: 0.4636628031730652, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.83s
Iteration: 373,  Mean reward: 2.015151515151515, Mean Entropy: 0.5643278956413269, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 374,  Mean reward: 3.358490566037736, Mean Entropy: 0.5385401248931885, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 375,  Mean reward: 3.963636363636364, Mean Entropy: 0.643683135509491, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 376,  Mean reward: 5.288135593220339, Mean Entropy: 0.5243529081344604, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.81s
Iteration: 377,  Mean reward: 3.638888888888889, Mean Entropy: 0.5653740167617798, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 378,  Mean reward: 4.813559322033898, Mean Entropy: 0.5222262740135193, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 379,  Mean reward: 5.211864406779661, Mean Entropy: 0.6398416757583618, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 380,  Mean reward: 4.764150943396227, Mean Entropy: 0.6407907009124756, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.90s
Iteration: 381,  Mean reward: 4.047169811320755, Mean Entropy: 0.46734869480133057, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 382,  Mean reward: -3.217948717948718, Mean Entropy: 0.43076395988464355, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 383,  Mean reward: 2.4375, Mean Entropy: 0.6421793699264526, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 384,  Mean reward: 5.3, Mean Entropy: 0.48975062370300293, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 385,  Mean reward: 5.745901639344262, Mean Entropy: 0.46969565749168396, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.84s
Iteration: 386,  Mean reward: 4.780701754385965, Mean Entropy: 0.5289833545684814, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 387,  Mean reward: 6.075757575757576, Mean Entropy: 0.42795658111572266, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 1.17s
Iteration: 388,  Mean reward: 0.6774193548387096, Mean Entropy: 0.5351676344871521, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 389,  Mean reward: 3.2758620689655173, Mean Entropy: 0.568901002407074, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.89s
Iteration: 390,  Mean reward: 4.203703703703703, Mean Entropy: 0.5553883910179138, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 391,  Mean reward: 5.398305084745763, Mean Entropy: 0.3814567029476166, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 392,  Mean reward: -1.0, Mean Entropy: 0.25053220987319946, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 393,  Mean reward: 4.388888888888889, Mean Entropy: 0.4952412545681, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 394,  Mean reward: 3.9491525423728815, Mean Entropy: 0.5687804222106934, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 395,  Mean reward: 5.064516129032258, Mean Entropy: 0.5005762577056885, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.95s
Iteration: 396,  Mean reward: 3.5, Mean Entropy: 0.47799152135849, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 397,  Mean reward: 4.5701754385964914, Mean Entropy: 0.5294123888015747, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.86s
Iteration: 398,  Mean reward: 5.7890625, Mean Entropy: 0.4261225163936615, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 399,  Mean reward: 3.992424242424242, Mean Entropy: 0.5282150506973267, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 400,  Mean reward: 3.9918032786885247, Mean Entropy: 0.6569040417671204, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.94s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 6.044776119402985, Mean Entropy: 0.41897448897361755, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 402,  Mean reward: 4.827272727272727, Mean Entropy: 0.5438203811645508, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 403,  Mean reward: 5.645161290322581, Mean Entropy: 0.4808834493160248, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 404,  Mean reward: 4.137931034482759, Mean Entropy: 0.6211947798728943, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.98s
Iteration: 405,  Mean reward: 5.451612903225806, Mean Entropy: 0.49633359909057617, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 406,  Mean reward: 4.4576271186440675, Mean Entropy: 0.5468761920928955, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.87s
Iteration: 407,  Mean reward: 4.593220338983051, Mean Entropy: 0.5216127634048462, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 408,  Mean reward: 6.798611111111111, Mean Entropy: 0.2640218436717987, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 409,  Mean reward: 6.701492537313433, Mean Entropy: 0.39283037185668945, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 410,  Mean reward: 6.507142857142857, Mean Entropy: 0.36930036544799805, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 411,  Mean reward: -0.8046875, Mean Entropy: 0.4847051501274109, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 412,  Mean reward: 3.814814814814815, Mean Entropy: 0.5126163363456726, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 413,  Mean reward: 6.884057971014493, Mean Entropy: 0.3665778338909149, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.94s
Iteration: 414,  Mean reward: -1.0, Mean Entropy: 0.25298357009887695, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 415,  Mean reward: 5.270491803278689, Mean Entropy: 0.36977142095565796, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 416,  Mean reward: 6.046153846153846, Mean Entropy: 0.3043637275695801, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 417,  Mean reward: 6.515151515151516, Mean Entropy: 0.40452441573143005, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 418,  Mean reward: 6.875, Mean Entropy: 0.4093991219997406, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 419,  Mean reward: 6.113636363636363, Mean Entropy: 0.5015078783035278, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 420,  Mean reward: 5.515625, Mean Entropy: 0.42524203658103943, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 421,  Mean reward: 6.282258064516129, Mean Entropy: 0.4194372594356537, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 422,  Mean reward: -2.75, Mean Entropy: 0.31000959873199463, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 423,  Mean reward: 3.8275862068965516, Mean Entropy: 0.6057180762290955, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 424,  Mean reward: 2.9237288135593222, Mean Entropy: 0.6482799053192139, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 425,  Mean reward: 4.163793103448276, Mean Entropy: 0.5500037670135498, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 2.08s
Iteration: 426,  Mean reward: 5.745454545454545, Mean Entropy: 0.5301357507705688, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 427,  Mean reward: 7.119718309859155, Mean Entropy: 0.33855104446411133, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 428,  Mean reward: 6.229508196721311, Mean Entropy: 0.38044488430023193, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 429,  Mean reward: -1.5723684210526316, Mean Entropy: 0.32015371322631836, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 430,  Mean reward: 5.532258064516129, Mean Entropy: 0.5190651416778564, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.93s
Iteration: 431,  Mean reward: 6.484615384615385, Mean Entropy: 0.30792754888534546, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 432,  Mean reward: -2.4675324675324677, Mean Entropy: 0.35234054923057556, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 433,  Mean reward: 4.30327868852459, Mean Entropy: 0.5741737484931946, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 434,  Mean reward: 2.242857142857143, Mean Entropy: 0.38696128129959106, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 435,  Mean reward: 5.925, Mean Entropy: 0.4669286012649536, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 436,  Mean reward: 5.059322033898305, Mean Entropy: 0.4915684163570404, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 437,  Mean reward: 4.852941176470588, Mean Entropy: 0.46410655975341797, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 438,  Mean reward: 4.911764705882353, Mean Entropy: 0.3680589199066162, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 439,  Mean reward: 6.5546875, Mean Entropy: 0.4675719738006592, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 440,  Mean reward: 1.775, Mean Entropy: 0.5416532754898071, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 441,  Mean reward: 6.6521739130434785, Mean Entropy: 0.3791677951812744, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.89s
Iteration: 442,  Mean reward: -0.9855072463768116, Mean Entropy: 0.48649072647094727, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 443,  Mean reward: 1.0508474576271187, Mean Entropy: 0.5464487671852112, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 444,  Mean reward: 5.830645161290323, Mean Entropy: 0.3522782325744629, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 445,  Mean reward: 6.955882352941177, Mean Entropy: 0.17409178614616394, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 446,  Mean reward: -2.25, Mean Entropy: 0.3317290246486664, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 447,  Mean reward: 3.919642857142857, Mean Entropy: 0.5077336430549622, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 448,  Mean reward: 6.527397260273973, Mean Entropy: 0.2680663466453552, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 449,  Mean reward: 6.477611940298507, Mean Entropy: 0.1543659120798111, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 450,  Mean reward: -2.25, Mean Entropy: 0.0001416732557117939, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 451,  Mean reward: -0.25, Mean Entropy: 0.26136836409568787, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 452,  Mean reward: 4.964912280701754, Mean Entropy: 0.3395204544067383, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 453,  Mean reward: -1.25, Mean Entropy: 0.28086671233177185, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 454,  Mean reward: 4.324074074074074, Mean Entropy: 0.39255496859550476, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.82s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 455,  Mean reward: 7.347222222222222, Mean Entropy: 0.25255489349365234, complete_episode_count: 72.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 456,  Mean reward: 3.2017543859649122, Mean Entropy: 0.5776011943817139, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 457,  Mean reward: 4.464285714285714, Mean Entropy: 0.3367614150047302, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 458,  Mean reward: -1.0, Mean Entropy: 0.2872324585914612, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 459,  Mean reward: 5.2155172413793105, Mean Entropy: 0.3543321490287781, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 460,  Mean reward: -1.4153846153846155, Mean Entropy: 0.36592739820480347, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 461,  Mean reward: 6.425373134328358, Mean Entropy: 0.23835192620754242, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 462,  Mean reward: 6.651515151515151, Mean Entropy: 0.2921081781387329, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 463,  Mean reward: -0.55, Mean Entropy: 0.49502214789390564, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 464,  Mean reward: 4.922413793103448, Mean Entropy: 0.4507717490196228, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 2.05s
Iteration: 465,  Mean reward: 3.572463768115942, Mean Entropy: 0.30945447087287903, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 466,  Mean reward: 7.267123287671233, Mean Entropy: 0.2798067033290863, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 467,  Mean reward: 0.8974358974358975, Mean Entropy: 0.19046303629875183, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 468,  Mean reward: 4.632352941176471, Mean Entropy: 0.2174663543701172, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.91s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 469,  Mean reward: 7.6118421052631575, Mean Entropy: 0.2700863480567932, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 470,  Mean reward: -1.4104477611940298, Mean Entropy: 0.355934202671051, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 471,  Mean reward: 2.3278688524590163, Mean Entropy: 0.5851150751113892, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 472,  Mean reward: 4.353448275862069, Mean Entropy: 0.29077014327049255, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.79s
Iteration: 473,  Mean reward: -1.25, Mean Entropy: 0.3195347189903259, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 474,  Mean reward: 6.416666666666667, Mean Entropy: 0.4271702766418457, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 475,  Mean reward: -1.2666666666666666, Mean Entropy: 0.3032185733318329, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 476,  Mean reward: 2.037878787878788, Mean Entropy: 0.4687100350856781, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 477,  Mean reward: 5.721311475409836, Mean Entropy: 0.4595057964324951, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 478,  Mean reward: 5.008771929824562, Mean Entropy: 0.48516595363616943, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 479,  Mean reward: 7.5, Mean Entropy: 0.3369389474391937, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 480,  Mean reward: 4.079365079365079, Mean Entropy: 0.29835033416748047, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 481,  Mean reward: 7.211267605633803, Mean Entropy: 0.3166750967502594, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 482,  Mean reward: 0.24305555555555555, Mean Entropy: 0.2571919858455658, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 483,  Mean reward: 1.7887323943661972, Mean Entropy: 0.3593379855155945, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 484,  Mean reward: 5.672131147540983, Mean Entropy: 0.4264647960662842, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 485,  Mean reward: 7.15, Mean Entropy: 0.25684136152267456, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 486,  Mean reward: 2.12, Mean Entropy: 0.2717844545841217, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 487,  Mean reward: 6.015873015873016, Mean Entropy: 0.20214848220348358, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.96s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 488,  Mean reward: 7.981012658227848, Mean Entropy: 0.018361225724220276, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 489,  Mean reward: -2.25, Mean Entropy: 0.005665884353220463, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 490,  Mean reward: -4.170886075949367, Mean Entropy: 0.27971285581588745, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 491,  Mean reward: 4.859375, Mean Entropy: 0.461913526058197, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 492,  Mean reward: 4.296610169491525, Mean Entropy: 0.41436582803726196, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 493,  Mean reward: 6.897058823529412, Mean Entropy: 0.420773983001709, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 494,  Mean reward: -0.512987012987013, Mean Entropy: 0.27178460359573364, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 495,  Mean reward: 3.7818181818181817, Mean Entropy: 0.3370913863182068, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 496,  Mean reward: 7.383561643835616, Mean Entropy: 0.29819297790527344, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 497,  Mean reward: 1.6428571428571428, Mean Entropy: 0.3008890151977539, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 498,  Mean reward: 5.866666666666666, Mean Entropy: 0.563790500164032, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 499,  Mean reward: 2.3771929824561404, Mean Entropy: 0.6346808075904846, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 500,  Mean reward: 6.934782608695652, Mean Entropy: 0.3214196562767029, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.90s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -0.5, Mean Entropy: 0.2174786776304245, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 502,  Mean reward: 5.176056338028169, Mean Entropy: 0.2147725522518158, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 503,  Mean reward: -1.25, Mean Entropy: 0.2558813989162445, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 504,  Mean reward: 6.756944444444445, Mean Entropy: 0.29869192838668823, complete_episode_count: 72.0, Gather time: 0.75s, Train time: 0.92s
Iteration: 505,  Mean reward: 7.466666666666667, Mean Entropy: 0.07840833067893982, complete_episode_count: 75.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 506,  Mean reward: -0.6265822784810127, Mean Entropy: 0.06718866527080536, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 507,  Mean reward: -1.5256410256410255, Mean Entropy: 0.24713274836540222, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 508,  Mean reward: 4.944444444444445, Mean Entropy: 0.41826093196868896, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 509,  Mean reward: 4.860655737704918, Mean Entropy: 0.33999377489089966, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 510,  Mean reward: -2.863013698630137, Mean Entropy: 0.14357368648052216, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.93s
Iteration: 511,  Mean reward: 1.3115942028985508, Mean Entropy: 0.4278453290462494, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 512,  Mean reward: 5.35, Mean Entropy: 0.23730473220348358, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 513,  Mean reward: 7.4411764705882355, Mean Entropy: 0.16458718478679657, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 514,  Mean reward: -3.25, Mean Entropy: 0.0020149999763816595, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 515,  Mean reward: -2.0, Mean Entropy: 0.02836395800113678, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 516,  Mean reward: 7.625, Mean Entropy: 0.038177505135536194, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 517,  Mean reward: 7.698717948717949, Mean Entropy: 0.06819707900285721, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 518,  Mean reward: -2.25, Mean Entropy: 0.0014494918286800385, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 519,  Mean reward: -3.8987341772151898, Mean Entropy: 0.031083298847079277, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 520,  Mean reward: 7.82051282051282, Mean Entropy: 0.021651502698659897, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.93s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 521,  Mean reward: 8.0, Mean Entropy: 0.007195671554654837, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 522,  Mean reward: -1.5, Mean Entropy: 0.0005362112424336374, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.98s
Iteration: 523,  Mean reward: -1.25, Mean Entropy: 0.24000488221645355, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.91s
Iteration: 524,  Mean reward: 7.506944444444445, Mean Entropy: 0.10980629920959473, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 525,  Mean reward: 7.801282051282051, Mean Entropy: 0.008007998578250408, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 526,  Mean reward: -2.0, Mean Entropy: 0.00044476205948740244, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 527,  Mean reward: -2.25, Mean Entropy: 0.2535967230796814, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 528,  Mean reward: 7.551470588235294, Mean Entropy: 0.26412898302078247, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 529,  Mean reward: 7.1875, Mean Entropy: 0.1803700178861618, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 530,  Mean reward: -0.5, Mean Entropy: 0.0018147171940654516, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 531,  Mean reward: -3.0, Mean Entropy: 0.28421759605407715, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 532,  Mean reward: 4.859375, Mean Entropy: 0.2083580195903778, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 533,  Mean reward: 7.58, Mean Entropy: 0.12809498608112335, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 534,  Mean reward: -4.424050632911392, Mean Entropy: 0.01839606463909149, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 535,  Mean reward: -2.4177215189873418, Mean Entropy: 0.2786579728126526, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 536,  Mean reward: 2.64, Mean Entropy: 0.6430641412734985, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 537,  Mean reward: 6.594202898550725, Mean Entropy: 0.1233503445982933, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 538,  Mean reward: -0.5, Mean Entropy: 0.0012837452813982964, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 539,  Mean reward: -1.75, Mean Entropy: 0.24087539315223694, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 540,  Mean reward: 6.436507936507937, Mean Entropy: 0.3751373887062073, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 541,  Mean reward: 5.766129032258065, Mean Entropy: 0.25057506561279297, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 542,  Mean reward: 6.7101449275362315, Mean Entropy: 0.3406076431274414, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 543,  Mean reward: 6.613636363636363, Mean Entropy: 0.31399500370025635, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 544,  Mean reward: 5.190476190476191, Mean Entropy: 0.3415851294994354, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 545,  Mean reward: 6.7, Mean Entropy: 0.349122017621994, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 546,  Mean reward: 3.956896551724138, Mean Entropy: 0.27578791975975037, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 2.06s
Iteration: 547,  Mean reward: 7.814102564102564, Mean Entropy: 0.021082624793052673, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 548,  Mean reward: 8.0, Mean Entropy: 0.11732557415962219, complete_episode_count: 80.0, Gather time: 0.68s, Train time: 0.92s
Iteration: 549,  Mean reward: 4.640350877192983, Mean Entropy: 0.23977594077587128, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 550,  Mean reward: 6.638461538461539, Mean Entropy: 0.2564847469329834, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 551,  Mean reward: 5.966101694915254, Mean Entropy: 0.24780242145061493, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 552,  Mean reward: 6.116666666666666, Mean Entropy: 0.30868202447891235, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.85s
Iteration: 553,  Mean reward: 2.475806451612903, Mean Entropy: 0.2428789585828781, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 554,  Mean reward: 7.756578947368421, Mean Entropy: 0.11808258295059204, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 555,  Mean reward: 7.555555555555555, Mean Entropy: 0.07422322034835815, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 556,  Mean reward: -3.75, Mean Entropy: 0.0001487959234509617, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.97s
Iteration: 557,  Mean reward: -2.0, Mean Entropy: 0.13908295333385468, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 558,  Mean reward: 2.5737704918032787, Mean Entropy: 0.047081269323825836, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 0.00022651624749414623, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 560,  Mean reward: 8.0, Mean Entropy: 0.206563338637352, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 561,  Mean reward: -2.75, Mean Entropy: 0.0014996598474681377, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 562,  Mean reward: -0.6265822784810127, Mean Entropy: 0.007807796820998192, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 563,  Mean reward: 8.0, Mean Entropy: 0.004929675720632076, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 564,  Mean reward: 8.0, Mean Entropy: 0.05642678961157799, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 565,  Mean reward: -3.5, Mean Entropy: 0.007348177023231983, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 566,  Mean reward: -1.25, Mean Entropy: 0.03405313566327095, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 567,  Mean reward: 8.0, Mean Entropy: 0.011285781860351562, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 568,  Mean reward: 8.0, Mean Entropy: 0.18775710463523865, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 569,  Mean reward: 6.944444444444445, Mean Entropy: 0.19046364724636078, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 570,  Mean reward: -2.0, Mean Entropy: 2.4093508272926556e-06, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 571,  Mean reward: -1.75, Mean Entropy: 0.1270308792591095, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 572,  Mean reward: 3.685483870967742, Mean Entropy: 0.35021600127220154, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.96s
Iteration: 573,  Mean reward: 6.75, Mean Entropy: 0.08881843090057373, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 574,  Mean reward: 7.901315789473684, Mean Entropy: 0.16427497565746307, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 575,  Mean reward: 7.046875, Mean Entropy: 0.08563318103551865, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 576,  Mean reward: 0.25, Mean Entropy: 0.0018906563054770231, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 577,  Mean reward: -0.25, Mean Entropy: 0.16319037973880768, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 578,  Mean reward: 7.52054794520548, Mean Entropy: 0.13122552633285522, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 579,  Mean reward: 7.06, Mean Entropy: 0.2288295328617096, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 580,  Mean reward: 7.6875, Mean Entropy: 0.24159064888954163, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 581,  Mean reward: 7.5588235294117645, Mean Entropy: 0.2409363090991974, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 582,  Mean reward: 7.311594202898551, Mean Entropy: 0.18679210543632507, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 583,  Mean reward: 7.8618421052631575, Mean Entropy: 0.08024720102548599, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 584,  Mean reward: 8.0, Mean Entropy: 0.09899931401014328, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 585,  Mean reward: 7.103896103896104, Mean Entropy: 0.060373805463314056, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 1.14s
Iteration: 586,  Mean reward: 7.409090909090909, Mean Entropy: 0.023569941520690918, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 587,  Mean reward: 8.0, Mean Entropy: 0.06694846600294113, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 588,  Mean reward: 7.980769230769231, Mean Entropy: 0.011313378810882568, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 589,  Mean reward: 8.0, Mean Entropy: 0.0364200621843338, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 590,  Mean reward: 7.685897435897436, Mean Entropy: 0.027208995074033737, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.97s
Iteration: 591,  Mean reward: 8.0, Mean Entropy: 0.04457169398665428, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 592,  Mean reward: 7.981012658227848, Mean Entropy: 0.0005633044056594372, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 593,  Mean reward: 8.0, Mean Entropy: 0.011941924691200256, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 594,  Mean reward: 8.0, Mean Entropy: 0.00389909278601408, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 595,  Mean reward: 8.0, Mean Entropy: 0.01400003395974636, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 596,  Mean reward: 8.0, Mean Entropy: 0.15627405047416687, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 597,  Mean reward: -2.8771929824561404, Mean Entropy: 0.2371625155210495, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 598,  Mean reward: 2.492857142857143, Mean Entropy: 0.20122425258159637, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 599,  Mean reward: 6.695945945945946, Mean Entropy: 0.04422270506620407, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 600,  Mean reward: 8.0, Mean Entropy: 0.07547689229249954, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 7.609589041095891, Mean Entropy: 0.0944904237985611, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 602,  Mean reward: 7.67948717948718, Mean Entropy: 0.06456325948238373, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 603,  Mean reward: 7.675324675324675, Mean Entropy: 0.028701474890112877, complete_episode_count: 77.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 604,  Mean reward: 8.0, Mean Entropy: 0.007979326881468296, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.97s
Iteration: 605,  Mean reward: 8.0, Mean Entropy: 0.00387529656291008, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.0007120731170289218, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 607,  Mean reward: 8.0, Mean Entropy: 0.005784439854323864, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 608,  Mean reward: 8.0, Mean Entropy: 5.5794414947740734e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 609,  Mean reward: 8.0, Mean Entropy: 1.7398144336766563e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 610,  Mean reward: 8.0, Mean Entropy: 2.662092992977705e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 611,  Mean reward: 8.0, Mean Entropy: 2.0525047148112208e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 612,  Mean reward: 8.0, Mean Entropy: 1.8769758753478527e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 613,  Mean reward: 8.0, Mean Entropy: 1.9386809071875177e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 614,  Mean reward: 8.0, Mean Entropy: 1.7602786101633683e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 615,  Mean reward: 8.0, Mean Entropy: 1.943848837981932e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 616,  Mean reward: 8.0, Mean Entropy: 1.7529680917505175e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 617,  Mean reward: 8.0, Mean Entropy: 1.748411523294635e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 618,  Mean reward: 8.0, Mean Entropy: 1.898247683129739e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 619,  Mean reward: 8.0, Mean Entropy: 2.1039242710685357e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 620,  Mean reward: 8.0, Mean Entropy: 1.7849150026449934e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 621,  Mean reward: 8.0, Mean Entropy: 1.7990183550864458e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 622,  Mean reward: 8.0, Mean Entropy: 1.9115206669084728e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 623,  Mean reward: 8.0, Mean Entropy: 2.0016996131744236e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 624,  Mean reward: 8.0, Mean Entropy: 1.9931900169467553e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 625,  Mean reward: 8.0, Mean Entropy: 1.7948372260434553e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 626,  Mean reward: 8.0, Mean Entropy: 1.925920878420584e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 627,  Mean reward: 8.0, Mean Entropy: 1.5759305824758485e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.93s
Iteration: 628,  Mean reward: 8.0, Mean Entropy: 2.3654502001591027e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 629,  Mean reward: 8.0, Mean Entropy: 2.1648365873261355e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 630,  Mean reward: 8.0, Mean Entropy: 1.638119647395797e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 631,  Mean reward: 8.0, Mean Entropy: 2.2975436877459288e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 632,  Mean reward: 8.0, Mean Entropy: 1.8909249774878845e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 633,  Mean reward: 8.0, Mean Entropy: 1.8630989870871417e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 634,  Mean reward: 8.0, Mean Entropy: 2.3125809093471617e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 635,  Mean reward: 8.0, Mean Entropy: 1.8978296793648042e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 636,  Mean reward: 8.0, Mean Entropy: 2.167396087315865e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 637,  Mean reward: 8.0, Mean Entropy: 2.2747530238120817e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 638,  Mean reward: 8.0, Mean Entropy: 1.8719136278377846e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 639,  Mean reward: 8.0, Mean Entropy: 1.809825698728673e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 640,  Mean reward: 8.0, Mean Entropy: 2.0134448277531192e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 641,  Mean reward: 8.0, Mean Entropy: 2.1197305613895878e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 642,  Mean reward: 8.0, Mean Entropy: 2.2131582227302715e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 643,  Mean reward: 8.0, Mean Entropy: 1.8458649719832465e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 644,  Mean reward: 8.0, Mean Entropy: 2.166684134863317e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 645,  Mean reward: 8.0, Mean Entropy: 2.0362258510431275e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 646,  Mean reward: 8.0, Mean Entropy: 2.194877561123576e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 647,  Mean reward: 8.0, Mean Entropy: 2.2933363652555272e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 648,  Mean reward: 8.0, Mean Entropy: 2.0752566342707723e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 649,  Mean reward: 8.0, Mean Entropy: 1.877201793831773e-05, complete_episode_count: 80.0, Gather time: 0.73s, Train time: 0.90s
Iteration: 650,  Mean reward: 8.0, Mean Entropy: 2.0352481442387216e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 651,  Mean reward: 8.0, Mean Entropy: 2.0370729544083588e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 652,  Mean reward: 8.0, Mean Entropy: 1.9485962184262462e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 653,  Mean reward: 8.0, Mean Entropy: 2.491872146492824e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 654,  Mean reward: 8.0, Mean Entropy: 2.1899864805163816e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 655,  Mean reward: 8.0, Mean Entropy: 2.4052191292867064e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 656,  Mean reward: 8.0, Mean Entropy: 2.3012149540591054e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.93s
Iteration: 657,  Mean reward: 8.0, Mean Entropy: 2.158062852686271e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 658,  Mean reward: 8.0, Mean Entropy: 2.0539233446470462e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 659,  Mean reward: 8.0, Mean Entropy: 2.356833101657685e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 660,  Mean reward: 8.0, Mean Entropy: 2.23825809371192e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 661,  Mean reward: 8.0, Mean Entropy: 2.5838910005404614e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 662,  Mean reward: 8.0, Mean Entropy: 2.084547668346204e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.97s
Iteration: 663,  Mean reward: 8.0, Mean Entropy: 1.92949955817312e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 664,  Mean reward: 8.0, Mean Entropy: 2.6419460482429713e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 665,  Mean reward: 8.0, Mean Entropy: 2.1459225536091253e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 666,  Mean reward: 8.0, Mean Entropy: 2.243745075247716e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 667,  Mean reward: 8.0, Mean Entropy: 1.9227794837206602e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 668,  Mean reward: 8.0, Mean Entropy: 2.5584717150195502e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 669,  Mean reward: 8.0, Mean Entropy: 2.3033198885968886e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 670,  Mean reward: 8.0, Mean Entropy: 2.479064278304577e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 671,  Mean reward: 8.0, Mean Entropy: 2.5541387003613636e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 672,  Mean reward: 8.0, Mean Entropy: 2.0804000087082386e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 673,  Mean reward: 8.0, Mean Entropy: 2.4165648937923834e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 674,  Mean reward: 8.0, Mean Entropy: 2.4945205950643867e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 675,  Mean reward: 8.0, Mean Entropy: 2.5549121346557513e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 676,  Mean reward: 8.0, Mean Entropy: 2.3521677576354705e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 677,  Mean reward: 8.0, Mean Entropy: 2.294842852279544e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 678,  Mean reward: 8.0, Mean Entropy: 2.8442402253858745e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 679,  Mean reward: 8.0, Mean Entropy: 2.6599762350087985e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 680,  Mean reward: 8.0, Mean Entropy: 2.566557304817252e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 681,  Mean reward: 8.0, Mean Entropy: 2.304183362866752e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 682,  Mean reward: 8.0, Mean Entropy: 2.982987462019082e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 683,  Mean reward: 8.0, Mean Entropy: 2.6389847334939986e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 684,  Mean reward: 8.0, Mean Entropy: 2.6852121663978323e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 685,  Mean reward: 8.0, Mean Entropy: 2.5566594558767974e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 686,  Mean reward: 8.0, Mean Entropy: 2.220602982561104e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 687,  Mean reward: 8.0, Mean Entropy: 2.6599422199069522e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 688,  Mean reward: 8.0, Mean Entropy: 2.7634574507828802e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 689,  Mean reward: 8.0, Mean Entropy: 2.8857595680165105e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 690,  Mean reward: 8.0, Mean Entropy: 2.538230000936892e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 691,  Mean reward: 8.0, Mean Entropy: 3.146938615827821e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 692,  Mean reward: 8.0, Mean Entropy: 2.9716407880187035e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.13s
Iteration: 693,  Mean reward: 8.0, Mean Entropy: 3.05833964375779e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 694,  Mean reward: 8.0, Mean Entropy: 2.511366255930625e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 695,  Mean reward: 8.0, Mean Entropy: 2.531064092181623e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 696,  Mean reward: 8.0, Mean Entropy: 2.520361158531159e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 697,  Mean reward: 8.0, Mean Entropy: 2.7295141990180127e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 698,  Mean reward: 8.0, Mean Entropy: 3.274793198215775e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.95s
Iteration: 699,  Mean reward: 8.0, Mean Entropy: 2.9590175472549163e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 700,  Mean reward: 8.0, Mean Entropy: 2.659592792042531e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: 8.0, Mean Entropy: 2.613050673971884e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 702,  Mean reward: 8.0, Mean Entropy: 2.7969952498096973e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 703,  Mean reward: 8.0, Mean Entropy: 2.8781400033039972e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 704,  Mean reward: 8.0, Mean Entropy: 2.869004492822569e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 705,  Mean reward: 8.0, Mean Entropy: 2.9471670131897554e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 706,  Mean reward: 8.0, Mean Entropy: 3.342560012242757e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 707,  Mean reward: 8.0, Mean Entropy: 2.7217698516324162e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 708,  Mean reward: 8.0, Mean Entropy: 2.694604336284101e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 709,  Mean reward: 8.0, Mean Entropy: 3.38231839123182e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 710,  Mean reward: 8.0, Mean Entropy: 2.9249433282529935e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 711,  Mean reward: 8.0, Mean Entropy: 2.991275687236339e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 712,  Mean reward: 8.0, Mean Entropy: 3.953365740017034e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 713,  Mean reward: 8.0, Mean Entropy: 3.639504575403407e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 714,  Mean reward: 8.0, Mean Entropy: 3.638815542217344e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 715,  Mean reward: 8.0, Mean Entropy: 2.6309671738999896e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 716,  Mean reward: 8.0, Mean Entropy: 4.089947833563201e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 717,  Mean reward: 8.0, Mean Entropy: 4.0163293306250125e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 718,  Mean reward: 8.0, Mean Entropy: 3.150948032271117e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 719,  Mean reward: 8.0, Mean Entropy: 3.9354148611892015e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 720,  Mean reward: 8.0, Mean Entropy: 3.86936153518036e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 721,  Mean reward: 8.0, Mean Entropy: 3.6368208384374157e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 722,  Mean reward: 8.0, Mean Entropy: 4.3415624531917274e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 723,  Mean reward: 8.0, Mean Entropy: 3.695729174069129e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 724,  Mean reward: 8.0, Mean Entropy: 3.9065176679287106e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 725,  Mean reward: 8.0, Mean Entropy: 4.140437158639543e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 726,  Mean reward: 8.0, Mean Entropy: 3.0135932320263237e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 727,  Mean reward: 8.0, Mean Entropy: 4.150102904532105e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 728,  Mean reward: 8.0, Mean Entropy: 3.6471523344516754e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 1.08s
Iteration: 729,  Mean reward: 8.0, Mean Entropy: 3.006649058079347e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 730,  Mean reward: 8.0, Mean Entropy: 3.711070166900754e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 731,  Mean reward: 8.0, Mean Entropy: 4.2045547161251307e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 732,  Mean reward: 8.0, Mean Entropy: 3.3706994145177305e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 733,  Mean reward: 8.0, Mean Entropy: 4.696350879385136e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 734,  Mean reward: 8.0, Mean Entropy: 3.605785605031997e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 735,  Mean reward: 8.0, Mean Entropy: 3.2447336707264185e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 736,  Mean reward: 8.0, Mean Entropy: 3.008865678566508e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 737,  Mean reward: 8.0, Mean Entropy: 3.601109710871242e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 738,  Mean reward: 8.0, Mean Entropy: 4.1395782318431884e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 739,  Mean reward: 8.0, Mean Entropy: 3.579453186830506e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 740,  Mean reward: 8.0, Mean Entropy: 3.842579826596193e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 741,  Mean reward: 8.0, Mean Entropy: 4.0432376408716664e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 742,  Mean reward: 8.0, Mean Entropy: 4.1865569073706865e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 743,  Mean reward: 8.0, Mean Entropy: 4.1417672036914155e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 744,  Mean reward: 8.0, Mean Entropy: 3.954315252485685e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 745,  Mean reward: 8.0, Mean Entropy: 3.817753167822957e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 746,  Mean reward: 8.0, Mean Entropy: 4.2033403587993234e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 747,  Mean reward: 8.0, Mean Entropy: 5.004522972740233e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 748,  Mean reward: 8.0, Mean Entropy: 4.857406747760251e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.00s
Iteration: 749,  Mean reward: 8.0, Mean Entropy: 4.980916128261015e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 750,  Mean reward: 8.0, Mean Entropy: 4.095854819752276e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 751,  Mean reward: 8.0, Mean Entropy: 4.54340988653712e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 752,  Mean reward: 8.0, Mean Entropy: 5.536645039683208e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 753,  Mean reward: 8.0, Mean Entropy: 5.4807140259072185e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 754,  Mean reward: 8.0, Mean Entropy: 5.0818354793591425e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 755,  Mean reward: 8.0, Mean Entropy: 5.0033238949254155e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 756,  Mean reward: 8.0, Mean Entropy: 4.2715080780908465e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 757,  Mean reward: 8.0, Mean Entropy: 5.112503276905045e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 758,  Mean reward: 8.0, Mean Entropy: 5.950484046479687e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 759,  Mean reward: 8.0, Mean Entropy: 5.560610952670686e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 760,  Mean reward: 8.0, Mean Entropy: 4.5209289964986965e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.89s
Iteration: 761,  Mean reward: 8.0, Mean Entropy: 5.165619222680107e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 762,  Mean reward: 8.0, Mean Entropy: 4.837723099626601e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 763,  Mean reward: 8.0, Mean Entropy: 4.690108107752167e-05, complete_episode_count: 80.0, Gather time: 0.76s, Train time: 0.94s
Iteration: 764,  Mean reward: 8.0, Mean Entropy: 4.502937008510344e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 765,  Mean reward: 8.0, Mean Entropy: 6.113816925790161e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 766,  Mean reward: 8.0, Mean Entropy: 5.731721466872841e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 767,  Mean reward: 8.0, Mean Entropy: 6.396305252565071e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 768,  Mean reward: 8.0, Mean Entropy: 5.783471715403721e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 769,  Mean reward: 8.0, Mean Entropy: 4.8332625738112256e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 770,  Mean reward: 8.0, Mean Entropy: 6.683879473712295e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 771,  Mean reward: 8.0, Mean Entropy: 6.35696342214942e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 772,  Mean reward: 8.0, Mean Entropy: 6.128325185272843e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 773,  Mean reward: 8.0, Mean Entropy: 6.406361353583634e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 774,  Mean reward: 8.0, Mean Entropy: 4.588802039506845e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 775,  Mean reward: 8.0, Mean Entropy: 5.86551905144006e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 776,  Mean reward: 8.0, Mean Entropy: 5.8221798099111766e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 777,  Mean reward: 8.0, Mean Entropy: 6.813999789301306e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 778,  Mean reward: 8.0, Mean Entropy: 5.342257645679638e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 779,  Mean reward: 8.0, Mean Entropy: 6.723435217281803e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 780,  Mean reward: 8.0, Mean Entropy: 6.539691094076261e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 781,  Mean reward: 8.0, Mean Entropy: 6.461435987148434e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 782,  Mean reward: 8.0, Mean Entropy: 5.7007673603948206e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 783,  Mean reward: 8.0, Mean Entropy: 7.186489528976381e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 784,  Mean reward: 8.0, Mean Entropy: 6.328120070975274e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 785,  Mean reward: 8.0, Mean Entropy: 6.039521758793853e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 786,  Mean reward: 8.0, Mean Entropy: 6.22145744273439e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 787,  Mean reward: 8.0, Mean Entropy: 8.348130359081551e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 788,  Mean reward: 8.0, Mean Entropy: 5.501065606949851e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 789,  Mean reward: 8.0, Mean Entropy: 8.785852696746588e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.98s
Iteration: 790,  Mean reward: 8.0, Mean Entropy: 5.6555549235781655e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 791,  Mean reward: 8.0, Mean Entropy: 6.717722135363147e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 792,  Mean reward: 8.0, Mean Entropy: 6.206588295754045e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 793,  Mean reward: 8.0, Mean Entropy: 7.275261305039749e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 794,  Mean reward: 8.0, Mean Entropy: 0.00016051603597588837, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 795,  Mean reward: 8.0, Mean Entropy: 0.0013576241908594966, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 796,  Mean reward: 8.0, Mean Entropy: 0.00010887454118346795, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 797,  Mean reward: -0.25, Mean Entropy: 0.0013225622242316604, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 798,  Mean reward: -3.0, Mean Entropy: 0.00611458346247673, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 799,  Mean reward: -1.5, Mean Entropy: 0.01263611949980259, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.93s
Iteration: 800,  Mean reward: -1.1474358974358974, Mean Entropy: 0.49297890067100525, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.92s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: 5.703389830508475, Mean Entropy: 0.20039641857147217, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.83s
Iteration: 802,  Mean reward: 7.901315789473684, Mean Entropy: 0.016152460128068924, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 803,  Mean reward: -3.5, Mean Entropy: 0.010243631899356842, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 804,  Mean reward: -3.0, Mean Entropy: 0.02499004267156124, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 805,  Mean reward: -2.0384615384615383, Mean Entropy: 0.026631221175193787, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 806,  Mean reward: -0.75, Mean Entropy: 0.1679212749004364, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.14s
Iteration: 807,  Mean reward: 7.86, Mean Entropy: 0.3796066641807556, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 808,  Mean reward: 6.68, Mean Entropy: 0.019581686705350876, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 809,  Mean reward: 7.974683544303797, Mean Entropy: 0.06828126311302185, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.93s
Iteration: 810,  Mean reward: -1.0, Mean Entropy: 0.02795799821615219, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 811,  Mean reward: -4.545454545454546, Mean Entropy: 0.1545916050672531, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 812,  Mean reward: 3.2467532467532467, Mean Entropy: 0.15332478284835815, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 813,  Mean reward: 6.955223880597015, Mean Entropy: 0.30333590507507324, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 814,  Mean reward: 3.81, Mean Entropy: 0.5557088851928711, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 815,  Mean reward: 2.8207547169811322, Mean Entropy: 0.7195843458175659, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 816,  Mean reward: 4.160714285714286, Mean Entropy: 0.36551815271377563, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.84s
Iteration: 817,  Mean reward: 6.333333333333333, Mean Entropy: 0.3307962119579315, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 818,  Mean reward: 6.873134328358209, Mean Entropy: 0.06312249600887299, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 819,  Mean reward: -3.75, Mean Entropy: 9.341881377622485e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 820,  Mean reward: -3.75, Mean Entropy: 0.04733004793524742, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 821,  Mean reward: 7.9423076923076925, Mean Entropy: 0.06681803613901138, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 822,  Mean reward: 7.962025316455696, Mean Entropy: 0.025881197303533554, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 823,  Mean reward: 8.0, Mean Entropy: 3.51366153950039e-08, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 824,  Mean reward: -1.75, Mean Entropy: 8.578342658438487e-08, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 825,  Mean reward: -1.5, Mean Entropy: 1.1752841601264663e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 826,  Mean reward: -0.5, Mean Entropy: 0.0009030954679474235, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 827,  Mean reward: -3.25, Mean Entropy: 0.04445379972457886, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 828,  Mean reward: -1.639240506329114, Mean Entropy: 0.28853338956832886, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 829,  Mean reward: 1.2205882352941178, Mean Entropy: 0.5693349242210388, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 830,  Mean reward: 3.0982142857142856, Mean Entropy: 0.44082579016685486, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 831,  Mean reward: 3.377551020408163, Mean Entropy: 0.4684314429759979, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 832,  Mean reward: 3.7547169811320753, Mean Entropy: 0.6317231059074402, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 833,  Mean reward: 3.7169811320754715, Mean Entropy: 0.5056624412536621, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 834,  Mean reward: 3.879032258064516, Mean Entropy: 0.3574368953704834, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 835,  Mean reward: 4.696428571428571, Mean Entropy: 0.598815381526947, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 836,  Mean reward: 4.392857142857143, Mean Entropy: 0.5559568405151367, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 837,  Mean reward: 4.598214285714286, Mean Entropy: 0.4939274787902832, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.86s
Iteration: 838,  Mean reward: 4.684210526315789, Mean Entropy: 0.46779903769493103, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.83s
Iteration: 839,  Mean reward: 5.0701754385964914, Mean Entropy: 0.3658033311367035, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.81s
Iteration: 840,  Mean reward: 6.223076923076923, Mean Entropy: 0.2533523738384247, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 841,  Mean reward: 7.616438356164384, Mean Entropy: 0.031115468591451645, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 842,  Mean reward: -0.75, Mean Entropy: 0.002481420524418354, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 843,  Mean reward: -1.5, Mean Entropy: 0.005593571811914444, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 844,  Mean reward: 1.3493150684931507, Mean Entropy: 0.2722088694572449, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 1.11s
Iteration: 845,  Mean reward: 6.166666666666667, Mean Entropy: 0.2143910974264145, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 846,  Mean reward: 7.875, Mean Entropy: 0.10742630809545517, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 847,  Mean reward: 7.82051282051282, Mean Entropy: 0.0026133700739592314, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 848,  Mean reward: -2.75, Mean Entropy: 0.0011198006104677916, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 849,  Mean reward: -2.0, Mean Entropy: 0.26021644473075867, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 850,  Mean reward: 7.777027027027027, Mean Entropy: 0.08611129224300385, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 851,  Mean reward: 7.6558441558441555, Mean Entropy: 0.24902212619781494, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 852,  Mean reward: 7.980769230769231, Mean Entropy: 0.1310092955827713, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 853,  Mean reward: 5.653846153846154, Mean Entropy: 0.0653807520866394, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 854,  Mean reward: 7.981012658227848, Mean Entropy: 0.009852433577179909, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 855,  Mean reward: 8.0, Mean Entropy: 0.014275590889155865, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 856,  Mean reward: 8.0, Mean Entropy: 0.04033335670828819, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 857,  Mean reward: 7.5, Mean Entropy: 0.0076332478784024715, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 858,  Mean reward: 8.0, Mean Entropy: 0.013310405425727367, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 859,  Mean reward: 8.0, Mean Entropy: 0.09158653020858765, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 860,  Mean reward: 7.981012658227848, Mean Entropy: 0.010399443097412586, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 861,  Mean reward: 8.0, Mean Entropy: 0.015147927217185497, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 862,  Mean reward: 8.0, Mean Entropy: 0.050294261425733566, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.94s
Iteration: 863,  Mean reward: 7.961538461538462, Mean Entropy: 0.02882370911538601, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.91s
Iteration: 864,  Mean reward: 7.962025316455696, Mean Entropy: 0.008353662677109241, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 865,  Mean reward: 7.980769230769231, Mean Entropy: 0.004103042650967836, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 866,  Mean reward: 8.0, Mean Entropy: 0.21493779122829437, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 867,  Mean reward: 2.1013513513513513, Mean Entropy: 0.0025965068489313126, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 868,  Mean reward: 8.0, Mean Entropy: 0.0019115079194307327, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 869,  Mean reward: 8.0, Mean Entropy: 0.004262186121195555, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 870,  Mean reward: 8.0, Mean Entropy: 0.0034935022704303265, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 871,  Mean reward: 8.0, Mean Entropy: 0.002944322768598795, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 872,  Mean reward: 8.0, Mean Entropy: 0.0025877852458506823, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 873,  Mean reward: 8.0, Mean Entropy: 0.0033502287697046995, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 874,  Mean reward: 8.0, Mean Entropy: 0.0027621218468993902, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 875,  Mean reward: 8.0, Mean Entropy: 0.0032464182004332542, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 876,  Mean reward: 8.0, Mean Entropy: 0.00313524785451591, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 877,  Mean reward: 7.981012658227848, Mean Entropy: 0.0020266419742256403, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 878,  Mean reward: 8.0, Mean Entropy: 0.0020679410081356764, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 879,  Mean reward: 8.0, Mean Entropy: 0.002332837088033557, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 880,  Mean reward: 8.0, Mean Entropy: 0.0013159032678231597, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 881,  Mean reward: 8.0, Mean Entropy: 0.0015649277484044433, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 882,  Mean reward: 8.0, Mean Entropy: 0.0015578779857605696, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 883,  Mean reward: 8.0, Mean Entropy: 0.0021736035123467445, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 884,  Mean reward: 8.0, Mean Entropy: 0.0013851209077984095, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.12s
Iteration: 885,  Mean reward: 8.0, Mean Entropy: 0.0017011899035423994, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 886,  Mean reward: 8.0, Mean Entropy: 0.0016704050358384848, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 887,  Mean reward: 8.0, Mean Entropy: 0.001336596324108541, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 888,  Mean reward: 8.0, Mean Entropy: 0.001390698365867138, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 889,  Mean reward: 8.0, Mean Entropy: 0.001974110258743167, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 890,  Mean reward: 8.0, Mean Entropy: 0.0017524512950330973, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 891,  Mean reward: 8.0, Mean Entropy: 0.00181590486317873, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 892,  Mean reward: 8.0, Mean Entropy: 0.0015560099855065346, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 893,  Mean reward: 8.0, Mean Entropy: 0.0018750783056020737, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 894,  Mean reward: 8.0, Mean Entropy: 0.0015498977154493332, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 895,  Mean reward: 8.0, Mean Entropy: 0.0016063123475760221, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 896,  Mean reward: 8.0, Mean Entropy: 0.0016264021396636963, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 897,  Mean reward: 8.0, Mean Entropy: 0.001855277456343174, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 898,  Mean reward: 8.0, Mean Entropy: 0.0016815136186778545, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 899,  Mean reward: 8.0, Mean Entropy: 0.0016998786013573408, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 900,  Mean reward: 8.0, Mean Entropy: 0.0014262627810239792, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: 8.0, Mean Entropy: 0.0018430587369948626, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 902,  Mean reward: 8.0, Mean Entropy: 0.001797115313820541, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 903,  Mean reward: 8.0, Mean Entropy: 0.0015311944298446178, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 904,  Mean reward: 8.0, Mean Entropy: 0.0015410108026117086, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 905,  Mean reward: 8.0, Mean Entropy: 0.0017550985794514418, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 906,  Mean reward: 8.0, Mean Entropy: 0.0013943606754764915, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 907,  Mean reward: 8.0, Mean Entropy: 0.002382277976721525, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 908,  Mean reward: 8.0, Mean Entropy: 0.0021123564802110195, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 909,  Mean reward: 8.0, Mean Entropy: 0.001429772237315774, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 910,  Mean reward: 8.0, Mean Entropy: 0.001891125924885273, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 911,  Mean reward: 8.0, Mean Entropy: 0.0020888587459921837, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 912,  Mean reward: 8.0, Mean Entropy: 0.0019322908483445644, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 913,  Mean reward: 8.0, Mean Entropy: 0.0018093166872859001, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 914,  Mean reward: 8.0, Mean Entropy: 0.0014502763515338302, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 915,  Mean reward: 8.0, Mean Entropy: 0.0021360781975090504, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 916,  Mean reward: 8.0, Mean Entropy: 0.0017629328649491072, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 917,  Mean reward: 8.0, Mean Entropy: 0.0017123417928814888, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 918,  Mean reward: 8.0, Mean Entropy: 0.001780730439350009, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 919,  Mean reward: 8.0, Mean Entropy: 0.001407131552696228, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 920,  Mean reward: 8.0, Mean Entropy: 0.0015988396480679512, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 921,  Mean reward: 8.0, Mean Entropy: 0.0018277224153280258, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 922,  Mean reward: 8.0, Mean Entropy: 0.0012092969845980406, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 923,  Mean reward: 8.0, Mean Entropy: 0.0017372926231473684, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 924,  Mean reward: 8.0, Mean Entropy: 0.00151749886572361, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.23s
Iteration: 925,  Mean reward: 8.0, Mean Entropy: 0.0012219867203384638, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 926,  Mean reward: 8.0, Mean Entropy: 0.001836193143390119, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 927,  Mean reward: 8.0, Mean Entropy: 0.0015996030997484922, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 928,  Mean reward: 8.0, Mean Entropy: 0.0013414663262665272, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 929,  Mean reward: 8.0, Mean Entropy: 0.0012771745678037405, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 930,  Mean reward: 8.0, Mean Entropy: 0.0012050461955368519, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 931,  Mean reward: 8.0, Mean Entropy: 0.001758789992891252, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 932,  Mean reward: 8.0, Mean Entropy: 0.001106862910091877, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 933,  Mean reward: 8.0, Mean Entropy: 0.0013411145191639662, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 934,  Mean reward: 8.0, Mean Entropy: 0.0014080945402383804, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 935,  Mean reward: 8.0, Mean Entropy: 0.0016699845436960459, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 936,  Mean reward: 8.0, Mean Entropy: 0.0015543680638074875, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 937,  Mean reward: 8.0, Mean Entropy: 0.0016042793868109584, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 938,  Mean reward: 8.0, Mean Entropy: 0.0013247074093669653, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 939,  Mean reward: 8.0, Mean Entropy: 0.0015287669375538826, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 940,  Mean reward: 8.0, Mean Entropy: 0.0013341957237571478, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 941,  Mean reward: 8.0, Mean Entropy: 0.0015210595447570086, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 942,  Mean reward: 8.0, Mean Entropy: 0.0013216783991083503, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 943,  Mean reward: 8.0, Mean Entropy: 0.0015803136629983783, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 944,  Mean reward: 8.0, Mean Entropy: 0.0013645713916048408, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 945,  Mean reward: 8.0, Mean Entropy: 0.0014851177111268044, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 946,  Mean reward: 8.0, Mean Entropy: 0.001452216412872076, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 947,  Mean reward: 8.0, Mean Entropy: 0.0014776702737435699, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 948,  Mean reward: 8.0, Mean Entropy: 0.0013258976396173239, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 949,  Mean reward: 8.0, Mean Entropy: 0.001431305892765522, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 950,  Mean reward: 8.0, Mean Entropy: 0.0013065582606941462, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 951,  Mean reward: 8.0, Mean Entropy: 0.001287628198042512, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 952,  Mean reward: 8.0, Mean Entropy: 0.0013731743674725294, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 953,  Mean reward: 8.0, Mean Entropy: 0.0016545569524168968, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 954,  Mean reward: 8.0, Mean Entropy: 0.0010891258716583252, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 955,  Mean reward: 8.0, Mean Entropy: 0.0014932937920093536, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 956,  Mean reward: 8.0, Mean Entropy: 0.0012325964635238051, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 957,  Mean reward: 8.0, Mean Entropy: 0.001477877958677709, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 958,  Mean reward: 8.0, Mean Entropy: 0.0014150997158139944, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 959,  Mean reward: 8.0, Mean Entropy: 0.001608339138329029, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 960,  Mean reward: 8.0, Mean Entropy: 0.0017533593345433474, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 961,  Mean reward: 8.0, Mean Entropy: 0.0016238478710874915, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 962,  Mean reward: 8.0, Mean Entropy: 0.0011527417227625847, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 963,  Mean reward: 8.0, Mean Entropy: 0.0015957225114107132, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 964,  Mean reward: 8.0, Mean Entropy: 0.0013184258714318275, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.05s
Iteration: 965,  Mean reward: 8.0, Mean Entropy: 0.0016176823992282152, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 966,  Mean reward: 8.0, Mean Entropy: 0.001346181146800518, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 967,  Mean reward: 8.0, Mean Entropy: 0.0015849422197788954, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 968,  Mean reward: 8.0, Mean Entropy: 0.0013756382977589965, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 969,  Mean reward: 8.0, Mean Entropy: 0.0015829992480576038, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 970,  Mean reward: 8.0, Mean Entropy: 0.0014950529439374804, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 971,  Mean reward: 8.0, Mean Entropy: 0.0012280767550691962, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 972,  Mean reward: 8.0, Mean Entropy: 0.0011347820982336998, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 973,  Mean reward: 8.0, Mean Entropy: 0.0012368632014840841, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 974,  Mean reward: 8.0, Mean Entropy: 0.0009532934054732323, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 975,  Mean reward: 8.0, Mean Entropy: 0.0014616664266213775, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 976,  Mean reward: 8.0, Mean Entropy: 0.0010654961224645376, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 977,  Mean reward: 8.0, Mean Entropy: 0.0012928007636219263, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 978,  Mean reward: 8.0, Mean Entropy: 0.0012680101208388805, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 979,  Mean reward: 8.0, Mean Entropy: 0.0013047419488430023, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.92s
Iteration: 980,  Mean reward: 8.0, Mean Entropy: 0.0009889609646052122, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 981,  Mean reward: 8.0, Mean Entropy: 0.0010371155804023147, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 982,  Mean reward: 8.0, Mean Entropy: 0.000967976578976959, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 983,  Mean reward: 8.0, Mean Entropy: 0.000879353319760412, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 984,  Mean reward: 8.0, Mean Entropy: 0.0009730446035973728, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.95s
Iteration: 985,  Mean reward: 8.0, Mean Entropy: 0.0007946925470605493, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 986,  Mean reward: 8.0, Mean Entropy: 0.0007536975899711251, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 987,  Mean reward: 8.0, Mean Entropy: 0.0008611975936219096, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 988,  Mean reward: 8.0, Mean Entropy: 0.0009242240339517593, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 989,  Mean reward: 8.0, Mean Entropy: 0.0007391522522084415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 990,  Mean reward: 8.0, Mean Entropy: 0.0006857782136648893, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 991,  Mean reward: 8.0, Mean Entropy: 0.0007749387877993286, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 992,  Mean reward: 8.0, Mean Entropy: 0.0006298685912042856, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 993,  Mean reward: 8.0, Mean Entropy: 0.0008731898851692677, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 994,  Mean reward: 8.0, Mean Entropy: 0.0006255765329115093, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 995,  Mean reward: 8.0, Mean Entropy: 0.0006019148277118802, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 996,  Mean reward: 8.0, Mean Entropy: 0.0006101671606302261, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 997,  Mean reward: 8.0, Mean Entropy: 0.0004881105851382017, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 998,  Mean reward: 8.0, Mean Entropy: 0.0005295361625030637, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 999,  Mean reward: 8.0, Mean Entropy: 0.0003688300494104624, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 1000,  Mean reward: 8.0, Mean Entropy: 0.0005214179982431233, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: 8.0, Mean Entropy: 0.0007481510983780026, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 1002,  Mean reward: 8.0, Mean Entropy: 0.0005288497195579112, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 1003,  Mean reward: 8.0, Mean Entropy: 0.0006076092249713838, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 1004,  Mean reward: 8.0, Mean Entropy: 0.0005362197989597917, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.08s
Iteration: 1005,  Mean reward: 8.0, Mean Entropy: 0.0004163776757195592, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 1006,  Mean reward: 8.0, Mean Entropy: 0.0005689844838343561, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 1007,  Mean reward: 8.0, Mean Entropy: 0.0003504128544591367, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 1008,  Mean reward: 8.0, Mean Entropy: 0.000518135610036552, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 1009,  Mean reward: 8.0, Mean Entropy: 0.0004953623283654451, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 1010,  Mean reward: 8.0, Mean Entropy: 0.0004803225747309625, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 1011,  Mean reward: 8.0, Mean Entropy: 0.0005066386074759066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 1012,  Mean reward: 8.0, Mean Entropy: 0.0005617709830403328, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 1013,  Mean reward: 8.0, Mean Entropy: 0.0005312415887601674, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 1014,  Mean reward: 8.0, Mean Entropy: 0.0005964405136182904, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 1015,  Mean reward: 8.0, Mean Entropy: 0.0004516850458458066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 1016,  Mean reward: 8.0, Mean Entropy: 0.000462688592961058, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 1017,  Mean reward: 8.0, Mean Entropy: 0.0006395053351297975, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 1018,  Mean reward: 8.0, Mean Entropy: 0.0005128568736836314, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 1019,  Mean reward: 8.0, Mean Entropy: 0.0005143657908774912, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 1020,  Mean reward: 8.0, Mean Entropy: 0.0005637846770696342, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 1021,  Mean reward: 8.0, Mean Entropy: 0.0004924454260617495, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.32051282051282, Mean Entropy: 0.9169759750366211, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.81s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.548780487804878, Mean Entropy: 0.9314165115356445, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -3.3191489361702127, Mean Entropy: 0.9025354385375977, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.82s
Iteration: 3,  Mean reward: -6.439024390243903, Mean Entropy: 0.9963991045951843, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 4,  Mean reward: -6.678571428571429, Mean Entropy: 0.9530773162841797, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 5,  Mean reward: -1.7790697674418605, Mean Entropy: 0.9097557067871094, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 6,  Mean reward: -5.951219512195122, Mean Entropy: 0.8880948424339294, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.82s
Iteration: 7,  Mean reward: -3.4125, Mean Entropy: 0.9025354385375977, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 8,  Mean reward: -3.686046511627907, Mean Entropy: 0.9097555875778198, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.80s
Iteration: 9,  Mean reward: -4.7631578947368425, Mean Entropy: 0.9314162135124207, complete_episode_count: 38.0, Gather time: 0.50s, Train time: 1.81s
Iteration: 10,  Mean reward: -3.2195121951219514, Mean Entropy: 0.9314160346984863, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.80s
Iteration: 11,  Mean reward: -3.8, Mean Entropy: 0.945857048034668, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.81s
Iteration: 12,  Mean reward: -4.616279069767442, Mean Entropy: 0.924195408821106, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 13,  Mean reward: -4.406976744186046, Mean Entropy: 0.9458469152450562, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.80s
Iteration: 14,  Mean reward: -4.5256410256410255, Mean Entropy: 0.9241775274276733, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.83s
Iteration: 15,  Mean reward: -3.8, Mean Entropy: 0.9169617891311646, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.84s
Iteration: 16,  Mean reward: -4.294871794871795, Mean Entropy: 0.9313462972640991, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.79s
Iteration: 17,  Mean reward: -4.5227272727272725, Mean Entropy: 0.9454050064086914, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 18,  Mean reward: -2.911111111111111, Mean Entropy: 0.9233281016349792, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 19,  Mean reward: -4.634146341463414, Mean Entropy: 0.9512395858764648, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.97s
Iteration: 20,  Mean reward: -4.217391304347826, Mean Entropy: 1.0019423961639404, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 21,  Mean reward: -4.011363636363637, Mean Entropy: 0.9874987006187439, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 22,  Mean reward: -5.826086956521739, Mean Entropy: 0.9136109352111816, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 23,  Mean reward: -3.141025641025641, Mean Entropy: 0.9358067512512207, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 24,  Mean reward: -3.9069767441860463, Mean Entropy: 0.9072855710983276, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 25,  Mean reward: -5.0, Mean Entropy: 1.0117802619934082, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 26,  Mean reward: -3.4555555555555557, Mean Entropy: 0.9137542247772217, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 27,  Mean reward: -3.2142857142857144, Mean Entropy: 0.8789423704147339, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.81s
Iteration: 28,  Mean reward: -5.7555555555555555, Mean Entropy: 0.9262725710868835, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.78s
Iteration: 29,  Mean reward: -4.4222222222222225, Mean Entropy: 0.9527053833007812, complete_episode_count: 45.0, Gather time: 0.62s, Train time: 1.82s
Iteration: 30,  Mean reward: -4.180851063829787, Mean Entropy: 0.8965582251548767, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 31,  Mean reward: -1.6477272727272727, Mean Entropy: 0.9347628951072693, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.83s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 32,  Mean reward: -1.5666666666666667, Mean Entropy: 1.0072708129882812, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 33,  Mean reward: -4.260869565217392, Mean Entropy: 0.9689559936523438, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.82s
Iteration: 34,  Mean reward: -5.822222222222222, Mean Entropy: 0.9280821681022644, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 35,  Mean reward: -4.421052631578948, Mean Entropy: 0.963210940361023, complete_episode_count: 38.0, Gather time: 0.50s, Train time: 1.79s
Iteration: 36,  Mean reward: -5.459183673469388, Mean Entropy: 0.8998078107833862, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 37,  Mean reward: -3.978723404255319, Mean Entropy: 0.9290367364883423, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 38,  Mean reward: -6.255813953488372, Mean Entropy: 0.9021539688110352, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.82s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 39,  Mean reward: -0.1511627906976744, Mean Entropy: 0.8908678889274597, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 40,  Mean reward: -5.84375, Mean Entropy: 0.9322115182876587, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.82s
Iteration: 41,  Mean reward: -3.8043478260869565, Mean Entropy: 0.9442343711853027, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 42,  Mean reward: -2.8222222222222224, Mean Entropy: 0.8795015215873718, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 43,  Mean reward: -3.4019607843137254, Mean Entropy: 0.8925232291221619, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 44,  Mean reward: -6.322222222222222, Mean Entropy: 0.9091325998306274, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 45,  Mean reward: -3.2735849056603774, Mean Entropy: 0.9104546308517456, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 46,  Mean reward: -3.18, Mean Entropy: 0.921956479549408, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 47,  Mean reward: -5.2023809523809526, Mean Entropy: 0.9122209548950195, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.81s
Iteration: 48,  Mean reward: -1.62, Mean Entropy: 0.9332318305969238, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 49,  Mean reward: -3.9555555555555557, Mean Entropy: 0.8909997940063477, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.83s
Iteration: 50,  Mean reward: -4.363636363636363, Mean Entropy: 0.8627829551696777, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 51,  Mean reward: -2.9183673469387754, Mean Entropy: 0.8856541514396667, complete_episode_count: 49.0, Gather time: 0.51s, Train time: 1.80s
Iteration: 52,  Mean reward: -2.8229166666666665, Mean Entropy: 0.8585564494132996, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 53,  Mean reward: -1.88, Mean Entropy: 0.9066224098205566, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.97s
Iteration: 54,  Mean reward: -5.4186046511627906, Mean Entropy: 0.8972246646881104, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.81s
Iteration: 55,  Mean reward: -4.315217391304348, Mean Entropy: 0.8868467211723328, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 56,  Mean reward: -2.42, Mean Entropy: 0.8381869196891785, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.81s
Iteration: 57,  Mean reward: -3.7549019607843137, Mean Entropy: 0.8257550001144409, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.79s
Iteration: 58,  Mean reward: -0.15178571428571427, Mean Entropy: 0.7984468340873718, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 59,  Mean reward: -4.16, Mean Entropy: 0.7547479867935181, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.83s
Iteration: 60,  Mean reward: -4.147058823529412, Mean Entropy: 0.7306072115898132, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 61,  Mean reward: -3.710526315789474, Mean Entropy: 0.6464141607284546, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.79s
Iteration: 62,  Mean reward: -2.2966101694915255, Mean Entropy: 0.6442641019821167, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.81s
Iteration: 63,  Mean reward: -3.9237288135593222, Mean Entropy: 0.58324134349823, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.82s
Iteration: 64,  Mean reward: -2.3833333333333333, Mean Entropy: 0.41431984305381775, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.80s
Iteration: 65,  Mean reward: -3.9338235294117645, Mean Entropy: 0.30128034949302673, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 66,  Mean reward: -1.6884057971014492, Mean Entropy: 0.22424185276031494, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.90s
Iteration: 67,  Mean reward: -2.9932432432432434, Mean Entropy: 0.19013738632202148, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 68,  Mean reward: -3.2635135135135136, Mean Entropy: 0.18066740036010742, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 69,  Mean reward: -2.493150684931507, Mean Entropy: 0.15969237685203552, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 1.05s
Iteration: 70,  Mean reward: -1.875, Mean Entropy: 0.12036576867103577, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 71,  Mean reward: -0.9090909090909091, Mean Entropy: 0.05567256733775139, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 72,  Mean reward: -1.3860759493670887, Mean Entropy: 0.028037279844284058, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 73,  Mean reward: -3.0, Mean Entropy: 0.01871979981660843, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 74,  Mean reward: -2.75, Mean Entropy: 0.01512888353317976, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 75,  Mean reward: -1.5, Mean Entropy: 0.0153821911662817, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 76,  Mean reward: 0.13291139240506328, Mean Entropy: 0.012628888711333275, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.92s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 77,  Mean reward: 0.25, Mean Entropy: 0.014560259878635406, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.90s
Iteration: 78,  Mean reward: -0.5, Mean Entropy: 0.01802121102809906, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 79,  Mean reward: -1.75, Mean Entropy: 0.013921309262514114, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 80,  Mean reward: -3.0, Mean Entropy: 0.010015808045864105, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 81,  Mean reward: -1.75, Mean Entropy: 0.011883659288287163, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 82,  Mean reward: -1.0, Mean Entropy: 0.011169889941811562, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 83,  Mean reward: -2.5, Mean Entropy: 0.006571599747985601, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 84,  Mean reward: -2.25, Mean Entropy: 0.007227258291095495, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 85,  Mean reward: -2.0, Mean Entropy: 0.0057241041213274, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 86,  Mean reward: -2.0, Mean Entropy: 0.005647490732371807, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 87,  Mean reward: -1.5, Mean Entropy: 0.0050838179886341095, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 88,  Mean reward: -0.5, Mean Entropy: 0.005130212754011154, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 89,  Mean reward: -2.5, Mean Entropy: 0.004446241073310375, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 90,  Mean reward: -3.75, Mean Entropy: 0.004588176961988211, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 1.07s
Iteration: 91,  Mean reward: -0.5, Mean Entropy: 0.0070458874106407166, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 92,  Mean reward: -1.75, Mean Entropy: 0.006160395219922066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 93,  Mean reward: -2.0, Mean Entropy: 0.006227512843906879, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 94,  Mean reward: -4.5, Mean Entropy: 0.0040963939391076565, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 95,  Mean reward: -1.75, Mean Entropy: 0.004881018307060003, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 96,  Mean reward: -1.75, Mean Entropy: 0.00647624721750617, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 97,  Mean reward: -3.411392405063291, Mean Entropy: 0.004910755902528763, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 98,  Mean reward: -2.25, Mean Entropy: 0.009285230189561844, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 99,  Mean reward: -1.3860759493670887, Mean Entropy: 0.006043191067874432, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 100,  Mean reward: -2.25, Mean Entropy: 0.005903981626033783, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -3.75, Mean Entropy: 0.0045560128055512905, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 102,  Mean reward: -2.5, Mean Entropy: 0.0041044787503778934, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 103,  Mean reward: -1.75, Mean Entropy: 0.0028387324418872595, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 104,  Mean reward: -4.25, Mean Entropy: 0.001760787097737193, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 105,  Mean reward: -1.5, Mean Entropy: 0.001668090233579278, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 106,  Mean reward: -4.0, Mean Entropy: 0.001375255873426795, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 107,  Mean reward: -3.0, Mean Entropy: 0.001957745524123311, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 108,  Mean reward: -1.5, Mean Entropy: 0.0021880450658500195, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 109,  Mean reward: -3.0, Mean Entropy: 0.002267862670123577, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 110,  Mean reward: 0.0, Mean Entropy: 0.002952060429379344, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 111,  Mean reward: -4.5, Mean Entropy: 0.0020861090160906315, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 112,  Mean reward: -2.5, Mean Entropy: 0.0023755356669425964, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 113,  Mean reward: -1.25, Mean Entropy: 0.0026831093709915876, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 114,  Mean reward: -3.25, Mean Entropy: 0.0026120038237422705, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 115,  Mean reward: -2.0, Mean Entropy: 0.0024661701172590256, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 116,  Mean reward: -0.25, Mean Entropy: 0.0029908320866525173, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 117,  Mean reward: -2.25, Mean Entropy: 0.001981831854209304, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 118,  Mean reward: -0.5, Mean Entropy: 0.0020298499148339033, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 119,  Mean reward: -2.1455696202531644, Mean Entropy: 0.0005402503302320838, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 120,  Mean reward: -2.0, Mean Entropy: 0.0002608852810226381, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 121,  Mean reward: -2.75, Mean Entropy: 0.00018290166917722672, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 122,  Mean reward: -4.0, Mean Entropy: 0.00017745268996804953, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 123,  Mean reward: -2.5, Mean Entropy: 0.00020010789739899337, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.92s
Iteration: 124,  Mean reward: -2.75, Mean Entropy: 0.00017570405907463282, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 125,  Mean reward: -4.25, Mean Entropy: 0.0001780656020855531, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 126,  Mean reward: -1.0, Mean Entropy: 0.0002073514915537089, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 127,  Mean reward: -1.5, Mean Entropy: 0.00020044861594215035, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.90s
Iteration: 128,  Mean reward: 0.0, Mean Entropy: 0.00021117318829055876, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 129,  Mean reward: -1.25, Mean Entropy: 0.00021256569016259164, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 130,  Mean reward: -2.25, Mean Entropy: 0.0001951098529389128, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.06s
Iteration: 131,  Mean reward: -2.25, Mean Entropy: 0.00021251084399409592, complete_episode_count: 80.0, Gather time: 0.66s, Train time: 0.91s
Iteration: 132,  Mean reward: -2.0, Mean Entropy: 0.0001977164502022788, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 133,  Mean reward: -1.5, Mean Entropy: 0.000216584128793329, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 134,  Mean reward: -3.75, Mean Entropy: 0.00020075749489478767, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 135,  Mean reward: -2.0, Mean Entropy: 0.0002063061110675335, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 136,  Mean reward: -3.75, Mean Entropy: 0.00020444911206141114, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 137,  Mean reward: -2.5, Mean Entropy: 0.0002096528623951599, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 138,  Mean reward: -2.0, Mean Entropy: 0.00020766780653502792, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 139,  Mean reward: -2.5, Mean Entropy: 0.0001999967935262248, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 140,  Mean reward: -2.75, Mean Entropy: 0.00019479644834063947, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 141,  Mean reward: -4.25, Mean Entropy: 0.00019577282364480197, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 142,  Mean reward: -2.0, Mean Entropy: 0.00019681683625094593, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 143,  Mean reward: -2.75, Mean Entropy: 0.00019254109065514058, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 144,  Mean reward: -1.75, Mean Entropy: 0.00020151174976490438, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 145,  Mean reward: -4.25, Mean Entropy: 0.00018885322788264602, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 146,  Mean reward: -4.25, Mean Entropy: 0.0001921177317854017, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 147,  Mean reward: 0.25, Mean Entropy: 0.00020060389942955226, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 148,  Mean reward: -2.5, Mean Entropy: 0.00019704599981196225, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 149,  Mean reward: -1.75, Mean Entropy: 0.00019998305651824921, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 150,  Mean reward: -1.5, Mean Entropy: 0.00020149972988292575, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 151,  Mean reward: -1.25, Mean Entropy: 0.00019450296531431377, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 152,  Mean reward: 0.75, Mean Entropy: 0.00020972281345166266, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 153,  Mean reward: -4.0, Mean Entropy: 0.00018731869931798428, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 154,  Mean reward: -1.0, Mean Entropy: 0.0001974956103367731, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 155,  Mean reward: -3.75, Mean Entropy: 0.0001938813365995884, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 156,  Mean reward: -2.25, Mean Entropy: 0.00020170887000858784, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 157,  Mean reward: 0.0, Mean Entropy: 0.0002110476780217141, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 158,  Mean reward: -2.5, Mean Entropy: 0.00020069326274096966, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 159,  Mean reward: -3.25, Mean Entropy: 0.00020274179405532777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 160,  Mean reward: -2.0, Mean Entropy: 0.00020705594215542078, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 161,  Mean reward: -2.25, Mean Entropy: 0.00020630336075555533, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 162,  Mean reward: 0.25, Mean Entropy: 0.00021289216238074005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 163,  Mean reward: -2.5, Mean Entropy: 0.00020691181998699903, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 164,  Mean reward: -2.25, Mean Entropy: 0.0002075203083222732, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 165,  Mean reward: -0.25, Mean Entropy: 0.00022130468278191984, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 166,  Mean reward: -0.75, Mean Entropy: 0.00021590542746707797, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 167,  Mean reward: -1.25, Mean Entropy: 0.00022242193517740816, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 168,  Mean reward: -3.25, Mean Entropy: 0.00022344649187289178, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 169,  Mean reward: -1.25, Mean Entropy: 0.00022045790683478117, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 170,  Mean reward: -2.5, Mean Entropy: 0.00022102925868239254, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.05s
Iteration: 171,  Mean reward: -3.5, Mean Entropy: 0.00021881531574763358, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 172,  Mean reward: -2.5, Mean Entropy: 0.000221934518776834, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 173,  Mean reward: -3.5, Mean Entropy: 0.00021754129556939006, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 174,  Mean reward: -2.25, Mean Entropy: 0.00022286555031314492, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 175,  Mean reward: -2.0, Mean Entropy: 0.00022411512327380478, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 176,  Mean reward: -3.5, Mean Entropy: 0.00022346503101289272, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 177,  Mean reward: -2.25, Mean Entropy: 0.000225918396608904, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 178,  Mean reward: -0.5, Mean Entropy: 0.00022870370594318956, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 179,  Mean reward: -1.5, Mean Entropy: 0.00023016423801891506, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 180,  Mean reward: -1.5, Mean Entropy: 0.00023351199342869222, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 181,  Mean reward: -2.25, Mean Entropy: 0.00022827586508356035, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 182,  Mean reward: -3.75, Mean Entropy: 0.00022845521743874997, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 183,  Mean reward: -2.25, Mean Entropy: 0.00023460299416910857, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 184,  Mean reward: -2.0, Mean Entropy: 0.00023505107674282044, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 185,  Mean reward: -1.75, Mean Entropy: 0.00023521883122157305, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 186,  Mean reward: -0.5, Mean Entropy: 0.0002344139793422073, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 187,  Mean reward: -1.75, Mean Entropy: 0.00023773677821736783, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 188,  Mean reward: -1.5, Mean Entropy: 0.00023712698020972311, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 189,  Mean reward: -2.25, Mean Entropy: 0.00023020501248538494, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 190,  Mean reward: 0.5, Mean Entropy: 0.00024239525373559445, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 191,  Mean reward: -3.0, Mean Entropy: 0.0002461474505253136, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 192,  Mean reward: -1.75, Mean Entropy: 0.0002449364692438394, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 193,  Mean reward: -1.0, Mean Entropy: 0.0002481619594618678, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.02s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 194,  Mean reward: 1.5, Mean Entropy: 0.00025233125779777765, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 195,  Mean reward: -1.75, Mean Entropy: 0.0002527121687307954, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 196,  Mean reward: -1.0, Mean Entropy: 0.00025247421581298113, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 197,  Mean reward: -1.75, Mean Entropy: 0.00025478750467300415, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 198,  Mean reward: -2.25, Mean Entropy: 0.00025897909654304385, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 199,  Mean reward: -3.0, Mean Entropy: 0.00026011085719801486, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 200,  Mean reward: -2.25, Mean Entropy: 0.00026512646581977606, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.75, Mean Entropy: 0.0002681864134501666, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 202,  Mean reward: -5.5, Mean Entropy: 0.0002613729448057711, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 203,  Mean reward: 0.0, Mean Entropy: 0.00026579428231343627, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 204,  Mean reward: -0.75, Mean Entropy: 0.00026516185607761145, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 205,  Mean reward: -1.75, Mean Entropy: 0.0002657850563991815, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 206,  Mean reward: -2.0, Mean Entropy: 0.00026681931922212243, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 207,  Mean reward: -3.25, Mean Entropy: 0.0002734559529926628, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 208,  Mean reward: -4.5, Mean Entropy: 0.0002662542974576354, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 209,  Mean reward: 0.5, Mean Entropy: 0.0002757731999736279, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 210,  Mean reward: -2.0, Mean Entropy: 0.0002730066771619022, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.10s
Iteration: 211,  Mean reward: -0.75, Mean Entropy: 0.00027975469129160047, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 212,  Mean reward: 0.25, Mean Entropy: 0.00028139803907833993, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 213,  Mean reward: -2.75, Mean Entropy: 0.00028258422389626503, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 214,  Mean reward: -4.25, Mean Entropy: 0.00028312712674960494, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 215,  Mean reward: -2.0, Mean Entropy: 0.0002851343888323754, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 216,  Mean reward: -1.25, Mean Entropy: 0.00028363961610011756, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 217,  Mean reward: -3.0, Mean Entropy: 0.00028136023320257664, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 218,  Mean reward: -2.25, Mean Entropy: 0.00028543523512780666, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 219,  Mean reward: -2.0, Mean Entropy: 0.00028440114692784846, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 220,  Mean reward: -1.75, Mean Entropy: 0.00028023027698509395, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.91s
Iteration: 221,  Mean reward: -0.75, Mean Entropy: 0.0002858558436855674, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 222,  Mean reward: -2.5, Mean Entropy: 0.00028664604178629816, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 223,  Mean reward: -2.75, Mean Entropy: 0.0002854256599675864, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 224,  Mean reward: -2.5, Mean Entropy: 0.00028837742866016924, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 225,  Mean reward: -2.75, Mean Entropy: 0.0002885496651288122, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.90s
Iteration: 226,  Mean reward: -2.25, Mean Entropy: 0.00029288019868545234, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 227,  Mean reward: -3.25, Mean Entropy: 0.00029164215084165335, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 228,  Mean reward: -0.75, Mean Entropy: 0.0002976727846544236, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 229,  Mean reward: -1.25, Mean Entropy: 0.00029755040304735303, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 230,  Mean reward: 0.0, Mean Entropy: 0.00030188760138116777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 231,  Mean reward: -1.0, Mean Entropy: 0.0003030821681022644, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 232,  Mean reward: -2.0, Mean Entropy: 0.00030159042216837406, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 233,  Mean reward: -0.75, Mean Entropy: 0.0003067300422117114, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.91s
Iteration: 234,  Mean reward: -1.75, Mean Entropy: 0.0003032372333109379, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 235,  Mean reward: -2.25, Mean Entropy: 0.000304366578347981, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 236,  Mean reward: 0.5, Mean Entropy: 0.0003030463121831417, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 237,  Mean reward: -0.75, Mean Entropy: 0.00029567256569862366, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 238,  Mean reward: -3.25, Mean Entropy: 0.0002952975337393582, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 239,  Mean reward: -3.5, Mean Entropy: 0.0002920890983659774, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 240,  Mean reward: -3.75, Mean Entropy: 0.00028497641324065626, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 241,  Mean reward: -0.75, Mean Entropy: 0.00029241066658869386, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 242,  Mean reward: -1.5, Mean Entropy: 0.0002866740105673671, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 243,  Mean reward: -1.75, Mean Entropy: 0.00027960207080468535, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 244,  Mean reward: -1.75, Mean Entropy: 0.0002847545256372541, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 245,  Mean reward: -1.0, Mean Entropy: 0.00028670765459537506, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 246,  Mean reward: -2.25, Mean Entropy: 0.0002847234718501568, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 247,  Mean reward: -2.0, Mean Entropy: 0.00028569012647494674, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 248,  Mean reward: -1.5, Mean Entropy: 0.00028748437762260437, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 249,  Mean reward: -1.0, Mean Entropy: 0.00028749846387654543, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 250,  Mean reward: -3.25, Mean Entropy: 0.0002802526287268847, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.10s
Iteration: 251,  Mean reward: -3.75, Mean Entropy: 0.00027602509362623096, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 252,  Mean reward: -2.75, Mean Entropy: 0.00027612625854089856, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 253,  Mean reward: -2.75, Mean Entropy: 0.00027656505699269474, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 254,  Mean reward: -0.5, Mean Entropy: 0.0002773729502223432, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 255,  Mean reward: -3.75, Mean Entropy: 0.0002800974762067199, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 256,  Mean reward: -2.5, Mean Entropy: 0.00028444669442251325, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 257,  Mean reward: -2.5, Mean Entropy: 0.0002846763818524778, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 258,  Mean reward: -1.5, Mean Entropy: 0.00028415577253326774, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.89s
Iteration: 259,  Mean reward: -2.5, Mean Entropy: 0.00028476270381361246, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 260,  Mean reward: -1.75, Mean Entropy: 0.0002847316791303456, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 261,  Mean reward: -2.5, Mean Entropy: 0.000284383975667879, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 262,  Mean reward: -2.75, Mean Entropy: 0.000286688533378765, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 263,  Mean reward: -3.75, Mean Entropy: 0.0002867078874260187, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 264,  Mean reward: -2.25, Mean Entropy: 0.0002873674384318292, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 265,  Mean reward: -1.0, Mean Entropy: 0.00028528706752695143, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 266,  Mean reward: 0.25, Mean Entropy: 0.00027969747316092253, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 267,  Mean reward: -3.25, Mean Entropy: 0.0002768023987300694, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 268,  Mean reward: -2.75, Mean Entropy: 0.00027621802291832864, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 269,  Mean reward: -3.75, Mean Entropy: 0.00027285993564873934, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.94s
Iteration: 270,  Mean reward: 0.75, Mean Entropy: 0.0002699729520827532, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 271,  Mean reward: -3.5, Mean Entropy: 0.0002832979953382164, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 272,  Mean reward: -4.0, Mean Entropy: 0.0002908935130108148, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 273,  Mean reward: -1.0, Mean Entropy: 0.00027914755628444254, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 274,  Mean reward: 0.5, Mean Entropy: 0.0002802533854264766, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 275,  Mean reward: -2.25, Mean Entropy: 0.0002948243636637926, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 276,  Mean reward: -2.0, Mean Entropy: 0.00030328071443364024, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 277,  Mean reward: -0.75, Mean Entropy: 0.00029589186306111515, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 278,  Mean reward: -2.25, Mean Entropy: 0.00030782647081650794, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 279,  Mean reward: -1.0, Mean Entropy: 0.00029983214335516095, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 280,  Mean reward: -3.75, Mean Entropy: 0.0003205231041647494, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 281,  Mean reward: -3.0, Mean Entropy: 0.0003169242409057915, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 282,  Mean reward: -2.25, Mean Entropy: 0.00030833890195935965, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 283,  Mean reward: -1.25, Mean Entropy: 0.00030190724646672606, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 284,  Mean reward: -1.0, Mean Entropy: 0.0003068539663217962, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 285,  Mean reward: 0.0, Mean Entropy: 0.000303408014588058, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.95s
Iteration: 286,  Mean reward: -1.0, Mean Entropy: 0.0003171199350617826, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 287,  Mean reward: -1.25, Mean Entropy: 0.0003235111362300813, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 288,  Mean reward: -3.5, Mean Entropy: 0.00033575529232621193, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 289,  Mean reward: -1.75, Mean Entropy: 0.0003280469391029328, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 290,  Mean reward: -0.75, Mean Entropy: 0.000330690061673522, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.05s
Iteration: 291,  Mean reward: -0.75, Mean Entropy: 0.000330518203554675, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 292,  Mean reward: -2.5, Mean Entropy: 0.00036186532815918326, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 293,  Mean reward: -3.0, Mean Entropy: 0.00035046093398705125, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 294,  Mean reward: -1.5, Mean Entropy: 0.00034850541851483285, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 295,  Mean reward: -1.5, Mean Entropy: 0.0003346600569784641, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 296,  Mean reward: -0.75, Mean Entropy: 0.00032061466481536627, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 297,  Mean reward: -3.25, Mean Entropy: 0.00032634701346978545, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.91s
Iteration: 298,  Mean reward: -1.75, Mean Entropy: 0.0003288963926024735, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 299,  Mean reward: -2.25, Mean Entropy: 0.00034757237881422043, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 300,  Mean reward: -1.0, Mean Entropy: 0.0003267680876888335, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -2.25, Mean Entropy: 0.00034628878347575665, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 302,  Mean reward: -2.0, Mean Entropy: 0.00035469478461891413, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 303,  Mean reward: -1.0, Mean Entropy: 0.00033561489544808865, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 304,  Mean reward: -1.25, Mean Entropy: 0.0003497584257274866, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 305,  Mean reward: 0.25, Mean Entropy: 0.000339902238920331, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 306,  Mean reward: -1.25, Mean Entropy: 0.00036388495936989784, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 307,  Mean reward: -1.25, Mean Entropy: 0.00035122287226840854, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 308,  Mean reward: -1.75, Mean Entropy: 0.0003582849749363959, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 309,  Mean reward: -2.5, Mean Entropy: 0.00037919203168712556, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 310,  Mean reward: -2.0, Mean Entropy: 0.00036959588760510087, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 311,  Mean reward: -2.25, Mean Entropy: 0.0003606481186579913, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 312,  Mean reward: -3.0, Mean Entropy: 0.00038197822868824005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 313,  Mean reward: -3.0, Mean Entropy: 0.0003721269313246012, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 314,  Mean reward: -2.0, Mean Entropy: 0.000367458414984867, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 315,  Mean reward: -2.5, Mean Entropy: 0.0003794544318225235, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 316,  Mean reward: -4.25, Mean Entropy: 0.00037544089718721807, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 317,  Mean reward: -2.0, Mean Entropy: 0.0003778403624892235, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 318,  Mean reward: -3.25, Mean Entropy: 0.0003877809504047036, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 319,  Mean reward: -3.5, Mean Entropy: 0.0003843531012535095, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 320,  Mean reward: -0.75, Mean Entropy: 0.00037441327003762126, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.94s
Iteration: 321,  Mean reward: -1.0, Mean Entropy: 0.0003893982502631843, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 322,  Mean reward: -1.25, Mean Entropy: 0.0003787847235798836, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 323,  Mean reward: -2.75, Mean Entropy: 0.000392439600545913, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 324,  Mean reward: -3.0, Mean Entropy: 0.00041025353129953146, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 325,  Mean reward: -2.75, Mean Entropy: 0.00040100159822031856, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 326,  Mean reward: -2.25, Mean Entropy: 0.00041989347664639354, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 327,  Mean reward: -3.0, Mean Entropy: 0.00045817295904271305, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 328,  Mean reward: -0.75, Mean Entropy: 0.00042204040801152587, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 329,  Mean reward: -3.5, Mean Entropy: 0.0004342138417996466, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 330,  Mean reward: -2.25, Mean Entropy: 0.0004663444997277111, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.06s
Iteration: 331,  Mean reward: -1.75, Mean Entropy: 0.0004745388578157872, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 332,  Mean reward: -0.75, Mean Entropy: 0.00046721118269488215, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 333,  Mean reward: -2.0, Mean Entropy: 0.0004931988078169525, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 334,  Mean reward: -3.0, Mean Entropy: 0.0005152528174221516, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 335,  Mean reward: -2.0, Mean Entropy: 0.0005231214454397559, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 336,  Mean reward: -1.5, Mean Entropy: 0.0004896054742857814, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 337,  Mean reward: -1.0, Mean Entropy: 0.0005362596129998565, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 338,  Mean reward: -2.75, Mean Entropy: 0.0005497528472915292, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 339,  Mean reward: -1.0, Mean Entropy: 0.0005450886092148721, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 340,  Mean reward: -2.0, Mean Entropy: 0.000541240384336561, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 341,  Mean reward: -3.25, Mean Entropy: 0.0005538946716114879, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 342,  Mean reward: -0.5, Mean Entropy: 0.0005479707615450025, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 343,  Mean reward: -0.5, Mean Entropy: 0.0005188094801269472, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 344,  Mean reward: -0.5, Mean Entropy: 0.0005216541467234492, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 345,  Mean reward: -2.0, Mean Entropy: 0.0005095744272693992, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 346,  Mean reward: -1.25, Mean Entropy: 0.0004752298700623214, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 347,  Mean reward: -1.25, Mean Entropy: 0.00045542753650806844, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 348,  Mean reward: -2.25, Mean Entropy: 0.00043305428698658943, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 349,  Mean reward: -3.25, Mean Entropy: 0.0004264975432306528, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 350,  Mean reward: -1.5, Mean Entropy: 0.00043168148840777576, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 351,  Mean reward: -3.25, Mean Entropy: 0.0004431890556588769, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 352,  Mean reward: -0.5, Mean Entropy: 0.0004279709537513554, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 353,  Mean reward: -0.75, Mean Entropy: 0.00045362344826571643, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 354,  Mean reward: -2.0, Mean Entropy: 0.00047919945791363716, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 355,  Mean reward: -2.0, Mean Entropy: 0.0004779769806191325, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 356,  Mean reward: -1.75, Mean Entropy: 0.0004678735858760774, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 357,  Mean reward: -2.0, Mean Entropy: 0.00046310535981319845, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 358,  Mean reward: 0.25, Mean Entropy: 0.00045201912871561944, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 359,  Mean reward: -4.0, Mean Entropy: 0.00047662784345448017, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.89s
Iteration: 360,  Mean reward: 0.0, Mean Entropy: 0.0004361664759926498, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 361,  Mean reward: -2.0, Mean Entropy: 0.00045951837091706693, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 362,  Mean reward: -0.75, Mean Entropy: 0.0004545708652585745, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 363,  Mean reward: -1.25, Mean Entropy: 0.0004660869890358299, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 364,  Mean reward: -3.25, Mean Entropy: 0.00048243324272334576, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 365,  Mean reward: -3.411392405063291, Mean Entropy: 0.00021704209211748093, complete_episode_count: 79.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 366,  Mean reward: -1.25, Mean Entropy: 0.00022306435857899487, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 367,  Mean reward: -3.0, Mean Entropy: 0.00022924455697648227, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 368,  Mean reward: -2.0, Mean Entropy: 0.00022041126794647425, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 369,  Mean reward: -3.5, Mean Entropy: 0.00017537210078444332, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 370,  Mean reward: -0.5, Mean Entropy: 0.00025522185023874044, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.08s
Iteration: 371,  Mean reward: -1.5, Mean Entropy: 0.0002351223083678633, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 372,  Mean reward: -3.25, Mean Entropy: 0.0002000957465497777, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 373,  Mean reward: -3.5, Mean Entropy: 0.00018522978643886745, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 374,  Mean reward: -2.5, Mean Entropy: 0.00019756580877583474, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 375,  Mean reward: -3.0, Mean Entropy: 0.0001819156459532678, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 376,  Mean reward: -2.5, Mean Entropy: 0.00020022722310386598, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 377,  Mean reward: -0.75, Mean Entropy: 0.00024701145594008267, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 378,  Mean reward: -2.0, Mean Entropy: 0.0001876890892162919, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 379,  Mean reward: -4.0, Mean Entropy: 0.00020235928241163492, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 380,  Mean reward: -1.25, Mean Entropy: 0.00020890773157589138, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 381,  Mean reward: -2.75, Mean Entropy: 0.00019880439504049718, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.90s
Iteration: 382,  Mean reward: -1.0, Mean Entropy: 0.00022455169528257102, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 383,  Mean reward: -1.5, Mean Entropy: 0.00020174492965452373, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 384,  Mean reward: -1.25, Mean Entropy: 0.00017909645976033062, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 385,  Mean reward: -1.0, Mean Entropy: 0.0002093930379487574, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 386,  Mean reward: -4.25, Mean Entropy: 0.00014167270273901522, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 387,  Mean reward: -1.75, Mean Entropy: 0.0001925669494085014, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 388,  Mean reward: -3.5, Mean Entropy: 0.00017140171257779002, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 389,  Mean reward: -0.75, Mean Entropy: 0.000166778321727179, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 390,  Mean reward: -2.5, Mean Entropy: 0.0001745505433063954, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 391,  Mean reward: -0.5, Mean Entropy: 0.00017890005256049335, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 392,  Mean reward: -2.75, Mean Entropy: 0.00013902614591643214, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 393,  Mean reward: -1.5, Mean Entropy: 0.00013588130241259933, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 394,  Mean reward: -1.75, Mean Entropy: 0.00014879830996505916, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 395,  Mean reward: -0.5, Mean Entropy: 0.00014689475938212126, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 396,  Mean reward: -1.75, Mean Entropy: 0.00014424262917600572, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 397,  Mean reward: -0.75, Mean Entropy: 0.00013072893489152193, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 398,  Mean reward: -2.75, Mean Entropy: 0.00012484216131269932, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 399,  Mean reward: -2.75, Mean Entropy: 0.0001225091255037114, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 400,  Mean reward: -1.5, Mean Entropy: 0.00014082524285186082, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -1.0, Mean Entropy: 0.0001276970433536917, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 402,  Mean reward: -2.25, Mean Entropy: 0.00011648047802736983, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 403,  Mean reward: -2.0, Mean Entropy: 0.00012974801938980818, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 404,  Mean reward: -1.5, Mean Entropy: 0.00012153790885349736, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 405,  Mean reward: 0.0, Mean Entropy: 0.0001313055690843612, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 406,  Mean reward: -1.75, Mean Entropy: 0.00012072137178620324, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 407,  Mean reward: -1.25, Mean Entropy: 0.00010755855328170583, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 408,  Mean reward: -3.5, Mean Entropy: 0.00010303521412424743, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 409,  Mean reward: -3.75, Mean Entropy: 8.544552838429809e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 410,  Mean reward: -2.25, Mean Entropy: 9.633065201342106e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.08s
Iteration: 411,  Mean reward: -4.0, Mean Entropy: 9.008118649944663e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 412,  Mean reward: -2.5, Mean Entropy: 0.00010066048707813025, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 413,  Mean reward: -3.5, Mean Entropy: 9.53984708758071e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 414,  Mean reward: -1.5, Mean Entropy: 0.00010112690506502986, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 415,  Mean reward: -2.0, Mean Entropy: 0.00010017788008553907, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 416,  Mean reward: -3.5, Mean Entropy: 8.880619134288281e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 417,  Mean reward: -1.75, Mean Entropy: 0.00010262952127959579, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 418,  Mean reward: -2.0, Mean Entropy: 9.980401955544949e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 419,  Mean reward: -1.5, Mean Entropy: 9.75043949438259e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 420,  Mean reward: -0.5, Mean Entropy: 9.764322021510452e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 421,  Mean reward: -1.75, Mean Entropy: 9.442189184483141e-05, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.88s
Iteration: 422,  Mean reward: -2.75, Mean Entropy: 8.8976084953174e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 423,  Mean reward: -1.5, Mean Entropy: 9.595508163329214e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 424,  Mean reward: -2.25, Mean Entropy: 0.00010173166811000556, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 425,  Mean reward: -1.5, Mean Entropy: 0.00010119412036146969, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 426,  Mean reward: -2.0, Mean Entropy: 0.00010618881060509011, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 427,  Mean reward: -2.5, Mean Entropy: 0.00010472953727003187, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 428,  Mean reward: -2.75, Mean Entropy: 9.680743096396327e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 429,  Mean reward: -0.5, Mean Entropy: 0.00011121961142634973, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 430,  Mean reward: -2.25, Mean Entropy: 9.98527102638036e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 431,  Mean reward: -2.0, Mean Entropy: 9.589958790456876e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 432,  Mean reward: -3.0, Mean Entropy: 0.00010718974226620048, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 433,  Mean reward: -2.5, Mean Entropy: 0.00010179127275478095, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 434,  Mean reward: -2.0, Mean Entropy: 9.365475125377998e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 435,  Mean reward: -3.25, Mean Entropy: 9.449969365959987e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 436,  Mean reward: -4.0, Mean Entropy: 9.869736095424742e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 437,  Mean reward: -3.0, Mean Entropy: 0.00010412109986646101, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 438,  Mean reward: -1.25, Mean Entropy: 0.00011592532973736525, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 439,  Mean reward: -1.5, Mean Entropy: 0.00011675222776830196, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 440,  Mean reward: -0.75, Mean Entropy: 0.00012642238289117813, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 441,  Mean reward: -1.75, Mean Entropy: 0.00012291215534787625, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 442,  Mean reward: -1.5, Mean Entropy: 0.00012309801240917295, complete_episode_count: 80.0, Gather time: 0.68s, Train time: 0.90s
Iteration: 443,  Mean reward: -2.0, Mean Entropy: 0.00012162528582848608, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 444,  Mean reward: -1.5, Mean Entropy: 0.00012527928629424423, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 445,  Mean reward: -2.0, Mean Entropy: 0.00012406785390339792, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 446,  Mean reward: -0.75, Mean Entropy: 0.00013827430666424334, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 447,  Mean reward: -1.5, Mean Entropy: 0.00012313462502788752, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 448,  Mean reward: -4.0, Mean Entropy: 0.00012588544632308185, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 449,  Mean reward: -1.0, Mean Entropy: 0.00013913238944951445, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 450,  Mean reward: -3.5, Mean Entropy: 0.000127995663206093, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.05s
Iteration: 451,  Mean reward: -3.5, Mean Entropy: 0.00011905963037861511, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 452,  Mean reward: -4.5, Mean Entropy: 0.00012326103751547635, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.96s
Iteration: 453,  Mean reward: -1.75, Mean Entropy: 0.00014233443653210998, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 454,  Mean reward: -1.0, Mean Entropy: 0.00013884060899727046, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 455,  Mean reward: -1.0, Mean Entropy: 0.00015450142382178456, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 456,  Mean reward: -2.75, Mean Entropy: 0.00013851121184416115, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 457,  Mean reward: -2.0, Mean Entropy: 0.00014314489089883864, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 458,  Mean reward: -2.25, Mean Entropy: 0.00015664742386434227, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 459,  Mean reward: -2.75, Mean Entropy: 0.00015357742086052895, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 460,  Mean reward: -0.5, Mean Entropy: 0.00015894910029601306, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 461,  Mean reward: -2.25, Mean Entropy: 0.00014674871636088938, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 462,  Mean reward: -0.5, Mean Entropy: 0.00015285219706129283, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 463,  Mean reward: -1.0, Mean Entropy: 0.00016489450354129076, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 464,  Mean reward: -2.75, Mean Entropy: 0.00014489248860627413, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 465,  Mean reward: -0.5, Mean Entropy: 0.0001749714429024607, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 466,  Mean reward: -4.0, Mean Entropy: 0.00014134214143268764, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 467,  Mean reward: -4.25, Mean Entropy: 0.00014118695980869234, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 468,  Mean reward: -1.0, Mean Entropy: 0.00016246942686848342, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 469,  Mean reward: -1.25, Mean Entropy: 0.000158791066496633, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 470,  Mean reward: -1.5, Mean Entropy: 0.0001594137866050005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 471,  Mean reward: -1.5, Mean Entropy: 0.00015064224135130644, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 472,  Mean reward: -1.0, Mean Entropy: 0.00015301954408641905, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 473,  Mean reward: -2.75, Mean Entropy: 0.00014520002878271043, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 474,  Mean reward: -3.0, Mean Entropy: 0.000141389318741858, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 475,  Mean reward: -2.0, Mean Entropy: 0.00013824872439727187, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 476,  Mean reward: -3.5, Mean Entropy: 0.0001270266657229513, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 477,  Mean reward: -3.25, Mean Entropy: 0.00012992118718102574, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 478,  Mean reward: -1.75, Mean Entropy: 0.00013667321763932705, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 479,  Mean reward: -3.25, Mean Entropy: 0.00013195090286899358, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 480,  Mean reward: -2.5, Mean Entropy: 0.00012739477097056806, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 481,  Mean reward: -1.75, Mean Entropy: 0.00012973678531125188, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 482,  Mean reward: -1.75, Mean Entropy: 0.00012813435751013458, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 483,  Mean reward: -3.75, Mean Entropy: 0.00012162091297795996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 484,  Mean reward: -3.0, Mean Entropy: 0.00012210351997055113, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 485,  Mean reward: -2.25, Mean Entropy: 0.0001240271085407585, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 486,  Mean reward: -0.75, Mean Entropy: 0.00013071960711386055, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 487,  Mean reward: -2.5, Mean Entropy: 0.00012412176874931902, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 488,  Mean reward: -0.75, Mean Entropy: 0.0001324533805018291, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 489,  Mean reward: -4.0, Mean Entropy: 0.00012050134682795033, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 490,  Mean reward: -1.75, Mean Entropy: 0.0001323961914749816, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.07s
Iteration: 491,  Mean reward: -3.5, Mean Entropy: 0.00012804815196432173, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 492,  Mean reward: -0.25, Mean Entropy: 0.00013090981519781053, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 493,  Mean reward: -2.25, Mean Entropy: 0.00013390721869654953, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 494,  Mean reward: -3.25, Mean Entropy: 0.0001284383179154247, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 495,  Mean reward: -4.0, Mean Entropy: 0.00012385673471726477, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 496,  Mean reward: -2.75, Mean Entropy: 0.00012972819968126714, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 497,  Mean reward: -0.75, Mean Entropy: 0.0001365286880172789, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 498,  Mean reward: -4.75, Mean Entropy: 0.00011950376210734248, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 499,  Mean reward: -3.0, Mean Entropy: 0.00012558430898934603, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 500,  Mean reward: -2.5, Mean Entropy: 0.00012115181016270071, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -1.75, Mean Entropy: 0.0001275284303119406, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 502,  Mean reward: 0.0, Mean Entropy: 0.00013002041669096798, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 503,  Mean reward: -0.25, Mean Entropy: 0.00013548614515457302, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 504,  Mean reward: -3.0, Mean Entropy: 0.00013190179015509784, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 505,  Mean reward: -1.25, Mean Entropy: 0.00013798159488942474, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 506,  Mean reward: -1.25, Mean Entropy: 0.0001417136227246374, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 507,  Mean reward: -0.5, Mean Entropy: 0.0001377053267788142, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 508,  Mean reward: -2.5, Mean Entropy: 0.00013901693455409259, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 509,  Mean reward: -1.75, Mean Entropy: 0.00013800949091091752, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 510,  Mean reward: -1.25, Mean Entropy: 0.00013940114877186716, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 511,  Mean reward: -1.0, Mean Entropy: 0.00014291862316895276, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 512,  Mean reward: -1.25, Mean Entropy: 0.00014535494847223163, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 513,  Mean reward: -2.5, Mean Entropy: 0.0001409455289831385, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 514,  Mean reward: -2.25, Mean Entropy: 0.0001442606298951432, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 515,  Mean reward: -3.25, Mean Entropy: 0.00014706887304782867, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 516,  Mean reward: -3.0, Mean Entropy: 0.0001507689303252846, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 517,  Mean reward: -2.5, Mean Entropy: 0.00014822247612755746, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 518,  Mean reward: -3.0, Mean Entropy: 0.0001507504639448598, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.96s
Iteration: 519,  Mean reward: -3.0, Mean Entropy: 0.00014636739797424525, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 520,  Mean reward: 1.25, Mean Entropy: 0.00015403918223455548, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 521,  Mean reward: -3.0, Mean Entropy: 0.00014128976908978075, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 522,  Mean reward: -2.25, Mean Entropy: 0.00014832318993285298, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 523,  Mean reward: -2.75, Mean Entropy: 0.00014464529522228986, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 524,  Mean reward: -1.5, Mean Entropy: 0.00014812187873758376, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 525,  Mean reward: -3.5, Mean Entropy: 0.00014162699517328292, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 526,  Mean reward: -2.0, Mean Entropy: 0.00014386375551111996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 527,  Mean reward: -1.5, Mean Entropy: 0.00014968588948249817, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 528,  Mean reward: -0.5, Mean Entropy: 0.00015009194612503052, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 529,  Mean reward: -1.75, Mean Entropy: 0.00014798465417698026, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.93s
Iteration: 530,  Mean reward: -1.25, Mean Entropy: 0.00015079193690326065, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.06s
Iteration: 531,  Mean reward: -1.75, Mean Entropy: 0.00015036488184705377, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 532,  Mean reward: -3.25, Mean Entropy: 0.00014407921116799116, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 533,  Mean reward: -0.25, Mean Entropy: 0.00015093208639882505, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 534,  Mean reward: -1.0, Mean Entropy: 0.00014723831554874778, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 535,  Mean reward: -2.75, Mean Entropy: 0.00014574021042790264, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 536,  Mean reward: -0.5, Mean Entropy: 0.00014604965690523386, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 537,  Mean reward: -1.75, Mean Entropy: 0.00014408216520678252, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 538,  Mean reward: -4.0, Mean Entropy: 0.00014167209155857563, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 539,  Mean reward: -1.0, Mean Entropy: 0.00014458737859968096, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 540,  Mean reward: -1.0, Mean Entropy: 0.00014614954125136137, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 541,  Mean reward: -1.0, Mean Entropy: 0.00014480843674391508, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 542,  Mean reward: -1.75, Mean Entropy: 0.00014379809726960957, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 543,  Mean reward: -2.75, Mean Entropy: 0.0001422180503141135, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.94s
Iteration: 544,  Mean reward: -0.25, Mean Entropy: 0.00014387042028829455, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 545,  Mean reward: -0.25, Mean Entropy: 0.00014232182002160698, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.95s
Iteration: 546,  Mean reward: -2.5, Mean Entropy: 0.00013696301903109998, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 547,  Mean reward: -2.25, Mean Entropy: 0.0001386169606121257, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.92s
Iteration: 548,  Mean reward: -0.25, Mean Entropy: 0.00014235013804864138, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 549,  Mean reward: -1.0, Mean Entropy: 0.000145424492075108, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 550,  Mean reward: -3.25, Mean Entropy: 0.00014623720198869705, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 551,  Mean reward: -4.5, Mean Entropy: 0.0001480225910199806, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.94s
Iteration: 552,  Mean reward: 0.0, Mean Entropy: 0.00015414709923788905, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 553,  Mean reward: -2.5, Mean Entropy: 0.00015538328443653882, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 554,  Mean reward: -1.5, Mean Entropy: 0.0001546230778330937, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.90s
Iteration: 555,  Mean reward: -1.5, Mean Entropy: 0.00015173637075349689, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 556,  Mean reward: -3.5, Mean Entropy: 0.00015236195758916438, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.90s
Iteration: 557,  Mean reward: -2.0, Mean Entropy: 0.00015162071213126183, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 558,  Mean reward: -1.5, Mean Entropy: 0.00015341567632276565, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.89s
Iteration: 559,  Mean reward: -1.25, Mean Entropy: 0.00015375306247733533, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 560,  Mean reward: -2.5, Mean Entropy: 0.00015379596152342856, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.91s
Iteration: 561,  Mean reward: -2.0, Mean Entropy: 0.00015517153951805085, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.91s
Iteration: 562,  Mean reward: -0.75, Mean Entropy: 0.00015610811533406377, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.93s
Iteration: 563,  Mean reward: -1.25, Mean Entropy: 0.0001589932362549007, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 564,  Mean reward: -1.0, Mean Entropy: 0.00016073706501629204, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 565,  Mean reward: -1.75, Mean Entropy: 0.00015949980297591537, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.92s
Iteration: 566,  Mean reward: -0.5, Mean Entropy: 0.00015916585107333958, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 567,  Mean reward: -2.25, Mean Entropy: 0.00016202667029574513, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 568,  Mean reward: -0.5, Mean Entropy: 0.00016618252266198397, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 569,  Mean reward: -2.75, Mean Entropy: 0.00016117066843435168, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 570,  Mean reward: -1.0, Mean Entropy: 0.00016406386566814035, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.02s
Iteration: 571,  Mean reward: -0.5, Mean Entropy: 0.00017109522013925016, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 572,  Mean reward: -3.5, Mean Entropy: 0.00016688118921592832, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 573,  Mean reward: -2.75, Mean Entropy: 0.00016427956870757043, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 574,  Mean reward: -5.0, Mean Entropy: 0.00015967056970112026, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 575,  Mean reward: -2.0, Mean Entropy: 0.0001678252301644534, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.87s
Iteration: 576,  Mean reward: -3.5, Mean Entropy: 0.00016842524928506464, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 577,  Mean reward: -1.5, Mean Entropy: 0.0001729039940983057, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 578,  Mean reward: -0.75, Mean Entropy: 0.00017358793411403894, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 579,  Mean reward: -0.75, Mean Entropy: 0.00017837478662841022, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 580,  Mean reward: -1.0, Mean Entropy: 0.00018217088654637337, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 581,  Mean reward: -2.75, Mean Entropy: 0.0001792248076526448, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 582,  Mean reward: -0.5, Mean Entropy: 0.00018374400679022074, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 583,  Mean reward: -1.75, Mean Entropy: 0.0001897453039418906, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 584,  Mean reward: -3.25, Mean Entropy: 0.00018699327483773232, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 585,  Mean reward: -1.25, Mean Entropy: 0.00019573325698729604, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 586,  Mean reward: -1.75, Mean Entropy: 0.00019651686307042837, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 587,  Mean reward: -3.5, Mean Entropy: 0.0001923959207488224, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 588,  Mean reward: -2.25, Mean Entropy: 0.000202154740691185, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 589,  Mean reward: -1.25, Mean Entropy: 0.00020550951012410223, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 590,  Mean reward: -2.25, Mean Entropy: 0.00020887941354885697, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 591,  Mean reward: -1.25, Mean Entropy: 0.00021666873362846673, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 592,  Mean reward: -1.5, Mean Entropy: 0.00021322064276318997, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 593,  Mean reward: -0.75, Mean Entropy: 0.00021578713494818658, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 594,  Mean reward: -2.25, Mean Entropy: 0.00021297342027537525, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 595,  Mean reward: -2.5, Mean Entropy: 0.00020436616614460945, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 596,  Mean reward: -1.5, Mean Entropy: 0.00020987819880247116, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 597,  Mean reward: -3.75, Mean Entropy: 0.00020207294437568635, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 598,  Mean reward: -2.5, Mean Entropy: 0.00019868134404532611, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 599,  Mean reward: -2.0, Mean Entropy: 0.00019341727602295578, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 600,  Mean reward: -2.0, Mean Entropy: 0.00019268543110229075, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -2.5, Mean Entropy: 0.00019075983436778188, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 602,  Mean reward: -1.75, Mean Entropy: 0.00019080033234786242, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 603,  Mean reward: -2.25, Mean Entropy: 0.00019456532027106732, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 604,  Mean reward: -3.5, Mean Entropy: 0.0001890861167339608, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 605,  Mean reward: -2.0, Mean Entropy: 0.00019456184236332774, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 606,  Mean reward: -1.25, Mean Entropy: 0.0001954019971890375, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 607,  Mean reward: -2.5, Mean Entropy: 0.00019451338448561728, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 608,  Mean reward: -1.25, Mean Entropy: 0.00019306704052723944, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 609,  Mean reward: 0.0, Mean Entropy: 0.0001934911124408245, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 610,  Mean reward: 0.0, Mean Entropy: 0.0001901195355458185, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.01s
Iteration: 611,  Mean reward: -1.5, Mean Entropy: 0.00019158107170369476, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 612,  Mean reward: -1.75, Mean Entropy: 0.00018620744231157005, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 613,  Mean reward: -2.5, Mean Entropy: 0.00018548403750173748, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 614,  Mean reward: -2.5, Mean Entropy: 0.00018640991766005754, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 615,  Mean reward: -1.0, Mean Entropy: 0.00019224865536671132, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 616,  Mean reward: -2.0, Mean Entropy: 0.00019847553630825132, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 617,  Mean reward: -0.25, Mean Entropy: 0.0002138046984327957, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 618,  Mean reward: -1.75, Mean Entropy: 0.00021436696988530457, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 619,  Mean reward: -0.5, Mean Entropy: 0.00022264872677624226, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 620,  Mean reward: -1.25, Mean Entropy: 0.0002197468129452318, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 621,  Mean reward: -4.5, Mean Entropy: 0.00022473307035397738, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 622,  Mean reward: -2.0, Mean Entropy: 0.00022666189761366695, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 623,  Mean reward: -0.75, Mean Entropy: 0.0002288392133777961, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 624,  Mean reward: -3.5, Mean Entropy: 0.00022940238704904914, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 625,  Mean reward: -2.75, Mean Entropy: 0.0002342356601729989, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 626,  Mean reward: -1.25, Mean Entropy: 0.00024018212570808828, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 627,  Mean reward: -3.0, Mean Entropy: 0.00023508071899414062, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 628,  Mean reward: -1.5, Mean Entropy: 0.0002376373449806124, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 629,  Mean reward: -0.25, Mean Entropy: 0.00023705359490122646, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 630,  Mean reward: -2.5, Mean Entropy: 0.00022778520360589027, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 631,  Mean reward: -2.0, Mean Entropy: 0.00021948151697870344, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 632,  Mean reward: 1.25, Mean Entropy: 0.0002203275653300807, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 633,  Mean reward: -0.5, Mean Entropy: 0.0002208686200901866, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 634,  Mean reward: -3.25, Mean Entropy: 0.0002250476973131299, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 635,  Mean reward: -1.75, Mean Entropy: 0.0002379154320806265, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 636,  Mean reward: -2.25, Mean Entropy: 0.0002411139867035672, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 637,  Mean reward: -0.25, Mean Entropy: 0.00025132318842224777, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 638,  Mean reward: 0.25, Mean Entropy: 0.00025248335441574454, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 639,  Mean reward: -2.0, Mean Entropy: 0.00023741472978144884, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 640,  Mean reward: -1.25, Mean Entropy: 0.000241545305470936, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 641,  Mean reward: -1.75, Mean Entropy: 0.00024634567671455443, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 642,  Mean reward: -2.0, Mean Entropy: 0.00024787994334474206, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.88s
Iteration: 643,  Mean reward: -1.5, Mean Entropy: 0.00025755216483958066, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 644,  Mean reward: -2.5, Mean Entropy: 0.0002625390188768506, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 645,  Mean reward: -3.5, Mean Entropy: 0.00026629629428498447, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 646,  Mean reward: -2.75, Mean Entropy: 0.0002708819811232388, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 647,  Mean reward: -1.75, Mean Entropy: 0.0002974412636831403, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 648,  Mean reward: 0.25, Mean Entropy: 0.00030721089569851756, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 649,  Mean reward: -1.75, Mean Entropy: 0.00029351512785069644, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 650,  Mean reward: -1.0, Mean Entropy: 0.000303203531075269, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.01s
Iteration: 651,  Mean reward: -2.0, Mean Entropy: 0.0002951670903712511, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 652,  Mean reward: 0.0, Mean Entropy: 0.000321823958074674, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 653,  Mean reward: -1.0, Mean Entropy: 0.00033363173133693635, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 654,  Mean reward: -1.25, Mean Entropy: 0.0003361643757671118, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 655,  Mean reward: -2.0, Mean Entropy: 0.00034346804022789, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 656,  Mean reward: -1.0, Mean Entropy: 0.0003646653494797647, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 657,  Mean reward: -2.75, Mean Entropy: 0.00037269791937433183, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 658,  Mean reward: -1.0, Mean Entropy: 0.00038541154935956, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 659,  Mean reward: -1.25, Mean Entropy: 0.00039981407462619245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 660,  Mean reward: -3.5, Mean Entropy: 0.00042144232429564, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 661,  Mean reward: -0.25, Mean Entropy: 0.00040646351408213377, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 662,  Mean reward: -2.25, Mean Entropy: 0.00041913753375411034, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 663,  Mean reward: -3.25, Mean Entropy: 0.0004387233639135957, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 664,  Mean reward: -1.25, Mean Entropy: 0.00044683608575724065, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 665,  Mean reward: -2.25, Mean Entropy: 0.0004424343933351338, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 666,  Mean reward: -2.25, Mean Entropy: 0.0004337205027695745, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 667,  Mean reward: -3.0, Mean Entropy: 0.0004344919871073216, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 668,  Mean reward: -1.0, Mean Entropy: 0.00044477550545707345, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 669,  Mean reward: -1.5, Mean Entropy: 0.00044341234024614096, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 670,  Mean reward: -4.25, Mean Entropy: 0.00048762967344373465, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 671,  Mean reward: -2.5, Mean Entropy: 0.0004648173926398158, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 672,  Mean reward: -2.25, Mean Entropy: 0.0004478174669202417, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 673,  Mean reward: -1.5, Mean Entropy: 0.00045979226706549525, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 674,  Mean reward: -3.0, Mean Entropy: 0.00042509008198976517, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 675,  Mean reward: -3.75, Mean Entropy: 0.0004119380027987063, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 676,  Mean reward: -3.25, Mean Entropy: 0.00044648509356193244, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 677,  Mean reward: -2.25, Mean Entropy: 0.00045405153650790453, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 678,  Mean reward: -1.75, Mean Entropy: 0.0004654938238672912, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 679,  Mean reward: -2.0, Mean Entropy: 0.00045844863052479923, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 680,  Mean reward: -2.0, Mean Entropy: 0.0004496001056395471, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 681,  Mean reward: -0.75, Mean Entropy: 0.0004513474996201694, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 682,  Mean reward: -1.0, Mean Entropy: 0.00045543076703324914, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 683,  Mean reward: -1.75, Mean Entropy: 0.00044662499567493796, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 684,  Mean reward: -1.25, Mean Entropy: 0.00045748334378004074, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 685,  Mean reward: -1.75, Mean Entropy: 0.00043433881364762783, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 686,  Mean reward: -4.25, Mean Entropy: 0.0004577868094202131, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 687,  Mean reward: -1.0, Mean Entropy: 0.00044004706433042884, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 688,  Mean reward: -3.5, Mean Entropy: 0.0004434674046933651, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 689,  Mean reward: -1.0, Mean Entropy: 0.0004322775057516992, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 690,  Mean reward: -2.75, Mean Entropy: 0.0004483574884943664, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.00s
Iteration: 691,  Mean reward: -3.0, Mean Entropy: 0.0004495240282267332, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 692,  Mean reward: -1.25, Mean Entropy: 0.0004396808799356222, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 693,  Mean reward: -1.5, Mean Entropy: 0.0004410425608512014, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 694,  Mean reward: -0.75, Mean Entropy: 0.00043972869752906263, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -3.960526315789474, Mean Entropy: 1.0036194324493408, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.69s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.166666666666667, Mean Entropy: 0.9386367797851562, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 2,  Mean reward: -4.366666666666666, Mean Entropy: 0.967517614364624, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.67s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -2.4390243902439024, Mean Entropy: 0.9025344252586365, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 4,  Mean reward: -3.2125, Mean Entropy: 0.8953118324279785, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 5,  Mean reward: -5.295454545454546, Mean Entropy: 0.9675067663192749, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 6,  Mean reward: -3.0, Mean Entropy: 1.0252702236175537, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 7,  Mean reward: -2.872093023255814, Mean Entropy: 0.8808702230453491, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 8,  Mean reward: -4.476190476190476, Mean Entropy: 1.0397188663482666, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 9,  Mean reward: -4.0625, Mean Entropy: 0.9530762434005737, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 10,  Mean reward: -4.951219512195122, Mean Entropy: 0.9530689716339111, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 11,  Mean reward: -6.5625, Mean Entropy: 0.9313820600509644, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 12,  Mean reward: -4.238095238095238, Mean Entropy: 0.9385473728179932, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 13,  Mean reward: -5.0, Mean Entropy: 0.9601505994796753, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 14,  Mean reward: -4.3125, Mean Entropy: 0.9024138450622559, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 15,  Mean reward: -4.5625, Mean Entropy: 0.8879752159118652, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.65s
Iteration: 16,  Mean reward: -4.548780487804878, Mean Entropy: 0.9674310684204102, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 17,  Mean reward: -5.233333333333333, Mean Entropy: 0.8735933303833008, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 18,  Mean reward: -5.733333333333333, Mean Entropy: 0.909701943397522, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 19,  Mean reward: -4.280487804878049, Mean Entropy: 0.9458369016647339, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 20,  Mean reward: -2.2142857142857144, Mean Entropy: 0.953056275844574, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 21,  Mean reward: -5.5, Mean Entropy: 0.916956901550293, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 22,  Mean reward: -4.25, Mean Entropy: 0.9313957691192627, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 23,  Mean reward: -4.602564102564102, Mean Entropy: 0.9674992561340332, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.68s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 24,  Mean reward: -2.1627906976744184, Mean Entropy: 0.9241899847984314, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.71s
Iteration: 25,  Mean reward: -3.1666666666666665, Mean Entropy: 0.9819578528404236, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 26,  Mean reward: -4.5875, Mean Entropy: 0.9386361837387085, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 27,  Mean reward: -4.560975609756097, Mean Entropy: 0.9602969884872437, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 28,  Mean reward: -4.486486486486487, Mean Entropy: 0.9314165115356445, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 29,  Mean reward: -3.880952380952381, Mean Entropy: 1.0036187171936035, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 30,  Mean reward: -5.059523809523809, Mean Entropy: 0.8953145742416382, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.85s
Iteration: 31,  Mean reward: -3.9878048780487805, Mean Entropy: 0.8880928754806519, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 32,  Mean reward: -4.5227272727272725, Mean Entropy: 1.003612995147705, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 33,  Mean reward: -3.2093023255813953, Mean Entropy: 0.9675175547599792, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 34,  Mean reward: -5.625, Mean Entropy: 0.9819560050964355, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 35,  Mean reward: -2.5375, Mean Entropy: 0.8880918025970459, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 36,  Mean reward: -6.45, Mean Entropy: 0.9169707298278809, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 37,  Mean reward: -4.7631578947368425, Mean Entropy: 0.9097540378570557, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 38,  Mean reward: -4.583333333333333, Mean Entropy: 0.9819552898406982, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 39,  Mean reward: -3.4767441860465116, Mean Entropy: 0.9314161539077759, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 40,  Mean reward: -6.416666666666667, Mean Entropy: 0.8736541271209717, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 41,  Mean reward: -3.7435897435897436, Mean Entropy: 0.8880939483642578, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 42,  Mean reward: -3.2375, Mean Entropy: 0.9530755281448364, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 43,  Mean reward: -6.371794871794871, Mean Entropy: 1.003615379333496, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 44,  Mean reward: -3.6777777777777776, Mean Entropy: 0.953075110912323, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 45,  Mean reward: -3.5232558139534884, Mean Entropy: 0.9314140677452087, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 46,  Mean reward: -2.2439024390243905, Mean Entropy: 0.9241929054260254, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.65s
Iteration: 47,  Mean reward: -5.682926829268292, Mean Entropy: 0.8880938291549683, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 48,  Mean reward: -5.644444444444445, Mean Entropy: 0.938636302947998, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.69s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 49,  Mean reward: -1.4318181818181819, Mean Entropy: 0.9314135313034058, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.70s
Iteration: 50,  Mean reward: -3.7906976744186047, Mean Entropy: 0.9241926074028015, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 51,  Mean reward: -4.390243902439025, Mean Entropy: 0.9386312365531921, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 52,  Mean reward: -4.2625, Mean Entropy: 0.9602922201156616, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 53,  Mean reward: -5.337837837837838, Mean Entropy: 0.9025344848632812, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 54,  Mean reward: -3.7111111111111112, Mean Entropy: 0.9747330546379089, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 55,  Mean reward: -6.559523809523809, Mean Entropy: 0.924191415309906, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 56,  Mean reward: -3.7738095238095237, Mean Entropy: 0.9458467364311218, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.72s
Iteration: 57,  Mean reward: -2.6785714285714284, Mean Entropy: 0.9458522796630859, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 58,  Mean reward: -3.858974358974359, Mean Entropy: 0.9530715346336365, complete_episode_count: 39.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 59,  Mean reward: -7.075, Mean Entropy: 0.9025293588638306, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 60,  Mean reward: -5.267441860465116, Mean Entropy: 0.8880882263183594, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 61,  Mean reward: -6.487179487179487, Mean Entropy: 0.9241843223571777, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 62,  Mean reward: -4.565789473684211, Mean Entropy: 0.9386140704154968, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 63,  Mean reward: -4.166666666666667, Mean Entropy: 0.9241654276847839, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 64,  Mean reward: -6.256410256410256, Mean Entropy: 0.851974606513977, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 65,  Mean reward: -5.5375, Mean Entropy: 0.974705696105957, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.85s
Iteration: 66,  Mean reward: -5.513513513513513, Mean Entropy: 0.909722089767456, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 67,  Mean reward: -5.821428571428571, Mean Entropy: 0.93137526512146, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 68,  Mean reward: -3.3214285714285716, Mean Entropy: 0.9602600336074829, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 69,  Mean reward: -4.329268292682927, Mean Entropy: 0.9025055170059204, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 70,  Mean reward: -5.548780487804878, Mean Entropy: 0.9386083483695984, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 71,  Mean reward: -5.546511627906977, Mean Entropy: 0.9458270072937012, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 72,  Mean reward: -3.965909090909091, Mean Entropy: 0.9458311796188354, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 73,  Mean reward: -5.2875, Mean Entropy: 0.9097223281860352, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 74,  Mean reward: -4.511904761904762, Mean Entropy: 0.9241330027580261, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.74s
Iteration: 75,  Mean reward: -5.321428571428571, Mean Entropy: 0.9962746500968933, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 76,  Mean reward: -1.6395348837209303, Mean Entropy: 0.9530021548271179, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.73s
Iteration: 77,  Mean reward: -3.1951219512195124, Mean Entropy: 0.9168956279754639, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 78,  Mean reward: -6.146341463414634, Mean Entropy: 0.9240702390670776, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.65s
Iteration: 79,  Mean reward: -5.583333333333333, Mean Entropy: 1.0178720951080322, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 80,  Mean reward: -6.7625, Mean Entropy: 0.9240521788597107, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 81,  Mean reward: -3.5434782608695654, Mean Entropy: 0.960153341293335, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 82,  Mean reward: -4.1375, Mean Entropy: 0.9385271072387695, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 83,  Mean reward: -6.654761904761905, Mean Entropy: 0.9312633275985718, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 84,  Mean reward: -4.5227272727272725, Mean Entropy: 0.9601356983184814, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 85,  Mean reward: -5.046511627906977, Mean Entropy: 0.960173487663269, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 86,  Mean reward: -4.125, Mean Entropy: 0.9313186407089233, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 87,  Mean reward: -4.568181818181818, Mean Entropy: 0.9600976705551147, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 88,  Mean reward: -4.052631578947368, Mean Entropy: 0.9023404121398926, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 89,  Mean reward: -3.8552631578947367, Mean Entropy: 0.8589646220207214, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 90,  Mean reward: -3.4456521739130435, Mean Entropy: 0.9021487236022949, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 91,  Mean reward: -4.170454545454546, Mean Entropy: 0.9379698038101196, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 92,  Mean reward: -5.2560975609756095, Mean Entropy: 0.9305390119552612, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 93,  Mean reward: -4.193181818181818, Mean Entropy: 0.9593267440795898, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 94,  Mean reward: -4.0227272727272725, Mean Entropy: 0.8730260729789734, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 95,  Mean reward: -4.784090909090909, Mean Entropy: 0.872832179069519, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 96,  Mean reward: -5.5476190476190474, Mean Entropy: 0.9297885894775391, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 97,  Mean reward: -4.939024390243903, Mean Entropy: 0.9154262542724609, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 98,  Mean reward: -4.4868421052631575, Mean Entropy: 0.8938464522361755, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 99,  Mean reward: -3.058139534883721, Mean Entropy: 0.9157952666282654, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 100,  Mean reward: -3.9146341463414633, Mean Entropy: 0.89433354139328, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.83s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -3.641025641025641, Mean Entropy: 0.908782422542572, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 102,  Mean reward: -5.666666666666667, Mean Entropy: 0.9368146657943726, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 103,  Mean reward: -4.5227272727272725, Mean Entropy: 0.9221607446670532, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 104,  Mean reward: -4.146341463414634, Mean Entropy: 0.9507741928100586, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 105,  Mean reward: -2.784090909090909, Mean Entropy: 0.9224767684936523, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 106,  Mean reward: -2.6904761904761907, Mean Entropy: 0.9374098181724548, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 107,  Mean reward: -4.318181818181818, Mean Entropy: 0.8934429883956909, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 108,  Mean reward: -3.1333333333333333, Mean Entropy: 0.9292733073234558, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 109,  Mean reward: -3.175, Mean Entropy: 0.943547248840332, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 110,  Mean reward: -3.5568181818181817, Mean Entropy: 0.9354998469352722, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.65s
Iteration: 111,  Mean reward: -5.963414634146342, Mean Entropy: 0.9347307682037354, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 112,  Mean reward: -4.956521739130435, Mean Entropy: 0.8905735015869141, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 113,  Mean reward: -3.0531914893617023, Mean Entropy: 0.9113317728042603, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 114,  Mean reward: -5.222222222222222, Mean Entropy: 0.9553377032279968, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 115,  Mean reward: -5.784090909090909, Mean Entropy: 0.909912109375, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 116,  Mean reward: -5.048780487804878, Mean Entropy: 0.9021713137626648, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.66s
Iteration: 117,  Mean reward: -3.475, Mean Entropy: 0.9741896986961365, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 118,  Mean reward: -4.569767441860465, Mean Entropy: 1.015662670135498, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 119,  Mean reward: -4.548780487804878, Mean Entropy: 0.9050785899162292, complete_episode_count: 41.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 120,  Mean reward: -3.0568181818181817, Mean Entropy: 0.9161558151245117, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 121,  Mean reward: -2.802325581395349, Mean Entropy: 0.9150841236114502, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 122,  Mean reward: -4.6063829787234045, Mean Entropy: 0.8872708082199097, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 123,  Mean reward: -4.0813953488372094, Mean Entropy: 0.9016647338867188, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 124,  Mean reward: -2.6625, Mean Entropy: 0.8922697901725769, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 125,  Mean reward: -5.579545454545454, Mean Entropy: 0.9366329908370972, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 126,  Mean reward: -1.7604166666666667, Mean Entropy: 0.9055001735687256, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 127,  Mean reward: -5.28, Mean Entropy: 0.9252550005912781, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 128,  Mean reward: -3.7934782608695654, Mean Entropy: 0.8777126669883728, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 129,  Mean reward: -5.4772727272727275, Mean Entropy: 0.9068584442138672, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 130,  Mean reward: -3.534090909090909, Mean Entropy: 0.8780456781387329, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 131,  Mean reward: -2.8260869565217392, Mean Entropy: 0.8369523286819458, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 132,  Mean reward: -3.3, Mean Entropy: 0.9540396928787231, complete_episode_count: 45.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 133,  Mean reward: -3.7444444444444445, Mean Entropy: 0.9388799667358398, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 134,  Mean reward: -4.1, Mean Entropy: 0.9349650144577026, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 135,  Mean reward: -3.7127659574468086, Mean Entropy: 0.9199439883232117, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.83s
Iteration: 136,  Mean reward: -4.3, Mean Entropy: 0.9137487411499023, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 137,  Mean reward: -3.8333333333333335, Mean Entropy: 0.8683632612228394, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.72s
Iteration: 138,  Mean reward: -3.7395833333333335, Mean Entropy: 0.8632916808128357, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 139,  Mean reward: -5.4523809523809526, Mean Entropy: 0.853736400604248, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.66s
Iteration: 140,  Mean reward: -3.4431818181818183, Mean Entropy: 0.8910737037658691, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 141,  Mean reward: -1.941860465116279, Mean Entropy: 0.8878092169761658, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 142,  Mean reward: -6.511111111111111, Mean Entropy: 0.8691720366477966, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 143,  Mean reward: -2.8222222222222224, Mean Entropy: 0.8874489068984985, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 144,  Mean reward: -4.032608695652174, Mean Entropy: 0.8186795115470886, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 145,  Mean reward: -6.34, Mean Entropy: 0.800981879234314, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 146,  Mean reward: -2.6037735849056602, Mean Entropy: 0.8088396787643433, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 147,  Mean reward: -2.4761904761904763, Mean Entropy: 0.8389686346054077, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 148,  Mean reward: -6.2727272727272725, Mean Entropy: 0.8179011344909668, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 149,  Mean reward: -6.144444444444445, Mean Entropy: 0.8181465864181519, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 150,  Mean reward: -5.233333333333333, Mean Entropy: 0.77083420753479, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 151,  Mean reward: -2.5319148936170213, Mean Entropy: 0.8246976137161255, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 152,  Mean reward: -3.967391304347826, Mean Entropy: 0.8650011420249939, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 153,  Mean reward: -1.5476190476190477, Mean Entropy: 0.8481184244155884, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 154,  Mean reward: -2.630434782608696, Mean Entropy: 0.8169991374015808, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 155,  Mean reward: -4.566666666666666, Mean Entropy: 0.7863329648971558, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 156,  Mean reward: -3.652173913043478, Mean Entropy: 0.7452161312103271, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 157,  Mean reward: -2.1037735849056602, Mean Entropy: 0.8484706878662109, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 158,  Mean reward: -3.8365384615384617, Mean Entropy: 0.6609673500061035, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 159,  Mean reward: -1.836734693877551, Mean Entropy: 0.6628150343894958, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 160,  Mean reward: -2.53, Mean Entropy: 0.7319297194480896, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 161,  Mean reward: -2.452830188679245, Mean Entropy: 0.7285512685775757, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 162,  Mean reward: -4.679245283018868, Mean Entropy: 0.6990112066268921, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 163,  Mean reward: -1.2843137254901962, Mean Entropy: 0.6933249235153198, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.65s
Iteration: 164,  Mean reward: -4.790909090909091, Mean Entropy: 0.7845994830131531, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 165,  Mean reward: -2.25, Mean Entropy: 0.79930579662323, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 166,  Mean reward: -3.7547169811320753, Mean Entropy: 0.8071908950805664, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.66s
Iteration: 167,  Mean reward: -4.637254901960785, Mean Entropy: 0.8323206901550293, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 168,  Mean reward: -4.99, Mean Entropy: 0.8374128341674805, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 169,  Mean reward: -2.8617021276595747, Mean Entropy: 0.8688077926635742, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.88s
Iteration: 170,  Mean reward: -2.1020408163265305, Mean Entropy: 0.8573259115219116, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 171,  Mean reward: -4.55, Mean Entropy: 0.7906119227409363, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 172,  Mean reward: -2.625, Mean Entropy: 0.7414929270744324, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 173,  Mean reward: -4.06, Mean Entropy: 0.7255746722221375, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 174,  Mean reward: -1.8421052631578947, Mean Entropy: 0.6836729049682617, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 175,  Mean reward: -3.74, Mean Entropy: 0.7438400983810425, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 176,  Mean reward: -2.4732142857142856, Mean Entropy: 0.7828823328018188, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 177,  Mean reward: -3.5104166666666665, Mean Entropy: 0.7366032600402832, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 178,  Mean reward: -2.8627450980392157, Mean Entropy: 0.7130688428878784, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.65s
Iteration: 179,  Mean reward: -6.160377358490566, Mean Entropy: 0.6669210195541382, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 180,  Mean reward: -4.020408163265306, Mean Entropy: 0.6515727639198303, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 181,  Mean reward: -3.7244897959183674, Mean Entropy: 0.6266558170318604, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 182,  Mean reward: -2.372549019607843, Mean Entropy: 0.6565698981285095, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 183,  Mean reward: -2.909090909090909, Mean Entropy: 0.6206213235855103, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 184,  Mean reward: -2.8846153846153846, Mean Entropy: 0.6182152032852173, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 185,  Mean reward: -4.2, Mean Entropy: 0.6567704677581787, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 186,  Mean reward: -2.3833333333333333, Mean Entropy: 0.6609103083610535, complete_episode_count: 60.0, Gather time: 0.52s, Train time: 1.65s
Iteration: 187,  Mean reward: -3.7830188679245285, Mean Entropy: 0.6983894109725952, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 188,  Mean reward: -6.586538461538462, Mean Entropy: 0.6625762581825256, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 189,  Mean reward: -5.3, Mean Entropy: 0.6900811195373535, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 190,  Mean reward: -1.5865384615384615, Mean Entropy: 0.7050929069519043, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 191,  Mean reward: -4.145833333333333, Mean Entropy: 0.6942383646965027, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 192,  Mean reward: -2.607142857142857, Mean Entropy: 0.655278205871582, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 193,  Mean reward: -3.3518518518518516, Mean Entropy: 0.6164296269416809, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.72s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 194,  Mean reward: -0.35714285714285715, Mean Entropy: 0.6273764371871948, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.72s
Iteration: 195,  Mean reward: -2.787037037037037, Mean Entropy: 0.5523876547813416, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.73s
Iteration: 196,  Mean reward: -4.179245283018868, Mean Entropy: 0.6119383573532104, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 197,  Mean reward: -2.207547169811321, Mean Entropy: 0.6158415675163269, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 198,  Mean reward: -1.8055555555555556, Mean Entropy: 0.6313880681991577, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.72s
Iteration: 199,  Mean reward: -4.056603773584905, Mean Entropy: 0.6274635195732117, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.75s
Iteration: 200,  Mean reward: -0.8660714285714286, Mean Entropy: 0.5777958035469055, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.75s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -5.3584905660377355, Mean Entropy: 0.6016116142272949, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 202,  Mean reward: -3.456896551724138, Mean Entropy: 0.6712309718132019, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.87s
Iteration: 203,  Mean reward: -1.1727272727272726, Mean Entropy: 0.6411587595939636, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 204,  Mean reward: -3.518181818181818, Mean Entropy: 0.666432797908783, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 205,  Mean reward: -4.351851851851852, Mean Entropy: 0.5995053052902222, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.73s
Iteration: 206,  Mean reward: -1.4181818181818182, Mean Entropy: 0.6711623072624207, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.71s
Iteration: 207,  Mean reward: -1.736842105263158, Mean Entropy: 0.6139106750488281, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 208,  Mean reward: -2.4722222222222223, Mean Entropy: 0.7064220309257507, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.69s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 209,  Mean reward: 0.07142857142857142, Mean Entropy: 0.7326713800430298, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 210,  Mean reward: -5.875, Mean Entropy: 0.6696394681930542, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.74s
Iteration: 211,  Mean reward: -3.6203703703703702, Mean Entropy: 0.7252674102783203, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 212,  Mean reward: -2.2, Mean Entropy: 0.7422456741333008, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 213,  Mean reward: -1.2118644067796611, Mean Entropy: 0.6521211862564087, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 214,  Mean reward: -0.15454545454545454, Mean Entropy: 0.6462459564208984, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 215,  Mean reward: -1.2181818181818183, Mean Entropy: 0.7168156504631042, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 216,  Mean reward: -3.981818181818182, Mean Entropy: 0.701557993888855, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 217,  Mean reward: -2.1636363636363636, Mean Entropy: 0.5943541526794434, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 218,  Mean reward: -0.3389830508474576, Mean Entropy: 0.5209475755691528, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 219,  Mean reward: -2.754385964912281, Mean Entropy: 0.5680053234100342, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 220,  Mean reward: -0.6810344827586207, Mean Entropy: 0.4918031096458435, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 221,  Mean reward: -4.203389830508475, Mean Entropy: 0.5521605014801025, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 222,  Mean reward: -1.103448275862069, Mean Entropy: 0.5812737941741943, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 223,  Mean reward: -2.418181818181818, Mean Entropy: 0.5825915932655334, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 224,  Mean reward: -0.45081967213114754, Mean Entropy: 0.6730572581291199, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 225,  Mean reward: -1.5510204081632653, Mean Entropy: 0.7915293574333191, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 226,  Mean reward: 0.35714285714285715, Mean Entropy: 0.6813011169433594, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.68s
Iteration: 227,  Mean reward: -1.3584905660377358, Mean Entropy: 0.8045471906661987, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 228,  Mean reward: -1.61, Mean Entropy: 0.8178513050079346, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 229,  Mean reward: -0.8111111111111111, Mean Entropy: 0.7608968019485474, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.67s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 230,  Mean reward: 0.41228070175438597, Mean Entropy: 0.6755973696708679, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.70s
Iteration: 231,  Mean reward: -1.5089285714285714, Mean Entropy: 0.7667157649993896, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 232,  Mean reward: -2.2244897959183674, Mean Entropy: 0.7893766164779663, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 233,  Mean reward: -2.2115384615384617, Mean Entropy: 0.8229084014892578, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.69s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 234,  Mean reward: 0.696078431372549, Mean Entropy: 0.8052223920822144, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.84s
Iteration: 235,  Mean reward: -2.0849056603773586, Mean Entropy: 0.8005574941635132, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 236,  Mean reward: -1.9574468085106382, Mean Entropy: 0.7822248339653015, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 237,  Mean reward: -2.2058823529411766, Mean Entropy: 0.614156186580658, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 238,  Mean reward: -0.288135593220339, Mean Entropy: 0.5377472639083862, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.73s
Iteration: 239,  Mean reward: -1.75, Mean Entropy: 0.5767498016357422, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 240,  Mean reward: -0.9824561403508771, Mean Entropy: 0.5982392430305481, complete_episode_count: 57.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 241,  Mean reward: -0.9918032786885246, Mean Entropy: 0.5797089338302612, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 242,  Mean reward: 1.1120689655172413, Mean Entropy: 0.5989614725112915, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.71s
Iteration: 243,  Mean reward: -2.0185185185185186, Mean Entropy: 0.7510050535202026, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 244,  Mean reward: -0.7636363636363637, Mean Entropy: 0.568468451499939, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 245,  Mean reward: -3.3363636363636364, Mean Entropy: 0.6282886266708374, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 246,  Mean reward: -0.3135593220338983, Mean Entropy: 0.6505515575408936, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 247,  Mean reward: -1.263157894736842, Mean Entropy: 0.5570217967033386, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 248,  Mean reward: -1.3114754098360655, Mean Entropy: 0.5769971609115601, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 249,  Mean reward: 0.8571428571428571, Mean Entropy: 0.560818076133728, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 250,  Mean reward: 0.25862068965517243, Mean Entropy: 0.6768805384635925, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 251,  Mean reward: -4.134615384615385, Mean Entropy: 0.7108221054077148, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 252,  Mean reward: -1.209090909090909, Mean Entropy: 0.6419119834899902, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 253,  Mean reward: 0.03773584905660377, Mean Entropy: 0.6833724975585938, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 254,  Mean reward: -0.7058823529411765, Mean Entropy: 0.8301390409469604, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 255,  Mean reward: -1.0357142857142858, Mean Entropy: 0.7145513296127319, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 256,  Mean reward: 0.4895833333333333, Mean Entropy: 0.7980197668075562, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 257,  Mean reward: -2.1458333333333335, Mean Entropy: 0.8210398554801941, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 258,  Mean reward: -3.0520833333333335, Mean Entropy: 0.7773405313491821, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 259,  Mean reward: 0.9259259259259259, Mean Entropy: 0.7284218668937683, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 260,  Mean reward: 0.8469387755102041, Mean Entropy: 0.8540099859237671, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 261,  Mean reward: -0.9607843137254902, Mean Entropy: 0.7848765850067139, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 262,  Mean reward: -2.6122448979591835, Mean Entropy: 0.7666231989860535, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.78s
Iteration: 263,  Mean reward: -0.5625, Mean Entropy: 0.7503463625907898, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.67s
Iteration: 264,  Mean reward: -2.7547169811320753, Mean Entropy: 0.6540911793708801, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.73s
Iteration: 265,  Mean reward: -2.3425925925925926, Mean Entropy: 0.7344707250595093, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 266,  Mean reward: -1.9807692307692308, Mean Entropy: 0.7831436991691589, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 267,  Mean reward: 0.5727272727272728, Mean Entropy: 0.6828739047050476, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.85s
Iteration: 268,  Mean reward: -0.6057692307692307, Mean Entropy: 0.7968686819076538, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 269,  Mean reward: 0.87, Mean Entropy: 0.8396856784820557, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 270,  Mean reward: -0.2604166666666667, Mean Entropy: 0.8811087608337402, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 271,  Mean reward: -2.7346938775510203, Mean Entropy: 0.8022913932800293, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 272,  Mean reward: -1.45, Mean Entropy: 0.7091001868247986, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 273,  Mean reward: -3.7788461538461537, Mean Entropy: 0.7385424971580505, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.68s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 274,  Mean reward: 1.1574074074074074, Mean Entropy: 0.5686972141265869, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.70s
Iteration: 275,  Mean reward: -0.6909090909090909, Mean Entropy: 0.6539592742919922, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.69s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 276,  Mean reward: 1.4722222222222223, Mean Entropy: 0.6459016799926758, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 277,  Mean reward: 0.22641509433962265, Mean Entropy: 0.8414761424064636, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.66s
Iteration: 278,  Mean reward: 0.85, Mean Entropy: 0.8121095895767212, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 279,  Mean reward: 0.6122448979591837, Mean Entropy: 0.8111937046051025, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 280,  Mean reward: 1.0416666666666667, Mean Entropy: 0.7546776533126831, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 281,  Mean reward: 1.0, Mean Entropy: 0.7196469306945801, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 282,  Mean reward: 1.263157894736842, Mean Entropy: 0.7134369611740112, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 283,  Mean reward: 1.0081967213114753, Mean Entropy: 0.6007645130157471, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.65s
Iteration: 284,  Mean reward: 1.1153846153846154, Mean Entropy: 0.6022830009460449, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.66s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 285,  Mean reward: 2.392857142857143, Mean Entropy: 0.5199205279350281, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.68s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 286,  Mean reward: 3.7222222222222223, Mean Entropy: 0.6514967679977417, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.67s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 287,  Mean reward: 4.974137931034483, Mean Entropy: 0.8850867748260498, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.69s
Iteration: 288,  Mean reward: -3.895348837209302, Mean Entropy: 0.8070416450500488, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 289,  Mean reward: -3.524390243902439, Mean Entropy: 0.8597239851951599, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 290,  Mean reward: -3.5375, Mean Entropy: 0.8324834704399109, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 291,  Mean reward: 0.6, Mean Entropy: 0.9277462959289551, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 292,  Mean reward: -4.25, Mean Entropy: 0.9010457396507263, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 293,  Mean reward: -1.1538461538461537, Mean Entropy: 0.9233449697494507, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 294,  Mean reward: -2.0510204081632653, Mean Entropy: 0.8993635177612305, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 295,  Mean reward: -3.297872340425532, Mean Entropy: 0.8399469256401062, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 296,  Mean reward: 0.48, Mean Entropy: 0.875576376914978, complete_episode_count: 50.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 297,  Mean reward: -1.712962962962963, Mean Entropy: 0.7696418166160583, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 298,  Mean reward: -0.4824561403508772, Mean Entropy: 0.542518138885498, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 299,  Mean reward: -2.0833333333333335, Mean Entropy: 0.5831160545349121, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 300,  Mean reward: -0.9112903225806451, Mean Entropy: 0.6182048320770264, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 1.01s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 0.4051724137931034, Mean Entropy: 0.6483914256095886, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 302,  Mean reward: 4.188524590163935, Mean Entropy: 0.5857217311859131, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 303,  Mean reward: 3.7868852459016393, Mean Entropy: 0.7929851412773132, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 304,  Mean reward: 0.7980769230769231, Mean Entropy: 0.8855142593383789, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 305,  Mean reward: -3.608108108108108, Mean Entropy: 0.9034522771835327, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 306,  Mean reward: -0.4583333333333333, Mean Entropy: 0.7709300518035889, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 307,  Mean reward: 2.1862745098039214, Mean Entropy: 0.7469139099121094, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 308,  Mean reward: 1.030612244897959, Mean Entropy: 0.6067315340042114, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 309,  Mean reward: 2.1739130434782608, Mean Entropy: 0.7014035582542419, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 310,  Mean reward: 3.6176470588235294, Mean Entropy: 0.7885904312133789, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 311,  Mean reward: 4.560344827586207, Mean Entropy: 0.787137508392334, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.66s
Iteration: 312,  Mean reward: 2.69, Mean Entropy: 0.7788864970207214, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 313,  Mean reward: 3.638888888888889, Mean Entropy: 0.682705283164978, complete_episode_count: 54.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 314,  Mean reward: 3.169642857142857, Mean Entropy: 0.7560896873474121, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.66s
Iteration: 315,  Mean reward: 0.10344827586206896, Mean Entropy: 0.8637106418609619, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 316,  Mean reward: -4.716216216216216, Mean Entropy: 0.9469279646873474, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 317,  Mean reward: -1.630952380952381, Mean Entropy: 0.9659892320632935, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 318,  Mean reward: -5.738636363636363, Mean Entropy: 0.969537615776062, complete_episode_count: 44.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 319,  Mean reward: -4.107142857142857, Mean Entropy: 0.8597363829612732, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 320,  Mean reward: -1.3076923076923077, Mean Entropy: 0.7557932734489441, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 321,  Mean reward: 0.7307692307692307, Mean Entropy: 0.4907374083995819, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 322,  Mean reward: -0.9375, Mean Entropy: 0.4183565378189087, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 323,  Mean reward: 0.03731343283582089, Mean Entropy: 0.5243124961853027, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.87s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 324,  Mean reward: 5.079365079365079, Mean Entropy: 0.5242832899093628, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 325,  Mean reward: 0.19166666666666668, Mean Entropy: 0.6489676833152771, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 326,  Mean reward: 3.3839285714285716, Mean Entropy: 0.6648818254470825, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 327,  Mean reward: 2.5, Mean Entropy: 0.737838625907898, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 328,  Mean reward: 4.147540983606557, Mean Entropy: 0.693871021270752, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 329,  Mean reward: 2.9152542372881354, Mean Entropy: 0.6964854001998901, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.72s
Iteration: 330,  Mean reward: 2.7203389830508473, Mean Entropy: 0.5650014877319336, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.66s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 331,  Mean reward: 5.434426229508197, Mean Entropy: 0.5766628980636597, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 332,  Mean reward: -1.1746031746031746, Mean Entropy: 0.5199607610702515, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 333,  Mean reward: 3.0238095238095237, Mean Entropy: 0.5089818835258484, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.83s
Iteration: 334,  Mean reward: 2.6016949152542375, Mean Entropy: 0.43674516677856445, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 335,  Mean reward: 5.901639344262295, Mean Entropy: 0.5191850662231445, complete_episode_count: 61.0, Gather time: 0.70s, Train time: 0.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 336,  Mean reward: 6.1716417910447765, Mean Entropy: 0.5272783041000366, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 337,  Mean reward: 3.3934426229508197, Mean Entropy: 0.7611681818962097, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 338,  Mean reward: -1.38, Mean Entropy: 0.6030418872833252, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 339,  Mean reward: 4.008474576271187, Mean Entropy: 0.6218092441558838, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 340,  Mean reward: 3.057377049180328, Mean Entropy: 0.5667518973350525, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.69s
Iteration: 341,  Mean reward: 2.2954545454545454, Mean Entropy: 0.4895910620689392, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 342,  Mean reward: 2.841666666666667, Mean Entropy: 0.5683410167694092, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 343,  Mean reward: 4.360655737704918, Mean Entropy: 0.46855616569519043, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 344,  Mean reward: -4.604477611940299, Mean Entropy: 0.39470118284225464, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 345,  Mean reward: 3.9420289855072466, Mean Entropy: 0.3220137059688568, complete_episode_count: 69.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 346,  Mean reward: 4.303030303030303, Mean Entropy: 0.3634197413921356, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 347,  Mean reward: 3.36231884057971, Mean Entropy: 0.41762933135032654, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 348,  Mean reward: 6.045454545454546, Mean Entropy: 0.4452509880065918, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.84s
Iteration: 349,  Mean reward: 5.126984126984127, Mean Entropy: 0.39171966910362244, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.84s
Iteration: 350,  Mean reward: 5.669354838709677, Mean Entropy: 0.35818397998809814, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 351,  Mean reward: 6.345588235294118, Mean Entropy: 0.5210438966751099, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 352,  Mean reward: -1.3384615384615384, Mean Entropy: 0.3877142369747162, complete_episode_count: 65.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 353,  Mean reward: 5.6461538461538465, Mean Entropy: 0.37740859389305115, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 354,  Mean reward: 5.681818181818182, Mean Entropy: 0.367144912481308, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 355,  Mean reward: 6.1911764705882355, Mean Entropy: 0.4174504280090332, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 356,  Mean reward: 2.716417910447761, Mean Entropy: 0.40550684928894043, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.84s
Iteration: 357,  Mean reward: -4.023809523809524, Mean Entropy: 0.5240623950958252, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 358,  Mean reward: -5.841666666666667, Mean Entropy: 0.7395739555358887, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 359,  Mean reward: -5.9411764705882355, Mean Entropy: 0.7827247381210327, complete_episode_count: 34.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 360,  Mean reward: -7.1911764705882355, Mean Entropy: 0.7102255821228027, complete_episode_count: 34.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 361,  Mean reward: -4.893939393939394, Mean Entropy: 0.7630583047866821, complete_episode_count: 33.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 362,  Mean reward: -6.554054054054054, Mean Entropy: 0.7706884145736694, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 363,  Mean reward: -6.606060606060606, Mean Entropy: 0.7916824221611023, complete_episode_count: 33.0, Gather time: 0.49s, Train time: 1.65s
Iteration: 364,  Mean reward: -7.09375, Mean Entropy: 0.6485714912414551, complete_episode_count: 32.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 365,  Mean reward: -6.21875, Mean Entropy: 0.6661679148674011, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 366,  Mean reward: -7.409090909090909, Mean Entropy: 0.707372784614563, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 367,  Mean reward: -7.390625, Mean Entropy: 0.6682812571525574, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 368,  Mean reward: -6.453125, Mean Entropy: 0.70305335521698, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 369,  Mean reward: -6.109375, Mean Entropy: 0.6938140392303467, complete_episode_count: 32.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 370,  Mean reward: -6.390625, Mean Entropy: 0.6710755228996277, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 371,  Mean reward: -4.828125, Mean Entropy: 0.7291338443756104, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.71s
Iteration: 372,  Mean reward: -6.546875, Mean Entropy: 0.7670270800590515, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.67s
Iteration: 373,  Mean reward: -8.046875, Mean Entropy: 0.6611587405204773, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.70s
Iteration: 374,  Mean reward: -5.484375, Mean Entropy: 0.6269640922546387, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.86s
Iteration: 375,  Mean reward: -5.954545454545454, Mean Entropy: 0.7083930969238281, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.70s
Iteration: 376,  Mean reward: -5.578125, Mean Entropy: 0.6868771314620972, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 377,  Mean reward: -6.333333333333333, Mean Entropy: 0.6962339282035828, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 378,  Mean reward: -6.484375, Mean Entropy: 0.6046369075775146, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 379,  Mean reward: -6.4375, Mean Entropy: 0.6474546790122986, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.71s
Iteration: 380,  Mean reward: -6.203125, Mean Entropy: 0.6795588731765747, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 381,  Mean reward: -4.859375, Mean Entropy: 0.7792199850082397, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 382,  Mean reward: -6.015625, Mean Entropy: 0.7147233486175537, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 383,  Mean reward: -7.375, Mean Entropy: 0.6416536569595337, complete_episode_count: 32.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 384,  Mean reward: -5.984375, Mean Entropy: 0.6379651427268982, complete_episode_count: 32.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 385,  Mean reward: -5.890625, Mean Entropy: 0.6608907580375671, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.67s
Iteration: 386,  Mean reward: -5.359375, Mean Entropy: 0.696914553642273, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.70s
Iteration: 387,  Mean reward: -6.681818181818182, Mean Entropy: 0.6681451797485352, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 388,  Mean reward: -5.863636363636363, Mean Entropy: 0.59112548828125, complete_episode_count: 33.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 389,  Mean reward: -5.265625, Mean Entropy: 0.6816028356552124, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 390,  Mean reward: -4.90625, Mean Entropy: 0.7321205139160156, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 391,  Mean reward: -5.606060606060606, Mean Entropy: 0.8445481061935425, complete_episode_count: 33.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 392,  Mean reward: -6.823529411764706, Mean Entropy: 0.7499610185623169, complete_episode_count: 34.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 393,  Mean reward: -5.2272727272727275, Mean Entropy: 0.7868819236755371, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.70s
Iteration: 394,  Mean reward: -7.151515151515151, Mean Entropy: 0.7934316992759705, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 395,  Mean reward: -5.46969696969697, Mean Entropy: 0.835965096950531, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.71s
Iteration: 396,  Mean reward: -6.203125, Mean Entropy: 0.8043274879455566, complete_episode_count: 32.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 397,  Mean reward: -5.882352941176471, Mean Entropy: 0.9034315943717957, complete_episode_count: 34.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 398,  Mean reward: -4.125, Mean Entropy: 0.8684918284416199, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 399,  Mean reward: -2.2916666666666665, Mean Entropy: 0.9111081957817078, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 400,  Mean reward: -6.105263157894737, Mean Entropy: 0.8884431719779968, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.70s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -4.342105263157895, Mean Entropy: 0.8807495832443237, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 402,  Mean reward: -3.5, Mean Entropy: 0.9507970809936523, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 403,  Mean reward: -4.448717948717949, Mean Entropy: 0.9393237829208374, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 404,  Mean reward: -5.972972972972973, Mean Entropy: 0.8968967199325562, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 405,  Mean reward: -4.337209302325581, Mean Entropy: 0.9284870028495789, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.65s
Iteration: 406,  Mean reward: -6.8, Mean Entropy: 0.9487761855125427, complete_episode_count: 40.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 407,  Mean reward: -6.421052631578948, Mean Entropy: 0.9337124228477478, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.66s
Iteration: 408,  Mean reward: -2.7162162162162162, Mean Entropy: 0.9280979633331299, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 409,  Mean reward: -5.5, Mean Entropy: 0.9287229776382446, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 410,  Mean reward: -4.8875, Mean Entropy: 0.8968358039855957, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.85s
Iteration: 411,  Mean reward: -3.6125, Mean Entropy: 0.9326280951499939, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 412,  Mean reward: -4.488095238095238, Mean Entropy: 0.8614449501037598, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 413,  Mean reward: -4.475609756097561, Mean Entropy: 0.9464634656906128, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 414,  Mean reward: -2.358974358974359, Mean Entropy: 0.8935386538505554, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 415,  Mean reward: -3.7804878048780486, Mean Entropy: 0.8998958468437195, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.66s
Iteration: 416,  Mean reward: -5.602564102564102, Mean Entropy: 0.8888144493103027, complete_episode_count: 39.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 417,  Mean reward: -2.4767441860465116, Mean Entropy: 0.8866173624992371, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 418,  Mean reward: -3.5476190476190474, Mean Entropy: 0.9633724093437195, complete_episode_count: 42.0, Gather time: 0.49s, Train time: 1.65s
Iteration: 419,  Mean reward: -4.916666666666667, Mean Entropy: 1.0006147623062134, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 420,  Mean reward: -3.223404255319149, Mean Entropy: 0.9109527468681335, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 421,  Mean reward: -2.5, Mean Entropy: 0.8994924426078796, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 422,  Mean reward: -4.48936170212766, Mean Entropy: 0.9158446192741394, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 423,  Mean reward: -4.913043478260869, Mean Entropy: 0.9176034927368164, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 424,  Mean reward: -4.173913043478261, Mean Entropy: 0.8682443499565125, complete_episode_count: 46.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 425,  Mean reward: -5.074468085106383, Mean Entropy: 0.8890346884727478, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.65s
Iteration: 426,  Mean reward: -5.548076923076923, Mean Entropy: 0.7964502573013306, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 427,  Mean reward: -4.240384615384615, Mean Entropy: 0.8224085569381714, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 428,  Mean reward: -0.673469387755102, Mean Entropy: 0.8487588763237, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 429,  Mean reward: -3.8454545454545452, Mean Entropy: 0.8517798185348511, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 430,  Mean reward: -2.0, Mean Entropy: 0.7961410284042358, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 431,  Mean reward: -5.820754716981132, Mean Entropy: 0.7944382429122925, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 432,  Mean reward: -4.581818181818182, Mean Entropy: 0.8292969465255737, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 433,  Mean reward: -4.382978723404255, Mean Entropy: 0.8534705638885498, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 434,  Mean reward: -2.1530612244897958, Mean Entropy: 0.8768601417541504, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 435,  Mean reward: -2.688679245283019, Mean Entropy: 0.6658385396003723, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 436,  Mean reward: 3.6904761904761907, Mean Entropy: 0.545589804649353, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 437,  Mean reward: -0.08333333333333333, Mean Entropy: 0.5042309761047363, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 438,  Mean reward: 3.846774193548387, Mean Entropy: 0.41930708289146423, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.87s
Iteration: 439,  Mean reward: -2.225806451612903, Mean Entropy: 0.6248242855072021, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 440,  Mean reward: 4.928571428571429, Mean Entropy: 0.44638165831565857, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 441,  Mean reward: 1.2258064516129032, Mean Entropy: 0.5559839606285095, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 442,  Mean reward: 2.843137254901961, Mean Entropy: 0.46432170271873474, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 443,  Mean reward: 2.7327586206896552, Mean Entropy: 0.35964593291282654, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 444,  Mean reward: 1.0113636363636365, Mean Entropy: 0.5192524194717407, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.65s
Iteration: 445,  Mean reward: 3.1041666666666665, Mean Entropy: 0.5654751062393188, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.87s
Iteration: 446,  Mean reward: -0.12195121951219512, Mean Entropy: 0.7050905227661133, complete_episode_count: 41.0, Gather time: 0.49s, Train time: 1.71s
Iteration: 447,  Mean reward: -0.1590909090909091, Mean Entropy: 0.746221661567688, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 448,  Mean reward: 3.372549019607843, Mean Entropy: 0.5843273997306824, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 449,  Mean reward: 1.7826086956521738, Mean Entropy: 0.7137424945831299, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 450,  Mean reward: 3.480769230769231, Mean Entropy: 0.6768853068351746, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 451,  Mean reward: 2.28, Mean Entropy: 0.6975197792053223, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.65s
Iteration: 452,  Mean reward: -2.033333333333333, Mean Entropy: 0.8455188274383545, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 453,  Mean reward: 2.21, Mean Entropy: 0.6159472465515137, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 454,  Mean reward: 3.37, Mean Entropy: 0.6639829874038696, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 455,  Mean reward: 1.6603773584905661, Mean Entropy: 0.7656776309013367, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.66s
Iteration: 456,  Mean reward: 3.827272727272727, Mean Entropy: 0.32005739212036133, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 457,  Mean reward: -0.75, Mean Entropy: 0.19525203108787537, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 458,  Mean reward: 1.1885245901639345, Mean Entropy: 0.7359148263931274, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 459,  Mean reward: 2.5384615384615383, Mean Entropy: 0.5834786295890808, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 460,  Mean reward: 4.081818181818182, Mean Entropy: 0.49571582674980164, complete_episode_count: 55.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 461,  Mean reward: 3.2314814814814814, Mean Entropy: 0.5275185704231262, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 462,  Mean reward: 2.452830188679245, Mean Entropy: 0.5530744194984436, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 463,  Mean reward: 0.8181818181818182, Mean Entropy: 0.7999712824821472, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 464,  Mean reward: 3.3529411764705883, Mean Entropy: 0.6951221823692322, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 465,  Mean reward: 4.508928571428571, Mean Entropy: 0.5802308320999146, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.72s
Iteration: 466,  Mean reward: 2.1666666666666665, Mean Entropy: 0.6839288473129272, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 467,  Mean reward: 3.6785714285714284, Mean Entropy: 0.6604436635971069, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 468,  Mean reward: 0.7788461538461539, Mean Entropy: 0.7327408790588379, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 469,  Mean reward: 4.4363636363636365, Mean Entropy: 0.5446761250495911, complete_episode_count: 55.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 470,  Mean reward: 3.6538461538461537, Mean Entropy: 0.5490436553955078, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 471,  Mean reward: 2.7551020408163267, Mean Entropy: 0.6166982650756836, complete_episode_count: 49.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 472,  Mean reward: 3.735294117647059, Mean Entropy: 0.42843958735466003, complete_episode_count: 51.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 473,  Mean reward: -0.04918032786885246, Mean Entropy: 0.6385208368301392, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 474,  Mean reward: 3.5588235294117645, Mean Entropy: 0.6876937747001648, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 475,  Mean reward: 4.627450980392157, Mean Entropy: 0.5575768947601318, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 476,  Mean reward: 6.024193548387097, Mean Entropy: 0.4704406261444092, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 477,  Mean reward: 2.8706896551724137, Mean Entropy: 0.651922345161438, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.65s
Iteration: 478,  Mean reward: 3.7264150943396226, Mean Entropy: 0.6596879959106445, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 479,  Mean reward: 4.044642857142857, Mean Entropy: 0.7224736213684082, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.87s
Iteration: 480,  Mean reward: 4.443396226415095, Mean Entropy: 0.6259307861328125, complete_episode_count: 53.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 481,  Mean reward: -1.3833333333333333, Mean Entropy: 0.7047684192657471, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 482,  Mean reward: 4.663934426229508, Mean Entropy: 0.5391204357147217, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.83s
Iteration: 483,  Mean reward: 4.254716981132075, Mean Entropy: 0.6085745692253113, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 484,  Mean reward: 2.8, Mean Entropy: 0.7271110415458679, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 485,  Mean reward: 3.6176470588235294, Mean Entropy: 0.6993893384933472, complete_episode_count: 51.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 486,  Mean reward: 5.87719298245614, Mean Entropy: 0.43340158462524414, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 487,  Mean reward: 0.819672131147541, Mean Entropy: 0.2628267705440521, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 488,  Mean reward: 1.935483870967742, Mean Entropy: 0.5676729679107666, complete_episode_count: 62.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 489,  Mean reward: 4.456896551724138, Mean Entropy: 0.7038209438323975, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 490,  Mean reward: 3.409090909090909, Mean Entropy: 0.6760488748550415, complete_episode_count: 55.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 491,  Mean reward: 5.008474576271187, Mean Entropy: 0.47616106271743774, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 492,  Mean reward: 3.98, Mean Entropy: 0.6384536623954773, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 493,  Mean reward: 1.5390625, Mean Entropy: 0.3927777409553528, complete_episode_count: 64.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 494,  Mean reward: 5.482142857142857, Mean Entropy: 0.31849390268325806, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 495,  Mean reward: 1.4191176470588236, Mean Entropy: 0.3204641342163086, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 496,  Mean reward: -1.0080645161290323, Mean Entropy: 0.4800894260406494, complete_episode_count: 62.0, Gather time: 0.50s, Train time: 0.83s
Iteration: 497,  Mean reward: 5.353448275862069, Mean Entropy: 0.6028622984886169, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 498,  Mean reward: 4.771929824561403, Mean Entropy: 0.4282497763633728, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 499,  Mean reward: 6.103448275862069, Mean Entropy: 0.376925528049469, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 500,  Mean reward: 5.935483870967742, Mean Entropy: 0.45163339376449585, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 1.7327586206896552, Mean Entropy: 0.5054619312286377, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 502,  Mean reward: 5.767241379310345, Mean Entropy: 0.46561282873153687, complete_episode_count: 58.0, Gather time: 0.51s, Train time: 1.68s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 503,  Mean reward: 7.076923076923077, Mean Entropy: 0.3592482805252075, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 504,  Mean reward: -1.5071428571428571, Mean Entropy: 0.43991655111312866, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 505,  Mean reward: 0.7089552238805971, Mean Entropy: 0.3476928174495697, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 506,  Mean reward: 6.61864406779661, Mean Entropy: 0.401612251996994, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 507,  Mean reward: 6.848484848484849, Mean Entropy: 0.3459930121898651, complete_episode_count: 66.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 508,  Mean reward: 0.1111111111111111, Mean Entropy: 0.4970495104789734, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 509,  Mean reward: 5.52542372881356, Mean Entropy: 0.4524051547050476, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.69s
Iteration: 510,  Mean reward: 5.552631578947368, Mean Entropy: 0.5091327428817749, complete_episode_count: 57.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 511,  Mean reward: 5.729508196721311, Mean Entropy: 0.4116213321685791, complete_episode_count: 61.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 512,  Mean reward: 3.6160714285714284, Mean Entropy: 0.49391263723373413, complete_episode_count: 56.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 513,  Mean reward: 6.360655737704918, Mean Entropy: 0.4662414491176605, complete_episode_count: 61.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 514,  Mean reward: 6.82089552238806, Mean Entropy: 0.40131616592407227, complete_episode_count: 67.0, Gather time: 0.52s, Train time: 0.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 515,  Mean reward: 7.352941176470588, Mean Entropy: 0.22514352202415466, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 1.01s
Iteration: 516,  Mean reward: -1.8924050632911393, Mean Entropy: 0.10555458068847656, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 517,  Mean reward: -2.4177215189873418, Mean Entropy: 0.3200331926345825, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 518,  Mean reward: 6.426470588235294, Mean Entropy: 0.31789398193359375, complete_episode_count: 68.0, Gather time: 0.52s, Train time: 0.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 519,  Mean reward: 7.5, Mean Entropy: 0.29473960399627686, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 520,  Mean reward: 7.649253731343284, Mean Entropy: 0.4711936414241791, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 521,  Mean reward: -2.6691176470588234, Mean Entropy: 0.5617231130599976, complete_episode_count: 68.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 522,  Mean reward: 0.6271186440677966, Mean Entropy: 0.37615570425987244, complete_episode_count: 59.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 523,  Mean reward: 6.443548387096774, Mean Entropy: 0.37146085500717163, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 524,  Mean reward: 7.246268656716418, Mean Entropy: 0.2711196541786194, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 525,  Mean reward: 6.888888888888889, Mean Entropy: 0.37050509452819824, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.85s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 526,  Mean reward: 7.688405797101449, Mean Entropy: 0.39012593030929565, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 527,  Mean reward: 0.9444444444444444, Mean Entropy: 0.3479079008102417, complete_episode_count: 72.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 528,  Mean reward: 6.626984126984127, Mean Entropy: 0.3630933165550232, complete_episode_count: 63.0, Gather time: 0.51s, Train time: 0.84s
Iteration: 529,  Mean reward: 4.5, Mean Entropy: 0.31363770365715027, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 530,  Mean reward: 7.788732394366197, Mean Entropy: 0.23048609495162964, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 531,  Mean reward: 7.863013698630137, Mean Entropy: 0.2033308446407318, complete_episode_count: 73.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 532,  Mean reward: 7.175675675675675, Mean Entropy: 0.14821141958236694, complete_episode_count: 74.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 533,  Mean reward: 7.7631578947368425, Mean Entropy: 0.1493903398513794, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 534,  Mean reward: 7.6506849315068495, Mean Entropy: 0.2595829665660858, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 535,  Mean reward: 6.314285714285714, Mean Entropy: 0.23722819983959198, complete_episode_count: 70.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 536,  Mean reward: 7.506849315068493, Mean Entropy: 0.13268932700157166, complete_episode_count: 73.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 537,  Mean reward: 7.448717948717949, Mean Entropy: 0.1350633203983307, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 538,  Mean reward: 7.626666666666667, Mean Entropy: 0.13594350218772888, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.84s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 539,  Mean reward: 7.881578947368421, Mean Entropy: 0.0760924369096756, complete_episode_count: 76.0, Gather time: 0.53s, Train time: 0.86s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 540,  Mean reward: 8.0, Mean Entropy: 0.0521031878888607, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 541,  Mean reward: 8.0, Mean Entropy: 0.2724359333515167, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 542,  Mean reward: 0.756578947368421, Mean Entropy: 0.22138449549674988, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 543,  Mean reward: -3.528985507246377, Mean Entropy: 0.2881947159767151, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 544,  Mean reward: -1.3405797101449275, Mean Entropy: 0.4862915873527527, complete_episode_count: 69.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 545,  Mean reward: -2.1739130434782608, Mean Entropy: 0.5027588605880737, complete_episode_count: 46.0, Gather time: 0.49s, Train time: 1.68s
Iteration: 546,  Mean reward: -1.3936170212765957, Mean Entropy: 0.46989724040031433, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 547,  Mean reward: -2.0348837209302326, Mean Entropy: 0.40521714091300964, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 548,  Mean reward: -2.2567567567567566, Mean Entropy: 0.44176000356674194, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.63s
Iteration: 549,  Mean reward: -6.662790697674419, Mean Entropy: 0.3279995918273926, complete_episode_count: 43.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 550,  Mean reward: -7.0, Mean Entropy: 0.2755882143974304, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.68s
Iteration: 551,  Mean reward: -6.875, Mean Entropy: 0.33814582228660583, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 552,  Mean reward: -6.03125, Mean Entropy: 0.42238670587539673, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 553,  Mean reward: -4.044117647058823, Mean Entropy: 0.5067955851554871, complete_episode_count: 34.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 554,  Mean reward: -2.04, Mean Entropy: 0.48937225341796875, complete_episode_count: 50.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 555,  Mean reward: -1.625, Mean Entropy: 0.419833779335022, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.84s
Iteration: 556,  Mean reward: -0.6382978723404256, Mean Entropy: 0.4309520423412323, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.71s
Iteration: 557,  Mean reward: -4.7023809523809526, Mean Entropy: 0.22415386140346527, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 558,  Mean reward: -6.421875, Mean Entropy: 0.2421492338180542, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 559,  Mean reward: -6.296875, Mean Entropy: 0.28449517488479614, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 560,  Mean reward: -6.203125, Mean Entropy: 0.35132989287376404, complete_episode_count: 32.0, Gather time: 0.48s, Train time: 1.69s
Iteration: 561,  Mean reward: -5.575757575757576, Mean Entropy: 0.4281298518180847, complete_episode_count: 33.0, Gather time: 0.48s, Train time: 1.67s
Iteration: 562,  Mean reward: -5.5625, Mean Entropy: 0.5607413053512573, complete_episode_count: 32.0, Gather time: 0.49s, Train time: 1.69s
Iteration: 563,  Mean reward: -6.297297297297297, Mean Entropy: 0.574417233467102, complete_episode_count: 37.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 564,  Mean reward: -2.875, Mean Entropy: 0.553541362285614, complete_episode_count: 36.0, Gather time: 0.49s, Train time: 1.67s
Iteration: 565,  Mean reward: -3.8684210526315788, Mean Entropy: 0.5799132585525513, complete_episode_count: 38.0, Gather time: 0.49s, Train time: 1.72s
Iteration: 566,  Mean reward: -3.642857142857143, Mean Entropy: 0.44328612089157104, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 567,  Mean reward: -3.519230769230769, Mean Entropy: 0.3797590732574463, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.72s
Iteration: 568,  Mean reward: -1.1825396825396826, Mean Entropy: 0.2864052653312683, complete_episode_count: 63.0, Gather time: 0.52s, Train time: 1.70s
Iteration: 569,  Mean reward: 0.5080645161290323, Mean Entropy: 0.37146398425102234, complete_episode_count: 62.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 570,  Mean reward: -1.9285714285714286, Mean Entropy: 0.3355855941772461, complete_episode_count: 56.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 571,  Mean reward: -1.4111111111111112, Mean Entropy: 0.3310388922691345, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 572,  Mean reward: 1.9259259259259258, Mean Entropy: 0.20311853289604187, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 573,  Mean reward: 0.7641509433962265, Mean Entropy: 0.24706070125102997, complete_episode_count: 53.0, Gather time: 0.51s, Train time: 1.70s
Iteration: 574,  Mean reward: 1.6595744680851063, Mean Entropy: 0.2548679709434509, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.65s
Iteration: 575,  Mean reward: -0.05319148936170213, Mean Entropy: 0.22690562903881073, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.70s
Iteration: 576,  Mean reward: -0.3875, Mean Entropy: 0.24926702678203583, complete_episode_count: 40.0, Gather time: 0.49s, Train time: 1.70s
Iteration: 577,  Mean reward: 0.4787234042553192, Mean Entropy: 0.1749562919139862, complete_episode_count: 47.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 578,  Mean reward: 1.7083333333333333, Mean Entropy: 0.16115842759609222, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.68s
Iteration: 579,  Mean reward: 0.9888888888888889, Mean Entropy: 0.19513165950775146, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 580,  Mean reward: 1.125, Mean Entropy: 0.1588388830423355, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 581,  Mean reward: 1.8645833333333333, Mean Entropy: 0.15194553136825562, complete_episode_count: 48.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 582,  Mean reward: -0.08139534883720931, Mean Entropy: 0.17942743003368378, complete_episode_count: 43.0, Gather time: 0.49s, Train time: 1.66s
Iteration: 583,  Mean reward: 1.211111111111111, Mean Entropy: 0.11058370769023895, complete_episode_count: 45.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 584,  Mean reward: 0.38636363636363635, Mean Entropy: 0.13560521602630615, complete_episode_count: 44.0, Gather time: 0.50s, Train time: 1.69s
Iteration: 585,  Mean reward: 1.1666666666666667, Mean Entropy: 0.38296791911125183, complete_episode_count: 42.0, Gather time: 0.50s, Train time: 1.67s
Iteration: 586,  Mean reward: 4.375, Mean Entropy: 0.08313573896884918, complete_episode_count: 52.0, Gather time: 0.50s, Train time: 1.66s
Iteration: 587,  Mean reward: 6.174242424242424, Mean Entropy: 0.08073271065950394, complete_episode_count: 66.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 588,  Mean reward: 0.006578947368421052, Mean Entropy: 0.16849389672279358, complete_episode_count: 76.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 589,  Mean reward: 7.27536231884058, Mean Entropy: 0.1200089305639267, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 590,  Mean reward: 7.698717948717949, Mean Entropy: 0.07881467789411545, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 1.03s
Iteration: 591,  Mean reward: 6.826666666666667, Mean Entropy: 0.1258493959903717, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 592,  Mean reward: 5.585714285714285, Mean Entropy: 0.27901825308799744, complete_episode_count: 70.0, Gather time: 0.51s, Train time: 0.86s
Iteration: 593,  Mean reward: 5.069230769230769, Mean Entropy: 0.41939830780029297, complete_episode_count: 65.0, Gather time: 0.51s, Train time: 0.83s
Iteration: 594,  Mean reward: 0.953125, Mean Entropy: 0.3357149362564087, complete_episode_count: 64.0, Gather time: 0.51s, Train time: 0.87s
Iteration: 595,  Mean reward: 4.134615384615385, Mean Entropy: 0.43400630354881287, complete_episode_count: 52.0, Gather time: 0.51s, Train time: 1.68s
Iteration: 596,  Mean reward: 3.925925925925926, Mean Entropy: 0.4511815905570984, complete_episode_count: 54.0, Gather time: 0.51s, Train time: 1.67s
Iteration: 597,  Mean reward: 4.745762711864407, Mean Entropy: 0.3537444770336151, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.68s
Iteration: 598,  Mean reward: 7.188405797101449, Mean Entropy: 0.09705902636051178, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 599,  Mean reward: 7.974683544303797, Mean Entropy: 0.19194017350673676, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 600,  Mean reward: 6.544776119402985, Mean Entropy: 0.1323808878660202, complete_episode_count: 67.0, Gather time: 0.51s, Train time: 0.84s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 7.955128205128205, Mean Entropy: 0.019615834578871727, complete_episode_count: 78.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 602,  Mean reward: 8.0, Mean Entropy: 0.008500458672642708, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 603,  Mean reward: 7.826923076923077, Mean Entropy: 0.021866697818040848, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 604,  Mean reward: 8.0, Mean Entropy: 0.004793287254869938, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 605,  Mean reward: 8.0, Mean Entropy: 0.0028031188994646072, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.05856633931398392, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 607,  Mean reward: 6.702898550724638, Mean Entropy: 0.12452204525470734, complete_episode_count: 69.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 608,  Mean reward: 8.0, Mean Entropy: 0.0016114676836878061, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 609,  Mean reward: 8.0, Mean Entropy: 0.003383986186236143, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 610,  Mean reward: 8.0, Mean Entropy: 0.01411588303744793, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 611,  Mean reward: 8.0, Mean Entropy: 0.02492653951048851, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 612,  Mean reward: 7.826923076923077, Mean Entropy: 0.046416521072387695, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 613,  Mean reward: 8.0, Mean Entropy: 0.010149158537387848, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 614,  Mean reward: 8.0, Mean Entropy: 0.012831412255764008, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 615,  Mean reward: 8.0, Mean Entropy: 0.007459842599928379, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 616,  Mean reward: 8.0, Mean Entropy: 0.00433657132089138, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 617,  Mean reward: 8.0, Mean Entropy: 0.002259340602904558, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 618,  Mean reward: 8.0, Mean Entropy: 0.0039962599985301495, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 619,  Mean reward: 8.0, Mean Entropy: 0.004302882589399815, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 620,  Mean reward: 8.0, Mean Entropy: 0.007452116813510656, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 621,  Mean reward: 8.0, Mean Entropy: 0.19397401809692383, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 622,  Mean reward: 5.666666666666667, Mean Entropy: 0.15694688260555267, complete_episode_count: 60.0, Gather time: 0.51s, Train time: 1.71s
Iteration: 623,  Mean reward: 8.0, Mean Entropy: 0.006303935311734676, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 624,  Mean reward: 7.623376623376624, Mean Entropy: 0.03120836615562439, complete_episode_count: 77.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 625,  Mean reward: 8.0, Mean Entropy: 0.009732804261147976, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 626,  Mean reward: 7.814102564102564, Mean Entropy: 0.023207541555166245, complete_episode_count: 78.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 627,  Mean reward: 8.0, Mean Entropy: 0.01030171848833561, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 628,  Mean reward: 8.0, Mean Entropy: 0.01646397076547146, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 629,  Mean reward: 8.0, Mean Entropy: 0.009000003337860107, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.01s
Iteration: 630,  Mean reward: 8.0, Mean Entropy: 0.0043618688359856606, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 631,  Mean reward: 8.0, Mean Entropy: 0.003608991391956806, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 632,  Mean reward: 8.0, Mean Entropy: 0.004929299000650644, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 633,  Mean reward: 8.0, Mean Entropy: 0.04898710548877716, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 634,  Mean reward: 7.746666666666667, Mean Entropy: 0.056608181446790695, complete_episode_count: 75.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 635,  Mean reward: 7.974683544303797, Mean Entropy: 0.033159028738737106, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 636,  Mean reward: 8.0, Mean Entropy: 0.0031610834412276745, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 637,  Mean reward: 8.0, Mean Entropy: 0.0011279466561973095, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 638,  Mean reward: 8.0, Mean Entropy: 0.0010359546868130565, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 639,  Mean reward: 8.0, Mean Entropy: 0.0010924887610599399, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.89s
Iteration: 640,  Mean reward: 8.0, Mean Entropy: 0.0014477018266916275, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 641,  Mean reward: 8.0, Mean Entropy: 0.001682019210420549, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 642,  Mean reward: 8.0, Mean Entropy: 0.0016610175371170044, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 643,  Mean reward: 8.0, Mean Entropy: 0.0012213611043989658, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 644,  Mean reward: 8.0, Mean Entropy: 0.0008500503608956933, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 645,  Mean reward: 8.0, Mean Entropy: 0.000552243203856051, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 646,  Mean reward: 8.0, Mean Entropy: 0.00048303892253898084, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 647,  Mean reward: 8.0, Mean Entropy: 0.0003135339939035475, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 648,  Mean reward: 8.0, Mean Entropy: 0.0003489959053695202, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 649,  Mean reward: 8.0, Mean Entropy: 0.00030032405629754066, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 650,  Mean reward: 8.0, Mean Entropy: 0.00026984873693436384, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 651,  Mean reward: 8.0, Mean Entropy: 0.00027136190328747034, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 652,  Mean reward: 8.0, Mean Entropy: 0.00033709220588207245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 653,  Mean reward: 8.0, Mean Entropy: 0.0003895287518389523, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 654,  Mean reward: 8.0, Mean Entropy: 0.0003627049445640296, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 655,  Mean reward: 8.0, Mean Entropy: 0.00045774487080052495, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 656,  Mean reward: 8.0, Mean Entropy: 0.0004541589296422899, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 657,  Mean reward: 8.0, Mean Entropy: 0.0003736004000529647, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.87s
Iteration: 658,  Mean reward: 8.0, Mean Entropy: 0.00030805551796220243, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 659,  Mean reward: 8.0, Mean Entropy: 0.0003308443701826036, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 660,  Mean reward: 8.0, Mean Entropy: 0.0002540860732551664, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 661,  Mean reward: 8.0, Mean Entropy: 0.00026708689983934164, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 662,  Mean reward: 8.0, Mean Entropy: 0.00020144649897702038, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 663,  Mean reward: 8.0, Mean Entropy: 0.00019858269661199301, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 664,  Mean reward: 8.0, Mean Entropy: 0.00019276005332358181, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 665,  Mean reward: 8.0, Mean Entropy: 0.0001857304887380451, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 666,  Mean reward: 8.0, Mean Entropy: 0.0001835839357227087, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 667,  Mean reward: 8.0, Mean Entropy: 0.00017282424960285425, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 668,  Mean reward: 8.0, Mean Entropy: 0.00015074304246809334, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 669,  Mean reward: 8.0, Mean Entropy: 0.00012181241618236527, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.01s
Iteration: 670,  Mean reward: 8.0, Mean Entropy: 0.00016546552069485188, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 671,  Mean reward: 8.0, Mean Entropy: 0.00013028320972807705, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 672,  Mean reward: 8.0, Mean Entropy: 0.00011948946485063061, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 673,  Mean reward: 8.0, Mean Entropy: 0.00011875513882841915, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 674,  Mean reward: 8.0, Mean Entropy: 0.0001497078046668321, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 675,  Mean reward: 8.0, Mean Entropy: 0.00013339218276087195, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 676,  Mean reward: 8.0, Mean Entropy: 0.00014382120571099222, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 677,  Mean reward: 8.0, Mean Entropy: 0.0001135356942540966, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 678,  Mean reward: 8.0, Mean Entropy: 0.00011258322047069669, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 679,  Mean reward: 8.0, Mean Entropy: 0.00011172039376106113, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 680,  Mean reward: 8.0, Mean Entropy: 0.00013002095511183143, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 681,  Mean reward: 8.0, Mean Entropy: 0.00011369212006684393, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 682,  Mean reward: 8.0, Mean Entropy: 0.00011006096610799432, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 683,  Mean reward: 8.0, Mean Entropy: 0.00014077808009460568, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 684,  Mean reward: 8.0, Mean Entropy: 0.00013043914805166423, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 685,  Mean reward: 8.0, Mean Entropy: 0.00011453517072368413, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 686,  Mean reward: 8.0, Mean Entropy: 0.00012004829477518797, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 687,  Mean reward: 8.0, Mean Entropy: 0.0001376559230266139, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 688,  Mean reward: 8.0, Mean Entropy: 0.00011438703950261697, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 689,  Mean reward: 8.0, Mean Entropy: 0.00014134205412119627, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 690,  Mean reward: 8.0, Mean Entropy: 0.00013896914606448263, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 691,  Mean reward: 8.0, Mean Entropy: 0.00011526772141223773, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 692,  Mean reward: 8.0, Mean Entropy: 0.0001231318892678246, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 693,  Mean reward: 8.0, Mean Entropy: 9.525548375677317e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 694,  Mean reward: 8.0, Mean Entropy: 0.00011880633974215016, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 695,  Mean reward: 8.0, Mean Entropy: 9.546775254420936e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 696,  Mean reward: 8.0, Mean Entropy: 8.638965664431453e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 697,  Mean reward: 8.0, Mean Entropy: 0.0001530328008811921, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 698,  Mean reward: 8.0, Mean Entropy: 0.0001090322039090097, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 699,  Mean reward: 8.0, Mean Entropy: 0.00010949948045890778, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 700,  Mean reward: 8.0, Mean Entropy: 0.0001424863439751789, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: 8.0, Mean Entropy: 0.00012239794887136668, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 702,  Mean reward: 8.0, Mean Entropy: 0.00010242175630992278, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 703,  Mean reward: 8.0, Mean Entropy: 0.00011085536971222609, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 704,  Mean reward: 8.0, Mean Entropy: 0.00013726303586736321, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 705,  Mean reward: 8.0, Mean Entropy: 8.763880759943277e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 706,  Mean reward: 8.0, Mean Entropy: 0.00010651627962943166, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 707,  Mean reward: 8.0, Mean Entropy: 8.951458585215732e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 708,  Mean reward: 8.0, Mean Entropy: 0.00010500011558178812, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 709,  Mean reward: 8.0, Mean Entropy: 0.00017703301273286343, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.03s
Iteration: 710,  Mean reward: 8.0, Mean Entropy: 0.00011754933802876621, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 711,  Mean reward: 8.0, Mean Entropy: 0.00010109039430972189, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 712,  Mean reward: 8.0, Mean Entropy: 0.00014531469787470996, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 713,  Mean reward: 8.0, Mean Entropy: 0.00018292837194167078, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 714,  Mean reward: 8.0, Mean Entropy: 0.0001776075514499098, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 715,  Mean reward: 8.0, Mean Entropy: 0.00016830470121931285, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 716,  Mean reward: 8.0, Mean Entropy: 0.000172399973962456, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 717,  Mean reward: 8.0, Mean Entropy: 0.00016273852088488638, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 718,  Mean reward: 8.0, Mean Entropy: 0.00014096609083935618, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 719,  Mean reward: 8.0, Mean Entropy: 0.0001426969247404486, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 720,  Mean reward: 8.0, Mean Entropy: 0.00016682216664776206, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 721,  Mean reward: 8.0, Mean Entropy: 0.0001382952614221722, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 722,  Mean reward: 8.0, Mean Entropy: 0.00011911385809071362, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 723,  Mean reward: 8.0, Mean Entropy: 0.00011233659461140633, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 724,  Mean reward: 8.0, Mean Entropy: 0.00015280349180102348, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.82s
Iteration: 725,  Mean reward: 8.0, Mean Entropy: 0.0001566334394738078, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 726,  Mean reward: 8.0, Mean Entropy: 0.00012609924306161702, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 727,  Mean reward: 8.0, Mean Entropy: 0.00012485278421081603, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 728,  Mean reward: 8.0, Mean Entropy: 0.0001383475901093334, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 729,  Mean reward: 8.0, Mean Entropy: 0.00011614083632593974, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 730,  Mean reward: 8.0, Mean Entropy: 0.00015020891441963613, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 731,  Mean reward: 8.0, Mean Entropy: 0.00015909492503851652, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 732,  Mean reward: 8.0, Mean Entropy: 0.0001489150890847668, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 733,  Mean reward: 8.0, Mean Entropy: 0.00019977346528321505, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 734,  Mean reward: 8.0, Mean Entropy: 0.0001494325406383723, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 735,  Mean reward: 8.0, Mean Entropy: 0.00018993247067555785, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 736,  Mean reward: 8.0, Mean Entropy: 0.00013325546751730144, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 737,  Mean reward: 8.0, Mean Entropy: 0.00010912260040640831, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 738,  Mean reward: 8.0, Mean Entropy: 0.00014245712372940034, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 739,  Mean reward: 8.0, Mean Entropy: 0.0001323901815339923, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 740,  Mean reward: 8.0, Mean Entropy: 0.0001533099712105468, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 741,  Mean reward: 8.0, Mean Entropy: 0.00012651726137846708, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 742,  Mean reward: 8.0, Mean Entropy: 0.00016148557187989354, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 743,  Mean reward: 8.0, Mean Entropy: 0.000136977803776972, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 744,  Mean reward: 8.0, Mean Entropy: 0.0001713987730909139, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 745,  Mean reward: 8.0, Mean Entropy: 0.00014425911649595946, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 746,  Mean reward: 8.0, Mean Entropy: 0.00013527249393519014, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 747,  Mean reward: 8.0, Mean Entropy: 0.00012953615805599838, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 748,  Mean reward: 8.0, Mean Entropy: 0.00017046293942257762, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 749,  Mean reward: 8.0, Mean Entropy: 0.000158988987095654, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.02s
Iteration: 750,  Mean reward: 8.0, Mean Entropy: 0.0001225528831128031, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 751,  Mean reward: 8.0, Mean Entropy: 0.0001500602811574936, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 752,  Mean reward: 8.0, Mean Entropy: 0.0001319471630267799, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 753,  Mean reward: 8.0, Mean Entropy: 0.00011281523620709777, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 754,  Mean reward: 8.0, Mean Entropy: 0.00010553860192885622, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 755,  Mean reward: 8.0, Mean Entropy: 8.433283073827624e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 756,  Mean reward: 8.0, Mean Entropy: 0.00011799818457802758, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 757,  Mean reward: 8.0, Mean Entropy: 7.601836114190519e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 758,  Mean reward: 8.0, Mean Entropy: 0.0001316417328780517, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 759,  Mean reward: 8.0, Mean Entropy: 9.303513070335612e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 760,  Mean reward: 8.0, Mean Entropy: 8.36188773973845e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 761,  Mean reward: 8.0, Mean Entropy: 0.00010073486191686243, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 762,  Mean reward: 8.0, Mean Entropy: 0.000106332081486471, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 763,  Mean reward: 8.0, Mean Entropy: 0.00010247882164549083, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 764,  Mean reward: 8.0, Mean Entropy: 0.00012933750986121595, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 765,  Mean reward: 8.0, Mean Entropy: 9.713248437037691e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 766,  Mean reward: 8.0, Mean Entropy: 7.922359509393573e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 767,  Mean reward: 8.0, Mean Entropy: 0.00011094831279478967, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 768,  Mean reward: 8.0, Mean Entropy: 9.485030022915453e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 769,  Mean reward: 8.0, Mean Entropy: 8.056913793552667e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 770,  Mean reward: 8.0, Mean Entropy: 0.00012949151278007776, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 771,  Mean reward: 8.0, Mean Entropy: 0.00014557642862200737, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 772,  Mean reward: 8.0, Mean Entropy: 0.00011794378224294633, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 773,  Mean reward: 8.0, Mean Entropy: 0.00013969498104415834, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 774,  Mean reward: 8.0, Mean Entropy: 0.00014553984510712326, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 775,  Mean reward: 8.0, Mean Entropy: 0.00011486583389341831, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 776,  Mean reward: 8.0, Mean Entropy: 0.0001634028449188918, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 777,  Mean reward: 8.0, Mean Entropy: 0.0001264487364096567, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 778,  Mean reward: 8.0, Mean Entropy: 0.00014436428318731487, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 779,  Mean reward: 8.0, Mean Entropy: 0.00017413136083632708, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 780,  Mean reward: 8.0, Mean Entropy: 0.00019388238433748484, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 781,  Mean reward: 8.0, Mean Entropy: 0.00019267256720922887, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 782,  Mean reward: 8.0, Mean Entropy: 0.00015528805670328438, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 783,  Mean reward: 8.0, Mean Entropy: 0.0001602440170245245, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 784,  Mean reward: 8.0, Mean Entropy: 0.0001478565245633945, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 785,  Mean reward: 8.0, Mean Entropy: 0.00018277773051522672, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 786,  Mean reward: 8.0, Mean Entropy: 0.0001985780254472047, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 787,  Mean reward: 8.0, Mean Entropy: 0.0001856393355410546, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 788,  Mean reward: 8.0, Mean Entropy: 0.00018495635595172644, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 789,  Mean reward: 8.0, Mean Entropy: 0.00019414641428738832, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.03s
Iteration: 790,  Mean reward: 8.0, Mean Entropy: 0.0001427196548320353, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 791,  Mean reward: 8.0, Mean Entropy: 0.00027854187646880746, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 792,  Mean reward: 8.0, Mean Entropy: 0.00017492064216639847, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 793,  Mean reward: 8.0, Mean Entropy: 0.00023161942954175174, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 794,  Mean reward: 8.0, Mean Entropy: 0.00024824863066896796, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 795,  Mean reward: 8.0, Mean Entropy: 0.00019043192151002586, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 796,  Mean reward: 8.0, Mean Entropy: 0.00021474626555573195, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 797,  Mean reward: 8.0, Mean Entropy: 0.00022259347315412015, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 798,  Mean reward: 8.0, Mean Entropy: 0.00017835290054790676, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 799,  Mean reward: 8.0, Mean Entropy: 0.0002864367561414838, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 800,  Mean reward: 8.0, Mean Entropy: 0.00032613141229376197, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: 8.0, Mean Entropy: 0.0002195678389398381, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 802,  Mean reward: 8.0, Mean Entropy: 0.00019208429148420691, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 803,  Mean reward: 8.0, Mean Entropy: 0.0002866360009647906, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 804,  Mean reward: 8.0, Mean Entropy: 0.0002048007445409894, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 805,  Mean reward: 8.0, Mean Entropy: 0.00023648954811505973, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 806,  Mean reward: 8.0, Mean Entropy: 0.0002113108494086191, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 807,  Mean reward: 8.0, Mean Entropy: 0.00018852755601983517, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 808,  Mean reward: 8.0, Mean Entropy: 0.0002399806398898363, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 809,  Mean reward: 8.0, Mean Entropy: 0.00029606063617393374, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 810,  Mean reward: 8.0, Mean Entropy: 0.00022269545297604054, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 811,  Mean reward: 8.0, Mean Entropy: 0.00024645746452733874, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 812,  Mean reward: 8.0, Mean Entropy: 0.0002881023974623531, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 813,  Mean reward: 8.0, Mean Entropy: 0.00022823360632173717, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 814,  Mean reward: 8.0, Mean Entropy: 0.0002190624800277874, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 815,  Mean reward: 8.0, Mean Entropy: 0.0002206635836046189, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 816,  Mean reward: 8.0, Mean Entropy: 0.00023731209512334317, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 817,  Mean reward: 8.0, Mean Entropy: 0.00025263652787543833, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 818,  Mean reward: 8.0, Mean Entropy: 0.0002474560169503093, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 819,  Mean reward: 8.0, Mean Entropy: 0.00024775671772658825, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 820,  Mean reward: 8.0, Mean Entropy: 0.00031952315475791693, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 821,  Mean reward: 8.0, Mean Entropy: 0.00029536281363107264, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 822,  Mean reward: 8.0, Mean Entropy: 0.00030435348162427545, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 823,  Mean reward: 8.0, Mean Entropy: 0.0003307982115074992, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 824,  Mean reward: 8.0, Mean Entropy: 0.0003384779847692698, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 825,  Mean reward: 8.0, Mean Entropy: 0.000313580414513126, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 826,  Mean reward: 8.0, Mean Entropy: 0.00034200327354483306, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 827,  Mean reward: 8.0, Mean Entropy: 0.00037035433342680335, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 828,  Mean reward: 8.0, Mean Entropy: 0.0003178564365953207, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 829,  Mean reward: 8.0, Mean Entropy: 0.00036638579331338406, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.02s
Iteration: 830,  Mean reward: 8.0, Mean Entropy: 0.0004148006555624306, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 831,  Mean reward: 8.0, Mean Entropy: 0.00029957768856547773, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 832,  Mean reward: 8.0, Mean Entropy: 0.0002131521177943796, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 833,  Mean reward: 8.0, Mean Entropy: 0.00031343474984169006, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 834,  Mean reward: 8.0, Mean Entropy: 0.0003543604980222881, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 835,  Mean reward: 8.0, Mean Entropy: 0.00036601180909201503, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 836,  Mean reward: 8.0, Mean Entropy: 0.0003030441584996879, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 837,  Mean reward: 7.7215189873417724, Mean Entropy: 0.0004205895238555968, complete_episode_count: 79.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 838,  Mean reward: 8.0, Mean Entropy: 0.003153496887534857, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 839,  Mean reward: 8.0, Mean Entropy: 0.0018714660545811057, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 840,  Mean reward: 8.0, Mean Entropy: 0.0003629640559665859, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 841,  Mean reward: 8.0, Mean Entropy: 0.00017695756105240434, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 842,  Mean reward: 8.0, Mean Entropy: 0.00039599332376383245, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 843,  Mean reward: 8.0, Mean Entropy: 0.00023181168944574893, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 844,  Mean reward: 8.0, Mean Entropy: 0.00021033202938269824, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 845,  Mean reward: 8.0, Mean Entropy: 0.0002751081483438611, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 846,  Mean reward: 8.0, Mean Entropy: 0.00014057754015084356, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 847,  Mean reward: 8.0, Mean Entropy: 0.0001743248285492882, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 848,  Mean reward: 8.0, Mean Entropy: 0.00019403971964493394, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 849,  Mean reward: 8.0, Mean Entropy: 0.0002125315077137202, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 850,  Mean reward: 8.0, Mean Entropy: 0.00013906467938795686, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 851,  Mean reward: 8.0, Mean Entropy: 0.00010537091293372214, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 852,  Mean reward: 8.0, Mean Entropy: 7.452440331690013e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 853,  Mean reward: 8.0, Mean Entropy: 6.735670467605814e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 854,  Mean reward: 8.0, Mean Entropy: 9.521092579234391e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 855,  Mean reward: 8.0, Mean Entropy: 9.306204447057098e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 856,  Mean reward: 8.0, Mean Entropy: 7.398742309305817e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 857,  Mean reward: 8.0, Mean Entropy: 6.38966666883789e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 858,  Mean reward: 8.0, Mean Entropy: 8.194758265744895e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.86s
Iteration: 859,  Mean reward: 8.0, Mean Entropy: 7.079895294737071e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.88s
Iteration: 860,  Mean reward: 8.0, Mean Entropy: 7.807133079040796e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 861,  Mean reward: 8.0, Mean Entropy: 5.508640606421977e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 862,  Mean reward: 8.0, Mean Entropy: 5.9819773014169186e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 863,  Mean reward: 8.0, Mean Entropy: 8.27437179395929e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 864,  Mean reward: 8.0, Mean Entropy: 7.749360520392656e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 865,  Mean reward: 8.0, Mean Entropy: 3.621180076152086e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 866,  Mean reward: 8.0, Mean Entropy: 7.232992356875911e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 867,  Mean reward: 8.0, Mean Entropy: 7.465183443855494e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 868,  Mean reward: 8.0, Mean Entropy: 6.330900941975415e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 869,  Mean reward: 8.0, Mean Entropy: 6.0502217820612714e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 1.03s
Iteration: 870,  Mean reward: 8.0, Mean Entropy: 9.346986189484596e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 871,  Mean reward: 8.0, Mean Entropy: 7.66260054660961e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 872,  Mean reward: 8.0, Mean Entropy: 5.868960215593688e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 873,  Mean reward: 8.0, Mean Entropy: 7.744217873550951e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 874,  Mean reward: 8.0, Mean Entropy: 6.585134542547166e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 875,  Mean reward: 8.0, Mean Entropy: 5.626562051475048e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 876,  Mean reward: 8.0, Mean Entropy: 5.1671893743332475e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 877,  Mean reward: 8.0, Mean Entropy: 4.191829793853685e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 878,  Mean reward: 8.0, Mean Entropy: 6.212349398992956e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 879,  Mean reward: 8.0, Mean Entropy: 6.82640093145892e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 880,  Mean reward: 8.0, Mean Entropy: 6.218891212483868e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 881,  Mean reward: 8.0, Mean Entropy: 4.8071051423903555e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 882,  Mean reward: 8.0, Mean Entropy: 4.3301690311636776e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 883,  Mean reward: 8.0, Mean Entropy: 5.711907215300016e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 884,  Mean reward: 8.0, Mean Entropy: 4.503166928770952e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 885,  Mean reward: 8.0, Mean Entropy: 3.158221807098016e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 886,  Mean reward: 8.0, Mean Entropy: 4.0850813093129545e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 887,  Mean reward: 8.0, Mean Entropy: 3.381490023457445e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 888,  Mean reward: 8.0, Mean Entropy: 3.84172672056593e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 889,  Mean reward: 8.0, Mean Entropy: 3.3204156352439895e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 890,  Mean reward: 8.0, Mean Entropy: 3.1298248359235004e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 891,  Mean reward: 8.0, Mean Entropy: 4.1545263229636475e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 892,  Mean reward: 8.0, Mean Entropy: 2.903499625972472e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.85s
Iteration: 893,  Mean reward: 8.0, Mean Entropy: 4.2596522689564154e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 894,  Mean reward: 8.0, Mean Entropy: 3.973764250986278e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 895,  Mean reward: 8.0, Mean Entropy: 3.530742833390832e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 896,  Mean reward: 8.0, Mean Entropy: 4.6264012780738994e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 897,  Mean reward: 8.0, Mean Entropy: 2.961908103316091e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 898,  Mean reward: 8.0, Mean Entropy: 4.546422860585153e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 899,  Mean reward: 8.0, Mean Entropy: 2.987505286000669e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 900,  Mean reward: 8.0, Mean Entropy: 3.9181661122711375e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: 8.0, Mean Entropy: 3.41745835612528e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 902,  Mean reward: 8.0, Mean Entropy: 2.6404788513900712e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 903,  Mean reward: 8.0, Mean Entropy: 4.006531889899634e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 904,  Mean reward: 8.0, Mean Entropy: 3.848444976028986e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 905,  Mean reward: 8.0, Mean Entropy: 3.23166350426618e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 906,  Mean reward: 8.0, Mean Entropy: 3.216143522877246e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 907,  Mean reward: 8.0, Mean Entropy: 2.7321908419253305e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 908,  Mean reward: 8.0, Mean Entropy: 3.202783045708202e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 909,  Mean reward: 8.0, Mean Entropy: 4.256475222064182e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.06s
Iteration: 910,  Mean reward: 8.0, Mean Entropy: 2.8683072741841897e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 911,  Mean reward: 8.0, Mean Entropy: 3.427530828048475e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 912,  Mean reward: 8.0, Mean Entropy: 3.590787673601881e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 913,  Mean reward: 8.0, Mean Entropy: 4.218348112772219e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 914,  Mean reward: 8.0, Mean Entropy: 2.868373121600598e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 915,  Mean reward: 8.0, Mean Entropy: 3.1409290386363864e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 916,  Mean reward: 8.0, Mean Entropy: 2.974219933093991e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 917,  Mean reward: 8.0, Mean Entropy: 3.818417098955251e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 918,  Mean reward: 8.0, Mean Entropy: 3.699104854604229e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 919,  Mean reward: 8.0, Mean Entropy: 4.513229214353487e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 920,  Mean reward: 8.0, Mean Entropy: 3.976285006501712e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 921,  Mean reward: 8.0, Mean Entropy: 3.538460441632196e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 922,  Mean reward: 8.0, Mean Entropy: 3.8621608837274835e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 923,  Mean reward: 8.0, Mean Entropy: 3.225660475436598e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 924,  Mean reward: 8.0, Mean Entropy: 3.1021998438518494e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 925,  Mean reward: 8.0, Mean Entropy: 4.3711799662560225e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 926,  Mean reward: 8.0, Mean Entropy: 4.4126598368166015e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 927,  Mean reward: 8.0, Mean Entropy: 3.882097007590346e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 928,  Mean reward: 8.0, Mean Entropy: 4.1145940485876054e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 929,  Mean reward: 8.0, Mean Entropy: 3.369005935383029e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 930,  Mean reward: 8.0, Mean Entropy: 3.91080611734651e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 931,  Mean reward: 8.0, Mean Entropy: 3.603289951570332e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 932,  Mean reward: 8.0, Mean Entropy: 3.784954606089741e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 933,  Mean reward: 8.0, Mean Entropy: 3.1725692679174244e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 934,  Mean reward: 8.0, Mean Entropy: 3.969882891396992e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 935,  Mean reward: 8.0, Mean Entropy: 4.672629802371375e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 936,  Mean reward: 8.0, Mean Entropy: 3.5154313081875443e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 937,  Mean reward: 8.0, Mean Entropy: 3.567400563042611e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 938,  Mean reward: 8.0, Mean Entropy: 3.839164492092095e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 939,  Mean reward: 8.0, Mean Entropy: 3.570310946088284e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 940,  Mean reward: 8.0, Mean Entropy: 3.100456888205372e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 941,  Mean reward: 8.0, Mean Entropy: 3.988714161096141e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 942,  Mean reward: 8.0, Mean Entropy: 2.7451878850115463e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 943,  Mean reward: 8.0, Mean Entropy: 3.604566882131621e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 944,  Mean reward: 8.0, Mean Entropy: 4.8956087994156405e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 945,  Mean reward: 8.0, Mean Entropy: 4.5130422222428024e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 946,  Mean reward: 8.0, Mean Entropy: 3.6916542740073055e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 947,  Mean reward: 8.0, Mean Entropy: 5.212367977946997e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 948,  Mean reward: 8.0, Mean Entropy: 5.393725587055087e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 949,  Mean reward: 8.0, Mean Entropy: 4.637278834707104e-05, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 1.00s
Iteration: 950,  Mean reward: 8.0, Mean Entropy: 5.2545998187270015e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 951,  Mean reward: 8.0, Mean Entropy: 5.772304939455353e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 952,  Mean reward: 8.0, Mean Entropy: 5.352745938580483e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 953,  Mean reward: 8.0, Mean Entropy: 3.985566218034364e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 954,  Mean reward: 8.0, Mean Entropy: 4.171270120423287e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 955,  Mean reward: 8.0, Mean Entropy: 3.79263874492608e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 956,  Mean reward: 8.0, Mean Entropy: 3.3110176445916295e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 957,  Mean reward: 8.0, Mean Entropy: 3.682701208163053e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 958,  Mean reward: 8.0, Mean Entropy: 2.8346945327939466e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 959,  Mean reward: 8.0, Mean Entropy: 2.5192272005369887e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 960,  Mean reward: 8.0, Mean Entropy: 3.055030174436979e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 961,  Mean reward: 8.0, Mean Entropy: 3.0153974876157008e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 962,  Mean reward: 8.0, Mean Entropy: 2.7423709980212152e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 963,  Mean reward: 8.0, Mean Entropy: 3.200338687747717e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 964,  Mean reward: 8.0, Mean Entropy: 3.6367397115100175e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 965,  Mean reward: 8.0, Mean Entropy: 3.667765122372657e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 966,  Mean reward: 8.0, Mean Entropy: 3.672872480819933e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 967,  Mean reward: 8.0, Mean Entropy: 2.9156288292142563e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 968,  Mean reward: 8.0, Mean Entropy: 3.972394188167527e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 969,  Mean reward: 8.0, Mean Entropy: 3.388711775187403e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 970,  Mean reward: 8.0, Mean Entropy: 3.615761670516804e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 971,  Mean reward: 8.0, Mean Entropy: 3.45502448908519e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 972,  Mean reward: 8.0, Mean Entropy: 4.182500561000779e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 973,  Mean reward: 8.0, Mean Entropy: 3.4043023333651945e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 974,  Mean reward: 8.0, Mean Entropy: 4.108277425984852e-05, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.88s
Iteration: 975,  Mean reward: 8.0, Mean Entropy: 3.929770173272118e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 976,  Mean reward: 8.0, Mean Entropy: 3.636713881860487e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 977,  Mean reward: 8.0, Mean Entropy: 3.7129408156033605e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 978,  Mean reward: 8.0, Mean Entropy: 3.0844745197100565e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 979,  Mean reward: 8.0, Mean Entropy: 4.247089964337647e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 980,  Mean reward: 8.0, Mean Entropy: 3.4845888876589015e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 981,  Mean reward: 8.0, Mean Entropy: 2.586123264336493e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 982,  Mean reward: 8.0, Mean Entropy: 3.155391459586099e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 983,  Mean reward: 8.0, Mean Entropy: 4.255289968568832e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 984,  Mean reward: 8.0, Mean Entropy: 4.565095514408313e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 985,  Mean reward: 8.0, Mean Entropy: 3.336449299240485e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 986,  Mean reward: 8.0, Mean Entropy: 3.9516628021374345e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 987,  Mean reward: 8.0, Mean Entropy: 3.516586730256677e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 988,  Mean reward: 8.0, Mean Entropy: 4.256103056832217e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 989,  Mean reward: 8.0, Mean Entropy: 3.843387821689248e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.02s
Iteration: 990,  Mean reward: 8.0, Mean Entropy: 3.532985647325404e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 991,  Mean reward: 8.0, Mean Entropy: 4.438292671693489e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 992,  Mean reward: 8.0, Mean Entropy: 5.2413728553801775e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 993,  Mean reward: 8.0, Mean Entropy: 3.0942188459448516e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 994,  Mean reward: 8.0, Mean Entropy: 3.242073580622673e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 995,  Mean reward: 8.0, Mean Entropy: 4.2012437916127965e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.84s
Iteration: 996,  Mean reward: 8.0, Mean Entropy: 4.4783559133065864e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 997,  Mean reward: 8.0, Mean Entropy: 3.962678601965308e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 998,  Mean reward: 8.0, Mean Entropy: 4.9392703658668324e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 999,  Mean reward: 8.0, Mean Entropy: 4.352122050477192e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 1000,  Mean reward: 8.0, Mean Entropy: 3.279340307926759e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.82s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: 8.0, Mean Entropy: 3.362026109243743e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 1002,  Mean reward: 8.0, Mean Entropy: 3.0846473237033933e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 1003,  Mean reward: 8.0, Mean Entropy: 3.581650526029989e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 1004,  Mean reward: 8.0, Mean Entropy: 2.173080065404065e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 1005,  Mean reward: 8.0, Mean Entropy: 2.2197564248926938e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 1006,  Mean reward: 8.0, Mean Entropy: 2.4254299205495045e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 1007,  Mean reward: 8.0, Mean Entropy: 2.2081869246903807e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 1008,  Mean reward: 8.0, Mean Entropy: 3.0122246243990958e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 1009,  Mean reward: 8.0, Mean Entropy: 2.985783612530213e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 1010,  Mean reward: 8.0, Mean Entropy: 2.4027689505601302e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 1011,  Mean reward: 8.0, Mean Entropy: 1.822596459533088e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.83s
Iteration: 1012,  Mean reward: 8.0, Mean Entropy: 1.988365693250671e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 1013,  Mean reward: 8.0, Mean Entropy: 1.9768796846619807e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 1014,  Mean reward: 8.0, Mean Entropy: 2.3173717636382207e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.84s
Iteration: 1015,  Mean reward: 8.0, Mean Entropy: 2.4253044102806598e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.85s
Iteration: 1016,  Mean reward: 8.0, Mean Entropy: 2.7080022846348584e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 1017,  Mean reward: 8.0, Mean Entropy: 3.131310950266197e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 1018,  Mean reward: 8.0, Mean Entropy: 3.846798063023016e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 1019,  Mean reward: 8.0, Mean Entropy: 2.209329977631569e-05, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.85s
Iteration: 1020,  Mean reward: 8.0, Mean Entropy: 2.479633803886827e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.85s
Iteration: 1021,  Mean reward: 8.0, Mean Entropy: 1.9059854821534827e-05, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.87s
Iteration: 1022,  Mean reward: 8.0, Mean Entropy: 2.8368367566145025e-05, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.87s
Iteration: 1023,  Mean reward: 8.0, Mean Entropy: 3.634602762758732e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 1024,  Mean reward: 8.0, Mean Entropy: 2.466224759700708e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 1025,  Mean reward: 8.0, Mean Entropy: 1.6366720956284553e-05, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.89s
Iteration: 1026,  Mean reward: 8.0, Mean Entropy: 3.0044973755138926e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 1027,  Mean reward: 8.0, Mean Entropy: 3.402021684451029e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 1028,  Mean reward: 8.0, Mean Entropy: 3.8961592508712783e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 1029,  Mean reward: 8.0, Mean Entropy: 3.3831245673354715e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 1.05s
Iteration: 1030,  Mean reward: 8.0, Mean Entropy: 4.048662958666682e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 1031,  Mean reward: 8.0, Mean Entropy: 3.302700861240737e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 1032,  Mean reward: 8.0, Mean Entropy: 2.833385588019155e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 1033,  Mean reward: 8.0, Mean Entropy: 2.6057234208565205e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 1034,  Mean reward: 8.0, Mean Entropy: 2.48438554990571e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.86s
Iteration: 1035,  Mean reward: 8.0, Mean Entropy: 2.256965308333747e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 1036,  Mean reward: 8.0, Mean Entropy: 2.3010961740510538e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 1037,  Mean reward: 8.0, Mean Entropy: 1.8002712749876082e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.87s
Iteration: 1038,  Mean reward: 8.0, Mean Entropy: 3.5457531339488924e-05, complete_episode_count: 80.0, Gather time: 0.53s, Train time: 0.87s
Iteration: 1039,  Mean reward: 8.0, Mean Entropy: 3.329167884658091e-05, complete_episode_count: 80.0, Gather time: 0.52s, Train time: 0.89s
Iteration: 1040,  Mean reward: 8.0, Mean Entropy: 2.228997982456349e-05, complete_episode_count: 80.0, Gather time: 0.51s, Train time: 0.88s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.50it/s]100%|| 1/1 [00:00<00:00,  1.50it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type FE
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.83it/s]100%|| 1/1 [00:00<00:00,  1.83it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type FE
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.83it/s]100%|| 1/1 [00:00<00:00,  1.83it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type FE
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_FE_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: 4.666666666666667
  std over seeds: 4.714045207910317
  per seed: [8.000 -2.000 8.000]

success_rate.......
  avg over seeds: 0.8333333333333334
  std over seeds: 0.23570226039551584
  per seed: [1.000 0.500 1.000]

batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: EMB
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: v
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
critic: v
node_dim: 7
lstm_on: True
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy_EMB_LSTM(
  (FE): FeatureExtractor(
    (gat): GATv2(7, 24, num_layers=5)
  )
  (LSTM): EMB_LSTM(
    (lstm): LSTM(24, 24)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (theta8_v): Linear(in_features=24, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with GATv2 feature extraction and LSTM applied on node embeddings
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
LSTM.lstm.weight_ih_l0   [96, 24]     requires_grad=True
LSTM.lstm.weight_hh_l0   [96, 24]     requires_grad=True
LSTM.lstm.bias_ih_l0     [96]         requires_grad=True
LSTM.lstm.bias_hh_l0     [96]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.theta8_v.weight        [1, 24]      requires_grad=True
V.theta8_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 11498
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.304878048780488, Mean Entropy: 1.0397207736968994, complete_episode_count: 41.0, Gather time: 5.72s, Train time: 3.45s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.288888888888889, Mean Entropy: 0.8953145742416382, complete_episode_count: 45.0, Gather time: 0.60s, Train time: 1.56s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -2.775, Mean Entropy: 0.8953143954277039, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -2.0729166666666665, Mean Entropy: 0.9675167798995972, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 4,  Mean reward: -4.226190476190476, Mean Entropy: 0.9097543954849243, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 5,  Mean reward: -3.325, Mean Entropy: 0.996396541595459, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 6,  Mean reward: -6.5, Mean Entropy: 0.9097539782524109, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 7,  Mean reward: -3.9782608695652173, Mean Entropy: 0.9458565711975098, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 8,  Mean reward: -5.953488372093023, Mean Entropy: 0.9819583296775818, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 9,  Mean reward: -5.3375, Mean Entropy: 0.8664339780807495, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 10,  Mean reward: -3.7875, Mean Entropy: 0.9314165115356445, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 11,  Mean reward: -5.585365853658536, Mean Entropy: 0.873654305934906, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 12,  Mean reward: -4.2125, Mean Entropy: 0.9314165115356445, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 13,  Mean reward: -3.892857142857143, Mean Entropy: 0.9530772566795349, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 14,  Mean reward: -4.159090909090909, Mean Entropy: 0.9097553491592407, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 15,  Mean reward: -5.093023255813954, Mean Entropy: 0.9530760645866394, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 16,  Mean reward: -5.590909090909091, Mean Entropy: 0.9458565711975098, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 17,  Mean reward: -4.523809523809524, Mean Entropy: 0.938636302947998, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 18,  Mean reward: -3.2625, Mean Entropy: 0.9891769289970398, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 19,  Mean reward: -4.27906976744186, Mean Entropy: 0.9241946935653687, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 20,  Mean reward: -5.275, Mean Entropy: 1.0108368396759033, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 21,  Mean reward: -3.9270833333333335, Mean Entropy: 0.938635528087616, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 22,  Mean reward: -2.960526315789474, Mean Entropy: 0.9458534121513367, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 23,  Mean reward: -5.928571428571429, Mean Entropy: 0.9314114451408386, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 24,  Mean reward: -5.2560975609756095, Mean Entropy: 0.9675167798995972, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 25,  Mean reward: -7.113636363636363, Mean Entropy: 0.9602972269058228, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.69s
Iteration: 26,  Mean reward: -3.6818181818181817, Mean Entropy: 0.9819573163986206, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 27,  Mean reward: -6.306818181818182, Mean Entropy: 0.9675171375274658, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 28,  Mean reward: -5.406976744186046, Mean Entropy: 0.945849597454071, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 29,  Mean reward: -3.6547619047619047, Mean Entropy: 0.9241888523101807, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 30,  Mean reward: -6.317073170731708, Mean Entropy: 0.9602906107902527, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 31,  Mean reward: -3.369047619047619, Mean Entropy: 0.9458531737327576, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 32,  Mean reward: -6.109756097560975, Mean Entropy: 0.9458484053611755, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 33,  Mean reward: -5.488636363636363, Mean Entropy: 0.9530559778213501, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 34,  Mean reward: -1.25, Mean Entropy: 0.9602879285812378, complete_episode_count: 44.0, Gather time: 0.67s, Train time: 1.49s
Iteration: 35,  Mean reward: -4.341463414634147, Mean Entropy: 0.9241893291473389, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 36,  Mean reward: -4.988095238095238, Mean Entropy: 0.895301342010498, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 37,  Mean reward: -4.686046511627907, Mean Entropy: 1.0180399417877197, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 38,  Mean reward: -4.088888888888889, Mean Entropy: 0.9313753843307495, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 39,  Mean reward: -4.392857142857143, Mean Entropy: 0.9674546718597412, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 40,  Mean reward: -8.0, Mean Entropy: 0.9241640567779541, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 41,  Mean reward: -4.119047619047619, Mean Entropy: 0.9097310304641724, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 42,  Mean reward: -1.9886363636363635, Mean Entropy: 0.9169695377349854, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 43,  Mean reward: -5.190476190476191, Mean Entropy: 0.9241914749145508, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 44,  Mean reward: -1.9545454545454546, Mean Entropy: 0.9458514451980591, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 45,  Mean reward: -3.522727272727273, Mean Entropy: 0.9458534121513367, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 46,  Mean reward: -3.5875, Mean Entropy: 0.8808650970458984, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 47,  Mean reward: -5.036585365853658, Mean Entropy: 0.9530577659606934, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 48,  Mean reward: -4.392857142857143, Mean Entropy: 0.938620924949646, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 49,  Mean reward: -4.794871794871795, Mean Entropy: 0.9241918921470642, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 50,  Mean reward: -7.865853658536586, Mean Entropy: 0.9963802695274353, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 51,  Mean reward: -4.662790697674419, Mean Entropy: 0.9241923689842224, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 52,  Mean reward: -3.3658536585365852, Mean Entropy: 0.9314141273498535, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 53,  Mean reward: -3.5853658536585367, Mean Entropy: 0.9458562135696411, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 54,  Mean reward: -6.744186046511628, Mean Entropy: 0.9386350512504578, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 55,  Mean reward: -4.4021739130434785, Mean Entropy: 0.9602953195571899, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 56,  Mean reward: -4.571428571428571, Mean Entropy: 0.9747343063354492, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 57,  Mean reward: -2.533333333333333, Mean Entropy: 0.9314017295837402, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 58,  Mean reward: -7.32051282051282, Mean Entropy: 0.8519903421401978, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 59,  Mean reward: -5.032608695652174, Mean Entropy: 0.9241833686828613, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 60,  Mean reward: -5.918918918918919, Mean Entropy: 0.9240121841430664, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 61,  Mean reward: -6.3108108108108105, Mean Entropy: 0.9383847713470459, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 62,  Mean reward: -1.8181818181818181, Mean Entropy: 0.9096732139587402, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.68s
Iteration: 63,  Mean reward: -4.121951219512195, Mean Entropy: 0.9385074377059937, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 64,  Mean reward: -4.818181818181818, Mean Entropy: 0.9385706186294556, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.53s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 65,  Mean reward: -1.0595238095238095, Mean Entropy: 0.9530543684959412, complete_episode_count: 42.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 66,  Mean reward: -5.6125, Mean Entropy: 0.9169737100601196, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 67,  Mean reward: -3.658536585365854, Mean Entropy: 0.8736535906791687, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 68,  Mean reward: -3.9456521739130435, Mean Entropy: 0.9386358261108398, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.43s
Iteration: 69,  Mean reward: -5.384615384615385, Mean Entropy: 0.9241904616355896, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 70,  Mean reward: -1.0714285714285714, Mean Entropy: 0.9891738295555115, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 71,  Mean reward: -5.1477272727272725, Mean Entropy: 0.9241902232170105, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 72,  Mean reward: -4.117021276595745, Mean Entropy: 0.9314098358154297, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 73,  Mean reward: -7.425, Mean Entropy: 0.9819453954696655, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 74,  Mean reward: -5.3977272727272725, Mean Entropy: 0.9097496271133423, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 75,  Mean reward: -4.965116279069767, Mean Entropy: 0.9241918325424194, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 76,  Mean reward: -3.8255813953488373, Mean Entropy: 0.953068733215332, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 77,  Mean reward: -2.7674418604651163, Mean Entropy: 1.0036128759384155, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 78,  Mean reward: -4.675, Mean Entropy: 0.902533233165741, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 79,  Mean reward: -3.3214285714285716, Mean Entropy: 0.8880929350852966, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 80,  Mean reward: -4.605263157894737, Mean Entropy: 0.9314142465591431, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 81,  Mean reward: -3.372093023255814, Mean Entropy: 0.9819560050964355, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 82,  Mean reward: -2.85, Mean Entropy: 0.8880913257598877, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 83,  Mean reward: -4.571428571428571, Mean Entropy: 0.9097493886947632, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 84,  Mean reward: -2.289473684210526, Mean Entropy: 0.9314098358154297, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 85,  Mean reward: -5.260869565217392, Mean Entropy: 0.9602825045585632, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 86,  Mean reward: -2.0, Mean Entropy: 0.9313914179801941, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 87,  Mean reward: -2.8452380952380953, Mean Entropy: 0.9530520439147949, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 88,  Mean reward: -6.538461538461538, Mean Entropy: 0.9241587519645691, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 89,  Mean reward: -5.681818181818182, Mean Entropy: 0.8952866792678833, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 90,  Mean reward: -4.883720930232558, Mean Entropy: 0.9169381856918335, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 91,  Mean reward: -1.8488372093023255, Mean Entropy: 0.9313062429428101, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 92,  Mean reward: -3.0128205128205128, Mean Entropy: 0.9601671695709229, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 93,  Mean reward: -4.128205128205129, Mean Entropy: 0.9312418699264526, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 94,  Mean reward: -5.010869565217392, Mean Entropy: 0.9240624904632568, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 95,  Mean reward: -2.5121951219512195, Mean Entropy: 0.9096355438232422, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 96,  Mean reward: -5.709302325581396, Mean Entropy: 0.9457873702049255, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 97,  Mean reward: -5.0125, Mean Entropy: 0.9025141596794128, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 98,  Mean reward: -5.329268292682927, Mean Entropy: 0.909691572189331, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 99,  Mean reward: -3.127906976744186, Mean Entropy: 0.9167906641960144, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 100,  Mean reward: -5.868421052631579, Mean Entropy: 0.902409553527832, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.52s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -3.8488372093023258, Mean Entropy: 0.9240798950195312, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 102,  Mean reward: -6.104651162790698, Mean Entropy: 0.9097136855125427, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 103,  Mean reward: -3.813953488372093, Mean Entropy: 0.9818655848503113, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 104,  Mean reward: -5.178571428571429, Mean Entropy: 0.9672722816467285, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 105,  Mean reward: -3.6511627906976742, Mean Entropy: 0.9238588213920593, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 106,  Mean reward: -3.7209302325581395, Mean Entropy: 0.9596636295318604, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 107,  Mean reward: -2.761904761904762, Mean Entropy: 0.9448118209838867, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 108,  Mean reward: -4.440476190476191, Mean Entropy: 0.9157246947288513, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 109,  Mean reward: -2.802325581395349, Mean Entropy: 0.9587962031364441, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 110,  Mean reward: -4.646341463414634, Mean Entropy: 0.9151825904846191, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 111,  Mean reward: -6.5375, Mean Entropy: 0.9668744802474976, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 112,  Mean reward: -4.3522727272727275, Mean Entropy: 0.895139217376709, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 113,  Mean reward: -4.9875, Mean Entropy: 0.9161831736564636, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 114,  Mean reward: -4.644444444444445, Mean Entropy: 0.9375259280204773, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 115,  Mean reward: -5.627906976744186, Mean Entropy: 0.9238006472587585, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 116,  Mean reward: -4.6395348837209305, Mean Entropy: 0.9168874621391296, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 117,  Mean reward: -4.621951219512195, Mean Entropy: 0.9234434366226196, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 118,  Mean reward: -2.9125, Mean Entropy: 1.0017273426055908, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 119,  Mean reward: -4.7625, Mean Entropy: 0.9237930178642273, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 120,  Mean reward: -4.043478260869565, Mean Entropy: 1.0099520683288574, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 121,  Mean reward: -5.1125, Mean Entropy: 0.9443931579589844, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 122,  Mean reward: -4.095238095238095, Mean Entropy: 1.0078996419906616, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 123,  Mean reward: -4.034883720930233, Mean Entropy: 0.9158648252487183, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 124,  Mean reward: -3.9047619047619047, Mean Entropy: 0.9953697919845581, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 125,  Mean reward: -5.6625, Mean Entropy: 0.9233253002166748, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 126,  Mean reward: -3.3095238095238093, Mean Entropy: 0.937736988067627, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 127,  Mean reward: -6.426829268292683, Mean Entropy: 0.9376866817474365, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 128,  Mean reward: -1.75, Mean Entropy: 0.887272834777832, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 129,  Mean reward: -3.3095238095238093, Mean Entropy: 0.9086469411849976, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 130,  Mean reward: -3.2738095238095237, Mean Entropy: 1.0378739833831787, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 131,  Mean reward: -5.195121951219512, Mean Entropy: 0.9223731756210327, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 132,  Mean reward: -1.0888888888888888, Mean Entropy: 0.8863219618797302, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 133,  Mean reward: -3.5833333333333335, Mean Entropy: 0.9434589147567749, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 134,  Mean reward: -3.7375, Mean Entropy: 0.9073079824447632, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 135,  Mean reward: -4.023809523809524, Mean Entropy: 0.9795839190483093, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 136,  Mean reward: -3.3372093023255816, Mean Entropy: 0.9372989535331726, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.70s
Iteration: 137,  Mean reward: -4.7125, Mean Entropy: 0.9150238037109375, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 138,  Mean reward: -2.813953488372093, Mean Entropy: 0.9423319101333618, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 139,  Mean reward: -3.607142857142857, Mean Entropy: 0.8708731532096863, complete_episode_count: 42.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 140,  Mean reward: -6.226190476190476, Mean Entropy: 0.9497270584106445, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 141,  Mean reward: -3.6511627906976742, Mean Entropy: 0.878503680229187, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 142,  Mean reward: -4.337209302325581, Mean Entropy: 0.8996763229370117, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 143,  Mean reward: -2.046511627906977, Mean Entropy: 0.904294490814209, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 144,  Mean reward: -2.8095238095238093, Mean Entropy: 0.8893305659294128, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 145,  Mean reward: -2.3372093023255816, Mean Entropy: 0.9369663000106812, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 146,  Mean reward: -4.693181818181818, Mean Entropy: 0.9220539927482605, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 147,  Mean reward: -4.309523809523809, Mean Entropy: 0.8864139318466187, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 148,  Mean reward: -4.946808510638298, Mean Entropy: 0.8548492193222046, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 149,  Mean reward: -4.208333333333333, Mean Entropy: 0.8878905177116394, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 150,  Mean reward: -4.822222222222222, Mean Entropy: 0.8510376214981079, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 151,  Mean reward: -6.797872340425532, Mean Entropy: 0.847486674785614, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 152,  Mean reward: -1.49, Mean Entropy: 0.8285908699035645, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 153,  Mean reward: -2.5098039215686274, Mean Entropy: 0.8834033012390137, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 154,  Mean reward: -5.511363636363637, Mean Entropy: 0.8445785045623779, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 155,  Mean reward: -5.184782608695652, Mean Entropy: 0.8026518225669861, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 156,  Mean reward: -2.202127659574468, Mean Entropy: 0.7350268363952637, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 157,  Mean reward: -1.41, Mean Entropy: 0.8204166889190674, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 158,  Mean reward: -2.4607843137254903, Mean Entropy: 0.8793658018112183, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 159,  Mean reward: -2.5520833333333335, Mean Entropy: 0.9171323776245117, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 160,  Mean reward: -3.1847826086956523, Mean Entropy: 0.839099645614624, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 161,  Mean reward: -3.8333333333333335, Mean Entropy: 0.7547120451927185, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 162,  Mean reward: -3.0408163265306123, Mean Entropy: 0.8072115778923035, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 163,  Mean reward: -1.6020408163265305, Mean Entropy: 0.916491687297821, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 164,  Mean reward: -6.5, Mean Entropy: 0.7670819163322449, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 165,  Mean reward: -4.245283018867925, Mean Entropy: 0.6275312900543213, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 166,  Mean reward: -2.42, Mean Entropy: 0.7301653623580933, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 167,  Mean reward: -3.6666666666666665, Mean Entropy: 0.6706945896148682, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 168,  Mean reward: -4.23, Mean Entropy: 0.556248664855957, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 169,  Mean reward: -4.027777777777778, Mean Entropy: 0.5873645544052124, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 170,  Mean reward: -2.2857142857142856, Mean Entropy: 0.6935939788818359, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 171,  Mean reward: -2.2142857142857144, Mean Entropy: 0.6512322425842285, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 172,  Mean reward: -2.7115384615384617, Mean Entropy: 0.6842374801635742, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.66s
Iteration: 173,  Mean reward: -3.452830188679245, Mean Entropy: 0.5200896263122559, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 174,  Mean reward: -4.125, Mean Entropy: 0.5583212375640869, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 175,  Mean reward: -2.127659574468085, Mean Entropy: 0.6531237959861755, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 176,  Mean reward: -3.9901960784313726, Mean Entropy: 0.4760759472846985, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 177,  Mean reward: -4.517241379310345, Mean Entropy: 0.5154352188110352, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 178,  Mean reward: -3.839622641509434, Mean Entropy: 0.5505404472351074, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 179,  Mean reward: -2.6372549019607843, Mean Entropy: 0.6070186495780945, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 180,  Mean reward: -3.232142857142857, Mean Entropy: 0.5592097640037537, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 181,  Mean reward: -1.9166666666666667, Mean Entropy: 0.6076750159263611, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 182,  Mean reward: -3.9, Mean Entropy: 0.4867294728755951, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 183,  Mean reward: -1.5277777777777777, Mean Entropy: 0.5567229986190796, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 184,  Mean reward: -5.852941176470588, Mean Entropy: 0.5280259847640991, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 185,  Mean reward: -3.292452830188679, Mean Entropy: 0.5450223684310913, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.42s
Iteration: 186,  Mean reward: -3.7641509433962264, Mean Entropy: 0.5214453339576721, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 187,  Mean reward: -4.294117647058823, Mean Entropy: 0.5572954416275024, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 188,  Mean reward: -5.37962962962963, Mean Entropy: 0.6169908046722412, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 189,  Mean reward: -6.009615384615385, Mean Entropy: 0.5725671052932739, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 190,  Mean reward: -3.388888888888889, Mean Entropy: 0.5297435522079468, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 191,  Mean reward: -3.5892857142857144, Mean Entropy: 0.5631370544433594, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 192,  Mean reward: -5.1, Mean Entropy: 0.5427678227424622, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 193,  Mean reward: -3.793103448275862, Mean Entropy: 0.528557538986206, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 194,  Mean reward: -5.472727272727273, Mean Entropy: 0.4930131435394287, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 195,  Mean reward: -0.7410714285714286, Mean Entropy: 0.7940022945404053, complete_episode_count: 56.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 196,  Mean reward: -3.2444444444444445, Mean Entropy: 0.8116574287414551, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 197,  Mean reward: -2.6153846153846154, Mean Entropy: 0.4586511552333832, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 198,  Mean reward: -2.7830188679245285, Mean Entropy: 0.6823537349700928, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 199,  Mean reward: -3.404255319148936, Mean Entropy: 0.5790639519691467, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 200,  Mean reward: -2.980769230769231, Mean Entropy: 0.7614161968231201, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.53s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -0.9150943396226415, Mean Entropy: 0.9053380489349365, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 202,  Mean reward: -4.813953488372093, Mean Entropy: 0.7644497156143188, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 203,  Mean reward: -3.644230769230769, Mean Entropy: 0.7794420123100281, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 204,  Mean reward: -5.659574468085107, Mean Entropy: 0.5129072666168213, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 205,  Mean reward: -2.8867924528301887, Mean Entropy: 0.6743267774581909, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 206,  Mean reward: -3.2547169811320753, Mean Entropy: 0.5432240962982178, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.67s
Iteration: 207,  Mean reward: -3.9166666666666665, Mean Entropy: 0.5238566994667053, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 208,  Mean reward: -3.8173076923076925, Mean Entropy: 0.6881711483001709, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 209,  Mean reward: -0.07, Mean Entropy: 0.8448623418807983, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 210,  Mean reward: -7.465116279069767, Mean Entropy: 0.5977027416229248, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 211,  Mean reward: -4.25, Mean Entropy: 0.5956319570541382, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 212,  Mean reward: -1.88, Mean Entropy: 0.6342410445213318, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 213,  Mean reward: -2.1037735849056602, Mean Entropy: 0.6177787780761719, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 214,  Mean reward: -1.3461538461538463, Mean Entropy: 0.5704089403152466, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 215,  Mean reward: -3.4150943396226414, Mean Entropy: 0.5768280029296875, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 216,  Mean reward: -0.7924528301886793, Mean Entropy: 0.5910269618034363, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 217,  Mean reward: -1.7641509433962264, Mean Entropy: 0.6122378706932068, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 218,  Mean reward: -4.233333333333333, Mean Entropy: 0.4689614772796631, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 219,  Mean reward: -2.889830508474576, Mean Entropy: 0.5008302927017212, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 220,  Mean reward: -4.192982456140351, Mean Entropy: 0.491705060005188, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 221,  Mean reward: -3.211864406779661, Mean Entropy: 0.5094399452209473, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 222,  Mean reward: -2.3157894736842106, Mean Entropy: 0.6685828566551208, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 223,  Mean reward: -3.480392156862745, Mean Entropy: 0.5917371511459351, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 224,  Mean reward: -2.2966101694915255, Mean Entropy: 0.564220666885376, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 225,  Mean reward: -4.009615384615385, Mean Entropy: 0.5683740973472595, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 226,  Mean reward: -2.5754716981132075, Mean Entropy: 0.6558087468147278, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 227,  Mean reward: -1.7037037037037037, Mean Entropy: 0.6138657927513123, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 228,  Mean reward: -4.731481481481482, Mean Entropy: 0.5399556159973145, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 229,  Mean reward: -4.754901960784314, Mean Entropy: 0.5464338660240173, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 230,  Mean reward: -2.7413793103448274, Mean Entropy: 0.6585532426834106, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 231,  Mean reward: -2.2457627118644066, Mean Entropy: 0.7567143440246582, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 232,  Mean reward: -4.814814814814815, Mean Entropy: 0.5222899913787842, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 233,  Mean reward: -5.95, Mean Entropy: 0.5902749300003052, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 234,  Mean reward: -4.703703703703703, Mean Entropy: 0.6517032384872437, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 235,  Mean reward: -5.194444444444445, Mean Entropy: 0.6693333983421326, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 236,  Mean reward: -3.088235294117647, Mean Entropy: 0.7385003566741943, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 237,  Mean reward: -2.574468085106383, Mean Entropy: 0.6545972228050232, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 238,  Mean reward: -3.839622641509434, Mean Entropy: 0.6448889970779419, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 239,  Mean reward: -1.8017241379310345, Mean Entropy: 0.7236291170120239, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 240,  Mean reward: -5.729166666666667, Mean Entropy: 0.5516538619995117, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 241,  Mean reward: -3.9622641509433962, Mean Entropy: 0.6208606362342834, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.72s
Iteration: 242,  Mean reward: -3.803921568627451, Mean Entropy: 0.5651687979698181, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 243,  Mean reward: -3.710526315789474, Mean Entropy: 0.5645841360092163, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 244,  Mean reward: -3.9454545454545453, Mean Entropy: 0.5530439615249634, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 245,  Mean reward: -2.82, Mean Entropy: 0.6078416109085083, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 246,  Mean reward: -2.8679245283018866, Mean Entropy: 0.6428103446960449, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 247,  Mean reward: -4.8584905660377355, Mean Entropy: 0.5178248286247253, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 248,  Mean reward: -3.8596491228070176, Mean Entropy: 0.5261049866676331, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 249,  Mean reward: -2.6666666666666665, Mean Entropy: 0.753880500793457, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 250,  Mean reward: -3.1122448979591835, Mean Entropy: 0.6537193059921265, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 251,  Mean reward: -6.204081632653061, Mean Entropy: 0.571407675743103, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 252,  Mean reward: -5.072727272727272, Mean Entropy: 0.6489700078964233, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 253,  Mean reward: -3.767857142857143, Mean Entropy: 0.6346679329872131, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 254,  Mean reward: -4.450980392156863, Mean Entropy: 0.6698839068412781, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 255,  Mean reward: -4.090909090909091, Mean Entropy: 0.6629085540771484, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 256,  Mean reward: -5.609090909090909, Mean Entropy: 0.601542055606842, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 257,  Mean reward: -2.3333333333333335, Mean Entropy: 0.7376817464828491, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 258,  Mean reward: -3.75, Mean Entropy: 0.7029008269309998, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 259,  Mean reward: -3.1226415094339623, Mean Entropy: 0.7129713296890259, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 260,  Mean reward: -2.7685185185185186, Mean Entropy: 0.6815826296806335, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 261,  Mean reward: -4.107142857142857, Mean Entropy: 0.5638590455055237, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 262,  Mean reward: -3.375, Mean Entropy: 0.5853802561759949, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 263,  Mean reward: -4.394230769230769, Mean Entropy: 0.5591005086898804, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 264,  Mean reward: -6.169642857142857, Mean Entropy: 0.5109086632728577, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 265,  Mean reward: -4.148148148148148, Mean Entropy: 0.5625103712081909, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 266,  Mean reward: -3.74468085106383, Mean Entropy: 0.5712696313858032, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 267,  Mean reward: -1.009090909090909, Mean Entropy: 0.8373288512229919, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.43s
Iteration: 268,  Mean reward: -2.4, Mean Entropy: 0.9065801501274109, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 269,  Mean reward: -5.0, Mean Entropy: 0.7675546407699585, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 270,  Mean reward: -4.179245283018868, Mean Entropy: 0.6461672186851501, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 271,  Mean reward: -5.444444444444445, Mean Entropy: 0.6621215343475342, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 272,  Mean reward: -3.6132075471698113, Mean Entropy: 0.7530865669250488, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 273,  Mean reward: -3.0113636363636362, Mean Entropy: 0.7930347323417664, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 274,  Mean reward: -2.4523809523809526, Mean Entropy: 0.9430475234985352, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 275,  Mean reward: -6.127906976744186, Mean Entropy: 0.6641519069671631, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 276,  Mean reward: -2.3846153846153846, Mean Entropy: 0.5968832969665527, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.66s
Iteration: 277,  Mean reward: -2.008771929824561, Mean Entropy: 0.7585365176200867, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 278,  Mean reward: -4.961538461538462, Mean Entropy: 0.5132747888565063, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 279,  Mean reward: -4.2727272727272725, Mean Entropy: 0.5494499206542969, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 280,  Mean reward: -3.5090909090909093, Mean Entropy: 0.6483389139175415, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 281,  Mean reward: -5.092592592592593, Mean Entropy: 0.5391454696655273, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 282,  Mean reward: -3.324561403508772, Mean Entropy: 0.5567846894264221, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 283,  Mean reward: -5.657894736842105, Mean Entropy: 0.5319168567657471, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 284,  Mean reward: -3.5, Mean Entropy: 0.6167796850204468, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 285,  Mean reward: -5.89622641509434, Mean Entropy: 0.6157568693161011, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 286,  Mean reward: -2.7264150943396226, Mean Entropy: 0.5956971645355225, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 287,  Mean reward: -2.849056603773585, Mean Entropy: 0.6463812589645386, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 288,  Mean reward: -2.4607843137254903, Mean Entropy: 0.6476473808288574, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 289,  Mean reward: -3.75, Mean Entropy: 0.615919828414917, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 290,  Mean reward: -5.311111111111111, Mean Entropy: 0.5493508577346802, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.43s
Iteration: 291,  Mean reward: -4.907894736842105, Mean Entropy: 0.5697481036186218, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 292,  Mean reward: -6.365853658536586, Mean Entropy: 0.5125774145126343, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 293,  Mean reward: -5.918918918918919, Mean Entropy: 0.5374515056610107, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 294,  Mean reward: -8.94186046511628, Mean Entropy: 0.6053367257118225, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 295,  Mean reward: -5.069767441860465, Mean Entropy: 0.4932480454444885, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 296,  Mean reward: -6.32051282051282, Mean Entropy: 0.736282229423523, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 297,  Mean reward: -6.325581395348837, Mean Entropy: 0.8742803335189819, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 298,  Mean reward: -4.4523809523809526, Mean Entropy: 0.9385744333267212, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 299,  Mean reward: -4.811111111111111, Mean Entropy: 0.9526827335357666, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 300,  Mean reward: -5.5625, Mean Entropy: 0.8948540687561035, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -6.761904761904762, Mean Entropy: 0.9160034656524658, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 302,  Mean reward: -4.3023255813953485, Mean Entropy: 0.8879436254501343, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 303,  Mean reward: -3.659090909090909, Mean Entropy: 0.9674575328826904, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 304,  Mean reward: -4.939024390243903, Mean Entropy: 0.9235569834709167, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 305,  Mean reward: -4.052631578947368, Mean Entropy: 0.8866283297538757, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 306,  Mean reward: -5.536585365853658, Mean Entropy: 0.9559789896011353, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 307,  Mean reward: -3.4523809523809526, Mean Entropy: 0.937291145324707, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 308,  Mean reward: -6.719512195121951, Mean Entropy: 0.9008326530456543, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 309,  Mean reward: -4.170731707317073, Mean Entropy: 0.8553742170333862, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 310,  Mean reward: -4.034090909090909, Mean Entropy: 0.9184659719467163, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 311,  Mean reward: -3.55, Mean Entropy: 0.8702518343925476, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 312,  Mean reward: -2.426829268292683, Mean Entropy: 0.9655951261520386, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.69s
Iteration: 313,  Mean reward: -3.857142857142857, Mean Entropy: 0.9450909495353699, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 314,  Mean reward: -5.423913043478261, Mean Entropy: 0.9485142230987549, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 315,  Mean reward: -3.7567567567567566, Mean Entropy: 0.8733090162277222, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 316,  Mean reward: -3.782608695652174, Mean Entropy: 0.9241346120834351, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 317,  Mean reward: -5.593023255813954, Mean Entropy: 0.9386267066001892, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 318,  Mean reward: -4.25, Mean Entropy: 0.9386219382286072, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 319,  Mean reward: -3.08974358974359, Mean Entropy: 0.9241331815719604, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 320,  Mean reward: -6.565789473684211, Mean Entropy: 0.9241744875907898, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 321,  Mean reward: -3.4186046511627906, Mean Entropy: 0.9602643847465515, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 322,  Mean reward: -4.890243902439025, Mean Entropy: 0.9602250456809998, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 323,  Mean reward: -4.024390243902439, Mean Entropy: 0.9746162295341492, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 324,  Mean reward: -4.934210526315789, Mean Entropy: 0.9384521245956421, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 325,  Mean reward: -5.226190476190476, Mean Entropy: 0.9415749311447144, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 326,  Mean reward: -3.441860465116279, Mean Entropy: 0.9018799662590027, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 327,  Mean reward: -4.223684210526316, Mean Entropy: 0.9095134735107422, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 328,  Mean reward: -4.025, Mean Entropy: 0.9238749742507935, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 329,  Mean reward: -3.875, Mean Entropy: 0.9521929621696472, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 330,  Mean reward: -3.1794871794871793, Mean Entropy: 0.9570847749710083, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 331,  Mean reward: -4.55, Mean Entropy: 0.9241856932640076, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 332,  Mean reward: -3.0375, Mean Entropy: 0.8971406817436218, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 333,  Mean reward: -4.2, Mean Entropy: 0.9899813532829285, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 334,  Mean reward: -3.8666666666666667, Mean Entropy: 0.857897937297821, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 335,  Mean reward: -2.7653061224489797, Mean Entropy: 0.8834875822067261, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 336,  Mean reward: -2.336734693877551, Mean Entropy: 0.7875185012817383, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 337,  Mean reward: -0.89, Mean Entropy: 0.7399289011955261, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 338,  Mean reward: -3.97, Mean Entropy: 0.6335081458091736, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 339,  Mean reward: -4.358974358974359, Mean Entropy: 0.5055919885635376, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 340,  Mean reward: -0.2159090909090909, Mean Entropy: 0.5373201370239258, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 341,  Mean reward: -4.1923076923076925, Mean Entropy: 0.8745881915092468, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 342,  Mean reward: 1.1, Mean Entropy: 0.6669220328330994, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 343,  Mean reward: -3.574468085106383, Mean Entropy: 0.8906095027923584, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 344,  Mean reward: -4.046511627906977, Mean Entropy: 0.928858757019043, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 345,  Mean reward: -4.666666666666667, Mean Entropy: 0.9330756068229675, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 346,  Mean reward: -4.546511627906977, Mean Entropy: 0.9672640562057495, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 347,  Mean reward: -3.402439024390244, Mean Entropy: 0.9313370585441589, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 348,  Mean reward: -6.575, Mean Entropy: 0.9385761618614197, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 349,  Mean reward: -3.566666666666667, Mean Entropy: 0.9746143817901611, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 350,  Mean reward: -2.0348837209302326, Mean Entropy: 0.9094926714897156, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 351,  Mean reward: -4.571428571428571, Mean Entropy: 0.9673568606376648, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 352,  Mean reward: -5.8, Mean Entropy: 0.9024704694747925, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 353,  Mean reward: -5.225, Mean Entropy: 0.9458421468734741, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 354,  Mean reward: -5.4743589743589745, Mean Entropy: 0.9386295676231384, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 355,  Mean reward: -7.475609756097561, Mean Entropy: 0.9673686027526855, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 356,  Mean reward: -4.825, Mean Entropy: 0.9304803609848022, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 357,  Mean reward: -4.0, Mean Entropy: 0.9014682769775391, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 358,  Mean reward: -3.6041666666666665, Mean Entropy: 0.8785478472709656, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 359,  Mean reward: -1.0648148148148149, Mean Entropy: 0.8099528551101685, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.59s
Iteration: 360,  Mean reward: -3.522222222222222, Mean Entropy: 0.8413372039794922, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 361,  Mean reward: -2.2111111111111112, Mean Entropy: 0.8112232685089111, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 362,  Mean reward: -4.988636363636363, Mean Entropy: 0.8272108435630798, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 363,  Mean reward: -1.2, Mean Entropy: 0.8105831146240234, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 364,  Mean reward: -1.5729166666666667, Mean Entropy: 0.9201381206512451, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 365,  Mean reward: -1.8111111111111111, Mean Entropy: 0.9116436243057251, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 366,  Mean reward: -2.902173913043478, Mean Entropy: 0.9240681529045105, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 367,  Mean reward: -2.8260869565217392, Mean Entropy: 0.8063468933105469, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 368,  Mean reward: -1.7317073170731707, Mean Entropy: 0.5799351930618286, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 369,  Mean reward: -5.144444444444445, Mean Entropy: 0.7830990552902222, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 370,  Mean reward: -5.0394736842105265, Mean Entropy: 0.9496323466300964, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.52s
Iteration: 371,  Mean reward: -5.726190476190476, Mean Entropy: 0.8753880262374878, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 372,  Mean reward: -1.6122448979591837, Mean Entropy: 0.7621602416038513, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 373,  Mean reward: -3.4565217391304346, Mean Entropy: 0.7964200973510742, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 374,  Mean reward: 0.05434782608695652, Mean Entropy: 0.773673415184021, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 375,  Mean reward: -4.618421052631579, Mean Entropy: 0.8550796508789062, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 376,  Mean reward: -3.488095238095238, Mean Entropy: 0.5654075145721436, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 377,  Mean reward: -2.127906976744186, Mean Entropy: 0.7745651006698608, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 378,  Mean reward: -5.976190476190476, Mean Entropy: 0.9566026926040649, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 379,  Mean reward: -3.7560975609756095, Mean Entropy: 0.8858429789543152, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 380,  Mean reward: -3.0543478260869565, Mean Entropy: 0.823066234588623, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 381,  Mean reward: -4.088888888888889, Mean Entropy: 0.7438170909881592, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 382,  Mean reward: -5.5, Mean Entropy: 0.8516265749931335, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 383,  Mean reward: -1.1704545454545454, Mean Entropy: 0.7847475409507751, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 384,  Mean reward: -3.7708333333333335, Mean Entropy: 0.847777783870697, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 385,  Mean reward: -1.9270833333333333, Mean Entropy: 0.9634327292442322, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.65s
Iteration: 386,  Mean reward: -2.7555555555555555, Mean Entropy: 0.890938401222229, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 387,  Mean reward: -4.806818181818182, Mean Entropy: 0.8624858856201172, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 388,  Mean reward: -2.697674418604651, Mean Entropy: 0.8903826475143433, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 389,  Mean reward: -3.3214285714285716, Mean Entropy: 0.8671972751617432, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 390,  Mean reward: -2.872093023255814, Mean Entropy: 0.8468796610832214, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 391,  Mean reward: -4.931818181818182, Mean Entropy: 0.8571834564208984, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 392,  Mean reward: -2.3255813953488373, Mean Entropy: 0.8527097702026367, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 393,  Mean reward: -3.595744680851064, Mean Entropy: 0.8777751922607422, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 394,  Mean reward: -0.8936170212765957, Mean Entropy: 0.8218022584915161, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 395,  Mean reward: -0.8936170212765957, Mean Entropy: 0.7484912872314453, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 396,  Mean reward: -2.5795454545454546, Mean Entropy: 0.7175328135490417, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 397,  Mean reward: -0.2558139534883721, Mean Entropy: 0.6320099234580994, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 398,  Mean reward: 0.6739130434782609, Mean Entropy: 0.739284873008728, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 399,  Mean reward: 2.32, Mean Entropy: 0.8154957294464111, complete_episode_count: 50.0, Gather time: 1.33s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 400,  Mean reward: 5.6415094339622645, Mean Entropy: 0.8808717727661133, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.48s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -1.5892857142857142, Mean Entropy: 0.8945764303207397, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 402,  Mean reward: -1.8478260869565217, Mean Entropy: 0.7382721304893494, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 403,  Mean reward: -1.4767441860465116, Mean Entropy: 0.7535686492919922, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 404,  Mean reward: -0.5104166666666666, Mean Entropy: 0.8312379717826843, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 405,  Mean reward: 2.022727272727273, Mean Entropy: 0.7766522169113159, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 406,  Mean reward: -0.23809523809523808, Mean Entropy: 0.7973823547363281, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 407,  Mean reward: 1.2857142857142858, Mean Entropy: 0.7517821788787842, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 408,  Mean reward: 1.8043478260869565, Mean Entropy: 0.7890510559082031, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 409,  Mean reward: -0.020833333333333332, Mean Entropy: 0.6496373414993286, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 410,  Mean reward: 3.1770833333333335, Mean Entropy: 0.7956743240356445, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 411,  Mean reward: 0.6666666666666666, Mean Entropy: 0.7854228019714355, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 412,  Mean reward: 1.4895833333333333, Mean Entropy: 0.6420351266860962, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 413,  Mean reward: 4.882352941176471, Mean Entropy: 0.7632667422294617, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 414,  Mean reward: -2.6538461538461537, Mean Entropy: 0.7004510164260864, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 415,  Mean reward: -0.22448979591836735, Mean Entropy: 0.6325963735580444, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 416,  Mean reward: 1.7209302325581395, Mean Entropy: 0.865138053894043, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 417,  Mean reward: -2.532608695652174, Mean Entropy: 0.8935216665267944, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 418,  Mean reward: -5.144736842105263, Mean Entropy: 0.8086196780204773, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 419,  Mean reward: -4.473684210526316, Mean Entropy: 0.8926522731781006, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 420,  Mean reward: -5.315789473684211, Mean Entropy: 0.926047682762146, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 421,  Mean reward: -4.511904761904762, Mean Entropy: 0.8966073989868164, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 422,  Mean reward: -4.8088235294117645, Mean Entropy: 0.8420159220695496, complete_episode_count: 34.0, Gather time: 0.53s, Train time: 1.73s
Iteration: 423,  Mean reward: -4.602564102564102, Mean Entropy: 0.8115231394767761, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 424,  Mean reward: -2.5365853658536586, Mean Entropy: 0.9531404972076416, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 425,  Mean reward: -5.158536585365853, Mean Entropy: 0.9011897444725037, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 426,  Mean reward: -4.225, Mean Entropy: 0.8959161639213562, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 427,  Mean reward: -3.090909090909091, Mean Entropy: 0.9845401644706726, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 428,  Mean reward: -4.0, Mean Entropy: 0.9664689302444458, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 429,  Mean reward: -3.5657894736842106, Mean Entropy: 0.8946126699447632, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 430,  Mean reward: -2.647727272727273, Mean Entropy: 0.7441447973251343, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 431,  Mean reward: -3.8214285714285716, Mean Entropy: 0.6704291105270386, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 432,  Mean reward: -3.7625, Mean Entropy: 0.7535881996154785, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 433,  Mean reward: -5.72972972972973, Mean Entropy: 0.658413827419281, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.61s
Iteration: 434,  Mean reward: -5.602564102564102, Mean Entropy: 0.7219076156616211, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 435,  Mean reward: -5.054054054054054, Mean Entropy: 0.7358306646347046, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 436,  Mean reward: -5.381578947368421, Mean Entropy: 0.5859681367874146, complete_episode_count: 38.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 437,  Mean reward: -6.180555555555555, Mean Entropy: 0.3572404384613037, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 438,  Mean reward: -6.409090909090909, Mean Entropy: 0.5278469324111938, complete_episode_count: 33.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 439,  Mean reward: -0.17391304347826086, Mean Entropy: 0.8305572271347046, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 440,  Mean reward: -6.5, Mean Entropy: 0.8994348049163818, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 441,  Mean reward: -6.0625, Mean Entropy: 0.8632148504257202, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 442,  Mean reward: -3.7195121951219514, Mean Entropy: 0.8880891799926758, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 443,  Mean reward: -3.125, Mean Entropy: 0.8657197952270508, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 444,  Mean reward: -5.256410256410256, Mean Entropy: 0.6461959481239319, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 445,  Mean reward: -5.27906976744186, Mean Entropy: 0.6174600720405579, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 446,  Mean reward: -4.987179487179487, Mean Entropy: 0.4838154911994934, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 447,  Mean reward: 1.1636363636363636, Mean Entropy: 0.8214664459228516, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 448,  Mean reward: -7.972972972972973, Mean Entropy: 0.6707768440246582, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 449,  Mean reward: -4.15, Mean Entropy: 0.6742377281188965, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 450,  Mean reward: -5.090909090909091, Mean Entropy: 0.5826792120933533, complete_episode_count: 33.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 451,  Mean reward: -3.022727272727273, Mean Entropy: 0.7996792197227478, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 452,  Mean reward: -4.044117647058823, Mean Entropy: 0.7129840850830078, complete_episode_count: 34.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 453,  Mean reward: -4.7875, Mean Entropy: 0.7365149259567261, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 454,  Mean reward: -0.6162790697674418, Mean Entropy: 0.7730685472488403, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 455,  Mean reward: -5.153846153846154, Mean Entropy: 0.9127303957939148, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 456,  Mean reward: -4.25, Mean Entropy: 0.8982406854629517, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 457,  Mean reward: -4.475609756097561, Mean Entropy: 0.936276912689209, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 458,  Mean reward: -6.576923076923077, Mean Entropy: 0.9020437002182007, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 459,  Mean reward: -4.571428571428571, Mean Entropy: 0.9674315452575684, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.67s
Iteration: 460,  Mean reward: -4.228260869565218, Mean Entropy: 0.9164136648178101, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 461,  Mean reward: -3.7125, Mean Entropy: 0.8895992636680603, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 462,  Mean reward: -6.7105263157894735, Mean Entropy: 0.8939485549926758, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 463,  Mean reward: -7.048780487804878, Mean Entropy: 0.916731059551239, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 464,  Mean reward: -4.125, Mean Entropy: 0.9744161367416382, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 465,  Mean reward: -3.6627906976744184, Mean Entropy: 0.9518022537231445, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 466,  Mean reward: -5.448717948717949, Mean Entropy: 0.9449977874755859, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 467,  Mean reward: -4.3604651162790695, Mean Entropy: 0.9448229670524597, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 468,  Mean reward: -4.6395348837209305, Mean Entropy: 0.887321949005127, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 469,  Mean reward: -4.489130434782608, Mean Entropy: 0.8961778879165649, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 470,  Mean reward: -4.736111111111111, Mean Entropy: 0.8040129542350769, complete_episode_count: 36.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 471,  Mean reward: 2.08, Mean Entropy: 0.8330553770065308, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 472,  Mean reward: -8.435897435897436, Mean Entropy: 0.9377728700637817, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 473,  Mean reward: -4.214285714285714, Mean Entropy: 0.9134461879730225, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 474,  Mean reward: -5.619047619047619, Mean Entropy: 0.9713885188102722, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 475,  Mean reward: -2.607142857142857, Mean Entropy: 0.9090849757194519, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 476,  Mean reward: -4.840909090909091, Mean Entropy: 0.948406457901001, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 477,  Mean reward: -4.616279069767442, Mean Entropy: 0.9411753416061401, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 478,  Mean reward: -3.9473684210526314, Mean Entropy: 0.9478722810745239, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 479,  Mean reward: -3.840909090909091, Mean Entropy: 0.950950026512146, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 480,  Mean reward: -4.5394736842105265, Mean Entropy: 0.8937276601791382, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 481,  Mean reward: -2.602272727272727, Mean Entropy: 0.9019150733947754, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 482,  Mean reward: -7.9375, Mean Entropy: 0.8876263499259949, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 483,  Mean reward: -5.2317073170731705, Mean Entropy: 0.8794251680374146, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 484,  Mean reward: -4.188888888888889, Mean Entropy: 0.9280256032943726, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 485,  Mean reward: -2.61, Mean Entropy: 0.8195487856864929, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 486,  Mean reward: -2.8958333333333335, Mean Entropy: 0.955855131149292, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 487,  Mean reward: -4.326086956521739, Mean Entropy: 0.8299834132194519, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 488,  Mean reward: -4.933962264150943, Mean Entropy: 0.7014939785003662, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 489,  Mean reward: -1.1634615384615385, Mean Entropy: 0.8721195459365845, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 490,  Mean reward: -1.3936170212765957, Mean Entropy: 0.79045569896698, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 491,  Mean reward: -4.735294117647059, Mean Entropy: 0.7270652055740356, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 492,  Mean reward: -1.5188679245283019, Mean Entropy: 0.6828755140304565, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 493,  Mean reward: -2.9655172413793105, Mean Entropy: 0.5253931283950806, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 494,  Mean reward: -3.95, Mean Entropy: 0.5810824632644653, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 495,  Mean reward: -2.841666666666667, Mean Entropy: 0.36350226402282715, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.67s
Iteration: 496,  Mean reward: -3.8805970149253732, Mean Entropy: 0.4603913426399231, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 497,  Mean reward: -3.2966101694915255, Mean Entropy: 0.6020801067352295, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.43s
Iteration: 498,  Mean reward: -3.1307692307692307, Mean Entropy: 0.660216212272644, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 499,  Mean reward: -3.245614035087719, Mean Entropy: 0.48855480551719666, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 500,  Mean reward: -1.3409090909090908, Mean Entropy: 0.6410168409347534, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.72s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -4.314814814814815, Mean Entropy: 0.854078471660614, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 502,  Mean reward: -4.416666666666667, Mean Entropy: 0.8599313497543335, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 503,  Mean reward: -3.595744680851064, Mean Entropy: 0.6161961555480957, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 504,  Mean reward: -0.06140350877192982, Mean Entropy: 0.7472052574157715, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.43s
Iteration: 505,  Mean reward: -5.1875, Mean Entropy: 0.9535074234008789, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 506,  Mean reward: -5.648936170212766, Mean Entropy: 0.9405418634414673, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 507,  Mean reward: -2.3854166666666665, Mean Entropy: 0.7426662445068359, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 508,  Mean reward: -4.4375, Mean Entropy: 0.6531388759613037, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 509,  Mean reward: -4.336734693877551, Mean Entropy: 0.8124194145202637, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 510,  Mean reward: -0.27450980392156865, Mean Entropy: 0.7570564150810242, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 511,  Mean reward: -5.4, Mean Entropy: 0.9551483988761902, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 512,  Mean reward: -5.011627906976744, Mean Entropy: 0.90142822265625, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 513,  Mean reward: -5.2976190476190474, Mean Entropy: 0.9170392751693726, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 514,  Mean reward: -2.607142857142857, Mean Entropy: 0.8923050761222839, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 515,  Mean reward: -5.9523809523809526, Mean Entropy: 0.8998562693595886, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 516,  Mean reward: -5.902439024390244, Mean Entropy: 0.9838484525680542, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 517,  Mean reward: -3.909090909090909, Mean Entropy: 0.952491283416748, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 518,  Mean reward: -4.7125, Mean Entropy: 0.9241397380828857, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 519,  Mean reward: -4.848837209302325, Mean Entropy: 0.9312667846679688, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 520,  Mean reward: -4.666666666666667, Mean Entropy: 0.958600640296936, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 521,  Mean reward: -4.35, Mean Entropy: 0.9004557132720947, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 522,  Mean reward: -6.829545454545454, Mean Entropy: 0.9431607127189636, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 523,  Mean reward: -3.383720930232558, Mean Entropy: 0.9588276743888855, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 524,  Mean reward: -1.6785714285714286, Mean Entropy: 0.85299152135849, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 525,  Mean reward: -3.197674418604651, Mean Entropy: 0.8677893877029419, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 526,  Mean reward: -3.8, Mean Entropy: 0.8739101886749268, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 527,  Mean reward: -3.902173913043478, Mean Entropy: 0.8978264331817627, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 528,  Mean reward: -8.19, Mean Entropy: 0.9014991521835327, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 529,  Mean reward: -3.2142857142857144, Mean Entropy: 0.9074455499649048, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 530,  Mean reward: -5.579545454545454, Mean Entropy: 0.8712687492370605, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 531,  Mean reward: -4.222222222222222, Mean Entropy: 0.8579750061035156, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 532,  Mean reward: -2.15625, Mean Entropy: 0.6533855199813843, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.68s
Iteration: 533,  Mean reward: -2.955357142857143, Mean Entropy: 0.8315216302871704, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 534,  Mean reward: -1.0326086956521738, Mean Entropy: 0.9429647326469421, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 535,  Mean reward: -4.181818181818182, Mean Entropy: 0.9413344264030457, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 536,  Mean reward: -4.628205128205129, Mean Entropy: 0.8202421069145203, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 537,  Mean reward: -0.8863636363636364, Mean Entropy: 0.9289302229881287, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 538,  Mean reward: -4.853658536585366, Mean Entropy: 0.9043070673942566, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 539,  Mean reward: -2.686046511627907, Mean Entropy: 0.8337928652763367, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 540,  Mean reward: -3.225, Mean Entropy: 0.8848183155059814, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 541,  Mean reward: -1.7875, Mean Entropy: 0.858040988445282, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 542,  Mean reward: -3.63953488372093, Mean Entropy: 0.8677479028701782, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 543,  Mean reward: -1.4130434782608696, Mean Entropy: 0.43175825476646423, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 544,  Mean reward: -9.5, Mean Entropy: 0.7447325587272644, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 545,  Mean reward: -2.2254901960784315, Mean Entropy: 0.6591694355010986, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 546,  Mean reward: -2.2403846153846154, Mean Entropy: 0.44779038429260254, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 547,  Mean reward: -1.1612903225806452, Mean Entropy: 0.4913521409034729, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 548,  Mean reward: -5.391666666666667, Mean Entropy: 0.5239490866661072, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.43s
Iteration: 549,  Mean reward: -3.2580645161290325, Mean Entropy: 0.6272631287574768, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 550,  Mean reward: -5.3431372549019605, Mean Entropy: 0.5387862920761108, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 551,  Mean reward: -3.425, Mean Entropy: 0.558803915977478, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 552,  Mean reward: -5.654545454545454, Mean Entropy: 0.636359691619873, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 553,  Mean reward: -4.830645161290323, Mean Entropy: 0.43951165676116943, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 554,  Mean reward: -2.2096774193548385, Mean Entropy: 0.5908118486404419, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 555,  Mean reward: -7.093220338983051, Mean Entropy: 0.4753682613372803, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 556,  Mean reward: -2.6693548387096775, Mean Entropy: 0.4944257140159607, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 557,  Mean reward: -3.5775862068965516, Mean Entropy: 0.44857704639434814, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 558,  Mean reward: -4.209090909090909, Mean Entropy: 0.532254159450531, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 559,  Mean reward: -7.418181818181818, Mean Entropy: 0.476034015417099, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 560,  Mean reward: -5.516949152542373, Mean Entropy: 0.42109745740890503, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 561,  Mean reward: -3.109375, Mean Entropy: 0.40315407514572144, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 562,  Mean reward: -2.5606060606060606, Mean Entropy: 0.4097523093223572, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 563,  Mean reward: -2.015625, Mean Entropy: 0.45742329955101013, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 564,  Mean reward: -1.523076923076923, Mean Entropy: 0.31824755668640137, complete_episode_count: 65.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 565,  Mean reward: -3.4140625, Mean Entropy: 0.345005601644516, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 566,  Mean reward: -2.5692307692307694, Mean Entropy: 0.4384542405605316, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 567,  Mean reward: -4.119047619047619, Mean Entropy: 0.3354092538356781, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.74s
Iteration: 568,  Mean reward: -6.063492063492063, Mean Entropy: 0.34589847922325134, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 569,  Mean reward: -4.818181818181818, Mean Entropy: 0.4198121428489685, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 570,  Mean reward: -5.575, Mean Entropy: 0.32720619440078735, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 571,  Mean reward: -5.140625, Mean Entropy: 0.3575165271759033, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.92s
Iteration: 572,  Mean reward: -3.2096774193548385, Mean Entropy: 0.34726405143737793, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.84s
Iteration: 573,  Mean reward: -1.921875, Mean Entropy: 0.35904788970947266, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 574,  Mean reward: -3.30327868852459, Mean Entropy: 0.381584107875824, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 575,  Mean reward: -1.8203125, Mean Entropy: 0.35203322768211365, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.76s
Iteration: 576,  Mean reward: -1.608695652173913, Mean Entropy: 0.3130415380001068, complete_episode_count: 69.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 577,  Mean reward: -5.78030303030303, Mean Entropy: 0.35278499126434326, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 578,  Mean reward: -4.476190476190476, Mean Entropy: 0.37917909026145935, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 579,  Mean reward: -0.12096774193548387, Mean Entropy: 0.35985955595970154, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 580,  Mean reward: -1.28125, Mean Entropy: 0.37847447395324707, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 581,  Mean reward: -1.8507462686567164, Mean Entropy: 0.42300352454185486, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 582,  Mean reward: -1.1923076923076923, Mean Entropy: 0.3220375180244446, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 583,  Mean reward: -0.18181818181818182, Mean Entropy: 0.3990550935268402, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 584,  Mean reward: -0.008333333333333333, Mean Entropy: 0.3697506785392761, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 585,  Mean reward: -2.467741935483871, Mean Entropy: 0.37041133642196655, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.77s
Iteration: 586,  Mean reward: -1.234375, Mean Entropy: 0.39182791113853455, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 587,  Mean reward: -3.305084745762712, Mean Entropy: 0.42688286304473877, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 588,  Mean reward: -0.6774193548387096, Mean Entropy: 0.48918434977531433, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 589,  Mean reward: -0.08771929824561403, Mean Entropy: 0.4987618327140808, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 590,  Mean reward: -0.8035714285714286, Mean Entropy: 0.43363404273986816, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 591,  Mean reward: 0.8113207547169812, Mean Entropy: 0.4207911491394043, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 592,  Mean reward: -0.9017857142857143, Mean Entropy: 0.396152138710022, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 593,  Mean reward: -2.871212121212121, Mean Entropy: 0.3866350054740906, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.80s
Iteration: 594,  Mean reward: -0.7457627118644068, Mean Entropy: 0.3970510959625244, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 595,  Mean reward: -2.556603773584906, Mean Entropy: 0.5064570307731628, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 596,  Mean reward: 0.027777777777777776, Mean Entropy: 0.46518802642822266, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 597,  Mean reward: -0.16363636363636364, Mean Entropy: 0.41045820713043213, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 598,  Mean reward: 0.3611111111111111, Mean Entropy: 0.53004390001297, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 599,  Mean reward: -1.33, Mean Entropy: 0.4319245219230652, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 600,  Mean reward: -0.3644067796610169, Mean Entropy: 0.4015134274959564, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.46s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -0.5660377358490566, Mean Entropy: 0.5430271029472351, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 602,  Mean reward: -1.7282608695652173, Mean Entropy: 0.5497952699661255, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 603,  Mean reward: 0.1015625, Mean Entropy: 0.328498899936676, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 604,  Mean reward: 0.680327868852459, Mean Entropy: 0.5882171392440796, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.78s
Iteration: 605,  Mean reward: 1.5576923076923077, Mean Entropy: 0.5371079444885254, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 606,  Mean reward: 2.146551724137931, Mean Entropy: 0.557523250579834, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 607,  Mean reward: 2.4224137931034484, Mean Entropy: 0.5059282779693604, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.59s
Iteration: 608,  Mean reward: -0.940677966101695, Mean Entropy: 0.4243147373199463, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 609,  Mean reward: -1.2380952380952381, Mean Entropy: 0.44826173782348633, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 610,  Mean reward: 0.775, Mean Entropy: 0.5807473063468933, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 611,  Mean reward: 0.765625, Mean Entropy: 0.5712999105453491, complete_episode_count: 64.0, Gather time: 0.56s, Train time: 0.98s
Iteration: 612,  Mean reward: -0.5677966101694916, Mean Entropy: 0.4086359441280365, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 613,  Mean reward: 1.1634615384615385, Mean Entropy: 0.5996017456054688, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 614,  Mean reward: -1.1721311475409837, Mean Entropy: 0.5561193227767944, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 615,  Mean reward: -3.547169811320755, Mean Entropy: 0.5426095128059387, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 616,  Mean reward: -0.8145161290322581, Mean Entropy: 0.5322954654693604, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.74s
Iteration: 617,  Mean reward: -1.7818181818181817, Mean Entropy: 0.5512097477912903, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 618,  Mean reward: 1.353448275862069, Mean Entropy: 0.6332849860191345, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 619,  Mean reward: -0.23958333333333334, Mean Entropy: 0.6868292689323425, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 620,  Mean reward: -1.2019230769230769, Mean Entropy: 0.6500661373138428, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 621,  Mean reward: -0.25, Mean Entropy: 0.7218102216720581, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 622,  Mean reward: -0.20754716981132076, Mean Entropy: 0.585845947265625, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 623,  Mean reward: 0.10784313725490197, Mean Entropy: 0.4287581145763397, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 624,  Mean reward: -2.0588235294117645, Mean Entropy: 0.655746579170227, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 625,  Mean reward: -0.07017543859649122, Mean Entropy: 0.6183966994285583, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 626,  Mean reward: 1.6470588235294117, Mean Entropy: 0.5939880609512329, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 627,  Mean reward: 3.537037037037037, Mean Entropy: 0.5384736061096191, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 628,  Mean reward: -2.543859649122807, Mean Entropy: 0.5757717490196228, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 629,  Mean reward: 1.2272727272727273, Mean Entropy: 0.5439493060112, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 630,  Mean reward: 1.368421052631579, Mean Entropy: 0.5324801802635193, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 631,  Mean reward: 3.790909090909091, Mean Entropy: 0.6812930107116699, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 632,  Mean reward: 3.7962962962962963, Mean Entropy: 0.6024140119552612, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 633,  Mean reward: 1.7105263157894737, Mean Entropy: 0.5991383790969849, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 634,  Mean reward: 1.1454545454545455, Mean Entropy: 0.6628700494766235, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 635,  Mean reward: 4.077586206896552, Mean Entropy: 0.6071045398712158, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 636,  Mean reward: 3.4814814814814814, Mean Entropy: 0.6281527280807495, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 637,  Mean reward: 5.161016949152542, Mean Entropy: 0.6071116924285889, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.64s
Iteration: 638,  Mean reward: -2.560344827586207, Mean Entropy: 0.9191646575927734, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 639,  Mean reward: -3.760869565217391, Mean Entropy: 0.9333744049072266, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 640,  Mean reward: -4.475609756097561, Mean Entropy: 0.8874239325523376, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 641,  Mean reward: -6.9125, Mean Entropy: 0.8849825859069824, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 642,  Mean reward: -3.9148936170212765, Mean Entropy: 0.8604571223258972, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 643,  Mean reward: -6.344444444444444, Mean Entropy: 0.7664355635643005, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 644,  Mean reward: -5.404761904761905, Mean Entropy: 0.8280601501464844, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 645,  Mean reward: -5.7439024390243905, Mean Entropy: 0.8062950968742371, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 646,  Mean reward: -3.7023809523809526, Mean Entropy: 0.7679034471511841, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.69s
Iteration: 647,  Mean reward: -5.439024390243903, Mean Entropy: 0.7931400537490845, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 648,  Mean reward: -2.142857142857143, Mean Entropy: 0.8869094848632812, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 649,  Mean reward: -4.678571428571429, Mean Entropy: 0.8999015092849731, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 650,  Mean reward: -4.321428571428571, Mean Entropy: 0.8664684295654297, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 651,  Mean reward: -6.197368421052632, Mean Entropy: 0.8667155504226685, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.53s
Iteration: 652,  Mean reward: -4.425, Mean Entropy: 0.9071303606033325, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 653,  Mean reward: -3.7906976744186047, Mean Entropy: 0.9813523292541504, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 654,  Mean reward: -4.209302325581396, Mean Entropy: 1.0105856657028198, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 655,  Mean reward: -2.4404761904761907, Mean Entropy: 0.9240741729736328, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 656,  Mean reward: -3.630952380952381, Mean Entropy: 0.9240314960479736, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 657,  Mean reward: -4.375, Mean Entropy: 0.894939661026001, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 658,  Mean reward: -5.134146341463414, Mean Entropy: 0.9385687112808228, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 659,  Mean reward: -6.785714285714286, Mean Entropy: 0.938634991645813, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 660,  Mean reward: -5.821428571428571, Mean Entropy: 0.9313847422599792, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 661,  Mean reward: -6.552631578947368, Mean Entropy: 0.9241206049919128, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 662,  Mean reward: -6.130434782608695, Mean Entropy: 0.9887921214103699, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 663,  Mean reward: -5.526315789473684, Mean Entropy: 0.9024410247802734, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 664,  Mean reward: -2.9125, Mean Entropy: 0.9530738592147827, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 665,  Mean reward: -3.0789473684210527, Mean Entropy: 0.8880936503410339, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 666,  Mean reward: -5.428571428571429, Mean Entropy: 0.9314078688621521, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 667,  Mean reward: -6.939024390243903, Mean Entropy: 0.9385641813278198, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 668,  Mean reward: -3.604651162790698, Mean Entropy: 0.996395468711853, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 669,  Mean reward: -4.425, Mean Entropy: 0.9673945307731628, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 670,  Mean reward: -3.782051282051282, Mean Entropy: 0.9457592964172363, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 671,  Mean reward: -3.723684210526316, Mean Entropy: 0.9169427752494812, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 672,  Mean reward: -4.926829268292683, Mean Entropy: 0.931102454662323, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 673,  Mean reward: -4.170731707317073, Mean Entropy: 0.9818999171257019, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 674,  Mean reward: -5.325, Mean Entropy: 0.9458087682723999, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 675,  Mean reward: -1.9222222222222223, Mean Entropy: 0.9458420276641846, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 676,  Mean reward: -2.6463414634146343, Mean Entropy: 0.9442336559295654, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 677,  Mean reward: -6.058139534883721, Mean Entropy: 0.9022294878959656, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 678,  Mean reward: -5.533333333333333, Mean Entropy: 0.9675095081329346, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 679,  Mean reward: -5.906976744186046, Mean Entropy: 0.9169657230377197, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 680,  Mean reward: -3.3375, Mean Entropy: 0.9747349619865417, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 681,  Mean reward: -3.0121951219512195, Mean Entropy: 0.9718837738037109, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 682,  Mean reward: -3.686046511627907, Mean Entropy: 0.9147445559501648, complete_episode_count: 43.0, Gather time: 0.70s, Train time: 1.44s
Iteration: 683,  Mean reward: -4.658536585365853, Mean Entropy: 0.9060635566711426, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.67s
Iteration: 684,  Mean reward: -3.4318181818181817, Mean Entropy: 1.0317736864089966, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 685,  Mean reward: -2.522727272727273, Mean Entropy: 1.0180201530456543, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 686,  Mean reward: -4.9375, Mean Entropy: 0.9025264978408813, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 687,  Mean reward: -4.615384615384615, Mean Entropy: 0.9638885259628296, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 688,  Mean reward: -5.840909090909091, Mean Entropy: 0.9626724720001221, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 689,  Mean reward: -5.590909090909091, Mean Entropy: 0.9566006660461426, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 690,  Mean reward: -4.3, Mean Entropy: 0.9458012580871582, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 691,  Mean reward: -3.011627906976744, Mean Entropy: 0.9313372373580933, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 692,  Mean reward: -4.883720930232558, Mean Entropy: 0.9150770902633667, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 693,  Mean reward: -4.226190476190476, Mean Entropy: 0.9742891788482666, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 694,  Mean reward: -5.5227272727272725, Mean Entropy: 0.9075771570205688, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 695,  Mean reward: -5.809523809523809, Mean Entropy: 0.9498356580734253, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 696,  Mean reward: -5.366666666666666, Mean Entropy: 0.9576764106750488, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.57s
Iteration: 697,  Mean reward: -3.5609756097560976, Mean Entropy: 0.9024438261985779, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.47s
Iteration: 698,  Mean reward: -5.5, Mean Entropy: 0.8794502019882202, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 699,  Mean reward: -3.675, Mean Entropy: 0.9741673469543457, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 700,  Mean reward: -2.372093023255814, Mean Entropy: 0.9313833713531494, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.46s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -2.6333333333333333, Mean Entropy: 0.9819262027740479, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 702,  Mean reward: -2.9625, Mean Entropy: 0.9458341598510742, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 703,  Mean reward: -4.641304347826087, Mean Entropy: 0.931376576423645, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 704,  Mean reward: -6.773809523809524, Mean Entropy: 0.9146405458450317, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 705,  Mean reward: -4.3604651162790695, Mean Entropy: 1.0459001064300537, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 706,  Mean reward: -6.056818181818182, Mean Entropy: 0.959732711315155, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 707,  Mean reward: -4.666666666666667, Mean Entropy: 0.9093450307846069, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 708,  Mean reward: -4.5, Mean Entropy: 0.8519377708435059, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 709,  Mean reward: -5.309523809523809, Mean Entropy: 0.9890073537826538, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 710,  Mean reward: -3.8026315789473686, Mean Entropy: 1.0106680393218994, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 711,  Mean reward: -6.695121951219512, Mean Entropy: 0.9594117999076843, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.61s
Iteration: 712,  Mean reward: -5.175, Mean Entropy: 1.0095477104187012, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 713,  Mean reward: -4.825581395348837, Mean Entropy: 0.9087615013122559, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 714,  Mean reward: -6.2023809523809526, Mean Entropy: 0.9158080816268921, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 715,  Mean reward: -3.3658536585365852, Mean Entropy: 0.9378350973129272, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 716,  Mean reward: -2.395348837209302, Mean Entropy: 0.9295637607574463, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 717,  Mean reward: -4.378048780487805, Mean Entropy: 0.9293555021286011, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 718,  Mean reward: -4.107142857142857, Mean Entropy: 0.9002385139465332, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 719,  Mean reward: -6.145833333333333, Mean Entropy: 0.9363371729850769, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 720,  Mean reward: -3.588888888888889, Mean Entropy: 0.972972571849823, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.65s
Iteration: 721,  Mean reward: -3.5652173913043477, Mean Entropy: 0.9461859464645386, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 722,  Mean reward: -4.1125, Mean Entropy: 0.8864555358886719, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 723,  Mean reward: -0.79, Mean Entropy: 0.9533205032348633, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 724,  Mean reward: -1.475609756097561, Mean Entropy: 0.9837086200714111, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 725,  Mean reward: -2.595744680851064, Mean Entropy: 0.9130287170410156, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 726,  Mean reward: -1.9479166666666667, Mean Entropy: 0.9558011889457703, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 727,  Mean reward: -1.5833333333333333, Mean Entropy: 0.90867680311203, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 728,  Mean reward: -4.635416666666667, Mean Entropy: 0.987087607383728, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 729,  Mean reward: -5.355263157894737, Mean Entropy: 0.8316986560821533, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 730,  Mean reward: -3.738095238095238, Mean Entropy: 0.9069178104400635, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 731,  Mean reward: -3.7777777777777777, Mean Entropy: 0.9701148867607117, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 732,  Mean reward: -4.372093023255814, Mean Entropy: 0.8732258677482605, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 733,  Mean reward: -4.744186046511628, Mean Entropy: 0.8969677686691284, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 734,  Mean reward: -0.11818181818181818, Mean Entropy: 1.0189318656921387, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 735,  Mean reward: -3.0833333333333335, Mean Entropy: 0.9301832914352417, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 736,  Mean reward: -4.5375, Mean Entropy: 0.6879290342330933, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 737,  Mean reward: -5.328947368421052, Mean Entropy: 0.7357808351516724, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 738,  Mean reward: -2.4555555555555557, Mean Entropy: 0.8467020988464355, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 739,  Mean reward: -3.953488372093023, Mean Entropy: 0.8519295454025269, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 740,  Mean reward: -4.976744186046512, Mean Entropy: 0.854674220085144, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 741,  Mean reward: -1.2790697674418605, Mean Entropy: 0.9868897795677185, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 742,  Mean reward: -6.3108108108108105, Mean Entropy: 0.9017521142959595, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 743,  Mean reward: -5.613636363636363, Mean Entropy: 0.8983451724052429, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 744,  Mean reward: -5.413043478260869, Mean Entropy: 0.8171188235282898, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 745,  Mean reward: -6.336956521739131, Mean Entropy: 0.8337851762771606, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 746,  Mean reward: -5.136363636363637, Mean Entropy: 0.8470573425292969, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 747,  Mean reward: -2.977777777777778, Mean Entropy: 0.8131084442138672, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 748,  Mean reward: -3.5568181818181817, Mean Entropy: 0.8791201114654541, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 749,  Mean reward: -7.453488372093023, Mean Entropy: 0.9354085326194763, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 750,  Mean reward: -6.095238095238095, Mean Entropy: 0.9564108848571777, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 751,  Mean reward: -3.261904761904762, Mean Entropy: 0.9273254871368408, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 752,  Mean reward: -3.989130434782609, Mean Entropy: 0.8124842643737793, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 753,  Mean reward: -2.4782608695652173, Mean Entropy: 0.8078686594963074, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 754,  Mean reward: -3.5, Mean Entropy: 0.8788188695907593, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 755,  Mean reward: -1.2954545454545454, Mean Entropy: 0.8700556755065918, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 756,  Mean reward: -5.607142857142857, Mean Entropy: 0.8775838613510132, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.67s
Iteration: 757,  Mean reward: -4.0777777777777775, Mean Entropy: 0.8935961723327637, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 758,  Mean reward: -6.630952380952381, Mean Entropy: 0.8530831336975098, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 759,  Mean reward: -3.3444444444444446, Mean Entropy: 0.9040905833244324, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 760,  Mean reward: -5.111111111111111, Mean Entropy: 0.8567096590995789, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 761,  Mean reward: -4.777777777777778, Mean Entropy: 0.9278427958488464, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 762,  Mean reward: -3.602272727272727, Mean Entropy: 0.8231620192527771, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 763,  Mean reward: -0.6979166666666666, Mean Entropy: 0.4855467677116394, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 764,  Mean reward: -4.69811320754717, Mean Entropy: 0.5614590644836426, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 765,  Mean reward: -2.7327586206896552, Mean Entropy: 0.6063216328620911, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.42s
Iteration: 766,  Mean reward: -4.916666666666667, Mean Entropy: 0.6319910287857056, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 767,  Mean reward: -3.462962962962963, Mean Entropy: 0.6098536849021912, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 768,  Mean reward: -0.5434782608695652, Mean Entropy: 0.6089904308319092, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 769,  Mean reward: -1.5576923076923077, Mean Entropy: 0.5991469621658325, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 770,  Mean reward: -1.6770833333333333, Mean Entropy: 0.6649003028869629, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 771,  Mean reward: -1.7941176470588236, Mean Entropy: 0.7094684839248657, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.57s
Iteration: 772,  Mean reward: -1.771186440677966, Mean Entropy: 0.6447307467460632, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 773,  Mean reward: -2.394230769230769, Mean Entropy: 0.6750382781028748, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 774,  Mean reward: -3.7058823529411766, Mean Entropy: 0.6629296541213989, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 775,  Mean reward: -2.849056603773585, Mean Entropy: 0.6829514503479004, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 776,  Mean reward: -2.2641509433962264, Mean Entropy: 0.7128288745880127, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 777,  Mean reward: -0.8297872340425532, Mean Entropy: 0.7335020899772644, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 778,  Mean reward: 0.2978723404255319, Mean Entropy: 0.8376336097717285, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 779,  Mean reward: -0.7142857142857143, Mean Entropy: 0.7241141200065613, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 780,  Mean reward: -5.755102040816326, Mean Entropy: 0.8072284460067749, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 781,  Mean reward: -3.6341463414634148, Mean Entropy: 0.6753394603729248, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 782,  Mean reward: -2.7755102040816326, Mean Entropy: 0.7628071308135986, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 783,  Mean reward: -2.077777777777778, Mean Entropy: 0.7486268281936646, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 784,  Mean reward: -0.13095238095238096, Mean Entropy: 0.6916708946228027, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 785,  Mean reward: 0.09302325581395349, Mean Entropy: 0.6826229095458984, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 786,  Mean reward: 4.208333333333333, Mean Entropy: 0.7911354899406433, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 787,  Mean reward: 1.4782608695652173, Mean Entropy: 0.7130728960037231, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 788,  Mean reward: -4.377551020408164, Mean Entropy: 0.7786328196525574, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 789,  Mean reward: -4.490909090909091, Mean Entropy: 0.8978331089019775, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 790,  Mean reward: -5.4772727272727275, Mean Entropy: 0.9273090362548828, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 791,  Mean reward: -4.641304347826087, Mean Entropy: 0.9669920802116394, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 792,  Mean reward: -6.086956521739131, Mean Entropy: 0.9106845855712891, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.73s
Iteration: 793,  Mean reward: -3.409090909090909, Mean Entropy: 0.8037039041519165, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 794,  Mean reward: -4.116279069767442, Mean Entropy: 0.8293783068656921, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 795,  Mean reward: -4.572916666666667, Mean Entropy: 0.9088419675827026, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 796,  Mean reward: -1.1195652173913044, Mean Entropy: 0.48433980345726013, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 797,  Mean reward: -2.701923076923077, Mean Entropy: 0.5220451951026917, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 798,  Mean reward: -3.9591836734693877, Mean Entropy: 0.5928338766098022, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 799,  Mean reward: -2.798076923076923, Mean Entropy: 0.5962659120559692, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 800,  Mean reward: -1.7596153846153846, Mean Entropy: 0.5426923036575317, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.45s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -1.3653846153846154, Mean Entropy: 0.48124605417251587, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 802,  Mean reward: -1.7413793103448276, Mean Entropy: 0.5445842742919922, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 803,  Mean reward: -0.35454545454545455, Mean Entropy: 0.5771255493164062, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 804,  Mean reward: -3.701923076923077, Mean Entropy: 0.5189807415008545, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 805,  Mean reward: -0.2692307692307692, Mean Entropy: 0.5752758383750916, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 806,  Mean reward: -1.5701754385964912, Mean Entropy: 0.6084023118019104, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 807,  Mean reward: 1.3363636363636364, Mean Entropy: 0.5595608949661255, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 808,  Mean reward: -3.3627450980392157, Mean Entropy: 0.6149587631225586, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 809,  Mean reward: -3.3771929824561404, Mean Entropy: 0.6383137702941895, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 810,  Mean reward: 1.0980392156862746, Mean Entropy: 0.5434364676475525, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 811,  Mean reward: -2.989795918367347, Mean Entropy: 0.6103907227516174, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 812,  Mean reward: -0.9134615384615384, Mean Entropy: 0.5904971361160278, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 813,  Mean reward: -6.438775510204081, Mean Entropy: 0.6596702337265015, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 814,  Mean reward: 0.3829787234042553, Mean Entropy: 0.5743358135223389, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 815,  Mean reward: -2.7142857142857144, Mean Entropy: 0.6074230074882507, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 816,  Mean reward: -1.99, Mean Entropy: 0.6029430627822876, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 817,  Mean reward: -0.7982456140350878, Mean Entropy: 0.5073338747024536, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 818,  Mean reward: -3.7363636363636363, Mean Entropy: 0.6601740121841431, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 819,  Mean reward: -3.9903846153846154, Mean Entropy: 0.7113122940063477, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 820,  Mean reward: -2.0638297872340425, Mean Entropy: 0.6764146685600281, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 821,  Mean reward: -1.39, Mean Entropy: 0.6015219688415527, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 822,  Mean reward: -2.1153846153846154, Mean Entropy: 0.6306160688400269, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 823,  Mean reward: -5.653846153846154, Mean Entropy: 0.698858916759491, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 824,  Mean reward: 0.24, Mean Entropy: 0.618267297744751, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 825,  Mean reward: -4.21, Mean Entropy: 0.7009617686271667, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 826,  Mean reward: -3.85, Mean Entropy: 0.6927691698074341, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 827,  Mean reward: -1.065217391304348, Mean Entropy: 0.7263317108154297, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.65s
Iteration: 828,  Mean reward: -0.34782608695652173, Mean Entropy: 0.7149984836578369, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 829,  Mean reward: 1.851063829787234, Mean Entropy: 0.7294236421585083, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 830,  Mean reward: -1.7391304347826086, Mean Entropy: 0.7420133352279663, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 831,  Mean reward: -1.5568181818181819, Mean Entropy: 0.643362283706665, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 832,  Mean reward: 1.5222222222222221, Mean Entropy: 0.5445465445518494, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 833,  Mean reward: -1.93, Mean Entropy: 0.7008130550384521, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 834,  Mean reward: 2.340909090909091, Mean Entropy: 0.671940803527832, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 835,  Mean reward: 0.9130434782608695, Mean Entropy: 0.5989428758621216, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 836,  Mean reward: -0.8627450980392157, Mean Entropy: 0.6416370868682861, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 837,  Mean reward: 2.0113636363636362, Mean Entropy: 0.6340605616569519, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 838,  Mean reward: 1.9777777777777779, Mean Entropy: 0.674473762512207, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 839,  Mean reward: -0.6122448979591837, Mean Entropy: 0.6413809657096863, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 840,  Mean reward: 2.244186046511628, Mean Entropy: 0.49129152297973633, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 841,  Mean reward: 0.36792452830188677, Mean Entropy: 0.6911000609397888, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 842,  Mean reward: 2.2555555555555555, Mean Entropy: 0.7105244994163513, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 843,  Mean reward: 3.5425531914893615, Mean Entropy: 0.6195346117019653, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 844,  Mean reward: -3.188679245283019, Mean Entropy: 0.6256402730941772, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 845,  Mean reward: 2.7872340425531914, Mean Entropy: 0.7266342043876648, complete_episode_count: 47.0, Gather time: 0.64s, Train time: 1.46s
Iteration: 846,  Mean reward: -3.488888888888889, Mean Entropy: 0.4054378867149353, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 847,  Mean reward: -3.7162162162162162, Mean Entropy: 0.3312993347644806, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 848,  Mean reward: -2.3958333333333335, Mean Entropy: 0.4650627076625824, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 849,  Mean reward: 1.3068181818181819, Mean Entropy: 0.46848607063293457, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 850,  Mean reward: 2.577777777777778, Mean Entropy: 0.5833792090415955, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 851,  Mean reward: 1.5348837209302326, Mean Entropy: 0.561309278011322, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 852,  Mean reward: 4.28125, Mean Entropy: 0.6692080497741699, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 853,  Mean reward: 1.255813953488372, Mean Entropy: 0.6014953255653381, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 854,  Mean reward: 3.010869565217391, Mean Entropy: 0.6670860052108765, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 855,  Mean reward: 2.7395833333333335, Mean Entropy: 0.736045777797699, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 856,  Mean reward: -0.8611111111111112, Mean Entropy: 0.6683627963066101, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 857,  Mean reward: 3.3152173913043477, Mean Entropy: 0.655561625957489, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 858,  Mean reward: -0.7019230769230769, Mean Entropy: 0.620928943157196, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 859,  Mean reward: 2.9895833333333335, Mean Entropy: 0.570879340171814, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 860,  Mean reward: 2.966666666666667, Mean Entropy: 0.6976930499076843, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 861,  Mean reward: 3.0714285714285716, Mean Entropy: 0.6255375146865845, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 862,  Mean reward: 0.391304347826087, Mean Entropy: 0.6964058876037598, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 863,  Mean reward: -2.6527777777777777, Mean Entropy: 0.6824909448623657, complete_episode_count: 36.0, Gather time: 0.54s, Train time: 1.70s
Iteration: 864,  Mean reward: -4.04054054054054, Mean Entropy: 0.6760033369064331, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 865,  Mean reward: -4.9393939393939394, Mean Entropy: 0.6115533709526062, complete_episode_count: 33.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 866,  Mean reward: -5.214285714285714, Mean Entropy: 0.7728428244590759, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 867,  Mean reward: -6.256410256410256, Mean Entropy: 0.6903426051139832, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 868,  Mean reward: -6.414285714285715, Mean Entropy: 0.6902605295181274, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.48s
Iteration: 869,  Mean reward: -5.9324324324324325, Mean Entropy: 0.6497008204460144, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 870,  Mean reward: -6.5394736842105265, Mean Entropy: 0.8210597038269043, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 871,  Mean reward: -5.8, Mean Entropy: 0.7101062536239624, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 872,  Mean reward: -5.666666666666667, Mean Entropy: 0.7683881521224976, complete_episode_count: 36.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 873,  Mean reward: -5.194444444444445, Mean Entropy: 0.6126973628997803, complete_episode_count: 36.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 874,  Mean reward: -3.730769230769231, Mean Entropy: 0.5895874500274658, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 875,  Mean reward: -1.775, Mean Entropy: 0.5716186761856079, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 876,  Mean reward: -1.2446808510638299, Mean Entropy: 0.6696339845657349, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 877,  Mean reward: 2.107843137254902, Mean Entropy: 0.7917569875717163, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 878,  Mean reward: -3.9324324324324325, Mean Entropy: 0.762384295463562, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 879,  Mean reward: -0.06818181818181818, Mean Entropy: 0.6784650683403015, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 880,  Mean reward: -1.2738095238095237, Mean Entropy: 0.6173843741416931, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 881,  Mean reward: 0.05319148936170213, Mean Entropy: 0.6419110298156738, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 882,  Mean reward: 1.0217391304347827, Mean Entropy: 0.665177583694458, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 883,  Mean reward: -1.98, Mean Entropy: 0.7014106512069702, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 884,  Mean reward: -1.8085106382978724, Mean Entropy: 0.873727023601532, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 885,  Mean reward: -0.8571428571428571, Mean Entropy: 0.8170641660690308, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 886,  Mean reward: -4.162790697674419, Mean Entropy: 0.8177993297576904, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 887,  Mean reward: -1.8297872340425532, Mean Entropy: 0.7916250228881836, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 888,  Mean reward: -0.3048780487804878, Mean Entropy: 0.9022749662399292, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 889,  Mean reward: -3.616279069767442, Mean Entropy: 0.8561476469039917, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.58s
Iteration: 890,  Mean reward: -1.9456521739130435, Mean Entropy: 0.8710064888000488, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 891,  Mean reward: -1.1904761904761905, Mean Entropy: 0.9037837982177734, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 892,  Mean reward: -2.104651162790698, Mean Entropy: 0.885835587978363, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 893,  Mean reward: -0.3488372093023256, Mean Entropy: 0.8160614967346191, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 894,  Mean reward: -2.2790697674418605, Mean Entropy: 0.8299126625061035, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 895,  Mean reward: -0.3617021276595745, Mean Entropy: 0.9300443530082703, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 896,  Mean reward: -3.2674418604651163, Mean Entropy: 0.8277058005332947, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 897,  Mean reward: 0.08888888888888889, Mean Entropy: 0.7991912364959717, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 898,  Mean reward: -3.673469387755102, Mean Entropy: 0.7402984499931335, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 899,  Mean reward: -0.6956521739130435, Mean Entropy: 0.8085098266601562, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 900,  Mean reward: -0.4318181818181818, Mean Entropy: 0.8749625086784363, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.70s
rec seq len 2
actor lr 0.0005
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.32051282051282, Mean Entropy: 0.9169759750366211, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.45s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.548780487804878, Mean Entropy: 0.9314165115356445, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -3.3191489361702127, Mean Entropy: 0.9025353789329529, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.59s
Iteration: 3,  Mean reward: -6.439024390243903, Mean Entropy: 0.9963991045951843, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 4,  Mean reward: -6.678571428571429, Mean Entropy: 0.9530774354934692, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 5,  Mean reward: -1.7790697674418605, Mean Entropy: 0.9097556471824646, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 6,  Mean reward: -5.951219512195122, Mean Entropy: 0.8880948424339294, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 7,  Mean reward: -3.4125, Mean Entropy: 0.9025353193283081, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 8,  Mean reward: -3.686046511627907, Mean Entropy: 0.9097555875778198, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 9,  Mean reward: -4.7631578947368425, Mean Entropy: 0.931416392326355, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 10,  Mean reward: -3.475609756097561, Mean Entropy: 0.9314162135124207, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 11,  Mean reward: -3.8, Mean Entropy: 0.9458565711975098, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 12,  Mean reward: -4.616279069767442, Mean Entropy: 0.9241952896118164, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 13,  Mean reward: -4.406976744186046, Mean Entropy: 0.9458565711975098, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 14,  Mean reward: -4.5256410256410255, Mean Entropy: 0.9241956472396851, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 15,  Mean reward: -3.8, Mean Entropy: 0.9169754981994629, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 16,  Mean reward: -5.15, Mean Entropy: 0.9314159154891968, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 17,  Mean reward: -4.4186046511627906, Mean Entropy: 0.9386359453201294, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 18,  Mean reward: -3.8863636363636362, Mean Entropy: 0.9530772566795349, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 19,  Mean reward: -6.365853658536586, Mean Entropy: 0.9530772566795349, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 20,  Mean reward: -4.659090909090909, Mean Entropy: 0.9891786575317383, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 21,  Mean reward: -4.4375, Mean Entropy: 0.9386366605758667, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 22,  Mean reward: -3.3333333333333335, Mean Entropy: 0.9097554683685303, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 23,  Mean reward: -7.012820512820513, Mean Entropy: 0.9747378826141357, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 24,  Mean reward: -5.780487804878049, Mean Entropy: 0.9819579124450684, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 25,  Mean reward: -5.358695652173913, Mean Entropy: 1.0108392238616943, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 26,  Mean reward: -5.056818181818182, Mean Entropy: 0.9458566904067993, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 27,  Mean reward: -2.573170731707317, Mean Entropy: 0.9169747233390808, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 28,  Mean reward: -6.093023255813954, Mean Entropy: 0.9314157366752625, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 29,  Mean reward: -5.056818181818182, Mean Entropy: 0.9819575548171997, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 30,  Mean reward: -4.488636363636363, Mean Entropy: 0.9891778230667114, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 31,  Mean reward: -2.107142857142857, Mean Entropy: 0.9530770182609558, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 32,  Mean reward: -6.666666666666667, Mean Entropy: 0.9241956472396851, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.63s
Iteration: 33,  Mean reward: -6.544444444444444, Mean Entropy: 0.9097542762756348, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 34,  Mean reward: -6.0, Mean Entropy: 0.9675168395042419, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 35,  Mean reward: -2.8658536585365852, Mean Entropy: 0.9819571375846863, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 36,  Mean reward: -4.284090909090909, Mean Entropy: 0.9386348724365234, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 37,  Mean reward: -5.174418604651163, Mean Entropy: 0.9097549915313721, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 38,  Mean reward: -8.55, Mean Entropy: 0.9963972568511963, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 39,  Mean reward: -5.023809523809524, Mean Entropy: 0.9314138293266296, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 40,  Mean reward: -6.023809523809524, Mean Entropy: 0.9530757665634155, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 41,  Mean reward: -3.5238095238095237, Mean Entropy: 0.9314160346984863, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 42,  Mean reward: -4.409090909090909, Mean Entropy: 0.9025353193283081, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 43,  Mean reward: -3.0795454545454546, Mean Entropy: 0.9097551107406616, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 44,  Mean reward: -2.7555555555555555, Mean Entropy: 0.9169721007347107, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 45,  Mean reward: -2.8452380952380953, Mean Entropy: 0.9169669151306152, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 46,  Mean reward: -5.744186046511628, Mean Entropy: 0.9963456988334656, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 47,  Mean reward: -5.25, Mean Entropy: 0.9241883158683777, complete_episode_count: 42.0, Gather time: 0.70s, Train time: 1.47s
Iteration: 48,  Mean reward: -4.428571428571429, Mean Entropy: 0.8880857825279236, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 49,  Mean reward: -3.6547619047619047, Mean Entropy: 0.9602756500244141, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 50,  Mean reward: -6.8023255813953485, Mean Entropy: 0.8953045606613159, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 51,  Mean reward: -5.223684210526316, Mean Entropy: 0.9025160074234009, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 52,  Mean reward: -4.815789473684211, Mean Entropy: 0.9241324663162231, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 53,  Mean reward: -3.383720930232558, Mean Entropy: 0.9528786540031433, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 54,  Mean reward: -3.1875, Mean Entropy: 0.9602227210998535, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 55,  Mean reward: -5.695121951219512, Mean Entropy: 0.9818115234375, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 56,  Mean reward: -1.853658536585366, Mean Entropy: 0.8735458850860596, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 57,  Mean reward: -3.5, Mean Entropy: 1.0104793310165405, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.51s
Iteration: 58,  Mean reward: -4.785714285714286, Mean Entropy: 0.9456107020378113, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 59,  Mean reward: -2.7195121951219514, Mean Entropy: 0.9372518062591553, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 60,  Mean reward: -7.1125, Mean Entropy: 0.8870576620101929, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 61,  Mean reward: -5.073170731707317, Mean Entropy: 0.9380852580070496, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 62,  Mean reward: -6.902439024390244, Mean Entropy: 0.9671193361282349, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 63,  Mean reward: -5.157894736842105, Mean Entropy: 0.9309765696525574, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 64,  Mean reward: -3.7, Mean Entropy: 0.8654934763908386, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 65,  Mean reward: -5.4523809523809526, Mean Entropy: 0.9519451856613159, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 66,  Mean reward: -4.355555555555555, Mean Entropy: 0.8876851797103882, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 67,  Mean reward: -5.8625, Mean Entropy: 0.9299583435058594, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 68,  Mean reward: -3.558139534883721, Mean Entropy: 0.9526638984680176, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 69,  Mean reward: -2.7972972972972974, Mean Entropy: 0.9600348472595215, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 70,  Mean reward: -5.342105263157895, Mean Entropy: 0.9017069339752197, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 71,  Mean reward: -5.380952380952381, Mean Entropy: 0.929865300655365, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 72,  Mean reward: -4.155555555555556, Mean Entropy: 0.9440363645553589, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.72s
Iteration: 73,  Mean reward: -3.2439024390243905, Mean Entropy: 0.9886411428451538, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 74,  Mean reward: -2.522727272727273, Mean Entropy: 0.9673112630844116, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 75,  Mean reward: -3.5543478260869565, Mean Entropy: 0.9095399379730225, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 76,  Mean reward: -4.6923076923076925, Mean Entropy: 0.9239344596862793, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 77,  Mean reward: -4.127906976744186, Mean Entropy: 1.0159857273101807, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 78,  Mean reward: -3.6547619047619047, Mean Entropy: 0.9018220901489258, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 79,  Mean reward: -5.425531914893617, Mean Entropy: 0.929970383644104, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 80,  Mean reward: -2.8, Mean Entropy: 0.9954532384872437, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 81,  Mean reward: -3.0, Mean Entropy: 0.9600266814231873, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 82,  Mean reward: -3.8488372093023258, Mean Entropy: 0.9165439009666443, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 83,  Mean reward: -3.619047619047619, Mean Entropy: 0.931062638759613, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 84,  Mean reward: -6.25, Mean Entropy: 0.8978358507156372, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 85,  Mean reward: -4.825581395348837, Mean Entropy: 0.972885012626648, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 86,  Mean reward: -3.0, Mean Entropy: 0.9078619480133057, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 87,  Mean reward: -1.6444444444444444, Mean Entropy: 1.0026142597198486, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 88,  Mean reward: -3.5625, Mean Entropy: 0.9153456687927246, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 89,  Mean reward: -6.6395348837209305, Mean Entropy: 0.9396734237670898, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.54s
Iteration: 90,  Mean reward: -5.72093023255814, Mean Entropy: 0.9310636520385742, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 91,  Mean reward: -5.394736842105263, Mean Entropy: 0.9018440246582031, complete_episode_count: 38.0, Gather time: 0.62s, Train time: 1.48s
Iteration: 92,  Mean reward: -7.605263157894737, Mean Entropy: 0.9449338912963867, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 93,  Mean reward: -3.7790697674418605, Mean Entropy: 0.9728612899780273, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 94,  Mean reward: -6.0813953488372094, Mean Entropy: 0.8784443736076355, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 95,  Mean reward: -4.8977272727272725, Mean Entropy: 0.8946884870529175, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 96,  Mean reward: -5.222222222222222, Mean Entropy: 0.9506995677947998, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 97,  Mean reward: -4.651162790697675, Mean Entropy: 0.94682776927948, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 98,  Mean reward: -4.5476190476190474, Mean Entropy: 0.9869334101676941, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 99,  Mean reward: -3.548780487804878, Mean Entropy: 0.8803833723068237, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 100,  Mean reward: -4.709302325581396, Mean Entropy: 0.9739702939987183, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.50s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -3.5232558139534884, Mean Entropy: 0.9094171524047852, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 102,  Mean reward: -5.888888888888889, Mean Entropy: 0.9524804949760437, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 103,  Mean reward: -6.033333333333333, Mean Entropy: 0.9558185935020447, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 104,  Mean reward: -5.795454545454546, Mean Entropy: 0.9023744463920593, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 105,  Mean reward: -3.0714285714285716, Mean Entropy: 0.8713202476501465, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 106,  Mean reward: -1.2934782608695652, Mean Entropy: 0.9525889158248901, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 107,  Mean reward: -3.05, Mean Entropy: 0.9597820043563843, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 108,  Mean reward: -3.875, Mean Entropy: 0.9884392023086548, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 109,  Mean reward: -4.146341463414634, Mean Entropy: 0.9455515742301941, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.67s
Iteration: 110,  Mean reward: -4.730769230769231, Mean Entropy: 0.9204689264297485, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 111,  Mean reward: -3.6627906976744184, Mean Entropy: 0.8946671485900879, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 112,  Mean reward: -5.325, Mean Entropy: 0.9523822665214539, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.50s
Iteration: 113,  Mean reward: -5.453488372093023, Mean Entropy: 0.9879114627838135, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 114,  Mean reward: -5.2894736842105265, Mean Entropy: 0.9268047213554382, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 115,  Mean reward: -5.318181818181818, Mean Entropy: 0.9354637861251831, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.51s
Iteration: 116,  Mean reward: -5.555555555555555, Mean Entropy: 0.9592933058738708, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 117,  Mean reward: -4.365853658536586, Mean Entropy: 0.9744402766227722, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 118,  Mean reward: -5.215909090909091, Mean Entropy: 0.9380167722702026, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 119,  Mean reward: -4.1625, Mean Entropy: 0.945142924785614, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 120,  Mean reward: -3.9625, Mean Entropy: 0.8950173258781433, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 121,  Mean reward: -6.317073170731708, Mean Entropy: 0.9092111587524414, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 122,  Mean reward: -3.5, Mean Entropy: 0.9815062284469604, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 123,  Mean reward: -4.166666666666667, Mean Entropy: 0.9957493543624878, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 124,  Mean reward: -3.1341463414634148, Mean Entropy: 0.9526402950286865, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 125,  Mean reward: -4.972972972972973, Mean Entropy: 0.9307223558425903, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.49s
Iteration: 126,  Mean reward: -2.2625, Mean Entropy: 0.9380277395248413, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 127,  Mean reward: -3.159090909090909, Mean Entropy: 0.95940101146698, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 128,  Mean reward: -4.243589743589744, Mean Entropy: 0.9725227355957031, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 129,  Mean reward: -4.441860465116279, Mean Entropy: 0.9519636034965515, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 130,  Mean reward: -6.3375, Mean Entropy: 0.9003056287765503, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 131,  Mean reward: -4.571428571428571, Mean Entropy: 1.0110467672348022, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 132,  Mean reward: -4.609756097560975, Mean Entropy: 0.8873738646507263, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 133,  Mean reward: -2.546511627906977, Mean Entropy: 1.0033578872680664, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 134,  Mean reward: -3.9555555555555557, Mean Entropy: 0.967171847820282, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 135,  Mean reward: -5.968085106382978, Mean Entropy: 0.9771876335144043, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 136,  Mean reward: -5.655555555555556, Mean Entropy: 0.903532862663269, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 137,  Mean reward: -6.141304347826087, Mean Entropy: 0.9476810693740845, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 138,  Mean reward: -2.5, Mean Entropy: 0.9862110018730164, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 139,  Mean reward: -5.9361702127659575, Mean Entropy: 0.9702072143554688, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 140,  Mean reward: -6.706521739130435, Mean Entropy: 0.9140992164611816, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 141,  Mean reward: -5.329545454545454, Mean Entropy: 0.88470458984375, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 142,  Mean reward: -3.411111111111111, Mean Entropy: 0.9480544924736023, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 143,  Mean reward: -2.65, Mean Entropy: 0.909018874168396, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 144,  Mean reward: -4.451219512195122, Mean Entropy: 0.9087921380996704, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 145,  Mean reward: -3.6931818181818183, Mean Entropy: 0.9068679809570312, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 146,  Mean reward: -6.023809523809524, Mean Entropy: 0.9201016426086426, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.64s
Iteration: 147,  Mean reward: -3.011904761904762, Mean Entropy: 0.943638265132904, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 148,  Mean reward: -4.833333333333333, Mean Entropy: 0.9672677516937256, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 149,  Mean reward: -3.2045454545454546, Mean Entropy: 0.948737621307373, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 150,  Mean reward: -3.1630434782608696, Mean Entropy: 0.9187560081481934, complete_episode_count: 46.0, Gather time: 0.69s, Train time: 1.52s
Iteration: 151,  Mean reward: -3.951219512195122, Mean Entropy: 0.9032682180404663, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 152,  Mean reward: -2.046511627906977, Mean Entropy: 0.982999324798584, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 153,  Mean reward: -2.313953488372093, Mean Entropy: 0.9484462738037109, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 154,  Mean reward: -4.136363636363637, Mean Entropy: 0.9307875037193298, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 155,  Mean reward: -3.9069767441860463, Mean Entropy: 0.909422755241394, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 156,  Mean reward: -4.663043478260869, Mean Entropy: 0.9513751864433289, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 157,  Mean reward: -4.3522727272727275, Mean Entropy: 0.9700109362602234, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 158,  Mean reward: -6.488372093023256, Mean Entropy: 0.9696331024169922, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 159,  Mean reward: -5.616279069767442, Mean Entropy: 0.9672471284866333, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 160,  Mean reward: -2.8947368421052633, Mean Entropy: 0.990415096282959, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 161,  Mean reward: -3.8205128205128207, Mean Entropy: 0.8845347762107849, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 162,  Mean reward: -4.095238095238095, Mean Entropy: 0.8710178136825562, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 163,  Mean reward: -4.322222222222222, Mean Entropy: 0.8920245170593262, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 164,  Mean reward: -2.2111111111111112, Mean Entropy: 0.9229224920272827, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 165,  Mean reward: -4.825581395348837, Mean Entropy: 0.8797876238822937, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 166,  Mean reward: -3.163265306122449, Mean Entropy: 0.7777232527732849, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 167,  Mean reward: -4.921052631578948, Mean Entropy: 0.709691047668457, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 168,  Mean reward: -3.267857142857143, Mean Entropy: 0.8711320757865906, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 169,  Mean reward: -0.5975609756097561, Mean Entropy: 0.9139041900634766, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 170,  Mean reward: -2.7888888888888888, Mean Entropy: 0.9447306394577026, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 171,  Mean reward: -5.102564102564102, Mean Entropy: 1.0221467018127441, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 172,  Mean reward: -2.433333333333333, Mean Entropy: 0.9382498264312744, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 173,  Mean reward: -3.9204545454545454, Mean Entropy: 0.27209553122520447, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 174,  Mean reward: -2.856060606060606, Mean Entropy: 0.3493262529373169, complete_episode_count: 66.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 175,  Mean reward: -4.112903225806452, Mean Entropy: 0.9034934043884277, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 176,  Mean reward: -8.5, Mean Entropy: 0.9134952425956726, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 177,  Mean reward: 0.7708333333333334, Mean Entropy: 0.9418448209762573, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 178,  Mean reward: -4.105263157894737, Mean Entropy: 0.9095930457115173, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 179,  Mean reward: -5.451219512195122, Mean Entropy: 0.8985451459884644, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 180,  Mean reward: -4.282051282051282, Mean Entropy: 0.9211399555206299, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.46s
Iteration: 181,  Mean reward: -5.2317073170731705, Mean Entropy: 0.9535242915153503, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 182,  Mean reward: -6.67948717948718, Mean Entropy: 0.6216479539871216, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 183,  Mean reward: -0.0673076923076923, Mean Entropy: 0.8928847312927246, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.65s
Iteration: 184,  Mean reward: -5.175, Mean Entropy: 0.8944486379623413, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 185,  Mean reward: -5.142857142857143, Mean Entropy: 0.9148086309432983, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 186,  Mean reward: -1.8863636363636365, Mean Entropy: 0.9086763262748718, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 187,  Mean reward: -6.256410256410256, Mean Entropy: 0.9581108093261719, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 188,  Mean reward: -4.125, Mean Entropy: 0.9375230073928833, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 189,  Mean reward: -4.222222222222222, Mean Entropy: 0.929886519908905, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 190,  Mean reward: -5.658536585365853, Mean Entropy: 0.9380513429641724, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 191,  Mean reward: -4.1, Mean Entropy: 0.9441289901733398, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 192,  Mean reward: -5.475609756097561, Mean Entropy: 0.9219982028007507, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 193,  Mean reward: -4.083333333333333, Mean Entropy: 0.9497801661491394, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 194,  Mean reward: -2.058139534883721, Mean Entropy: 0.9303671717643738, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 195,  Mean reward: -6.280487804878049, Mean Entropy: 0.9672832489013672, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 196,  Mean reward: -5.488372093023256, Mean Entropy: 0.8954834342002869, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 197,  Mean reward: -3.630434782608696, Mean Entropy: 0.9631778001785278, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 198,  Mean reward: -5.377777777777778, Mean Entropy: 0.9383234977722168, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 199,  Mean reward: -5.458333333333333, Mean Entropy: 0.9451870918273926, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 200,  Mean reward: -2.5106382978723403, Mean Entropy: 0.9264370203018188, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -4.174418604651163, Mean Entropy: 0.8785761594772339, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 202,  Mean reward: -1.711111111111111, Mean Entropy: 0.9603519439697266, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 203,  Mean reward: -6.142857142857143, Mean Entropy: 0.9440499544143677, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 204,  Mean reward: -5.1063829787234045, Mean Entropy: 0.9081872701644897, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 205,  Mean reward: -2.37, Mean Entropy: 0.9036433100700378, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 206,  Mean reward: -3.311111111111111, Mean Entropy: 0.9458404779434204, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 207,  Mean reward: -3.4166666666666665, Mean Entropy: 0.8638908267021179, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 208,  Mean reward: -2.604651162790698, Mean Entropy: 0.9444023966789246, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 209,  Mean reward: -3.5, Mean Entropy: 0.9983570575714111, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 210,  Mean reward: -3.7209302325581395, Mean Entropy: 0.9515904188156128, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 211,  Mean reward: -3.0595238095238093, Mean Entropy: 0.8506028056144714, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 212,  Mean reward: -4.858695652173913, Mean Entropy: 0.9563760757446289, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 213,  Mean reward: 1.3867924528301887, Mean Entropy: 0.925731360912323, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.45s
Iteration: 214,  Mean reward: -1.3928571428571428, Mean Entropy: 0.8350714445114136, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 215,  Mean reward: -4.73, Mean Entropy: 0.8743986487388611, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 216,  Mean reward: -1.0714285714285714, Mean Entropy: 0.8722135424613953, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 217,  Mean reward: -0.8586956521739131, Mean Entropy: 0.9122635722160339, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 218,  Mean reward: -3.9183673469387754, Mean Entropy: 0.8837155103683472, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 219,  Mean reward: -1.9583333333333333, Mean Entropy: 0.877498984336853, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.69s
Iteration: 220,  Mean reward: 1.2959183673469388, Mean Entropy: 0.8070621490478516, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 221,  Mean reward: -4.945652173913044, Mean Entropy: 0.8882511854171753, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 222,  Mean reward: -3.96875, Mean Entropy: 0.8930972814559937, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 223,  Mean reward: 0.68, Mean Entropy: 0.9122905731201172, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 224,  Mean reward: -2.2111111111111112, Mean Entropy: 0.8681247234344482, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 225,  Mean reward: -3.0217391304347827, Mean Entropy: 0.830215573310852, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 226,  Mean reward: -1.25, Mean Entropy: 0.8528323173522949, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 227,  Mean reward: 1.22, Mean Entropy: 0.8961795568466187, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 228,  Mean reward: -2.49, Mean Entropy: 0.912470817565918, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 229,  Mean reward: -4.3977272727272725, Mean Entropy: 0.803917407989502, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 230,  Mean reward: 1.1326530612244898, Mean Entropy: 0.8437355756759644, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 231,  Mean reward: -0.3404255319148936, Mean Entropy: 0.6948593854904175, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 232,  Mean reward: -2.0096153846153846, Mean Entropy: 0.7183451652526855, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 233,  Mean reward: 0.9166666666666666, Mean Entropy: 0.8752659559249878, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 234,  Mean reward: -3.8260869565217392, Mean Entropy: 0.7721174955368042, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 235,  Mean reward: -1.3541666666666667, Mean Entropy: 0.8782098293304443, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 236,  Mean reward: 0.3617021276595745, Mean Entropy: 0.8362991213798523, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 237,  Mean reward: -2.3660714285714284, Mean Entropy: 0.7044398188591003, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 238,  Mean reward: 1.31, Mean Entropy: 0.8419017791748047, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 239,  Mean reward: -1.85, Mean Entropy: 0.8151130676269531, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 240,  Mean reward: -0.08163265306122448, Mean Entropy: 0.8453044295310974, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 241,  Mean reward: 2.21, Mean Entropy: 0.7809009552001953, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 242,  Mean reward: -2.120967741935484, Mean Entropy: 0.5571399927139282, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 243,  Mean reward: -2.212962962962963, Mean Entropy: 0.7332029342651367, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 244,  Mean reward: 2.5350877192982457, Mean Entropy: 0.8827452659606934, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.46s
Iteration: 245,  Mean reward: 0.05660377358490566, Mean Entropy: 0.7762855291366577, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 246,  Mean reward: -1.0092592592592593, Mean Entropy: 0.6647405624389648, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 247,  Mean reward: 0.6018518518518519, Mean Entropy: 0.8874416351318359, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 248,  Mean reward: 0.00980392156862745, Mean Entropy: 0.8061277270317078, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.44s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 249,  Mean reward: 3.0980392156862746, Mean Entropy: 0.7671360969543457, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 250,  Mean reward: -3.2636363636363637, Mean Entropy: 0.7249512672424316, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 251,  Mean reward: 0.8627450980392157, Mean Entropy: 0.7430548667907715, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 252,  Mean reward: 3.117021276595745, Mean Entropy: 0.8328051567077637, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 253,  Mean reward: 0.6224489795918368, Mean Entropy: 0.8040021657943726, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 254,  Mean reward: 0.8981481481481481, Mean Entropy: 0.8132362365722656, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 255,  Mean reward: 0.7555555555555555, Mean Entropy: 0.6502763628959656, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.67s
Iteration: 256,  Mean reward: 1.3235294117647058, Mean Entropy: 0.803784966468811, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 257,  Mean reward: 1.5196078431372548, Mean Entropy: 0.7444941401481628, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 258,  Mean reward: 1.9351851851851851, Mean Entropy: 0.7857331037521362, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 259,  Mean reward: 2.9019607843137254, Mean Entropy: 0.7695072889328003, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 260,  Mean reward: -3.795918367346939, Mean Entropy: 0.7204537391662598, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 261,  Mean reward: 1.2065217391304348, Mean Entropy: 0.8639283180236816, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 262,  Mean reward: 1.0729166666666667, Mean Entropy: 0.7595836520195007, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 263,  Mean reward: 3.118181818181818, Mean Entropy: 0.7650508880615234, complete_episode_count: 55.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 264,  Mean reward: 2.0849056603773586, Mean Entropy: 0.7261773347854614, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 265,  Mean reward: 2.792452830188679, Mean Entropy: 0.8594984412193298, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 266,  Mean reward: -0.14444444444444443, Mean Entropy: 0.7858869433403015, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 267,  Mean reward: -1.3666666666666667, Mean Entropy: 0.7481299638748169, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 268,  Mean reward: 1.5326086956521738, Mean Entropy: 0.6813538074493408, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 269,  Mean reward: 1.702127659574468, Mean Entropy: 0.8318690061569214, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 270,  Mean reward: -2.883720930232558, Mean Entropy: 0.7998555898666382, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 271,  Mean reward: 1.7444444444444445, Mean Entropy: 0.6309647560119629, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 272,  Mean reward: 3.357142857142857, Mean Entropy: 0.6344966292381287, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 273,  Mean reward: 3.549019607843137, Mean Entropy: 0.8174813985824585, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.44s
Iteration: 274,  Mean reward: 1.6122448979591837, Mean Entropy: 0.6479671001434326, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 275,  Mean reward: 2.8627450980392157, Mean Entropy: 0.7853306531906128, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 276,  Mean reward: 4.43, Mean Entropy: 0.7001890540122986, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 277,  Mean reward: -1.6388888888888888, Mean Entropy: 0.4879034459590912, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 278,  Mean reward: 3.15, Mean Entropy: 0.7333792448043823, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 279,  Mean reward: 3.5980392156862746, Mean Entropy: 0.7033950090408325, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 280,  Mean reward: 1.0333333333333334, Mean Entropy: 0.6786479353904724, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 281,  Mean reward: 3.4693877551020407, Mean Entropy: 0.7584857940673828, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 282,  Mean reward: 4.372727272727273, Mean Entropy: 0.6184303164482117, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 283,  Mean reward: 3.51, Mean Entropy: 0.7727025747299194, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 284,  Mean reward: 0.5340909090909091, Mean Entropy: 0.669704794883728, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 285,  Mean reward: 3.7755102040816326, Mean Entropy: 0.6299616098403931, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 286,  Mean reward: 3.5, Mean Entropy: 0.62921541929245, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 287,  Mean reward: -1.7543859649122806, Mean Entropy: 0.5660930275917053, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 288,  Mean reward: 3.7549019607843137, Mean Entropy: 0.574661135673523, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 289,  Mean reward: 5.5092592592592595, Mean Entropy: 0.6978179812431335, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 290,  Mean reward: 0.54, Mean Entropy: 0.5917478203773499, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.69s
Iteration: 291,  Mean reward: 4.39622641509434, Mean Entropy: 0.6856050491333008, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 292,  Mean reward: 4.076923076923077, Mean Entropy: 0.8015146255493164, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 293,  Mean reward: -0.8777777777777778, Mean Entropy: 0.607050895690918, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 294,  Mean reward: 4.041666666666667, Mean Entropy: 0.6462104320526123, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 295,  Mean reward: 3.836734693877551, Mean Entropy: 0.6734813451766968, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 296,  Mean reward: 1.24, Mean Entropy: 0.6860082149505615, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 297,  Mean reward: 4.7075471698113205, Mean Entropy: 0.705708384513855, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 298,  Mean reward: 3.343137254901961, Mean Entropy: 0.7441121935844421, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 299,  Mean reward: 1.3942307692307692, Mean Entropy: 0.7988328337669373, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.58s
Iteration: 300,  Mean reward: 0.79, Mean Entropy: 0.8325725793838501, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.47s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -2.152173913043478, Mean Entropy: 0.7896983027458191, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 302,  Mean reward: 4.4907407407407405, Mean Entropy: 0.6553577780723572, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 303,  Mean reward: 5.415094339622642, Mean Entropy: 0.7013208270072937, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 304,  Mean reward: 5.116071428571429, Mean Entropy: 0.6987996101379395, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 305,  Mean reward: 1.351063829787234, Mean Entropy: 0.6993840932846069, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 306,  Mean reward: 5.355769230769231, Mean Entropy: 0.6767624616622925, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 307,  Mean reward: 2.5, Mean Entropy: 0.741689920425415, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 308,  Mean reward: 3.26, Mean Entropy: 0.7052943706512451, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 309,  Mean reward: 4.06, Mean Entropy: 0.684151291847229, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 310,  Mean reward: -4.389830508474576, Mean Entropy: 0.683667778968811, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.45s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 311,  Mean reward: 6.02542372881356, Mean Entropy: 0.6746923327445984, complete_episode_count: 59.0, Gather time: 0.58s, Train time: 1.45s
Iteration: 312,  Mean reward: 1.4433962264150944, Mean Entropy: 0.671220064163208, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 313,  Mean reward: 4.153061224489796, Mean Entropy: 0.5402909517288208, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 314,  Mean reward: 4.611111111111111, Mean Entropy: 0.6687319278717041, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 315,  Mean reward: 5.464912280701754, Mean Entropy: 0.5979955196380615, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 316,  Mean reward: 0.2641509433962264, Mean Entropy: 0.5225629210472107, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 317,  Mean reward: 4.781818181818182, Mean Entropy: 0.7341906428337097, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 318,  Mean reward: 0.7906976744186046, Mean Entropy: 0.7311185598373413, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 319,  Mean reward: -3.6470588235294117, Mean Entropy: 0.4586929678916931, complete_episode_count: 34.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 320,  Mean reward: -3.175675675675676, Mean Entropy: 0.5804034471511841, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 321,  Mean reward: -4.159090909090909, Mean Entropy: 0.8905133008956909, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 322,  Mean reward: -3.7888888888888888, Mean Entropy: 0.7942352294921875, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 323,  Mean reward: -3.4239130434782608, Mean Entropy: 0.8631281852722168, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 324,  Mean reward: -3.5681818181818183, Mean Entropy: 0.8573939800262451, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 325,  Mean reward: 0.1568627450980392, Mean Entropy: 0.7671552896499634, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.71s
Iteration: 326,  Mean reward: -2.735294117647059, Mean Entropy: 0.7896448373794556, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 327,  Mean reward: 0.5431034482758621, Mean Entropy: 0.7839676141738892, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 328,  Mean reward: -4.5625, Mean Entropy: 0.44411128759384155, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 329,  Mean reward: -3.1186440677966103, Mean Entropy: 0.4101046919822693, complete_episode_count: 59.0, Gather time: 0.71s, Train time: 1.46s
Iteration: 330,  Mean reward: -1.21875, Mean Entropy: 0.7028061747550964, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 331,  Mean reward: -4.153846153846154, Mean Entropy: 0.7979006767272949, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 332,  Mean reward: -11.133333333333333, Mean Entropy: 0.5089911222457886, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 333,  Mean reward: 0.9056603773584906, Mean Entropy: 0.7117671966552734, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 334,  Mean reward: -1.489795918367347, Mean Entropy: 0.6826028227806091, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 335,  Mean reward: 0.34210526315789475, Mean Entropy: 0.575058102607727, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 336,  Mean reward: 2.6346153846153846, Mean Entropy: 0.7530031204223633, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 337,  Mean reward: -2.3839285714285716, Mean Entropy: 0.6277483701705933, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 338,  Mean reward: 3.3260869565217392, Mean Entropy: 0.6532081961631775, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 339,  Mean reward: 3.2083333333333335, Mean Entropy: 0.7127568125724792, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 340,  Mean reward: 1.9081632653061225, Mean Entropy: 0.7149394750595093, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 341,  Mean reward: 2.9069767441860463, Mean Entropy: 0.580390989780426, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 342,  Mean reward: 1.705128205128205, Mean Entropy: 0.4680027663707733, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 343,  Mean reward: -6.371428571428571, Mean Entropy: 0.46401247382164, complete_episode_count: 35.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 344,  Mean reward: -7.161764705882353, Mean Entropy: 0.7065873146057129, complete_episode_count: 34.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 345,  Mean reward: -7.385714285714286, Mean Entropy: 0.7227758169174194, complete_episode_count: 35.0, Gather time: 0.53s, Train time: 1.44s
Iteration: 346,  Mean reward: -7.128205128205129, Mean Entropy: 0.9645799398422241, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 347,  Mean reward: -5.646341463414634, Mean Entropy: 0.9127930402755737, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 348,  Mean reward: -1.925, Mean Entropy: 0.9220923781394958, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 349,  Mean reward: -5.105263157894737, Mean Entropy: 0.9527891874313354, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 350,  Mean reward: -5.833333333333333, Mean Entropy: 0.9501932859420776, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 351,  Mean reward: -2.8333333333333335, Mean Entropy: 0.9802401065826416, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.47s
Iteration: 352,  Mean reward: -4.232558139534884, Mean Entropy: 0.9672607183456421, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 353,  Mean reward: -4.052083333333333, Mean Entropy: 0.9793987274169922, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 354,  Mean reward: -3.369047619047619, Mean Entropy: 0.9616637229919434, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 355,  Mean reward: -1.2857142857142858, Mean Entropy: 0.8606265783309937, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 356,  Mean reward: -2.4375, Mean Entropy: 0.9306906461715698, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 357,  Mean reward: -5.034883720930233, Mean Entropy: 0.8912215232849121, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 358,  Mean reward: -1.872093023255814, Mean Entropy: 0.9642328023910522, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 359,  Mean reward: -2.7674418604651163, Mean Entropy: 0.9225006103515625, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 360,  Mean reward: -0.47674418604651164, Mean Entropy: 0.8579564094543457, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 361,  Mean reward: -2.7613636363636362, Mean Entropy: 0.9040178656578064, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 362,  Mean reward: -4.170731707317073, Mean Entropy: 0.8691631555557251, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 363,  Mean reward: -7.3625, Mean Entropy: 0.8542680740356445, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 364,  Mean reward: -0.84, Mean Entropy: 0.9014352560043335, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 365,  Mean reward: -7.0875, Mean Entropy: 0.945382297039032, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 366,  Mean reward: -4.9875, Mean Entropy: 0.8574806451797485, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 367,  Mean reward: -5.097560975609756, Mean Entropy: 0.8703784942626953, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 368,  Mean reward: -5.456521739130435, Mean Entropy: 0.8552526235580444, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 369,  Mean reward: -2.6914893617021276, Mean Entropy: 0.8466118574142456, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 370,  Mean reward: -6.205128205128205, Mean Entropy: 0.523581862449646, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 371,  Mean reward: -0.5, Mean Entropy: 0.9340159893035889, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 372,  Mean reward: -5.846153846153846, Mean Entropy: 0.7771782875061035, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 373,  Mean reward: -3.902439024390244, Mean Entropy: 0.7498421669006348, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 374,  Mean reward: -3.75, Mean Entropy: 0.6810280084609985, complete_episode_count: 46.0, Gather time: 0.67s, Train time: 1.45s
Iteration: 375,  Mean reward: -0.9433962264150944, Mean Entropy: 0.7637548446655273, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 376,  Mean reward: -2.2386363636363638, Mean Entropy: 0.812324047088623, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 377,  Mean reward: -5.733333333333333, Mean Entropy: 0.7501899600028992, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 378,  Mean reward: -1.04, Mean Entropy: 0.765128493309021, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 379,  Mean reward: 0.25555555555555554, Mean Entropy: 0.9112417101860046, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 380,  Mean reward: -3.727272727272727, Mean Entropy: 0.6210376620292664, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 381,  Mean reward: -0.30392156862745096, Mean Entropy: 0.8845860958099365, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 382,  Mean reward: -6.775, Mean Entropy: 0.801334023475647, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 383,  Mean reward: -7.141025641025641, Mean Entropy: 0.7700738906860352, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 384,  Mean reward: -4.894736842105263, Mean Entropy: 0.8341953158378601, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 385,  Mean reward: -3.3625, Mean Entropy: 0.873677670955658, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 386,  Mean reward: -4.6625, Mean Entropy: 0.7869333028793335, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 387,  Mean reward: -3.375, Mean Entropy: 0.912473201751709, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 388,  Mean reward: -3.0121951219512195, Mean Entropy: 0.7695145010948181, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 389,  Mean reward: -1.1071428571428572, Mean Entropy: 0.8100360631942749, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 390,  Mean reward: -2.159090909090909, Mean Entropy: 0.7582799792289734, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 391,  Mean reward: -3.1666666666666665, Mean Entropy: 0.7951793670654297, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 392,  Mean reward: -0.8898305084745762, Mean Entropy: 0.8280321955680847, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 393,  Mean reward: -3.7857142857142856, Mean Entropy: 0.8841099143028259, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 394,  Mean reward: -3.0816326530612246, Mean Entropy: 0.8618078231811523, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 395,  Mean reward: -5.5875, Mean Entropy: 0.7519232630729675, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 396,  Mean reward: -2.1363636363636362, Mean Entropy: 0.7882389426231384, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 397,  Mean reward: 1.1607142857142858, Mean Entropy: 0.7790416479110718, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 398,  Mean reward: -7.6875, Mean Entropy: 0.7923605442047119, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.67s
Iteration: 399,  Mean reward: -3.25, Mean Entropy: 0.9479743838310242, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 400,  Mean reward: -4.2926829268292686, Mean Entropy: 0.8273892402648926, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -3.4069767441860463, Mean Entropy: 0.8621615171432495, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 402,  Mean reward: -0.3645833333333333, Mean Entropy: 0.827643096446991, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 403,  Mean reward: -5.420454545454546, Mean Entropy: 0.749575138092041, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.60s
Iteration: 404,  Mean reward: 0.05319148936170213, Mean Entropy: 0.8799532651901245, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 405,  Mean reward: -4.475, Mean Entropy: 0.9419540166854858, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 406,  Mean reward: -8.420454545454545, Mean Entropy: 0.9055169820785522, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 407,  Mean reward: -4.558139534883721, Mean Entropy: 0.9052981734275818, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 408,  Mean reward: -1.5476190476190477, Mean Entropy: 0.9041702151298523, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 409,  Mean reward: -5.136363636363637, Mean Entropy: 0.9674142599105835, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 410,  Mean reward: -3.951219512195122, Mean Entropy: 0.9030323028564453, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 411,  Mean reward: -5.045454545454546, Mean Entropy: 0.882384717464447, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 412,  Mean reward: -4.341463414634147, Mean Entropy: 0.9608401656150818, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 413,  Mean reward: -5.404761904761905, Mean Entropy: 0.9255499839782715, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 414,  Mean reward: -2.926829268292683, Mean Entropy: 0.8941448926925659, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 415,  Mean reward: -3.4, Mean Entropy: 0.9532880783081055, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 416,  Mean reward: -2.0357142857142856, Mean Entropy: 0.9034726619720459, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 417,  Mean reward: -0.9625, Mean Entropy: 0.9159151911735535, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 418,  Mean reward: -5.75531914893617, Mean Entropy: 0.9299740791320801, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.58s
Iteration: 419,  Mean reward: -0.9418604651162791, Mean Entropy: 0.9692255854606628, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 420,  Mean reward: -2.225, Mean Entropy: 0.8881070017814636, complete_episode_count: 40.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 421,  Mean reward: -2.9615384615384617, Mean Entropy: 0.8714760541915894, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 422,  Mean reward: 2.1382978723404253, Mean Entropy: 0.8487550020217896, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 423,  Mean reward: -3.388888888888889, Mean Entropy: 0.8623013496398926, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 424,  Mean reward: -2.0952380952380953, Mean Entropy: 0.8328983187675476, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 425,  Mean reward: -3.872093023255814, Mean Entropy: 0.8321658968925476, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 426,  Mean reward: 1.0, Mean Entropy: 0.8092269897460938, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 427,  Mean reward: -0.5357142857142857, Mean Entropy: 0.8227316737174988, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 428,  Mean reward: -8.421052631578947, Mean Entropy: 0.5339483618736267, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 429,  Mean reward: -6.038461538461538, Mean Entropy: 0.8378794193267822, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.45s
Iteration: 430,  Mean reward: -6.355263157894737, Mean Entropy: 0.9250503778457642, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 431,  Mean reward: -1.9883720930232558, Mean Entropy: 0.9268430471420288, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 432,  Mean reward: -4.975609756097561, Mean Entropy: 0.9568278789520264, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 433,  Mean reward: -4.848837209302325, Mean Entropy: 0.9124060869216919, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 434,  Mean reward: -3.261904761904762, Mean Entropy: 0.979736328125, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 435,  Mean reward: -5.357142857142857, Mean Entropy: 0.8050710558891296, complete_episode_count: 35.0, Gather time: 0.54s, Train time: 1.68s
Iteration: 436,  Mean reward: -3.7282608695652173, Mean Entropy: 0.8456038236618042, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 437,  Mean reward: -0.6632653061224489, Mean Entropy: 0.8414427042007446, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 438,  Mean reward: -3.0416666666666665, Mean Entropy: 0.908183753490448, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 439,  Mean reward: -5.2727272727272725, Mean Entropy: 0.7474149465560913, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 440,  Mean reward: -4.654761904761905, Mean Entropy: 0.853979766368866, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 441,  Mean reward: -3.311111111111111, Mean Entropy: 0.8979997634887695, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 442,  Mean reward: -3.130434782608696, Mean Entropy: 0.759087324142456, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 443,  Mean reward: -1.8909090909090909, Mean Entropy: 0.8468384742736816, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 444,  Mean reward: -3.1862745098039214, Mean Entropy: 0.8531668186187744, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 445,  Mean reward: -2.2604166666666665, Mean Entropy: 0.9307288527488708, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 446,  Mean reward: -5.404255319148936, Mean Entropy: 0.8805669546127319, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 447,  Mean reward: -3.2717391304347827, Mean Entropy: 0.9000759124755859, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 448,  Mean reward: -5.174418604651163, Mean Entropy: 0.8949422836303711, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 449,  Mean reward: -2.1808510638297873, Mean Entropy: 0.8756766319274902, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 450,  Mean reward: -0.38636363636363635, Mean Entropy: 0.846988320350647, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 451,  Mean reward: -2.3076923076923075, Mean Entropy: 0.8876770734786987, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 452,  Mean reward: -3.772727272727273, Mean Entropy: 0.9230844378471375, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.47s
Iteration: 453,  Mean reward: -0.75, Mean Entropy: 0.8719854354858398, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 454,  Mean reward: -6.53, Mean Entropy: 0.9284531474113464, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 455,  Mean reward: -4.9222222222222225, Mean Entropy: 0.8964459300041199, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 456,  Mean reward: -1.4583333333333333, Mean Entropy: 0.8798155188560486, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 457,  Mean reward: -3.2346938775510203, Mean Entropy: 0.8056553602218628, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 458,  Mean reward: -3.9767441860465116, Mean Entropy: 0.8788961172103882, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 459,  Mean reward: -1.5638297872340425, Mean Entropy: 0.9131366610527039, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 460,  Mean reward: -2.588235294117647, Mean Entropy: 0.7690753936767578, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 461,  Mean reward: 0.7555555555555555, Mean Entropy: 0.7960357666015625, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.46s
Iteration: 462,  Mean reward: -2.21875, Mean Entropy: 0.8655858039855957, complete_episode_count: 48.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 463,  Mean reward: 0.7209302325581395, Mean Entropy: 0.7916709184646606, complete_episode_count: 43.0, Gather time: 0.59s, Train time: 1.46s
Iteration: 464,  Mean reward: -1.0909090909090908, Mean Entropy: 0.7282611727714539, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.45s
Iteration: 465,  Mean reward: -0.79, Mean Entropy: 0.7999058961868286, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 466,  Mean reward: 2.7452830188679247, Mean Entropy: 0.6361754536628723, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 467,  Mean reward: 1.4134615384615385, Mean Entropy: 0.7375832796096802, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 468,  Mean reward: -3.69811320754717, Mean Entropy: 0.7664114832878113, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 469,  Mean reward: -1.3061224489795917, Mean Entropy: 0.7816221117973328, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 470,  Mean reward: 1.0612244897959184, Mean Entropy: 0.7341781854629517, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.71s
Iteration: 471,  Mean reward: 2.2830188679245285, Mean Entropy: 0.6565955877304077, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 472,  Mean reward: -1.2272727272727273, Mean Entropy: 0.6223318576812744, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 473,  Mean reward: 1.9915254237288136, Mean Entropy: 0.6136519908905029, complete_episode_count: 59.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 474,  Mean reward: 3.3524590163934427, Mean Entropy: 0.4669223725795746, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 475,  Mean reward: 1.0, Mean Entropy: 0.6942827105522156, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 476,  Mean reward: 1.3272727272727274, Mean Entropy: 0.655465841293335, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 477,  Mean reward: 0.6792452830188679, Mean Entropy: 0.6938904523849487, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 478,  Mean reward: 2.8796296296296298, Mean Entropy: 0.7379280924797058, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 479,  Mean reward: 0.018518518518518517, Mean Entropy: 0.7567062377929688, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 480,  Mean reward: 2.2, Mean Entropy: 0.771073043346405, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 481,  Mean reward: 1.6754385964912282, Mean Entropy: 0.6382262706756592, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 482,  Mean reward: -0.3793103448275862, Mean Entropy: 0.7305053472518921, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 483,  Mean reward: 0.4732142857142857, Mean Entropy: 0.7244834899902344, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 484,  Mean reward: 0.6886792452830188, Mean Entropy: 0.744145393371582, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 485,  Mean reward: 0.16346153846153846, Mean Entropy: 0.7816238403320312, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 486,  Mean reward: 1.1634615384615385, Mean Entropy: 0.7384423017501831, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 487,  Mean reward: 2.1481481481481484, Mean Entropy: 0.7322349548339844, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 488,  Mean reward: 1.2980769230769231, Mean Entropy: 0.8065404891967773, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 489,  Mean reward: 1.7941176470588236, Mean Entropy: 0.7532965540885925, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 490,  Mean reward: 0.057692307692307696, Mean Entropy: 0.7713780403137207, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 491,  Mean reward: -0.01818181818181818, Mean Entropy: 0.7227053642272949, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 492,  Mean reward: 0.3645833333333333, Mean Entropy: 0.8760703802108765, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 493,  Mean reward: 2.4705882352941178, Mean Entropy: 0.8160775899887085, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 494,  Mean reward: -0.7448979591836735, Mean Entropy: 0.7578359842300415, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 495,  Mean reward: 1.3660714285714286, Mean Entropy: 0.725712239742279, complete_episode_count: 56.0, Gather time: 0.56s, Train time: 1.43s
Iteration: 496,  Mean reward: 2.690909090909091, Mean Entropy: 0.6991720199584961, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 497,  Mean reward: 2.6923076923076925, Mean Entropy: 0.7520726919174194, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 498,  Mean reward: 2.560344827586207, Mean Entropy: 0.6881281733512878, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 499,  Mean reward: 2.4134615384615383, Mean Entropy: 0.762354850769043, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 500,  Mean reward: 1.0851063829787233, Mean Entropy: 0.795210063457489, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.40s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 1.1354166666666667, Mean Entropy: 0.7324435114860535, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 502,  Mean reward: 1.6226415094339623, Mean Entropy: 0.7352051734924316, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 503,  Mean reward: -0.20408163265306123, Mean Entropy: 0.7407299876213074, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 504,  Mean reward: 1.5686274509803921, Mean Entropy: 0.7225855588912964, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 505,  Mean reward: 3.1538461538461537, Mean Entropy: 0.74293053150177, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.66s
Iteration: 506,  Mean reward: 1.4019607843137254, Mean Entropy: 0.7361390590667725, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 507,  Mean reward: 2.75, Mean Entropy: 0.6767891645431519, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 508,  Mean reward: 2.603448275862069, Mean Entropy: 0.6707186698913574, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 509,  Mean reward: 0.20212765957446807, Mean Entropy: 0.7241140604019165, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 510,  Mean reward: 4.008333333333334, Mean Entropy: 0.6969663500785828, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 511,  Mean reward: 2.225806451612903, Mean Entropy: 0.6267358660697937, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 512,  Mean reward: 0.4722222222222222, Mean Entropy: 0.7130786180496216, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 513,  Mean reward: 2.3684210526315788, Mean Entropy: 0.7721025943756104, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 514,  Mean reward: 2.7966101694915255, Mean Entropy: 0.7353585958480835, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 515,  Mean reward: 0.55, Mean Entropy: 0.7195957899093628, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 516,  Mean reward: 1.3, Mean Entropy: 0.7431797981262207, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 517,  Mean reward: 1.8265306122448979, Mean Entropy: 0.7347651720046997, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 518,  Mean reward: 1.1764705882352942, Mean Entropy: 0.8194419741630554, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 519,  Mean reward: 3.792452830188679, Mean Entropy: 0.8009745478630066, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 520,  Mean reward: -0.5851063829787234, Mean Entropy: 0.7386150360107422, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 521,  Mean reward: 2.12, Mean Entropy: 0.7519888281822205, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 522,  Mean reward: 4.990909090909091, Mean Entropy: 0.715140700340271, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 523,  Mean reward: 3.0816326530612246, Mean Entropy: 0.7567047476768494, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.38s
Iteration: 524,  Mean reward: 3.423076923076923, Mean Entropy: 0.73588627576828, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.38s
Iteration: 525,  Mean reward: 2.843137254901961, Mean Entropy: 0.7390446066856384, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 526,  Mean reward: 3.519607843137255, Mean Entropy: 0.6734258532524109, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 527,  Mean reward: 4.326530612244898, Mean Entropy: 0.6561365127563477, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 528,  Mean reward: 3.0980392156862746, Mean Entropy: 0.6110864877700806, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 529,  Mean reward: 4.370689655172414, Mean Entropy: 0.5968970656394958, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 530,  Mean reward: 4.0636363636363635, Mean Entropy: 0.7104615569114685, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 531,  Mean reward: 4.296296296296297, Mean Entropy: 0.7035092711448669, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 532,  Mean reward: 2.269230769230769, Mean Entropy: 0.7039939761161804, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 533,  Mean reward: 2.4642857142857144, Mean Entropy: 0.8052834272384644, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 534,  Mean reward: 0.5909090909090909, Mean Entropy: 0.7375293970108032, complete_episode_count: 44.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 535,  Mean reward: 3.095744680851064, Mean Entropy: 0.7235090136528015, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 536,  Mean reward: 2.1666666666666665, Mean Entropy: 0.6892813444137573, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 537,  Mean reward: 3.5416666666666665, Mean Entropy: 0.6234021186828613, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 538,  Mean reward: 1.5096153846153846, Mean Entropy: 0.7142521142959595, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 539,  Mean reward: 1.836734693877551, Mean Entropy: 0.7572557330131531, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 540,  Mean reward: 3.27, Mean Entropy: 0.7458615303039551, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.61s
Iteration: 541,  Mean reward: 3.107142857142857, Mean Entropy: 0.610917329788208, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 542,  Mean reward: 2.99, Mean Entropy: 0.6675969958305359, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 543,  Mean reward: 3.2653061224489797, Mean Entropy: 0.7024153470993042, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 544,  Mean reward: 2.1666666666666665, Mean Entropy: 0.6171666383743286, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 545,  Mean reward: 4.508620689655173, Mean Entropy: 0.5936253070831299, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 546,  Mean reward: 3.452830188679245, Mean Entropy: 0.6181275844573975, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 547,  Mean reward: 2.6403508771929824, Mean Entropy: 0.7497722506523132, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 548,  Mean reward: 3.2333333333333334, Mean Entropy: 0.7176138758659363, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 549,  Mean reward: 0.9761904761904762, Mean Entropy: 0.711913526058197, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 550,  Mean reward: 5.017857142857143, Mean Entropy: 0.6190685033798218, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 551,  Mean reward: 2.7203389830508473, Mean Entropy: 0.7062755823135376, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 552,  Mean reward: 4.183673469387755, Mean Entropy: 0.6203884482383728, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 553,  Mean reward: 5.264150943396227, Mean Entropy: 0.6344299912452698, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 554,  Mean reward: -0.22727272727272727, Mean Entropy: 0.7291433215141296, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 555,  Mean reward: 1.173913043478261, Mean Entropy: 0.7759610414505005, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 556,  Mean reward: 2.4910714285714284, Mean Entropy: 0.6871749758720398, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 557,  Mean reward: 3.9489795918367347, Mean Entropy: 0.6579370498657227, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 558,  Mean reward: 4.018867924528302, Mean Entropy: 0.5572230815887451, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 559,  Mean reward: 3.881818181818182, Mean Entropy: 0.6929120421409607, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 560,  Mean reward: 1.81, Mean Entropy: 0.7302705645561218, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 561,  Mean reward: -0.7790697674418605, Mean Entropy: 0.7791334390640259, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 562,  Mean reward: 0.25, Mean Entropy: 0.7424921989440918, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 563,  Mean reward: 2.0510204081632653, Mean Entropy: 0.6639862060546875, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 564,  Mean reward: 3.163265306122449, Mean Entropy: 0.6668908596038818, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 565,  Mean reward: 3.481818181818182, Mean Entropy: 0.6428745985031128, complete_episode_count: 55.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 566,  Mean reward: 2.2142857142857144, Mean Entropy: 0.6499315500259399, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.44s
Iteration: 567,  Mean reward: 4.452830188679245, Mean Entropy: 0.6328099370002747, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 568,  Mean reward: 2.4893617021276597, Mean Entropy: 0.6443936824798584, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 569,  Mean reward: 4.588235294117647, Mean Entropy: 0.7072973251342773, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 570,  Mean reward: -1.3518518518518519, Mean Entropy: 0.5943468809127808, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 571,  Mean reward: 1.4326923076923077, Mean Entropy: 0.788689136505127, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.43s
Iteration: 572,  Mean reward: 4.046296296296297, Mean Entropy: 0.58189857006073, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 573,  Mean reward: 3.688679245283019, Mean Entropy: 0.674778938293457, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 574,  Mean reward: 3.3979591836734695, Mean Entropy: 0.6705888509750366, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 575,  Mean reward: 0.40625, Mean Entropy: 0.6845688819885254, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.65s
Iteration: 576,  Mean reward: 4.0, Mean Entropy: 0.755042552947998, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 577,  Mean reward: 4.223214285714286, Mean Entropy: 0.630133330821991, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 578,  Mean reward: 0.8018867924528302, Mean Entropy: 0.7018893957138062, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 579,  Mean reward: 3.2777777777777777, Mean Entropy: 0.5951332449913025, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 580,  Mean reward: 2.5454545454545454, Mean Entropy: 0.6671985387802124, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 581,  Mean reward: 1.8823529411764706, Mean Entropy: 0.7149256467819214, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 582,  Mean reward: 3.561224489795918, Mean Entropy: 0.5440554022789001, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 583,  Mean reward: 4.0, Mean Entropy: 0.7170811891555786, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 584,  Mean reward: 1.9607843137254901, Mean Entropy: 0.6780268549919128, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 585,  Mean reward: 0.9270833333333334, Mean Entropy: 0.626384973526001, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 586,  Mean reward: 4.096153846153846, Mean Entropy: 0.5765178799629211, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 587,  Mean reward: 4.407407407407407, Mean Entropy: 0.5510870218276978, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 588,  Mean reward: 3.855769230769231, Mean Entropy: 0.6040024757385254, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 589,  Mean reward: 4.462962962962963, Mean Entropy: 0.5937557220458984, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 590,  Mean reward: 2.4456521739130435, Mean Entropy: 0.5911680459976196, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 591,  Mean reward: 4.769230769230769, Mean Entropy: 0.6515463590621948, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 592,  Mean reward: 3.481132075471698, Mean Entropy: 0.58592289686203, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 593,  Mean reward: 4.472222222222222, Mean Entropy: 0.591660737991333, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 594,  Mean reward: 4.87037037037037, Mean Entropy: 0.5835995078086853, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 595,  Mean reward: 0.33653846153846156, Mean Entropy: 0.7461241483688354, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 596,  Mean reward: 3.3444444444444446, Mean Entropy: 0.760854184627533, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 597,  Mean reward: 2.095744680851064, Mean Entropy: 0.6625356674194336, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 598,  Mean reward: 4.273584905660377, Mean Entropy: 0.5614364147186279, complete_episode_count: 53.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 599,  Mean reward: 3.125, Mean Entropy: 0.6138044595718384, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 600,  Mean reward: 4.230769230769231, Mean Entropy: 0.6281644105911255, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.41s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 5.612068965517241, Mean Entropy: 0.6014485955238342, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 602,  Mean reward: -0.90625, Mean Entropy: 0.55329430103302, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 603,  Mean reward: 4.084905660377358, Mean Entropy: 0.582645058631897, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 604,  Mean reward: 1.82, Mean Entropy: 0.6644148826599121, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 605,  Mean reward: 3.2755102040816326, Mean Entropy: 0.6364263296127319, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.43s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 606,  Mean reward: 6.116666666666666, Mean Entropy: 0.8240672945976257, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.41s
Iteration: 607,  Mean reward: -4.785714285714286, Mean Entropy: 0.7762120962142944, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 608,  Mean reward: 0.0851063829787234, Mean Entropy: 0.6889628171920776, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 609,  Mean reward: 1.3303571428571428, Mean Entropy: 0.6355607509613037, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 610,  Mean reward: 4.415094339622642, Mean Entropy: 0.5014035701751709, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.55s
Iteration: 611,  Mean reward: 4.39, Mean Entropy: 0.7128056883811951, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 612,  Mean reward: 5.214285714285714, Mean Entropy: 0.7049484252929688, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 613,  Mean reward: -3.1203703703703702, Mean Entropy: 0.6903895139694214, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 614,  Mean reward: 3.3222222222222224, Mean Entropy: 0.7575377225875854, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 615,  Mean reward: 1.846938775510204, Mean Entropy: 0.8212884664535522, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 616,  Mean reward: 2.9423076923076925, Mean Entropy: 0.698495090007782, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 617,  Mean reward: 4.13, Mean Entropy: 0.709648847579956, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 618,  Mean reward: 4.009090909090909, Mean Entropy: 0.7645663619041443, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 619,  Mean reward: 2.9107142857142856, Mean Entropy: 0.5874514579772949, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 620,  Mean reward: 3.4489795918367347, Mean Entropy: 0.6346012949943542, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 621,  Mean reward: 3.46, Mean Entropy: 0.7072840929031372, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 622,  Mean reward: 1.3953488372093024, Mean Entropy: 0.6775292754173279, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 623,  Mean reward: -2.4054054054054053, Mean Entropy: 0.7899156808853149, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 624,  Mean reward: -4.290697674418604, Mean Entropy: 0.9237650632858276, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 625,  Mean reward: -3.225, Mean Entropy: 0.8912329077720642, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 626,  Mean reward: -4.013513513513513, Mean Entropy: 0.8965334892272949, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 627,  Mean reward: -5.15, Mean Entropy: 0.9215333461761475, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 628,  Mean reward: -5.283783783783784, Mean Entropy: 0.9508868455886841, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 629,  Mean reward: -5.6976744186046515, Mean Entropy: 0.9285674095153809, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 630,  Mean reward: -5.658536585365853, Mean Entropy: 0.9281796216964722, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 631,  Mean reward: -4.940476190476191, Mean Entropy: 0.958645224571228, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 632,  Mean reward: -3.9375, Mean Entropy: 0.9282225370407104, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 633,  Mean reward: -4.256410256410256, Mean Entropy: 0.8945116996765137, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 634,  Mean reward: -5.115384615384615, Mean Entropy: 0.9085096120834351, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 635,  Mean reward: -5.329268292682927, Mean Entropy: 0.9445191621780396, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 636,  Mean reward: -5.2125, Mean Entropy: 0.927807092666626, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 637,  Mean reward: -6.104651162790698, Mean Entropy: 0.9248156547546387, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 638,  Mean reward: -5.593023255813954, Mean Entropy: 0.9131550788879395, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 639,  Mean reward: -4.963414634146342, Mean Entropy: 0.8788748979568481, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 640,  Mean reward: -4.024390243902439, Mean Entropy: 0.8791885375976562, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 641,  Mean reward: -4.8375, Mean Entropy: 0.9490179419517517, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 642,  Mean reward: -6.0394736842105265, Mean Entropy: 0.9456816911697388, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 643,  Mean reward: -2.448717948717949, Mean Entropy: 0.8730936050415039, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 644,  Mean reward: -4.3375, Mean Entropy: 0.8534214496612549, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 645,  Mean reward: -6.081081081081081, Mean Entropy: 0.8685833811759949, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 646,  Mean reward: -5.186046511627907, Mean Entropy: 0.8618277907371521, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.56s
Iteration: 647,  Mean reward: -6.5, Mean Entropy: 0.9158021807670593, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 648,  Mean reward: -5.554054054054054, Mean Entropy: 0.9189122915267944, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 649,  Mean reward: -6.75, Mean Entropy: 0.9010190963745117, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 650,  Mean reward: -2.7738095238095237, Mean Entropy: 0.8945674896240234, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 651,  Mean reward: -4.282051282051282, Mean Entropy: 0.9171748161315918, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 652,  Mean reward: -3.923076923076923, Mean Entropy: 0.8772408366203308, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 653,  Mean reward: -2.4078947368421053, Mean Entropy: 1.023389220237732, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 654,  Mean reward: -3.9186046511627906, Mean Entropy: 0.8863914012908936, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 655,  Mean reward: -4.095238095238095, Mean Entropy: 0.9065009951591492, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 656,  Mean reward: -5.059523809523809, Mean Entropy: 0.8795134425163269, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 657,  Mean reward: -4.5125, Mean Entropy: 0.9069398641586304, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 658,  Mean reward: -2.0, Mean Entropy: 1.0046072006225586, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 659,  Mean reward: -3.7023809523809526, Mean Entropy: 0.8905815482139587, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 660,  Mean reward: -2.395348837209302, Mean Entropy: 0.9214531779289246, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 661,  Mean reward: -2.3625, Mean Entropy: 0.9143532514572144, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 662,  Mean reward: -4.193181818181818, Mean Entropy: 0.9596008658409119, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 663,  Mean reward: -5.463414634146342, Mean Entropy: 0.9660741090774536, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 664,  Mean reward: -2.451219512195122, Mean Entropy: 0.9321538805961609, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 665,  Mean reward: -2.6739130434782608, Mean Entropy: 0.9159978628158569, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 666,  Mean reward: -5.035714285714286, Mean Entropy: 0.9184737205505371, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 667,  Mean reward: -4.378048780487805, Mean Entropy: 0.9365150332450867, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 668,  Mean reward: -2.3452380952380953, Mean Entropy: 0.9800207018852234, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 669,  Mean reward: -4.0813953488372094, Mean Entropy: 0.9627489447593689, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 670,  Mean reward: -4.72093023255814, Mean Entropy: 0.8878723978996277, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 671,  Mean reward: -3.433333333333333, Mean Entropy: 0.8943964838981628, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 672,  Mean reward: -3.5, Mean Entropy: 0.9268379211425781, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 673,  Mean reward: -5.217948717948718, Mean Entropy: 0.848080575466156, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 674,  Mean reward: -2.872093023255814, Mean Entropy: 0.9311850070953369, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 675,  Mean reward: -4.670731707317073, Mean Entropy: 0.9271757006645203, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 676,  Mean reward: -3.986842105263158, Mean Entropy: 0.8872997164726257, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 677,  Mean reward: -5.166666666666667, Mean Entropy: 0.9271652698516846, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 678,  Mean reward: -5.678571428571429, Mean Entropy: 0.9354304671287537, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 679,  Mean reward: -4.2976190476190474, Mean Entropy: 0.9185700416564941, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 680,  Mean reward: -4.653846153846154, Mean Entropy: 0.9642822742462158, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 681,  Mean reward: -3.231707317073171, Mean Entropy: 0.923972487449646, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 682,  Mean reward: -4.107142857142857, Mean Entropy: 0.9186648726463318, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 683,  Mean reward: -4.909090909090909, Mean Entropy: 0.955439567565918, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.58s
Iteration: 684,  Mean reward: -3.4204545454545454, Mean Entropy: 0.9746216535568237, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 685,  Mean reward: -1.5340909090909092, Mean Entropy: 0.9812759160995483, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 686,  Mean reward: -2.8048780487804876, Mean Entropy: 0.9356938600540161, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 687,  Mean reward: -5.925, Mean Entropy: 0.9745593070983887, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 688,  Mean reward: -2.840909090909091, Mean Entropy: 0.9382538795471191, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 689,  Mean reward: -3.244186046511628, Mean Entropy: 0.952960193157196, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 690,  Mean reward: -5.142857142857143, Mean Entropy: 0.8735888004302979, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 691,  Mean reward: -2.9642857142857144, Mean Entropy: 0.8952791690826416, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 692,  Mean reward: -4.925, Mean Entropy: 0.9458162188529968, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 693,  Mean reward: -2.9878048780487805, Mean Entropy: 0.9458293914794922, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 694,  Mean reward: -5.3, Mean Entropy: 0.9456612467765808, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 695,  Mean reward: -3.9625, Mean Entropy: 1.0028069019317627, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 696,  Mean reward: -5.297297297297297, Mean Entropy: 0.9298025965690613, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 697,  Mean reward: -3.75, Mean Entropy: 0.9955193400382996, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 698,  Mean reward: -3.761904761904762, Mean Entropy: 0.902373731136322, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 699,  Mean reward: -4.627906976744186, Mean Entropy: 0.9384293556213379, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 700,  Mean reward: -5.397435897435898, Mean Entropy: 0.931350588798523, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: -4.5476190476190474, Mean Entropy: 0.9241719245910645, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 702,  Mean reward: -2.533333333333333, Mean Entropy: 0.9236253499984741, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 703,  Mean reward: -6.125, Mean Entropy: 0.9671553373336792, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 704,  Mean reward: -6.128205128205129, Mean Entropy: 0.8447672724723816, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 705,  Mean reward: -6.232558139534884, Mean Entropy: 0.9097191691398621, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 706,  Mean reward: -4.790697674418604, Mean Entropy: 0.9447282552719116, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 707,  Mean reward: -3.2875, Mean Entropy: 0.9384584426879883, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 708,  Mean reward: -5.317073170731708, Mean Entropy: 0.9886471629142761, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 709,  Mean reward: -2.8295454545454546, Mean Entropy: 0.9023089408874512, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 710,  Mean reward: -3.792682926829268, Mean Entropy: 0.9096423983573914, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 711,  Mean reward: -4.290697674418604, Mean Entropy: 0.89518803358078, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 712,  Mean reward: -4.290697674418604, Mean Entropy: 0.9310253858566284, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 713,  Mean reward: -4.976744186046512, Mean Entropy: 0.9022237062454224, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 714,  Mean reward: -5.6875, Mean Entropy: 0.9458138942718506, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 715,  Mean reward: -5.0, Mean Entropy: 0.974555253982544, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 716,  Mean reward: -6.410256410256411, Mean Entropy: 0.851798951625824, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 717,  Mean reward: -5.313953488372093, Mean Entropy: 0.9386024475097656, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 718,  Mean reward: -2.8026315789473686, Mean Entropy: 0.9745163917541504, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 719,  Mean reward: -3.738095238095238, Mean Entropy: 0.9457255601882935, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 720,  Mean reward: -5.2926829268292686, Mean Entropy: 0.9168328642845154, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.54s
Iteration: 721,  Mean reward: -5.038461538461538, Mean Entropy: 0.9169034361839294, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 722,  Mean reward: -4.416666666666667, Mean Entropy: 0.9743307828903198, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 723,  Mean reward: -5.175, Mean Entropy: 0.945797860622406, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 724,  Mean reward: -3.022727272727273, Mean Entropy: 1.0076388120651245, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 725,  Mean reward: -5.166666666666667, Mean Entropy: 0.9291853904724121, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 726,  Mean reward: -5.3375, Mean Entropy: 0.930982232093811, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 727,  Mean reward: -4.109756097560975, Mean Entropy: 0.9313408732414246, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 728,  Mean reward: -7.317073170731708, Mean Entropy: 1.0099366903305054, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 729,  Mean reward: -3.725, Mean Entropy: 0.9386142492294312, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 730,  Mean reward: -5.5125, Mean Entropy: 0.9530459642410278, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 731,  Mean reward: -4.353658536585366, Mean Entropy: 0.9599722623825073, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 732,  Mean reward: -2.7386363636363638, Mean Entropy: 0.9021608233451843, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 733,  Mean reward: -4.653846153846154, Mean Entropy: 0.9742201566696167, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 734,  Mean reward: -1.8255813953488371, Mean Entropy: 0.924176812171936, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 735,  Mean reward: -4.2073170731707314, Mean Entropy: 0.9819382429122925, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 736,  Mean reward: -3.546511627906977, Mean Entropy: 0.9530400037765503, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 737,  Mean reward: -4.4523809523809526, Mean Entropy: 0.9385566711425781, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 738,  Mean reward: -3.825, Mean Entropy: 0.916892409324646, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 739,  Mean reward: -3.6538461538461537, Mean Entropy: 0.924152135848999, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 740,  Mean reward: -6.538461538461538, Mean Entropy: 0.9530627131462097, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 741,  Mean reward: -4.988636363636363, Mean Entropy: 1.0036009550094604, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 742,  Mean reward: -5.383720930232558, Mean Entropy: 0.9674529433250427, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 743,  Mean reward: -2.3375, Mean Entropy: 0.9458401203155518, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 744,  Mean reward: -7.0125, Mean Entropy: 0.9819304347038269, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 745,  Mean reward: -3.158536585365854, Mean Entropy: 0.9241785407066345, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 746,  Mean reward: -5.453488372093023, Mean Entropy: 0.9386283159255981, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 747,  Mean reward: -4.341463414634147, Mean Entropy: 1.0396490097045898, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 748,  Mean reward: -6.785714285714286, Mean Entropy: 0.8878986835479736, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 749,  Mean reward: -3.8536585365853657, Mean Entropy: 0.945592999458313, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 750,  Mean reward: -7.23, Mean Entropy: 0.9673723578453064, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 751,  Mean reward: -5.226190476190476, Mean Entropy: 0.9457483291625977, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 752,  Mean reward: -4.476190476190476, Mean Entropy: 0.931402325630188, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 753,  Mean reward: -5.560975609756097, Mean Entropy: 0.9313468337059021, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 754,  Mean reward: -6.551282051282051, Mean Entropy: 0.9382194876670837, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 755,  Mean reward: -3.2888888888888888, Mean Entropy: 0.9312170743942261, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 756,  Mean reward: -3.8452380952380953, Mean Entropy: 0.9096404910087585, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 757,  Mean reward: -5.136363636363637, Mean Entropy: 0.9671005606651306, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 758,  Mean reward: -4.630434782608695, Mean Entropy: 0.9238771200180054, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.60s
Iteration: 759,  Mean reward: -3.2435897435897436, Mean Entropy: 0.9456087350845337, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 760,  Mean reward: -0.8604651162790697, Mean Entropy: 0.9747290015220642, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 761,  Mean reward: -4.4375, Mean Entropy: 0.9241807460784912, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 762,  Mean reward: -6.083333333333333, Mean Entropy: 0.9241397380828857, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 763,  Mean reward: -2.9767441860465116, Mean Entropy: 0.9601925611495972, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 764,  Mean reward: -6.5813953488372094, Mean Entropy: 0.9096686244010925, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 765,  Mean reward: -5.611111111111111, Mean Entropy: 0.9086569547653198, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 766,  Mean reward: -2.426829268292683, Mean Entropy: 0.9671242237091064, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 767,  Mean reward: -5.309523809523809, Mean Entropy: 0.9386047720909119, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 768,  Mean reward: -2.761904761904762, Mean Entropy: 0.9458513259887695, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 769,  Mean reward: -4.951219512195122, Mean Entropy: 0.95306795835495, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 770,  Mean reward: -6.569767441860465, Mean Entropy: 0.9746968150138855, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 771,  Mean reward: -4.453488372093023, Mean Entropy: 0.8952633738517761, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 772,  Mean reward: -3.217948717948718, Mean Entropy: 0.9602762460708618, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 773,  Mean reward: -4.388888888888889, Mean Entropy: 0.97472083568573, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 774,  Mean reward: -5.371794871794871, Mean Entropy: 0.9025315046310425, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 775,  Mean reward: -6.068181818181818, Mean Entropy: 0.9314103126525879, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 776,  Mean reward: -3.0795454545454546, Mean Entropy: 0.9241899251937866, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 777,  Mean reward: -4.395348837209302, Mean Entropy: 0.9675062894821167, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 778,  Mean reward: -5.928571428571429, Mean Entropy: 0.9458370208740234, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 779,  Mean reward: -5.954545454545454, Mean Entropy: 0.9241859912872314, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 780,  Mean reward: -3.8214285714285716, Mean Entropy: 0.9747263789176941, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 781,  Mean reward: -1.6829268292682926, Mean Entropy: 0.9890884160995483, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 782,  Mean reward: -7.77906976744186, Mean Entropy: 1.0035427808761597, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 783,  Mean reward: -6.8625, Mean Entropy: 0.9169495105743408, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 784,  Mean reward: -6.121951219512195, Mean Entropy: 0.9097203016281128, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 785,  Mean reward: -2.1818181818181817, Mean Entropy: 0.9674891233444214, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 786,  Mean reward: -5.186046511627907, Mean Entropy: 0.9384193420410156, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 787,  Mean reward: -5.357142857142857, Mean Entropy: 0.9601945877075195, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 788,  Mean reward: -3.871794871794872, Mean Entropy: 0.8879184722900391, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 789,  Mean reward: -3.2125, Mean Entropy: 0.9301456212997437, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 790,  Mean reward: -6.1125, Mean Entropy: 0.9309366941452026, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 791,  Mean reward: -4.524390243902439, Mean Entropy: 0.9312596321105957, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 792,  Mean reward: -6.975609756097561, Mean Entropy: 0.9024195671081543, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 793,  Mean reward: -7.5131578947368425, Mean Entropy: 0.9383432865142822, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 794,  Mean reward: -5.011627906976744, Mean Entropy: 0.9815036058425903, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 795,  Mean reward: -4.666666666666667, Mean Entropy: 0.9744515419006348, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.57s
Iteration: 796,  Mean reward: -4.85, Mean Entropy: 0.9312626123428345, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 797,  Mean reward: -4.0777777777777775, Mean Entropy: 0.9385382533073425, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 798,  Mean reward: -5.205128205128205, Mean Entropy: 0.989063024520874, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 799,  Mean reward: -3.802325581395349, Mean Entropy: 0.9313063621520996, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 800,  Mean reward: -4.32051282051282, Mean Entropy: 0.8952394723892212, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -4.906976744186046, Mean Entropy: 0.9457948207855225, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 802,  Mean reward: -4.105263157894737, Mean Entropy: 0.9384719729423523, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 803,  Mean reward: -4.315789473684211, Mean Entropy: 0.9087589979171753, complete_episode_count: 38.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 804,  Mean reward: -6.011904761904762, Mean Entropy: 0.9746912717819214, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 805,  Mean reward: -3.311111111111111, Mean Entropy: 0.930772066116333, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 806,  Mean reward: -2.4125, Mean Entropy: 0.902458667755127, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 807,  Mean reward: -4.659090909090909, Mean Entropy: 0.9530431628227234, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 808,  Mean reward: -4.205128205128205, Mean Entropy: 0.9314071536064148, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 809,  Mean reward: -3.453488372093023, Mean Entropy: 0.9963787794113159, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 810,  Mean reward: -4.488095238095238, Mean Entropy: 0.9386043548583984, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 811,  Mean reward: -4.961538461538462, Mean Entropy: 0.9025191068649292, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 812,  Mean reward: -3.6627906976744184, Mean Entropy: 0.9747373461723328, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 813,  Mean reward: -5.554054054054054, Mean Entropy: 0.8880288004875183, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 814,  Mean reward: -2.5875, Mean Entropy: 0.9314128756523132, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 815,  Mean reward: -3.3295454545454546, Mean Entropy: 0.9530676603317261, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 816,  Mean reward: -3.3372093023255816, Mean Entropy: 0.9675120115280151, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 817,  Mean reward: -4.534883720930233, Mean Entropy: 0.8736491203308105, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 818,  Mean reward: -3.090909090909091, Mean Entropy: 0.9386242628097534, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 819,  Mean reward: -4.441860465116279, Mean Entropy: 0.9310989379882812, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 820,  Mean reward: -2.5256410256410255, Mean Entropy: 0.9301328063011169, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 821,  Mean reward: -5.2023809523809526, Mean Entropy: 0.9021891355514526, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 822,  Mean reward: -5.651162790697675, Mean Entropy: 0.931316614151001, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 823,  Mean reward: -3.6904761904761907, Mean Entropy: 0.9159640073776245, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 824,  Mean reward: -5.662162162162162, Mean Entropy: 0.9020018577575684, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 825,  Mean reward: -4.3125, Mean Entropy: 0.8805379867553711, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 826,  Mean reward: -6.162790697674419, Mean Entropy: 0.9662479758262634, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 827,  Mean reward: -3.1555555555555554, Mean Entropy: 0.9600133895874023, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 828,  Mean reward: -6.463414634146342, Mean Entropy: 0.9378724098205566, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 829,  Mean reward: -3.3214285714285716, Mean Entropy: 0.938260555267334, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 830,  Mean reward: -4.4523809523809526, Mean Entropy: 0.9806782603263855, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 831,  Mean reward: -4.725, Mean Entropy: 0.9590842127799988, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 832,  Mean reward: -4.1125, Mean Entropy: 0.9664925336837769, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.55s
Iteration: 833,  Mean reward: -4.621951219512195, Mean Entropy: 0.9669691920280457, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 834,  Mean reward: -5.6875, Mean Entropy: 0.9304930567741394, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 835,  Mean reward: -6.230769230769231, Mean Entropy: 0.9425233602523804, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 836,  Mean reward: -3.3536585365853657, Mean Entropy: 0.9260672926902771, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 837,  Mean reward: -3.189189189189189, Mean Entropy: 0.9207954406738281, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 838,  Mean reward: 2.3653846153846154, Mean Entropy: 0.7660154104232788, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 839,  Mean reward: -6.486111111111111, Mean Entropy: 0.6652616262435913, complete_episode_count: 36.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 840,  Mean reward: -0.3181818181818182, Mean Entropy: 0.8419763445854187, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 841,  Mean reward: -0.9534883720930233, Mean Entropy: 0.7978338003158569, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 842,  Mean reward: 1.5, Mean Entropy: 0.7098010778427124, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 843,  Mean reward: 1.3636363636363635, Mean Entropy: 0.5693681836128235, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 844,  Mean reward: 3.4148936170212765, Mean Entropy: 0.6887229681015015, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 845,  Mean reward: 1.0, Mean Entropy: 0.6593202948570251, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 846,  Mean reward: 3.6382978723404253, Mean Entropy: 0.7711771726608276, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 847,  Mean reward: 1.6442307692307692, Mean Entropy: 0.669167697429657, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 848,  Mean reward: 4.2924528301886795, Mean Entropy: 0.8095313906669617, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 849,  Mean reward: -0.5729166666666666, Mean Entropy: 0.601800262928009, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 850,  Mean reward: 2.5729166666666665, Mean Entropy: 0.5774763226509094, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 851,  Mean reward: 5.669491525423729, Mean Entropy: 0.7269783020019531, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 852,  Mean reward: 2.4464285714285716, Mean Entropy: 0.5741324424743652, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 853,  Mean reward: 4.38, Mean Entropy: 0.6746541857719421, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 854,  Mean reward: 4.201754385964913, Mean Entropy: 0.9555360674858093, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 855,  Mean reward: 1.0465116279069768, Mean Entropy: 0.5160986185073853, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 856,  Mean reward: 3.1538461538461537, Mean Entropy: 0.7027155756950378, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 857,  Mean reward: 2.715686274509804, Mean Entropy: 0.6181451678276062, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 858,  Mean reward: 5.677966101694915, Mean Entropy: 0.4926270842552185, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 859,  Mean reward: 0.016666666666666666, Mean Entropy: 0.42468035221099854, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 860,  Mean reward: 5.072727272727272, Mean Entropy: 0.5245756506919861, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 861,  Mean reward: 5.348214285714286, Mean Entropy: 0.6512213349342346, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 862,  Mean reward: 0.19298245614035087, Mean Entropy: 0.6393696069717407, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 863,  Mean reward: 5.241666666666666, Mean Entropy: 0.407792866230011, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 864,  Mean reward: 5.6440677966101696, Mean Entropy: 0.5622236132621765, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 865,  Mean reward: -2.5531914893617023, Mean Entropy: 0.3632985055446625, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.39s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 866,  Mean reward: 6.349206349206349, Mean Entropy: 0.3055787682533264, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.72s
Iteration: 867,  Mean reward: -0.175, Mean Entropy: 0.35234302282333374, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 0.87s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 868,  Mean reward: 6.359649122807017, Mean Entropy: 0.3836996555328369, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 869,  Mean reward: 3.7966101694915255, Mean Entropy: 0.5062934756278992, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.41s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 870,  Mean reward: 6.387931034482759, Mean Entropy: 0.5110945701599121, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.41s
Iteration: 871,  Mean reward: 1.7549019607843137, Mean Entropy: 0.5051568746566772, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 872,  Mean reward: 3.734042553191489, Mean Entropy: 0.518884539604187, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 873,  Mean reward: 4.433962264150943, Mean Entropy: 0.4985926151275635, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 874,  Mean reward: 6.358333333333333, Mean Entropy: 0.5060087442398071, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 875,  Mean reward: 4.953703703703703, Mean Entropy: 0.4698730409145355, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 876,  Mean reward: 3.6052631578947367, Mean Entropy: 0.5196757316589355, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 877,  Mean reward: 0.8877551020408163, Mean Entropy: 0.5219742059707642, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.41s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 878,  Mean reward: 6.603448275862069, Mean Entropy: 0.4878341555595398, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.40s
Iteration: 879,  Mean reward: 5.064814814814815, Mean Entropy: 0.40926575660705566, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 880,  Mean reward: 1.9017857142857142, Mean Entropy: 0.32377147674560547, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.43s
Iteration: 881,  Mean reward: 4.672727272727273, Mean Entropy: 0.3277805745601654, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 882,  Mean reward: 4.75, Mean Entropy: 0.5997312664985657, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 883,  Mean reward: 4.438775510204081, Mean Entropy: 0.3953835964202881, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 884,  Mean reward: 4.4245283018867925, Mean Entropy: 0.6616958379745483, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 885,  Mean reward: 0.023255813953488372, Mean Entropy: 0.49061307311058044, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 886,  Mean reward: 5.9, Mean Entropy: 0.7438125610351562, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 887,  Mean reward: -4.575757575757576, Mean Entropy: 0.6898936033248901, complete_episode_count: 33.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 888,  Mean reward: 3.9895833333333335, Mean Entropy: 0.5241149663925171, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 889,  Mean reward: 0.872093023255814, Mean Entropy: 0.31638339161872864, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 890,  Mean reward: -3.0535714285714284, Mean Entropy: 0.5498915910720825, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 891,  Mean reward: -5.122222222222222, Mean Entropy: 0.5395269393920898, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 892,  Mean reward: -3.8076923076923075, Mean Entropy: 0.4883803129196167, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 893,  Mean reward: -6.326086956521739, Mean Entropy: 0.6226911544799805, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 894,  Mean reward: 0.5681818181818182, Mean Entropy: 0.6305798292160034, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 895,  Mean reward: -2.369565217391304, Mean Entropy: 0.5407743453979492, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 896,  Mean reward: -0.33783783783783783, Mean Entropy: 0.46873974800109863, complete_episode_count: 37.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 897,  Mean reward: -4.683673469387755, Mean Entropy: 0.5152297616004944, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 898,  Mean reward: -5.066666666666666, Mean Entropy: 0.5590249300003052, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 899,  Mean reward: -5.963414634146342, Mean Entropy: 0.5485605597496033, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 900,  Mean reward: -5.381578947368421, Mean Entropy: 0.7054137587547302, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.39s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -8.121621621621621, Mean Entropy: 0.7003390789031982, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 902,  Mean reward: -5.541666666666667, Mean Entropy: 0.8141993284225464, complete_episode_count: 36.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 903,  Mean reward: -0.3645833333333333, Mean Entropy: 0.7161705493927002, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.57s
Iteration: 904,  Mean reward: -9.80188679245283, Mean Entropy: 0.9563179016113281, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 905,  Mean reward: -6.402439024390244, Mean Entropy: 0.851546049118042, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 906,  Mean reward: -4.597222222222222, Mean Entropy: 0.7955431342124939, complete_episode_count: 36.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 907,  Mean reward: -0.9886363636363636, Mean Entropy: 0.6961301565170288, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 908,  Mean reward: -1.2291666666666667, Mean Entropy: 0.5952772498130798, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 909,  Mean reward: -0.1951219512195122, Mean Entropy: 0.618209183216095, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 910,  Mean reward: 2.1875, Mean Entropy: 0.5015207529067993, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 911,  Mean reward: 4.063492063492063, Mean Entropy: 0.6371400356292725, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 912,  Mean reward: -1.8214285714285714, Mean Entropy: 0.22993728518486023, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 913,  Mean reward: 4.230769230769231, Mean Entropy: 0.36436185240745544, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 914,  Mean reward: 3.72, Mean Entropy: 0.4367676377296448, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 915,  Mean reward: -0.03488372093023256, Mean Entropy: 0.3498867452144623, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 916,  Mean reward: 5.175925925925926, Mean Entropy: 0.4866616129875183, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 917,  Mean reward: 1.3771929824561404, Mean Entropy: 0.5790359973907471, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 918,  Mean reward: -6.877358490566038, Mean Entropy: 0.6713569164276123, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 919,  Mean reward: -7.897435897435898, Mean Entropy: 0.661409318447113, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 920,  Mean reward: -5.071428571428571, Mean Entropy: 0.7521438598632812, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 921,  Mean reward: -6.036585365853658, Mean Entropy: 0.7660359144210815, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 922,  Mean reward: -4.846153846153846, Mean Entropy: 0.6937419176101685, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 923,  Mean reward: -5.131578947368421, Mean Entropy: 0.6786214113235474, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 924,  Mean reward: -5.282051282051282, Mean Entropy: 0.7888171076774597, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 925,  Mean reward: -3.891891891891892, Mean Entropy: 0.7678828239440918, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 926,  Mean reward: -3.9523809523809526, Mean Entropy: 0.8192161321640015, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 927,  Mean reward: -5.0476190476190474, Mean Entropy: 0.8101158142089844, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 928,  Mean reward: -3.5625, Mean Entropy: 0.753682017326355, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 929,  Mean reward: -2.2439024390243905, Mean Entropy: 0.7357913851737976, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 930,  Mean reward: -4.321428571428571, Mean Entropy: 0.8132085800170898, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 931,  Mean reward: -5.7625, Mean Entropy: 0.6372655630111694, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 932,  Mean reward: -6.666666666666667, Mean Entropy: 0.6719262599945068, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 933,  Mean reward: -4.175, Mean Entropy: 0.6802266836166382, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 934,  Mean reward: -3.627906976744186, Mean Entropy: 0.7942508459091187, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 935,  Mean reward: -8.012820512820513, Mean Entropy: 0.6453105211257935, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 936,  Mean reward: 1.2604166666666667, Mean Entropy: 0.7954003214836121, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 937,  Mean reward: -7.538461538461538, Mean Entropy: 0.7778508067131042, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 938,  Mean reward: -2.895348837209302, Mean Entropy: 0.7543627619743347, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 939,  Mean reward: -2.871794871794872, Mean Entropy: 0.8206298351287842, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.56s
Iteration: 940,  Mean reward: 0.031914893617021274, Mean Entropy: 0.6470381617546082, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 941,  Mean reward: -2.1530612244897958, Mean Entropy: 0.7521356344223022, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 942,  Mean reward: -2.95, Mean Entropy: 1.00630521774292, complete_episode_count: 50.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 943,  Mean reward: -6.773809523809524, Mean Entropy: 0.9006568193435669, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 944,  Mean reward: -6.1625, Mean Entropy: 0.8620151281356812, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 945,  Mean reward: -4.071428571428571, Mean Entropy: 0.8051493167877197, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 946,  Mean reward: -5.282051282051282, Mean Entropy: 0.75384521484375, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 947,  Mean reward: -4.048780487804878, Mean Entropy: 0.8048974871635437, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 948,  Mean reward: -6.441860465116279, Mean Entropy: 0.8293285369873047, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 949,  Mean reward: -4.877777777777778, Mean Entropy: 0.755379855632782, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 950,  Mean reward: -5.229166666666667, Mean Entropy: 0.8725576400756836, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 951,  Mean reward: -5.817073170731708, Mean Entropy: 0.8970944285392761, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 952,  Mean reward: -4.571428571428571, Mean Entropy: 0.8359900712966919, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 953,  Mean reward: -3.7790697674418605, Mean Entropy: 0.7685294151306152, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 954,  Mean reward: -6.222222222222222, Mean Entropy: 0.5034598112106323, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 955,  Mean reward: 3.282258064516129, Mean Entropy: 0.6583985090255737, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 956,  Mean reward: -1.7641509433962264, Mean Entropy: 0.8684419393539429, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 957,  Mean reward: -3.59375, Mean Entropy: 0.8958660960197449, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 958,  Mean reward: -2.25531914893617, Mean Entropy: 0.7196145057678223, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 959,  Mean reward: -2.313953488372093, Mean Entropy: 0.7447475790977478, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 960,  Mean reward: -1.9878048780487805, Mean Entropy: 0.6491576433181763, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 961,  Mean reward: -2.130952380952381, Mean Entropy: 0.7417246699333191, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 962,  Mean reward: -5.829545454545454, Mean Entropy: 0.8405518531799316, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 963,  Mean reward: -5.392857142857143, Mean Entropy: 0.9407763481140137, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 964,  Mean reward: -5.822222222222222, Mean Entropy: 0.8905783891677856, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 965,  Mean reward: -4.044444444444444, Mean Entropy: 0.9716912508010864, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 966,  Mean reward: -4.875, Mean Entropy: 0.9106688499450684, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 967,  Mean reward: -3.423076923076923, Mean Entropy: 0.909747302532196, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 968,  Mean reward: -5.635135135135135, Mean Entropy: 0.9458460807800293, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 969,  Mean reward: -3.659090909090909, Mean Entropy: 0.9817973375320435, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 970,  Mean reward: -4.045454545454546, Mean Entropy: 0.9238535761833191, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 971,  Mean reward: -1.9111111111111112, Mean Entropy: 0.8827207088470459, complete_episode_count: 45.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 972,  Mean reward: -2.988372093023256, Mean Entropy: 0.9519824385643005, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 973,  Mean reward: -4.670731707317073, Mean Entropy: 0.9818775057792664, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 974,  Mean reward: -5.914634146341464, Mean Entropy: 0.9524960517883301, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 975,  Mean reward: -4.0, Mean Entropy: 0.9434489011764526, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 976,  Mean reward: -2.046511627906977, Mean Entropy: 0.9276635050773621, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.58s
Iteration: 977,  Mean reward: -4.344444444444444, Mean Entropy: 0.9669527411460876, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.43s
Iteration: 978,  Mean reward: -5.397435897435898, Mean Entropy: 0.9594206809997559, complete_episode_count: 39.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 979,  Mean reward: -5.916666666666667, Mean Entropy: 0.9247364401817322, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 980,  Mean reward: -5.086956521739131, Mean Entropy: 0.9555239677429199, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 981,  Mean reward: -3.782608695652174, Mean Entropy: 0.9098523855209351, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 982,  Mean reward: -3.3, Mean Entropy: 0.9820966720581055, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 983,  Mean reward: -4.173913043478261, Mean Entropy: 0.9283058047294617, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 984,  Mean reward: -1.3111111111111111, Mean Entropy: 0.9306402206420898, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 985,  Mean reward: -4.211111111111111, Mean Entropy: 0.8742485046386719, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 986,  Mean reward: -4.904255319148936, Mean Entropy: 0.8588624000549316, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 987,  Mean reward: -5.9186046511627906, Mean Entropy: 0.9455828666687012, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 988,  Mean reward: -1.6224489795918366, Mean Entropy: 0.9435078501701355, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 989,  Mean reward: -2.988888888888889, Mean Entropy: 0.9455147981643677, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 990,  Mean reward: -3.402439024390244, Mean Entropy: 0.9691640734672546, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 991,  Mean reward: -1.5, Mean Entropy: 0.9682459831237793, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 992,  Mean reward: -5.290697674418604, Mean Entropy: 0.8516599535942078, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 993,  Mean reward: -6.222222222222222, Mean Entropy: 0.6590861678123474, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 994,  Mean reward: -0.5819672131147541, Mean Entropy: 0.9065666198730469, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 995,  Mean reward: -3.4680851063829787, Mean Entropy: 0.6364227533340454, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 996,  Mean reward: 2.305084745762712, Mean Entropy: 0.778633713722229, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 997,  Mean reward: -3.5595238095238093, Mean Entropy: 0.9354133605957031, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 998,  Mean reward: -5.034090909090909, Mean Entropy: 0.9226285219192505, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 999,  Mean reward: -5.17948717948718, Mean Entropy: 0.936935305595398, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1000,  Mean reward: -3.911111111111111, Mean Entropy: 0.9963513612747192, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: -5.107142857142857, Mean Entropy: 0.9675161838531494, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 1002,  Mean reward: -2.2261904761904763, Mean Entropy: 0.9746932983398438, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1003,  Mean reward: -4.0256410256410255, Mean Entropy: 0.900256335735321, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1004,  Mean reward: -4.1375, Mean Entropy: 0.9111489057540894, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1005,  Mean reward: -3.6363636363636362, Mean Entropy: 0.8955855369567871, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1006,  Mean reward: -2.5384615384615383, Mean Entropy: 0.8814200162887573, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1007,  Mean reward: -6.651162790697675, Mean Entropy: 0.8133548498153687, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1008,  Mean reward: -1.1764705882352942, Mean Entropy: 0.40119415521621704, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1009,  Mean reward: -1.3877551020408163, Mean Entropy: 0.5622416734695435, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1010,  Mean reward: 0.14285714285714285, Mean Entropy: 0.5240753889083862, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.36s
Iteration: 1011,  Mean reward: -3.6, Mean Entropy: 0.573092520236969, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1012,  Mean reward: 0.7333333333333333, Mean Entropy: 0.5663352012634277, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1013,  Mean reward: -7.736111111111111, Mean Entropy: 0.4221687912940979, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.55s
Iteration: 1014,  Mean reward: 4.405172413793103, Mean Entropy: 0.5199832916259766, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1015,  Mean reward: -1.0625, Mean Entropy: 0.44085022807121277, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1016,  Mean reward: -4.111111111111111, Mean Entropy: 0.223732590675354, complete_episode_count: 36.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1017,  Mean reward: -3.7346938775510203, Mean Entropy: 0.31407245993614197, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1018,  Mean reward: -0.42727272727272725, Mean Entropy: 0.5334763526916504, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1019,  Mean reward: -4.585106382978723, Mean Entropy: 0.6734296679496765, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1020,  Mean reward: -4.385416666666667, Mean Entropy: 0.6829688549041748, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1021,  Mean reward: -3.1547619047619047, Mean Entropy: 0.6729881763458252, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1022,  Mean reward: -4.3478260869565215, Mean Entropy: 0.6394755244255066, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1023,  Mean reward: -1.4666666666666666, Mean Entropy: 0.6500332355499268, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 1024,  Mean reward: -0.6382978723404256, Mean Entropy: 0.6021617650985718, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1025,  Mean reward: -2.7169811320754715, Mean Entropy: 0.5841296911239624, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1026,  Mean reward: -1.9210526315789473, Mean Entropy: 0.6559278964996338, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1027,  Mean reward: -7.0227272727272725, Mean Entropy: 0.6168217062950134, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1028,  Mean reward: -2.92, Mean Entropy: 0.5491214990615845, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 1029,  Mean reward: -3.8333333333333335, Mean Entropy: 0.550017237663269, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1030,  Mean reward: -0.29347826086956524, Mean Entropy: 0.5336204767227173, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1031,  Mean reward: 0.40384615384615385, Mean Entropy: 0.564038872718811, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1032,  Mean reward: -1.9081632653061225, Mean Entropy: 0.5664069652557373, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 1033,  Mean reward: -1.8369565217391304, Mean Entropy: 0.6014688014984131, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1034,  Mean reward: -2.4903846153846154, Mean Entropy: 0.5872475504875183, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1035,  Mean reward: -2.0754716981132075, Mean Entropy: 0.5183860659599304, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1036,  Mean reward: -0.38235294117647056, Mean Entropy: 0.4416069984436035, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1037,  Mean reward: -2.510204081632653, Mean Entropy: 0.49734705686569214, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1038,  Mean reward: -1.6145833333333333, Mean Entropy: 0.6193169355392456, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1039,  Mean reward: -1.3365384615384615, Mean Entropy: 0.5522311925888062, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1040,  Mean reward: -2.544642857142857, Mean Entropy: 0.4820382595062256, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1041,  Mean reward: 0.9905660377358491, Mean Entropy: 0.4173662066459656, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1042,  Mean reward: 2.6145833333333335, Mean Entropy: 0.45941051840782166, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1043,  Mean reward: 0.10526315789473684, Mean Entropy: 0.49626481533050537, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1044,  Mean reward: -0.96875, Mean Entropy: 0.4503582715988159, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1045,  Mean reward: 3.411764705882353, Mean Entropy: 0.5080749988555908, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1046,  Mean reward: 1.7391304347826086, Mean Entropy: 0.5038003325462341, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1047,  Mean reward: 1.8369565217391304, Mean Entropy: 0.5337365865707397, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1048,  Mean reward: 4.37962962962963, Mean Entropy: 0.26696670055389404, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.58s
Iteration: 1049,  Mean reward: 3.409090909090909, Mean Entropy: 0.26889273524284363, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1050,  Mean reward: 2.730769230769231, Mean Entropy: 0.4997939467430115, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 1051,  Mean reward: 3.764705882352941, Mean Entropy: 0.4061688780784607, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1052,  Mean reward: 4.614035087719298, Mean Entropy: 0.285037636756897, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1053,  Mean reward: 1.6111111111111112, Mean Entropy: 0.4595418870449066, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1054,  Mean reward: 1.621212121212121, Mean Entropy: 0.45829710364341736, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1055,  Mean reward: -0.6176470588235294, Mean Entropy: 0.4503977298736572, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1056,  Mean reward: 7.122950819672131, Mean Entropy: 0.38528525829315186, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 1057,  Mean reward: -3.276595744680851, Mean Entropy: 0.31644207239151, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1058,  Mean reward: 7.110294117647059, Mean Entropy: 0.33475375175476074, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1059,  Mean reward: 6.61864406779661, Mean Entropy: 0.20850324630737305, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.38s
Iteration: 1060,  Mean reward: 1.3017241379310345, Mean Entropy: 0.5138699412345886, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1061,  Mean reward: 4.045454545454546, Mean Entropy: 0.4753551483154297, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1062,  Mean reward: 2.2169811320754715, Mean Entropy: 0.4882825016975403, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1063,  Mean reward: 2.0538461538461537, Mean Entropy: 0.5331519842147827, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1064,  Mean reward: 4.952830188679245, Mean Entropy: 0.4096022844314575, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1065,  Mean reward: 4.598360655737705, Mean Entropy: 0.4824143350124359, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1066,  Mean reward: 1.9754098360655739, Mean Entropy: 0.5713829398155212, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1067,  Mean reward: 3.990566037735849, Mean Entropy: 0.369607537984848, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1068,  Mean reward: 5.87719298245614, Mean Entropy: 0.3524828851222992, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1069,  Mean reward: -3.1594202898550723, Mean Entropy: 0.39901506900787354, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1070,  Mean reward: -0.45454545454545453, Mean Entropy: 0.36763426661491394, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1071,  Mean reward: 5.7631578947368425, Mean Entropy: 0.4303067624568939, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 1072,  Mean reward: 2.296875, Mean Entropy: 0.32754236459732056, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1073,  Mean reward: 6.946153846153846, Mean Entropy: 0.33707350492477417, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1074,  Mean reward: 6.915254237288136, Mean Entropy: 0.30046796798706055, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 1075,  Mean reward: 6.113207547169812, Mean Entropy: 0.29470303654670715, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1076,  Mean reward: 4.818181818181818, Mean Entropy: 0.266452431678772, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1077,  Mean reward: 7.467741935483871, Mean Entropy: 0.30263030529022217, complete_episode_count: 62.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1078,  Mean reward: 7.283333333333333, Mean Entropy: 0.3307196795940399, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 1079,  Mean reward: 5.525862068965517, Mean Entropy: 0.47946760058403015, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1080,  Mean reward: 5.1796875, Mean Entropy: 0.14037993550300598, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1081,  Mean reward: -2.8133333333333335, Mean Entropy: 0.35856279730796814, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1082,  Mean reward: 3.7, Mean Entropy: 0.14292815327644348, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1083,  Mean reward: 5.23015873015873, Mean Entropy: 0.16931818425655365, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1084,  Mean reward: -3.0933333333333333, Mean Entropy: 0.3151358366012573, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1085,  Mean reward: -1.1928571428571428, Mean Entropy: 0.3069213628768921, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1086,  Mean reward: 6.101851851851852, Mean Entropy: 0.35188183188438416, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1087,  Mean reward: -4.820754716981132, Mean Entropy: 0.24717912077903748, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.54s
Iteration: 1088,  Mean reward: -3.411764705882353, Mean Entropy: 0.36874276399612427, complete_episode_count: 34.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1089,  Mean reward: -7.581632653061225, Mean Entropy: 0.6224982142448425, complete_episode_count: 49.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1090,  Mean reward: -5.311320754716981, Mean Entropy: 0.7695626020431519, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1091,  Mean reward: -5.044117647058823, Mean Entropy: 0.3796565532684326, complete_episode_count: 34.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1092,  Mean reward: -4.776315789473684, Mean Entropy: 0.2642526924610138, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1093,  Mean reward: -5.256756756756757, Mean Entropy: 0.2465527355670929, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1094,  Mean reward: -3.984848484848485, Mean Entropy: 0.24662688374519348, complete_episode_count: 33.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1095,  Mean reward: -5.27027027027027, Mean Entropy: 0.28316450119018555, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1096,  Mean reward: -2.2777777777777777, Mean Entropy: 0.3396918475627899, complete_episode_count: 36.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 1097,  Mean reward: -0.9264705882352942, Mean Entropy: 0.16182368993759155, complete_episode_count: 34.0, Gather time: 0.52s, Train time: 1.43s
Iteration: 1098,  Mean reward: -5.833333333333333, Mean Entropy: 0.7286900877952576, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1099,  Mean reward: -6.975, Mean Entropy: 0.682900607585907, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1100,  Mean reward: 0.69, Mean Entropy: 0.9061059951782227, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.41s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: -6.385714285714286, Mean Entropy: 0.3183977007865906, complete_episode_count: 35.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1102,  Mean reward: -5.511627906976744, Mean Entropy: 0.7640483379364014, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1103,  Mean reward: -6.0212765957446805, Mean Entropy: 0.4756191670894623, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1104,  Mean reward: -5.771428571428571, Mean Entropy: 0.20313981175422668, complete_episode_count: 35.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1105,  Mean reward: -6.22972972972973, Mean Entropy: 0.311648964881897, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 1106,  Mean reward: -1.4054054054054055, Mean Entropy: 0.2783404588699341, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1107,  Mean reward: -6.842105263157895, Mean Entropy: 0.3522264361381531, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1108,  Mean reward: -3.7435897435897436, Mean Entropy: 0.398979127407074, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1109,  Mean reward: -5.0625, Mean Entropy: 0.29520297050476074, complete_episode_count: 32.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 1110,  Mean reward: -0.9324324324324325, Mean Entropy: 0.2800854444503784, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1111,  Mean reward: -2.2127659574468086, Mean Entropy: 0.5461410880088806, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1112,  Mean reward: -3.159090909090909, Mean Entropy: 0.32506972551345825, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1113,  Mean reward: 1.4166666666666667, Mean Entropy: 0.3941165804862976, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1114,  Mean reward: -1.7295081967213115, Mean Entropy: 0.24570414423942566, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1115,  Mean reward: 1.2028985507246377, Mean Entropy: 0.6021488308906555, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1116,  Mean reward: -1.5384615384615385, Mean Entropy: 0.37583374977111816, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1117,  Mean reward: -0.08139534883720931, Mean Entropy: 0.13247844576835632, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1118,  Mean reward: 0.38095238095238093, Mean Entropy: 0.18943682312965393, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1119,  Mean reward: -0.6578947368421053, Mean Entropy: 0.4341382682323456, complete_episode_count: 76.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 1120,  Mean reward: 0.4673913043478261, Mean Entropy: 0.13015085458755493, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 1121,  Mean reward: -0.1511627906976744, Mean Entropy: 0.3105420470237732, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1122,  Mean reward: -1.6216216216216217, Mean Entropy: 0.6567453742027283, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1123,  Mean reward: -1.6730769230769231, Mean Entropy: 0.39990752935409546, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1124,  Mean reward: 2.017857142857143, Mean Entropy: 0.7015314102172852, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.60s
Iteration: 1125,  Mean reward: -0.4880952380952381, Mean Entropy: 0.4969613552093506, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1126,  Mean reward: -4.885964912280702, Mean Entropy: 0.629206657409668, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1127,  Mean reward: 0.5306122448979592, Mean Entropy: 0.5236081480979919, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1128,  Mean reward: -1.456140350877193, Mean Entropy: 0.6817572116851807, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1129,  Mean reward: -2.4722222222222223, Mean Entropy: 0.7733073234558105, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1130,  Mean reward: -2.5754716981132075, Mean Entropy: 0.7092094421386719, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 1131,  Mean reward: -0.5350877192982456, Mean Entropy: 0.6612796187400818, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1132,  Mean reward: 1.009090909090909, Mean Entropy: 0.5618802309036255, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1133,  Mean reward: 4.481481481481482, Mean Entropy: 0.5843656063079834, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1134,  Mean reward: 3.8359375, Mean Entropy: 0.41668257117271423, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1135,  Mean reward: 6.016129032258065, Mean Entropy: 0.4199686050415039, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 1136,  Mean reward: 7.1796875, Mean Entropy: 0.45048317313194275, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1137,  Mean reward: 0.6666666666666666, Mean Entropy: 0.16716895997524261, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1138,  Mean reward: -6.909090909090909, Mean Entropy: 0.33882564306259155, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1139,  Mean reward: -6.2439024390243905, Mean Entropy: 0.45522746443748474, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1140,  Mean reward: -6.215909090909091, Mean Entropy: 0.5761545896530151, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1141,  Mean reward: -5.989583333333333, Mean Entropy: 0.4805663526058197, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1142,  Mean reward: -7.537735849056604, Mean Entropy: 0.4140552282333374, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1143,  Mean reward: -5.052238805970149, Mean Entropy: 0.7053457498550415, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1144,  Mean reward: -7.267241379310345, Mean Entropy: 0.6837853789329529, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1145,  Mean reward: -4.804347826086956, Mean Entropy: 0.38609910011291504, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1146,  Mean reward: -8.287878787878787, Mean Entropy: 0.14506468176841736, complete_episode_count: 33.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 1147,  Mean reward: -7.9411764705882355, Mean Entropy: 0.13382968306541443, complete_episode_count: 34.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1148,  Mean reward: -2.5697674418604652, Mean Entropy: 0.2832638621330261, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1149,  Mean reward: -8.333333333333334, Mean Entropy: 0.14118823409080505, complete_episode_count: 33.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 1150,  Mean reward: -8.117647058823529, Mean Entropy: 0.19063600897789001, complete_episode_count: 34.0, Gather time: 0.51s, Train time: 1.42s
Iteration: 1151,  Mean reward: -8.106060606060606, Mean Entropy: 0.24160701036453247, complete_episode_count: 33.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1152,  Mean reward: -7.106060606060606, Mean Entropy: 0.38615307211875916, complete_episode_count: 33.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 1153,  Mean reward: -6.828571428571428, Mean Entropy: 0.3999108672142029, complete_episode_count: 35.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1154,  Mean reward: -6.986111111111111, Mean Entropy: 0.32467615604400635, complete_episode_count: 36.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1155,  Mean reward: -6.46969696969697, Mean Entropy: 0.23115622997283936, complete_episode_count: 33.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 1156,  Mean reward: -6.424242424242424, Mean Entropy: 0.21924206614494324, complete_episode_count: 33.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 1157,  Mean reward: -6.4393939393939394, Mean Entropy: 0.29161012172698975, complete_episode_count: 33.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 1158,  Mean reward: -6.136363636363637, Mean Entropy: 0.21389997005462646, complete_episode_count: 33.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 1159,  Mean reward: -6.046875, Mean Entropy: 0.35021328926086426, complete_episode_count: 32.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 1160,  Mean reward: -5.478260869565218, Mean Entropy: 0.5402382612228394, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1161,  Mean reward: -3.3333333333333335, Mean Entropy: 0.6168807744979858, complete_episode_count: 48.0, Gather time: 0.54s, Train time: 1.61s
Iteration: 1162,  Mean reward: -5.627659574468085, Mean Entropy: 0.7187306880950928, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1163,  Mean reward: -2.940677966101695, Mean Entropy: 0.639378547668457, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1164,  Mean reward: -1.9333333333333333, Mean Entropy: 0.8920317888259888, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.38s
Iteration: 1165,  Mean reward: -7.25, Mean Entropy: 0.375140905380249, complete_episode_count: 32.0, Gather time: 0.51s, Train time: 1.39s
Iteration: 1166,  Mean reward: -3.144230769230769, Mean Entropy: 0.7959503531455994, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1167,  Mean reward: -1.4215686274509804, Mean Entropy: 0.8809320330619812, complete_episode_count: 51.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1168,  Mean reward: -3.3536585365853657, Mean Entropy: 0.6558570861816406, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1169,  Mean reward: -4.527272727272727, Mean Entropy: 0.7098509073257446, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1170,  Mean reward: -1.6833333333333333, Mean Entropy: 0.8691630363464355, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1171,  Mean reward: -3.581818181818182, Mean Entropy: 0.8711277842521667, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 1172,  Mean reward: -2.5789473684210527, Mean Entropy: 0.8774290084838867, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1173,  Mean reward: -6.389830508474576, Mean Entropy: 0.790317952632904, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 1174,  Mean reward: -5.355769230769231, Mean Entropy: 0.8396021723747253, complete_episode_count: 52.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1175,  Mean reward: 0.10714285714285714, Mean Entropy: 0.7022117972373962, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1176,  Mean reward: -3.3425925925925926, Mean Entropy: 0.8761427402496338, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1177,  Mean reward: -0.375, Mean Entropy: 0.8268387317657471, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1178,  Mean reward: -0.746031746031746, Mean Entropy: 0.8421468138694763, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1179,  Mean reward: -2.6578947368421053, Mean Entropy: 0.9168652296066284, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1180,  Mean reward: -2.4913793103448274, Mean Entropy: 0.7729741930961609, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1181,  Mean reward: -3.5636363636363635, Mean Entropy: 0.6431077718734741, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1182,  Mean reward: -1.5859375, Mean Entropy: 0.8274833559989929, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1183,  Mean reward: -3.663265306122449, Mean Entropy: 0.5609433650970459, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1184,  Mean reward: -2.875, Mean Entropy: 0.7919936180114746, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1185,  Mean reward: -1.8307692307692307, Mean Entropy: 0.7990853190422058, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1186,  Mean reward: -1.8916666666666666, Mean Entropy: 0.6313775777816772, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1187,  Mean reward: -4.189655172413793, Mean Entropy: 0.7520347833633423, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1188,  Mean reward: -4.383928571428571, Mean Entropy: 0.7870159149169922, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1189,  Mean reward: -2.5847457627118646, Mean Entropy: 0.8725913763046265, complete_episode_count: 59.0, Gather time: 0.55s, Train time: 1.37s
Iteration: 1190,  Mean reward: -2.0681818181818183, Mean Entropy: 0.8013509511947632, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1191,  Mean reward: -2.0080645161290325, Mean Entropy: 0.8414518237113953, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1192,  Mean reward: -5.151785714285714, Mean Entropy: 0.7811013460159302, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1193,  Mean reward: 0.5, Mean Entropy: 0.8360905647277832, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1194,  Mean reward: -4.39622641509434, Mean Entropy: 0.5020333528518677, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.36s
Iteration: 1195,  Mean reward: -2.3482142857142856, Mean Entropy: 0.6846074461936951, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1196,  Mean reward: -0.36153846153846153, Mean Entropy: 0.805809497833252, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1197,  Mean reward: -3.533333333333333, Mean Entropy: 0.6746248602867126, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1198,  Mean reward: -2.425, Mean Entropy: 0.8241176009178162, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 1199,  Mean reward: -2.7818181818181817, Mean Entropy: 0.7338046431541443, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1200,  Mean reward: -2.836206896551724, Mean Entropy: 0.784533679485321, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.39s
rec seq len 2
actor lr 0.0005
Iteration: 1201,  Mean reward: -2.9453125, Mean Entropy: 0.7916175723075867, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1202,  Mean reward: -2.245614035087719, Mean Entropy: 0.8259860873222351, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1203,  Mean reward: -1.7118644067796611, Mean Entropy: 0.6787501573562622, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1204,  Mean reward: -1.7033898305084745, Mean Entropy: 0.7760300636291504, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1205,  Mean reward: -2.7916666666666665, Mean Entropy: 0.7854616045951843, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1206,  Mean reward: -2.4909090909090907, Mean Entropy: 0.7845891714096069, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1207,  Mean reward: -2.060344827586207, Mean Entropy: 0.7576515674591064, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1208,  Mean reward: -3.0258620689655173, Mean Entropy: 0.6757316589355469, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.38s
Iteration: 1209,  Mean reward: -2.7457627118644066, Mean Entropy: 0.7449873685836792, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1210,  Mean reward: -0.9307692307692308, Mean Entropy: 0.7920546531677246, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1211,  Mean reward: -4.631578947368421, Mean Entropy: 0.940905749797821, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1212,  Mean reward: -3.7589285714285716, Mean Entropy: 0.7154328227043152, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.37s
Iteration: 1213,  Mean reward: -2.575, Mean Entropy: 0.8585759401321411, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 1214,  Mean reward: -6.387931034482759, Mean Entropy: 0.7127959132194519, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1215,  Mean reward: -2.4655172413793105, Mean Entropy: 0.7720736265182495, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1216,  Mean reward: -4.175438596491228, Mean Entropy: 0.8714359998703003, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1217,  Mean reward: -2.9035087719298245, Mean Entropy: 0.8002868890762329, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.40s
Iteration: 1218,  Mean reward: -4.967741935483871, Mean Entropy: 0.7819921374320984, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1219,  Mean reward: -2.819672131147541, Mean Entropy: 0.8758184909820557, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1220,  Mean reward: -1.3303571428571428, Mean Entropy: 0.8653227686882019, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 1221,  Mean reward: -3.889830508474576, Mean Entropy: 0.8818454742431641, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1222,  Mean reward: -1.675, Mean Entropy: 0.8285390734672546, complete_episode_count: 60.0, Gather time: 0.55s, Train time: 1.41s
Iteration: 1223,  Mean reward: -2.1491228070175437, Mean Entropy: 0.8495683073997498, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1224,  Mean reward: -4.6, Mean Entropy: 0.8569850325584412, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1225,  Mean reward: -2.3448275862068964, Mean Entropy: 0.7061458826065063, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1226,  Mean reward: -3.3181818181818183, Mean Entropy: 0.8056958913803101, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1227,  Mean reward: -2.5714285714285716, Mean Entropy: 0.8385210037231445, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1228,  Mean reward: -3.806451612903226, Mean Entropy: 0.7938329577445984, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1229,  Mean reward: -4.366666666666666, Mean Entropy: 0.820107638835907, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.35s
Iteration: 1230,  Mean reward: -3.9285714285714284, Mean Entropy: 0.7523568868637085, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1231,  Mean reward: -5.5, Mean Entropy: 0.7656585574150085, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1232,  Mean reward: -1.6484375, Mean Entropy: 0.6993308067321777, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1233,  Mean reward: -1.734375, Mean Entropy: 0.8040618896484375, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.88s
Iteration: 1234,  Mean reward: -2.625, Mean Entropy: 0.8904870748519897, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1235,  Mean reward: -2.6796875, Mean Entropy: 0.803464412689209, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1236,  Mean reward: -0.6515151515151515, Mean Entropy: 0.7578415274620056, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1237,  Mean reward: -2.675, Mean Entropy: 0.7913769483566284, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1238,  Mean reward: -2.287878787878788, Mean Entropy: 0.7292232513427734, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1239,  Mean reward: -2.2238805970149254, Mean Entropy: 0.6976751089096069, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1240,  Mean reward: -3.7661290322580645, Mean Entropy: 0.7559887766838074, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1241,  Mean reward: -2.443548387096774, Mean Entropy: 0.7985782027244568, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1242,  Mean reward: -4.21875, Mean Entropy: 0.764085054397583, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1243,  Mean reward: -2.65625, Mean Entropy: 0.7879867553710938, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1244,  Mean reward: -3.5, Mean Entropy: 0.7277225852012634, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1245,  Mean reward: -1.8360655737704918, Mean Entropy: 0.7671429514884949, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1246,  Mean reward: -2.4461538461538463, Mean Entropy: 0.7002328634262085, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1247,  Mean reward: -2.890625, Mean Entropy: 0.7645027041435242, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1248,  Mean reward: -2.475806451612903, Mean Entropy: 0.7712103724479675, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1249,  Mean reward: -2.242424242424242, Mean Entropy: 0.7597715258598328, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1250,  Mean reward: -3.6271186440677967, Mean Entropy: 0.766974925994873, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.35s
Iteration: 1251,  Mean reward: -2.261904761904762, Mean Entropy: 0.7148216962814331, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1252,  Mean reward: -0.8, Mean Entropy: 0.5647980570793152, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1253,  Mean reward: -2.009433962264151, Mean Entropy: 0.4571090638637543, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.36s
Iteration: 1254,  Mean reward: 1.6935483870967742, Mean Entropy: 0.5941745638847351, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 1.36s
Iteration: 1255,  Mean reward: -2.871212121212121, Mean Entropy: 0.6683750152587891, complete_episode_count: 66.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1256,  Mean reward: -1.8360655737704918, Mean Entropy: 0.6661660075187683, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1257,  Mean reward: -1.6415094339622642, Mean Entropy: 0.6890844106674194, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1258,  Mean reward: 0.13636363636363635, Mean Entropy: 0.6372967958450317, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1259,  Mean reward: -1.6754385964912282, Mean Entropy: 0.46578449010849, complete_episode_count: 57.0, Gather time: 0.55s, Train time: 1.37s
Iteration: 1260,  Mean reward: 4.406666666666666, Mean Entropy: 0.32791292667388916, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1261,  Mean reward: 3.0072463768115942, Mean Entropy: 0.4545210301876068, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1262,  Mean reward: 2.6858974358974357, Mean Entropy: 0.33862775564193726, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1263,  Mean reward: 2.2533333333333334, Mean Entropy: 0.3764641284942627, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1264,  Mean reward: 2.5428571428571427, Mean Entropy: 0.21843135356903076, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1265,  Mean reward: 6.37012987012987, Mean Entropy: 0.28560882806777954, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1266,  Mean reward: 5.493150684931507, Mean Entropy: 0.20156803727149963, complete_episode_count: 73.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1267,  Mean reward: 5.688405797101449, Mean Entropy: 0.20191828906536102, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1268,  Mean reward: 6.2272727272727275, Mean Entropy: 0.2908879518508911, complete_episode_count: 77.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 1269,  Mean reward: 4.653846153846154, Mean Entropy: 0.1871916949748993, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1270,  Mean reward: 7.215189873417722, Mean Entropy: 0.06639418751001358, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.70s
Iteration: 1271,  Mean reward: 7.381578947368421, Mean Entropy: 0.14382652938365936, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1272,  Mean reward: 7.381578947368421, Mean Entropy: 0.10118617862462997, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1273,  Mean reward: 7.75, Mean Entropy: 0.07241054624319077, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.68s
Iteration: 1274,  Mean reward: 6.868852459016393, Mean Entropy: 0.020472101867198944, complete_episode_count: 61.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1275,  Mean reward: -0.6774193548387096, Mean Entropy: 0.2074764221906662, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1276,  Mean reward: 5.4772727272727275, Mean Entropy: 0.05028359591960907, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 1277,  Mean reward: 1.1818181818181819, Mean Entropy: 0.8190277814865112, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1278,  Mean reward: 6.203125, Mean Entropy: 0.2525066137313843, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1279,  Mean reward: 1.434782608695652, Mean Entropy: 0.12689808011054993, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1280,  Mean reward: 1.1693548387096775, Mean Entropy: 0.5152036547660828, complete_episode_count: 62.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1281,  Mean reward: 1.5070422535211268, Mean Entropy: 0.45817744731903076, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.73s
Iteration: 1282,  Mean reward: -1.3125, Mean Entropy: 0.23656468093395233, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1283,  Mean reward: -1.6018518518518519, Mean Entropy: 0.8336684107780457, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1284,  Mean reward: -1.358974358974359, Mean Entropy: 0.509190559387207, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1285,  Mean reward: -3.6702127659574466, Mean Entropy: 0.44533345103263855, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1286,  Mean reward: 3.9827586206896552, Mean Entropy: 0.37340304255485535, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.42s
Iteration: 1287,  Mean reward: -6.560344827586207, Mean Entropy: 0.6474193334579468, complete_episode_count: 58.0, Gather time: 0.55s, Train time: 1.38s
Iteration: 1288,  Mean reward: -6.338028169014085, Mean Entropy: 0.0017404095269739628, complete_episode_count: 71.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 1289,  Mean reward: -3.0, Mean Entropy: 0.5591576099395752, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.68s
Iteration: 1290,  Mean reward: -2.239130434782609, Mean Entropy: 0.8364443778991699, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1291,  Mean reward: 0.9021739130434783, Mean Entropy: 0.013340308330953121, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1292,  Mean reward: -2.651898734177215, Mean Entropy: 0.6681889295578003, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 1293,  Mean reward: -7.744897959183674, Mean Entropy: 0.36504578590393066, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1294,  Mean reward: -2.263888888888889, Mean Entropy: 0.29610970616340637, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1295,  Mean reward: -4.964285714285714, Mean Entropy: 0.24060308933258057, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1296,  Mean reward: -1.7361111111111112, Mean Entropy: 0.23447629809379578, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1297,  Mean reward: -0.9571428571428572, Mean Entropy: 0.376542866230011, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1298,  Mean reward: -2.566666666666667, Mean Entropy: 0.23634925484657288, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1299,  Mean reward: -1.1492537313432836, Mean Entropy: 0.19066661596298218, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1300,  Mean reward: -3.3676470588235294, Mean Entropy: 0.21069931983947754, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 1301,  Mean reward: -3.0074626865671643, Mean Entropy: 0.26555606722831726, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1302,  Mean reward: -0.375, Mean Entropy: 0.2687091827392578, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1303,  Mean reward: -4.5625, Mean Entropy: 0.35320794582366943, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1304,  Mean reward: -2.8839285714285716, Mean Entropy: 0.30230170488357544, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1305,  Mean reward: -2.8442622950819674, Mean Entropy: 0.4169192910194397, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1306,  Mean reward: -3.1354166666666665, Mean Entropy: 0.4016537368297577, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1307,  Mean reward: -3.1354166666666665, Mean Entropy: 0.4747302532196045, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1308,  Mean reward: -9.784313725490197, Mean Entropy: 0.6968790888786316, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1309,  Mean reward: -7.17, Mean Entropy: 0.7147996425628662, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1310,  Mean reward: -9.453488372093023, Mean Entropy: 0.7580407857894897, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1311,  Mean reward: -7.989130434782608, Mean Entropy: 0.8447731733322144, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1312,  Mean reward: -6.369565217391305, Mean Entropy: 0.7686343193054199, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 1313,  Mean reward: -10.9, Mean Entropy: 0.6246923804283142, complete_episode_count: 45.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1314,  Mean reward: -9.0, Mean Entropy: 0.7148079872131348, complete_episode_count: 49.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1315,  Mean reward: -8.666666666666666, Mean Entropy: 0.6642457246780396, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.55s
Iteration: 1316,  Mean reward: -9.361702127659575, Mean Entropy: 0.7122175693511963, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1317,  Mean reward: -10.033333333333333, Mean Entropy: 0.5995909571647644, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1318,  Mean reward: -8.625, Mean Entropy: 0.6726701259613037, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1319,  Mean reward: -9.181818181818182, Mean Entropy: 0.7375792264938354, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1320,  Mean reward: -6.346938775510204, Mean Entropy: 0.3761975169181824, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.42s
Iteration: 1321,  Mean reward: -2.4166666666666665, Mean Entropy: 0.5175954699516296, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1322,  Mean reward: -9.6875, Mean Entropy: 0.1993207186460495, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1323,  Mean reward: -8.716216216216216, Mean Entropy: 0.2626104950904846, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1324,  Mean reward: -10.739583333333334, Mean Entropy: 0.6681728363037109, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1325,  Mean reward: -9.847826086956522, Mean Entropy: 0.4372839629650116, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1326,  Mean reward: -9.845238095238095, Mean Entropy: 0.4377362132072449, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1327,  Mean reward: -8.852272727272727, Mean Entropy: 0.5405210852622986, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1328,  Mean reward: -10.652173913043478, Mean Entropy: 0.45410656929016113, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1329,  Mean reward: -9.012820512820513, Mean Entropy: 0.3784119486808777, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1330,  Mean reward: -10.738636363636363, Mean Entropy: 0.4346636235713959, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1331,  Mean reward: -8.865853658536585, Mean Entropy: 0.21464762091636658, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1332,  Mean reward: -8.797297297297296, Mean Entropy: 0.2595476508140564, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.40s
Iteration: 1333,  Mean reward: -8.775, Mean Entropy: 0.4810130000114441, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1334,  Mean reward: -8.0, Mean Entropy: 0.5799740552902222, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1335,  Mean reward: -7.654761904761905, Mean Entropy: 0.5253093242645264, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1336,  Mean reward: -6.1521739130434785, Mean Entropy: 0.48816901445388794, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1337,  Mean reward: -7.488372093023256, Mean Entropy: 0.5370640754699707, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1338,  Mean reward: -10.076923076923077, Mean Entropy: 0.49146467447280884, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 1339,  Mean reward: -7.079545454545454, Mean Entropy: 0.2906702160835266, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1340,  Mean reward: -8.58108108108108, Mean Entropy: 0.0711711198091507, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1341,  Mean reward: -6.885714285714286, Mean Entropy: 0.439612478017807, complete_episode_count: 35.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 1342,  Mean reward: -5.15, Mean Entropy: 0.551300048828125, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1343,  Mean reward: -9.871794871794872, Mean Entropy: 0.5261988639831543, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1344,  Mean reward: -9.790697674418604, Mean Entropy: 0.7077339887619019, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1345,  Mean reward: -6.670454545454546, Mean Entropy: 0.6310526132583618, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1346,  Mean reward: -7.5256410256410255, Mean Entropy: 0.47671040892601013, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1347,  Mean reward: -5.576923076923077, Mean Entropy: 0.5733203887939453, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1348,  Mean reward: -5.565789473684211, Mean Entropy: 0.7441784739494324, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1349,  Mean reward: -3.858974358974359, Mean Entropy: 0.7877900004386902, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1350,  Mean reward: -4.8125, Mean Entropy: 0.8068920969963074, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 1351,  Mean reward: -2.813953488372093, Mean Entropy: 0.9071463346481323, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1352,  Mean reward: -5.7375, Mean Entropy: 0.9077188372612, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.56s
Iteration: 1353,  Mean reward: -6.5, Mean Entropy: 0.8993299007415771, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1354,  Mean reward: -4.328947368421052, Mean Entropy: 0.8650593757629395, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1355,  Mean reward: -4.2875, Mean Entropy: 0.9129941463470459, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1356,  Mean reward: -2.897727272727273, Mean Entropy: 0.9297930598258972, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1357,  Mean reward: -4.941860465116279, Mean Entropy: 0.9566000699996948, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1358,  Mean reward: -1.4111111111111112, Mean Entropy: 0.8975744247436523, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1359,  Mean reward: -1.5425531914893618, Mean Entropy: 0.7909208536148071, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1360,  Mean reward: 0.31521739130434784, Mean Entropy: 0.7528094053268433, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1361,  Mean reward: -2.509259259259259, Mean Entropy: 0.8669410943984985, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1362,  Mean reward: -1.0851063829787233, Mean Entropy: 0.8133996725082397, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1363,  Mean reward: -5.597222222222222, Mean Entropy: 0.546784520149231, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 1364,  Mean reward: 0.6382978723404256, Mean Entropy: 0.9016364812850952, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1365,  Mean reward: -3.125, Mean Entropy: 0.7586778402328491, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1366,  Mean reward: 1.358695652173913, Mean Entropy: 0.7355896830558777, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1367,  Mean reward: 2.5245901639344264, Mean Entropy: 0.8966808319091797, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1368,  Mean reward: -6.435897435897436, Mean Entropy: 0.5438276529312134, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1369,  Mean reward: 0.203125, Mean Entropy: 0.5540388822555542, complete_episode_count: 64.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1370,  Mean reward: -0.07142857142857142, Mean Entropy: 0.5785781145095825, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1371,  Mean reward: -4.144736842105263, Mean Entropy: 0.4258500337600708, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1372,  Mean reward: -1.59375, Mean Entropy: 0.6087672114372253, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1373,  Mean reward: -9.09375, Mean Entropy: 0.5921617150306702, complete_episode_count: 32.0, Gather time: 0.51s, Train time: 1.41s
Iteration: 1374,  Mean reward: -2.0833333333333335, Mean Entropy: 0.7794827818870544, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1375,  Mean reward: -7.014285714285714, Mean Entropy: 0.3943363428115845, complete_episode_count: 35.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 1376,  Mean reward: -6.109756097560975, Mean Entropy: 0.9354979991912842, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1377,  Mean reward: -5.0, Mean Entropy: 0.9672046899795532, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1378,  Mean reward: -2.25, Mean Entropy: 0.8951249122619629, complete_episode_count: 42.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1379,  Mean reward: -5.8, Mean Entropy: 0.9311017394065857, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1380,  Mean reward: -3.8875, Mean Entropy: 0.8951324224472046, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1381,  Mean reward: -3.891891891891892, Mean Entropy: 0.9674772620201111, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1382,  Mean reward: -4.841463414634147, Mean Entropy: 0.8808587789535522, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1383,  Mean reward: -4.25, Mean Entropy: 0.924181342124939, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 1384,  Mean reward: -1.625, Mean Entropy: 0.9891685247421265, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1385,  Mean reward: -5.295454545454546, Mean Entropy: 0.9314114451408386, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1386,  Mean reward: -2.2625, Mean Entropy: 0.9241837859153748, complete_episode_count: 40.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1387,  Mean reward: -3.902173913043478, Mean Entropy: 0.9529812335968018, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1388,  Mean reward: -4.675, Mean Entropy: 0.895220935344696, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1389,  Mean reward: -4.871794871794871, Mean Entropy: 0.8669284582138062, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.53s
Iteration: 1390,  Mean reward: -4.3522727272727275, Mean Entropy: 0.8687556982040405, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1391,  Mean reward: -5.243589743589744, Mean Entropy: 0.8974698781967163, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1392,  Mean reward: -1.558139534883721, Mean Entropy: 0.8327979445457458, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1393,  Mean reward: -1.0348837209302326, Mean Entropy: 0.8682585954666138, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 1394,  Mean reward: 1.8137254901960784, Mean Entropy: 0.7504019737243652, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1395,  Mean reward: -6.317073170731708, Mean Entropy: 0.5159589648246765, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1396,  Mean reward: -2.5348837209302326, Mean Entropy: 0.6053926348686218, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1397,  Mean reward: 3.588235294117647, Mean Entropy: 0.4449900984764099, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.34s
Iteration: 1398,  Mean reward: 0.21739130434782608, Mean Entropy: 0.456707239151001, complete_episode_count: 46.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1399,  Mean reward: -0.7380952380952381, Mean Entropy: 0.585378885269165, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1400,  Mean reward: 3.9537037037037037, Mean Entropy: 0.6471571922302246, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.43s
rec seq len 2
actor lr 0.0005
Iteration: 1401,  Mean reward: -6.291666666666667, Mean Entropy: 0.3117275834083557, complete_episode_count: 36.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1402,  Mean reward: 2.6545454545454548, Mean Entropy: 0.5342786312103271, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1403,  Mean reward: 1.6721311475409837, Mean Entropy: 0.484685480594635, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1404,  Mean reward: 3.644230769230769, Mean Entropy: 0.4587113857269287, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.42s
Iteration: 1405,  Mean reward: 3.210526315789474, Mean Entropy: 0.5135210752487183, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1406,  Mean reward: 1.6226415094339623, Mean Entropy: 0.47608765959739685, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1407,  Mean reward: 3.4545454545454546, Mean Entropy: 0.4123915433883667, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1408,  Mean reward: 3.618181818181818, Mean Entropy: 0.3883427083492279, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1409,  Mean reward: 5.567796610169491, Mean Entropy: 0.35222434997558594, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.36s
Iteration: 1410,  Mean reward: 4.081818181818182, Mean Entropy: 0.5950367450714111, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1411,  Mean reward: -2.284313725490196, Mean Entropy: 0.6243841648101807, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1412,  Mean reward: -1.6491228070175439, Mean Entropy: 0.4739238917827606, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1413,  Mean reward: -0.7542372881355932, Mean Entropy: 0.5110551118850708, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1414,  Mean reward: 1.6, Mean Entropy: 0.4739813208580017, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1415,  Mean reward: 0.6428571428571429, Mean Entropy: 0.5269908905029297, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1416,  Mean reward: 1.3333333333333333, Mean Entropy: 0.4969460368156433, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1417,  Mean reward: 2.824561403508772, Mean Entropy: 0.5492120981216431, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1418,  Mean reward: 2.8135593220338984, Mean Entropy: 0.6368710994720459, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1419,  Mean reward: 0.5277777777777778, Mean Entropy: 0.5184791684150696, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1420,  Mean reward: 4.231481481481482, Mean Entropy: 0.38765066862106323, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1421,  Mean reward: 3.890909090909091, Mean Entropy: 0.541083037853241, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1422,  Mean reward: 4.711538461538462, Mean Entropy: 0.4329812526702881, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1423,  Mean reward: 0.16363636363636364, Mean Entropy: 0.4740513265132904, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1424,  Mean reward: -3.694915254237288, Mean Entropy: 0.6036123037338257, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1425,  Mean reward: 2.0172413793103448, Mean Entropy: 0.6593884229660034, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.55s
Iteration: 1426,  Mean reward: 0.19298245614035087, Mean Entropy: 0.5918596386909485, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1427,  Mean reward: 4.037037037037037, Mean Entropy: 0.530585527420044, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1428,  Mean reward: -7.420454545454546, Mean Entropy: 0.3078557848930359, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1429,  Mean reward: 1.871212121212121, Mean Entropy: 0.33081531524658203, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1430,  Mean reward: 3.2452830188679247, Mean Entropy: 0.5784655213356018, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1431,  Mean reward: 1.2916666666666667, Mean Entropy: 0.48929738998413086, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1432,  Mean reward: 1.4107142857142858, Mean Entropy: 0.46979355812072754, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1433,  Mean reward: -1.2040816326530612, Mean Entropy: 0.3808286190032959, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 1434,  Mean reward: -7.644736842105263, Mean Entropy: 0.3172094523906708, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1435,  Mean reward: -0.6226415094339622, Mean Entropy: 0.1258123517036438, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1436,  Mean reward: -10.569767441860465, Mean Entropy: 0.15260648727416992, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 1437,  Mean reward: 1.1916666666666667, Mean Entropy: 0.278465211391449, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1438,  Mean reward: -3.0865384615384617, Mean Entropy: 0.2751576900482178, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1439,  Mean reward: -1.4642857142857142, Mean Entropy: 0.3301600217819214, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1440,  Mean reward: 0.5294117647058824, Mean Entropy: 0.5040296912193298, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1441,  Mean reward: 1.18, Mean Entropy: 0.1711217612028122, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1442,  Mean reward: 4.726415094339623, Mean Entropy: 0.4311498999595642, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1443,  Mean reward: -6.296875, Mean Entropy: 0.1632075309753418, complete_episode_count: 32.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 1444,  Mean reward: -8.457142857142857, Mean Entropy: 0.5840311050415039, complete_episode_count: 35.0, Gather time: 0.51s, Train time: 1.38s
Iteration: 1445,  Mean reward: 0.65, Mean Entropy: 0.6610915660858154, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1446,  Mean reward: -3.606382978723404, Mean Entropy: 0.7685744762420654, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1447,  Mean reward: -0.38461538461538464, Mean Entropy: 0.560590922832489, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1448,  Mean reward: -0.6696428571428571, Mean Entropy: 0.5267503261566162, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1449,  Mean reward: -3.294642857142857, Mean Entropy: 0.5045697093009949, complete_episode_count: 56.0, Gather time: 0.54s, Train time: 1.36s
Iteration: 1450,  Mean reward: -1.8529411764705883, Mean Entropy: 0.5550584197044373, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1451,  Mean reward: -4.324074074074074, Mean Entropy: 0.5249897241592407, complete_episode_count: 54.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1452,  Mean reward: -1.2295081967213115, Mean Entropy: 0.44465798139572144, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1453,  Mean reward: -5.2075471698113205, Mean Entropy: 0.4886104464530945, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1454,  Mean reward: -4.311320754716981, Mean Entropy: 0.534841775894165, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1455,  Mean reward: -2.759259259259259, Mean Entropy: 0.46771568059921265, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1456,  Mean reward: -3.3135593220338984, Mean Entropy: 0.48132336139678955, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1457,  Mean reward: -3.3508771929824563, Mean Entropy: 0.4434104859828949, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1458,  Mean reward: -2.4411764705882355, Mean Entropy: 0.36205217242240906, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1459,  Mean reward: -0.7246376811594203, Mean Entropy: 0.34423136711120605, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1460,  Mean reward: -1.8307692307692307, Mean Entropy: 0.28758013248443604, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1461,  Mean reward: -3.0074626865671643, Mean Entropy: 0.3321390748023987, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1462,  Mean reward: -2.2578125, Mean Entropy: 0.3135083317756653, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 1463,  Mean reward: -1.5546875, Mean Entropy: 0.33909353613853455, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1464,  Mean reward: -2.7761194029850746, Mean Entropy: 0.34986448287963867, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1465,  Mean reward: -2.046875, Mean Entropy: 0.34503287076950073, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1466,  Mean reward: -3.1641791044776117, Mean Entropy: 0.32428306341171265, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1467,  Mean reward: -1.0144927536231885, Mean Entropy: 0.31957826018333435, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1468,  Mean reward: -2.1615384615384614, Mean Entropy: 0.38445138931274414, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1469,  Mean reward: -1.6567164179104477, Mean Entropy: 0.34923839569091797, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1470,  Mean reward: -1.7272727272727273, Mean Entropy: 0.32391035556793213, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1471,  Mean reward: -2.246031746031746, Mean Entropy: 0.369234561920166, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1472,  Mean reward: -1.119047619047619, Mean Entropy: 0.3894641399383545, complete_episode_count: 63.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1473,  Mean reward: -3.238095238095238, Mean Entropy: 0.38217657804489136, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1474,  Mean reward: -3.778688524590164, Mean Entropy: 0.371018648147583, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1475,  Mean reward: -2.9153846153846152, Mean Entropy: 0.35613393783569336, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1476,  Mean reward: -1.3088235294117647, Mean Entropy: 0.2763904929161072, complete_episode_count: 68.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1477,  Mean reward: -2.962686567164179, Mean Entropy: 0.3022671937942505, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1478,  Mean reward: -1.355072463768116, Mean Entropy: 0.24038991332054138, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.73s
Iteration: 1479,  Mean reward: -1.2214285714285715, Mean Entropy: 0.2608657777309418, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1480,  Mean reward: -3.0833333333333335, Mean Entropy: 0.3413519859313965, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1481,  Mean reward: -2.891304347826087, Mean Entropy: 0.3597191274166107, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1482,  Mean reward: -1.4296875, Mean Entropy: 0.3167611360549927, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1483,  Mean reward: -4.777777777777778, Mean Entropy: 0.3749452829360962, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1484,  Mean reward: -1.2540983606557377, Mean Entropy: 0.42905157804489136, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1485,  Mean reward: -3.9846153846153847, Mean Entropy: 0.33648282289505005, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1486,  Mean reward: -3.8253968253968256, Mean Entropy: 0.42043155431747437, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1487,  Mean reward: -3.0, Mean Entropy: 0.4642508625984192, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1488,  Mean reward: -0.46296296296296297, Mean Entropy: 0.5170568823814392, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.38s
Iteration: 1489,  Mean reward: -6.132075471698113, Mean Entropy: 0.5486523509025574, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1490,  Mean reward: -2.7788461538461537, Mean Entropy: 0.5560691356658936, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1491,  Mean reward: -0.6181818181818182, Mean Entropy: 0.5284874439239502, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1492,  Mean reward: -3.75, Mean Entropy: 0.4656796455383301, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1493,  Mean reward: -0.5423728813559322, Mean Entropy: 0.5135645866394043, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1494,  Mean reward: -2.3596491228070176, Mean Entropy: 0.43258604407310486, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.37s
Iteration: 1495,  Mean reward: -3.2017543859649122, Mean Entropy: 0.4378703534603119, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1496,  Mean reward: -2.152542372881356, Mean Entropy: 0.47091278433799744, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.38s
Iteration: 1497,  Mean reward: -0.47619047619047616, Mean Entropy: 0.3473741412162781, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1498,  Mean reward: -2.721311475409836, Mean Entropy: 0.4630601108074188, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1499,  Mean reward: -3.230769230769231, Mean Entropy: 0.38840746879577637, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1500,  Mean reward: -1.0081967213114753, Mean Entropy: 0.49381446838378906, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.70s
rec seq len 2
actor lr 0.0005
Iteration: 1501,  Mean reward: -1.309090909090909, Mean Entropy: 0.47245916724205017, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1502,  Mean reward: -3.308333333333333, Mean Entropy: 0.45119354128837585, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1503,  Mean reward: -2.043103448275862, Mean Entropy: 0.4133005142211914, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.57s
Iteration: 1504,  Mean reward: -2.3706896551724137, Mean Entropy: 0.3949768543243408, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1505,  Mean reward: -1.7461538461538462, Mean Entropy: 0.37351295351982117, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1506,  Mean reward: -3.5555555555555554, Mean Entropy: 0.6233135461807251, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1507,  Mean reward: -7.55, Mean Entropy: 0.641025185585022, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1508,  Mean reward: -5.349056603773585, Mean Entropy: 0.5705798864364624, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1509,  Mean reward: -4.237704918032787, Mean Entropy: 0.4274560809135437, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1510,  Mean reward: -1.5923076923076922, Mean Entropy: 0.4495666027069092, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1511,  Mean reward: -4.330188679245283, Mean Entropy: 0.5264909863471985, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1512,  Mean reward: -1.5175438596491229, Mean Entropy: 0.5340932607650757, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1513,  Mean reward: -2.956140350877193, Mean Entropy: 0.49239835143089294, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1514,  Mean reward: -4.298076923076923, Mean Entropy: 0.5501413345336914, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1515,  Mean reward: -2.4642857142857144, Mean Entropy: 0.5000130534172058, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.39s
Iteration: 1516,  Mean reward: -1.9090909090909092, Mean Entropy: 0.5518523454666138, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1517,  Mean reward: -4.663636363636364, Mean Entropy: 0.47840386629104614, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1518,  Mean reward: -3.3333333333333335, Mean Entropy: 0.5476180911064148, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1519,  Mean reward: -5.9818181818181815, Mean Entropy: 0.6640421152114868, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1520,  Mean reward: -11.375, Mean Entropy: 0.6881000399589539, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 1521,  Mean reward: -1.79, Mean Entropy: 0.5976988077163696, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.41s
Iteration: 1522,  Mean reward: -4.9245283018867925, Mean Entropy: 0.6131092309951782, complete_episode_count: 53.0, Gather time: 0.54s, Train time: 1.41s
Iteration: 1523,  Mean reward: -6.009433962264151, Mean Entropy: 0.6473580598831177, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1524,  Mean reward: -5.708333333333333, Mean Entropy: 0.5165705680847168, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1525,  Mean reward: -4.365384615384615, Mean Entropy: 0.5732287764549255, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 1526,  Mean reward: -4.362068965517241, Mean Entropy: 0.48006749153137207, complete_episode_count: 58.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1527,  Mean reward: -4.975, Mean Entropy: 0.480521023273468, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.35s
Iteration: 1528,  Mean reward: -0.5555555555555556, Mean Entropy: 0.05778512358665466, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1529,  Mean reward: -3.449275362318841, Mean Entropy: 0.41055282950401306, complete_episode_count: 69.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1530,  Mean reward: -1.0175438596491229, Mean Entropy: 0.4168620705604553, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.39s
Iteration: 1531,  Mean reward: -1.1574074074074074, Mean Entropy: 0.1567605435848236, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.44s
Iteration: 1532,  Mean reward: -1.5, Mean Entropy: 0.3265193700790405, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1533,  Mean reward: 0.6052631578947368, Mean Entropy: 0.11907656490802765, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.36s
Iteration: 1534,  Mean reward: -2.037878787878788, Mean Entropy: 0.3159918487071991, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1535,  Mean reward: -6.625, Mean Entropy: 0.3174894452095032, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1536,  Mean reward: -3.524193548387097, Mean Entropy: 0.13724985718727112, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 1.36s
Iteration: 1537,  Mean reward: -1.6666666666666667, Mean Entropy: 0.12284529209136963, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1538,  Mean reward: -2.76056338028169, Mean Entropy: 0.2691182494163513, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1539,  Mean reward: -1.356060606060606, Mean Entropy: 0.32235777378082275, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1540,  Mean reward: -4.286885245901639, Mean Entropy: 0.3381428122520447, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.86s
Iteration: 1541,  Mean reward: -3.5, Mean Entropy: 0.3070197105407715, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1542,  Mean reward: -4.030769230769231, Mean Entropy: 0.31091660261154175, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.72s
Iteration: 1543,  Mean reward: -3.639705882352941, Mean Entropy: 0.25305888056755066, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1544,  Mean reward: -2.5642857142857145, Mean Entropy: 0.2515929937362671, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1545,  Mean reward: -2.893939393939394, Mean Entropy: 0.2652512192726135, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1546,  Mean reward: -1.7462686567164178, Mean Entropy: 0.1640407294034958, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1547,  Mean reward: -1.2642857142857142, Mean Entropy: 0.16281068325042725, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1548,  Mean reward: -3.0211267605633805, Mean Entropy: 0.17685572803020477, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1549,  Mean reward: -0.8714285714285714, Mean Entropy: 0.1554328203201294, complete_episode_count: 70.0, Gather time: 0.56s, Train time: 0.69s
Iteration: 1550,  Mean reward: -1.710144927536232, Mean Entropy: 0.15912102162837982, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1551,  Mean reward: -2.3642857142857143, Mean Entropy: 0.12496497482061386, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1552,  Mean reward: -1.7569444444444444, Mean Entropy: 0.12279310077428818, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1553,  Mean reward: -2.0347222222222223, Mean Entropy: 0.15216200053691864, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1554,  Mean reward: -2.847826086956522, Mean Entropy: 0.14425697922706604, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1555,  Mean reward: -0.7971014492753623, Mean Entropy: 0.13666492700576782, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1556,  Mean reward: -2.1, Mean Entropy: 0.13916675746440887, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1557,  Mean reward: -3.739130434782609, Mean Entropy: 0.13162025809288025, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1558,  Mean reward: -4.007246376811594, Mean Entropy: 0.16718414425849915, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1559,  Mean reward: -3.1159420289855073, Mean Entropy: 0.1171504408121109, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1560,  Mean reward: -2.4788732394366195, Mean Entropy: 0.12464257329702377, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1561,  Mean reward: -2.7794117647058822, Mean Entropy: 0.1496184766292572, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1562,  Mean reward: -1.0136986301369864, Mean Entropy: 0.11029833555221558, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1563,  Mean reward: -3.301470588235294, Mean Entropy: 0.16096022725105286, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1564,  Mean reward: -5.347222222222222, Mean Entropy: 0.15812250971794128, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1565,  Mean reward: -3.739130434782609, Mean Entropy: 0.17353294789791107, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1566,  Mean reward: -0.8142857142857143, Mean Entropy: 0.23825407028198242, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1567,  Mean reward: 0.3181818181818182, Mean Entropy: 0.23329606652259827, complete_episode_count: 66.0, Gather time: 0.53s, Train time: 0.71s
Iteration: 1568,  Mean reward: -3.7803030303030303, Mean Entropy: 0.21956728398799896, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1569,  Mean reward: -3.1594202898550723, Mean Entropy: 0.16660353541374207, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1570,  Mean reward: -3.2611940298507465, Mean Entropy: 0.1516411453485489, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.71s
Iteration: 1571,  Mean reward: -2.3857142857142857, Mean Entropy: 0.1175113320350647, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.70s
Iteration: 1572,  Mean reward: -3.0294117647058822, Mean Entropy: 0.14598816633224487, complete_episode_count: 68.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1573,  Mean reward: -1.9930555555555556, Mean Entropy: 0.16253453493118286, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1574,  Mean reward: -3.0422535211267605, Mean Entropy: 0.15787473320960999, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1575,  Mean reward: -0.6041666666666666, Mean Entropy: 0.12532737851142883, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.72s
Iteration: 1576,  Mean reward: -1.7361111111111112, Mean Entropy: 0.1153796911239624, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1577,  Mean reward: -2.6164383561643834, Mean Entropy: 0.12287121266126633, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.71s
Iteration: 1578,  Mean reward: -1.662162162162162, Mean Entropy: 0.14671486616134644, complete_episode_count: 74.0, Gather time: 0.56s, Train time: 0.71s
Iteration: 1579,  Mean reward: -3.3, Mean Entropy: 0.26080724596977234, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.70s
Iteration: 1580,  Mean reward: 0.5357142857142857, Mean Entropy: 0.24740533530712128, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.69s
Iteration: 1581,  Mean reward: 4.842105263157895, Mean Entropy: 0.22062775492668152, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1582,  Mean reward: 4.732142857142857, Mean Entropy: 0.20566484332084656, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1583,  Mean reward: 4.7407407407407405, Mean Entropy: 0.30189943313598633, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.57s
Iteration: 1584,  Mean reward: 5.568965517241379, Mean Entropy: 0.24760469794273376, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1585,  Mean reward: 4.254545454545455, Mean Entropy: 0.37863701581954956, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.40s
Iteration: 1586,  Mean reward: -1.1634615384615385, Mean Entropy: 0.13848523795604706, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1587,  Mean reward: -1.728813559322034, Mean Entropy: 0.5505658388137817, complete_episode_count: 59.0, Gather time: 0.54s, Train time: 1.42s
Iteration: 1588,  Mean reward: -5.341463414634147, Mean Entropy: 0.34332507848739624, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1589,  Mean reward: -7.153846153846154, Mean Entropy: 0.28463488817214966, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1590,  Mean reward: -2.926829268292683, Mean Entropy: 0.40652430057525635, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.40s
Iteration: 1591,  Mean reward: -2.76, Mean Entropy: 0.34584373235702515, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.41s
Iteration: 1592,  Mean reward: 1.8936170212765957, Mean Entropy: 0.2839657664299011, complete_episode_count: 47.0, Gather time: 0.53s, Train time: 1.40s
Iteration: 1593,  Mean reward: -5.454545454545454, Mean Entropy: 0.1767200082540512, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1594,  Mean reward: -3.7966101694915255, Mean Entropy: 0.46262919902801514, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.39s
Iteration: 1595,  Mean reward: -5.27, Mean Entropy: 0.32013028860092163, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 1596,  Mean reward: -1.8181818181818181, Mean Entropy: 0.17645692825317383, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1597,  Mean reward: -3.4925373134328357, Mean Entropy: 0.2894100546836853, complete_episode_count: 67.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1598,  Mean reward: -2.044776119402985, Mean Entropy: 0.3350088596343994, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1599,  Mean reward: -2.5163934426229506, Mean Entropy: 0.38691478967666626, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1600,  Mean reward: -1.5416666666666667, Mean Entropy: 0.41590020060539246, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 0.66s
rec seq len 2
actor lr 0.0005
Iteration: 1601,  Mean reward: -2.1615384615384614, Mean Entropy: 0.38638943433761597, complete_episode_count: 65.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1602,  Mean reward: -2.183333333333333, Mean Entropy: 0.33762359619140625, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 1603,  Mean reward: -2.941666666666667, Mean Entropy: 0.4290730953216553, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.31s
Iteration: 1604,  Mean reward: -3.406779661016949, Mean Entropy: 0.38543757796287537, complete_episode_count: 59.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 1605,  Mean reward: -1.1825396825396826, Mean Entropy: 0.28532618284225464, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 1606,  Mean reward: -0.9841269841269841, Mean Entropy: 0.34432217478752136, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1607,  Mean reward: -2.8951612903225805, Mean Entropy: 0.36233848333358765, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1608,  Mean reward: -2.237704918032787, Mean Entropy: 0.3186984062194824, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 1609,  Mean reward: -1.8307692307692307, Mean Entropy: 0.16543824970722198, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 1610,  Mean reward: -3.427536231884058, Mean Entropy: 0.1549692004919052, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 1611,  Mean reward: -3.7534246575342465, Mean Entropy: 0.22685179114341736, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1612,  Mean reward: -1.6041666666666667, Mean Entropy: 0.27408120036125183, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1613,  Mean reward: -3.1333333333333333, Mean Entropy: 0.11185818910598755, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1614,  Mean reward: -1.8157894736842106, Mean Entropy: 0.033723607659339905, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1615,  Mean reward: -1.639240506329114, Mean Entropy: 0.1741541624069214, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1616,  Mean reward: -3.38, Mean Entropy: 0.1416005939245224, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1617,  Mean reward: -0.9090909090909091, Mean Entropy: 0.016463633626699448, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1618,  Mean reward: -3.25, Mean Entropy: 0.19939957559108734, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1619,  Mean reward: -4.172413793103448, Mean Entropy: 0.30447638034820557, complete_episode_count: 58.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 1620,  Mean reward: -2.651898734177215, Mean Entropy: 0.003632328240200877, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1621,  Mean reward: -1.1329113924050633, Mean Entropy: 0.03463619574904442, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1622,  Mean reward: 0.6195652173913043, Mean Entropy: 0.15239787101745605, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 1623,  Mean reward: -4.2155172413793105, Mean Entropy: 0.10111489146947861, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 1624,  Mean reward: -4.050724637681159, Mean Entropy: 0.20925453305244446, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1625,  Mean reward: -0.8716216216216216, Mean Entropy: 0.11303441226482391, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1626,  Mean reward: -0.3055555555555556, Mean Entropy: 0.11291579902172089, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1627,  Mean reward: -2.595890410958904, Mean Entropy: 0.09654355049133301, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1628,  Mean reward: -3.0064935064935066, Mean Entropy: 0.0802854523062706, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1629,  Mean reward: -3.33974358974359, Mean Entropy: 0.04894015192985535, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1630,  Mean reward: -1.4285714285714286, Mean Entropy: 0.048370346426963806, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1631,  Mean reward: -1.639240506329114, Mean Entropy: 0.01215987466275692, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1632,  Mean reward: -1.75, Mean Entropy: 0.1943303346633911, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1633,  Mean reward: -0.9236111111111112, Mean Entropy: 0.14995718002319336, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1634,  Mean reward: -2.4788732394366195, Mean Entropy: 0.03313298523426056, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1635,  Mean reward: -2.9675324675324677, Mean Entropy: 0.24811041355133057, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1636,  Mean reward: -0.879746835443038, Mean Entropy: 0.15924593806266785, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1637,  Mean reward: -2.548611111111111, Mean Entropy: 0.12754766643047333, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1638,  Mean reward: -2.4577464788732395, Mean Entropy: 0.020938117057085037, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1639,  Mean reward: -1.4050632911392404, Mean Entropy: 0.006053378339856863, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1640,  Mean reward: -1.0, Mean Entropy: 0.0037302556447684765, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1641,  Mean reward: -1.5, Mean Entropy: 0.002449844731017947, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1642,  Mean reward: -3.0, Mean Entropy: 0.0009193400619551539, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1643,  Mean reward: -1.25, Mean Entropy: 0.0004029919800814241, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1644,  Mean reward: -2.25, Mean Entropy: 0.00037222643732093275, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1645,  Mean reward: -3.5, Mean Entropy: 0.0005066963494755328, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1646,  Mean reward: -2.0, Mean Entropy: 0.0006576680461876094, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1647,  Mean reward: -2.5, Mean Entropy: 0.0008008781587705016, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1648,  Mean reward: -1.75, Mean Entropy: 0.0010062032379209995, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1649,  Mean reward: -2.75, Mean Entropy: 0.0008001748938113451, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1650,  Mean reward: -3.75, Mean Entropy: 0.0008912912453524768, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1651,  Mean reward: -1.0, Mean Entropy: 0.0007214031647890806, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1652,  Mean reward: -1.25, Mean Entropy: 0.000698539603035897, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1653,  Mean reward: -2.0, Mean Entropy: 0.0008253738051280379, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1654,  Mean reward: -2.5, Mean Entropy: 0.000962644990067929, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1655,  Mean reward: -2.25, Mean Entropy: 0.0009913435205817223, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1656,  Mean reward: -3.0, Mean Entropy: 0.0008992712828330696, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1657,  Mean reward: -3.25, Mean Entropy: 0.0009170995326712728, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1658,  Mean reward: -2.0, Mean Entropy: 0.0008620410226285458, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1659,  Mean reward: -0.75, Mean Entropy: 0.0008568716584704816, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1660,  Mean reward: -2.0, Mean Entropy: 0.0009360401891171932, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1661,  Mean reward: -2.75, Mean Entropy: 0.0011178203858435154, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1662,  Mean reward: -1.75, Mean Entropy: 0.0009721487294882536, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.69s
Iteration: 1663,  Mean reward: -3.5, Mean Entropy: 0.0013738954439759254, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.85s
Iteration: 1664,  Mean reward: -0.25, Mean Entropy: 0.0009207034017890692, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1665,  Mean reward: -3.75, Mean Entropy: 0.0010628486052155495, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1666,  Mean reward: -3.25, Mean Entropy: 0.0009804910514503717, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.67s
Iteration: 1667,  Mean reward: -1.5, Mean Entropy: 0.0007505097310058773, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1668,  Mean reward: -2.25, Mean Entropy: 0.0008748624240979552, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1669,  Mean reward: -1.75, Mean Entropy: 0.0009728437289595604, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1670,  Mean reward: -3.5, Mean Entropy: 0.0010956204496324062, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1671,  Mean reward: -2.5, Mean Entropy: 0.0011196195846423507, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1672,  Mean reward: -2.5, Mean Entropy: 0.0009209222625941038, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1673,  Mean reward: -0.75, Mean Entropy: 0.0011136395623907447, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1674,  Mean reward: -3.25, Mean Entropy: 0.001288356725126505, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1675,  Mean reward: -1.25, Mean Entropy: 0.0013053454458713531, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1676,  Mean reward: -1.5, Mean Entropy: 0.0016922702779993415, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1677,  Mean reward: -1.5, Mean Entropy: 0.0020980171393603086, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1678,  Mean reward: -2.0, Mean Entropy: 0.0016546787228435278, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1679,  Mean reward: -1.5, Mean Entropy: 0.0017286472721025348, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1680,  Mean reward: -3.0, Mean Entropy: 0.002244897186756134, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1681,  Mean reward: -3.0, Mean Entropy: 0.002449987456202507, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1682,  Mean reward: -1.5, Mean Entropy: 0.002343455795198679, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1683,  Mean reward: -3.0, Mean Entropy: 0.0021137965377420187, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1684,  Mean reward: -0.5, Mean Entropy: 0.0016811530804261565, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1685,  Mean reward: -3.0, Mean Entropy: 0.0017237220890820026, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1686,  Mean reward: -2.75, Mean Entropy: 0.001422589411959052, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1687,  Mean reward: -2.0, Mean Entropy: 0.0012762735132128, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1688,  Mean reward: -1.0, Mean Entropy: 0.0009557013399899006, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1689,  Mean reward: -2.75, Mean Entropy: 0.0010453505674377084, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1690,  Mean reward: -2.75, Mean Entropy: 0.0016512213042005897, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1691,  Mean reward: -2.5, Mean Entropy: 0.0015808189054951072, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1692,  Mean reward: -3.0, Mean Entropy: 0.0025641871616244316, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1693,  Mean reward: -3.25, Mean Entropy: 0.002021683380007744, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1694,  Mean reward: -3.25, Mean Entropy: 0.001871488057076931, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1695,  Mean reward: -3.0, Mean Entropy: 0.0018922988092526793, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1696,  Mean reward: -1.5, Mean Entropy: 0.0014511793851852417, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1697,  Mean reward: -3.25, Mean Entropy: 0.0016684038564562798, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1698,  Mean reward: -3.0, Mean Entropy: 0.002245486481115222, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1699,  Mean reward: -1.5, Mean Entropy: 0.001732995267957449, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1700,  Mean reward: -3.0, Mean Entropy: 0.0018893828382715583, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
rec seq len 2
actor lr 0.0005
Iteration: 1701,  Mean reward: -2.25, Mean Entropy: 0.0019534581806510687, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1702,  Mean reward: -1.75, Mean Entropy: 0.0013899708865210414, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1703,  Mean reward: -0.5, Mean Entropy: 0.0014554986264556646, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1704,  Mean reward: -2.5, Mean Entropy: 0.0016551399603486061, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 1705,  Mean reward: -3.0, Mean Entropy: 0.0018458187114447355, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1706,  Mean reward: -1.0, Mean Entropy: 0.0015243723755702376, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1707,  Mean reward: -1.75, Mean Entropy: 0.0018062861636281013, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1708,  Mean reward: -2.5, Mean Entropy: 0.0020092809572815895, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1709,  Mean reward: -2.5, Mean Entropy: 0.0020842920057475567, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1710,  Mean reward: -0.75, Mean Entropy: 0.001943569048307836, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1711,  Mean reward: -0.75, Mean Entropy: 0.0017628574278205633, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1712,  Mean reward: -3.25, Mean Entropy: 0.003445851383730769, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1713,  Mean reward: -0.75, Mean Entropy: 0.0015185227384790778, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1714,  Mean reward: -0.5, Mean Entropy: 0.001738638966344297, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1715,  Mean reward: -1.25, Mean Entropy: 0.0017755980370566249, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1716,  Mean reward: -2.75, Mean Entropy: 0.0025062316562980413, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1717,  Mean reward: -1.75, Mean Entropy: 0.0014775521121919155, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1718,  Mean reward: -1.75, Mean Entropy: 0.0018806178122758865, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1719,  Mean reward: -1.75, Mean Entropy: 0.001984473317861557, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1720,  Mean reward: -1.5, Mean Entropy: 0.00202384777367115, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1721,  Mean reward: -3.5, Mean Entropy: 0.0021278802305459976, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1722,  Mean reward: -1.75, Mean Entropy: 0.0030901513528078794, complete_episode_count: 80.0, Gather time: 0.56s, Train time: 0.68s
Iteration: 1723,  Mean reward: -2.25, Mean Entropy: 0.0020383503288030624, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1724,  Mean reward: -1.0, Mean Entropy: 0.0010239885887131095, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1725,  Mean reward: -3.0, Mean Entropy: 0.000763850926887244, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1726,  Mean reward: -2.25, Mean Entropy: 0.00043364561861380935, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1727,  Mean reward: -2.75, Mean Entropy: 0.0006042641471140087, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1728,  Mean reward: -0.75, Mean Entropy: 0.00048078675172291696, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1729,  Mean reward: -0.5, Mean Entropy: 0.000490483595058322, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.65s
Iteration: 1730,  Mean reward: -2.5, Mean Entropy: 0.00053863984066993, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1731,  Mean reward: -1.5, Mean Entropy: 0.0004948450950905681, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.65s
Iteration: 1732,  Mean reward: -1.0, Mean Entropy: 0.00042380447848699987, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1733,  Mean reward: -1.75, Mean Entropy: 0.00048350170254707336, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 1734,  Mean reward: -2.5, Mean Entropy: 0.0006110994145274162, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1735,  Mean reward: -2.0, Mean Entropy: 0.0004619452520273626, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1736,  Mean reward: -2.5, Mean Entropy: 0.0006859296699985862, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1737,  Mean reward: -3.5, Mean Entropy: 0.0005487942835316062, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1738,  Mean reward: -1.0, Mean Entropy: 0.000519852910656482, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.65s
Iteration: 1739,  Mean reward: -4.0, Mean Entropy: 0.0005573952803388238, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1740,  Mean reward: -2.75, Mean Entropy: 0.00046673553879372776, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.65s
Iteration: 1741,  Mean reward: -0.5, Mean Entropy: 0.0005368981510400772, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1742,  Mean reward: -3.5, Mean Entropy: 0.0004217501846142113, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1743,  Mean reward: -1.0, Mean Entropy: 0.0005552025977522135, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1744,  Mean reward: -1.0, Mean Entropy: 0.00040450802771374583, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1745,  Mean reward: -1.75, Mean Entropy: 0.0011087257880717516, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.82s
Iteration: 1746,  Mean reward: -0.879746835443038, Mean Entropy: 0.0006414797389879823, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1747,  Mean reward: 0.25, Mean Entropy: 0.0003265920968260616, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1748,  Mean reward: -3.25, Mean Entropy: 0.048498719930648804, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1749,  Mean reward: -1.5921052631578947, Mean Entropy: 0.0036232788115739822, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1750,  Mean reward: -2.75, Mean Entropy: 0.04798029363155365, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1751,  Mean reward: -3.278688524590164, Mean Entropy: 0.11550982296466827, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1752,  Mean reward: -1.8356164383561644, Mean Entropy: 0.14427140355110168, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 1753,  Mean reward: -1.6338028169014085, Mean Entropy: 0.12280714511871338, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1754,  Mean reward: -6.071428571428571, Mean Entropy: 0.22037196159362793, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 1755,  Mean reward: -2.2857142857142856, Mean Entropy: 0.3333723247051239, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1756,  Mean reward: -0.92, Mean Entropy: 0.1970376968383789, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 1757,  Mean reward: -10.991666666666667, Mean Entropy: 0.36048027873039246, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 1758,  Mean reward: -11.660377358490566, Mean Entropy: 0.526007354259491, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 1759,  Mean reward: -7.625, Mean Entropy: 0.45423609018325806, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 1760,  Mean reward: -2.8360655737704916, Mean Entropy: 0.13335232436656952, complete_episode_count: 61.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1761,  Mean reward: -0.825, Mean Entropy: 0.07859104126691818, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 1762,  Mean reward: 0.9166666666666666, Mean Entropy: 0.1844111531972885, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 1763,  Mean reward: -6.833333333333333, Mean Entropy: 0.3620225787162781, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 1764,  Mean reward: -3.982456140350877, Mean Entropy: 0.43818387389183044, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.29s
Iteration: 1765,  Mean reward: -2.1587301587301586, Mean Entropy: 0.1992238461971283, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 1766,  Mean reward: -1.4029850746268657, Mean Entropy: 0.5310870409011841, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 1767,  Mean reward: -2.316326530612245, Mean Entropy: 0.3124609589576721, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 1768,  Mean reward: 4.634615384615385, Mean Entropy: 0.42955145239830017, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 1769,  Mean reward: -3.0365853658536586, Mean Entropy: 0.5052132606506348, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 1770,  Mean reward: -2.1984126984126986, Mean Entropy: 0.5895224809646606, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.69s
Iteration: 1771,  Mean reward: -3.127659574468085, Mean Entropy: 0.5243220925331116, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 1772,  Mean reward: -3.259259259259259, Mean Entropy: 0.46917518973350525, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 1773,  Mean reward: -2.491228070175439, Mean Entropy: 0.5197230577468872, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.32s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -3.960526315789474, Mean Entropy: 1.0036189556121826, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.166666666666667, Mean Entropy: 0.9386352300643921, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 2,  Mean reward: -4.366666666666666, Mean Entropy: 0.9675167798995972, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 3,  Mean reward: -2.4390243902439024, Mean Entropy: 0.9025344848632812, complete_episode_count: 41.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 4,  Mean reward: -3.2125, Mean Entropy: 0.8953133821487427, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 5,  Mean reward: -5.295454545454546, Mean Entropy: 0.9675156474113464, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 6,  Mean reward: -3.4642857142857144, Mean Entropy: 1.0324989557266235, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 7,  Mean reward: -2.872093023255814, Mean Entropy: 0.8808732032775879, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 8,  Mean reward: -4.476190476190476, Mean Entropy: 1.0397181510925293, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.29s
Iteration: 9,  Mean reward: -4.0625, Mean Entropy: 0.9530762434005737, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 10,  Mean reward: -4.951219512195122, Mean Entropy: 0.9530770182609558, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 11,  Mean reward: -6.5625, Mean Entropy: 0.9314165115356445, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 12,  Mean reward: -4.0, Mean Entropy: 0.9386367797851562, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 13,  Mean reward: -5.0, Mean Entropy: 0.960297703742981, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 14,  Mean reward: -4.3125, Mean Entropy: 0.9097556471824646, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 15,  Mean reward: -5.2875, Mean Entropy: 0.9241962432861328, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 16,  Mean reward: -4.2023809523809526, Mean Entropy: 0.9602974653244019, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 17,  Mean reward: -4.880434782608695, Mean Entropy: 0.9169759154319763, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 18,  Mean reward: -4.6, Mean Entropy: 0.9169759750366211, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 19,  Mean reward: -2.817073170731707, Mean Entropy: 0.9458563327789307, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 20,  Mean reward: -4.595238095238095, Mean Entropy: 0.9530773162841797, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 21,  Mean reward: -8.035714285714286, Mean Entropy: 0.8953151702880859, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 22,  Mean reward: -2.966666666666667, Mean Entropy: 0.9169759750366211, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 23,  Mean reward: -3.051282051282051, Mean Entropy: 0.9675177931785583, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 24,  Mean reward: -3.558139534883721, Mean Entropy: 0.9241961240768433, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 25,  Mean reward: -3.642857142857143, Mean Entropy: 0.9819570779800415, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 26,  Mean reward: -4.0875, Mean Entropy: 0.9386365413665771, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 27,  Mean reward: -5.048780487804878, Mean Entropy: 0.9602972865104675, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 28,  Mean reward: -5.027027027027027, Mean Entropy: 0.9314157366752625, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 29,  Mean reward: -6.261904761904762, Mean Entropy: 1.0036194324493408, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 30,  Mean reward: -1.7261904761904763, Mean Entropy: 0.8953150510787964, complete_episode_count: 42.0, Gather time: 0.63s, Train time: 1.32s
Iteration: 31,  Mean reward: -4.963414634146342, Mean Entropy: 0.8880948424339294, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 32,  Mean reward: -4.9772727272727275, Mean Entropy: 1.0036194324493408, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.31s
Iteration: 33,  Mean reward: -4.604651162790698, Mean Entropy: 0.9675177335739136, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 34,  Mean reward: -5.170454545454546, Mean Entropy: 0.9819583296775818, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 35,  Mean reward: -3.5375, Mean Entropy: 0.8880947828292847, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 36,  Mean reward: -4.45, Mean Entropy: 0.9169744849205017, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 37,  Mean reward: -5.552631578947368, Mean Entropy: 0.9025346040725708, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 38,  Mean reward: -5.138888888888889, Mean Entropy: 0.9819562435150146, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 39,  Mean reward: -4.872093023255814, Mean Entropy: 0.9314157962799072, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 40,  Mean reward: -4.75, Mean Entropy: 0.8736540079116821, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 41,  Mean reward: -6.82051282051282, Mean Entropy: 0.8880934715270996, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 42,  Mean reward: -5.7125, Mean Entropy: 0.9530773162841797, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 43,  Mean reward: -3.8076923076923075, Mean Entropy: 1.0036194324493408, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 44,  Mean reward: -3.688888888888889, Mean Entropy: 0.9602975845336914, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 45,  Mean reward: -3.988372093023256, Mean Entropy: 0.9025348424911499, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 46,  Mean reward: -7.609756097560975, Mean Entropy: 0.9530773162841797, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 47,  Mean reward: -4.7073170731707314, Mean Entropy: 0.8808745741844177, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 48,  Mean reward: -5.2, Mean Entropy: 0.9314165115356445, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 49,  Mean reward: -5.5227272727272725, Mean Entropy: 0.9891786575317383, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 50,  Mean reward: -3.7906976744186047, Mean Entropy: 0.9458569288253784, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 51,  Mean reward: -3.4146341463414633, Mean Entropy: 0.9241961240768433, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 52,  Mean reward: -2.7625, Mean Entropy: 0.9819583892822266, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 53,  Mean reward: -5.878378378378378, Mean Entropy: 0.9314165115356445, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 54,  Mean reward: -2.3777777777777778, Mean Entropy: 0.945855975151062, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 55,  Mean reward: -4.666666666666667, Mean Entropy: 0.9097515940666199, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 56,  Mean reward: -2.3452380952380953, Mean Entropy: 0.9386357069015503, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 57,  Mean reward: -4.107142857142857, Mean Entropy: 0.9314160346984863, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 58,  Mean reward: -3.3461538461538463, Mean Entropy: 0.9458569288253784, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 59,  Mean reward: -7.075, Mean Entropy: 0.873653769493103, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 60,  Mean reward: -3.872093023255814, Mean Entropy: 0.9386365413665771, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.37s
Iteration: 61,  Mean reward: -5.1923076923076925, Mean Entropy: 0.9530754089355469, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 62,  Mean reward: -5.355263157894737, Mean Entropy: 0.9458497762680054, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 63,  Mean reward: -5.1923076923076925, Mean Entropy: 0.9530691504478455, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 64,  Mean reward: -4.717948717948718, Mean Entropy: 0.8880810737609863, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 65,  Mean reward: -3.5375, Mean Entropy: 0.9458504915237427, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 66,  Mean reward: -4.972972972972973, Mean Entropy: 0.9169651865959167, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 67,  Mean reward: -4.869047619047619, Mean Entropy: 0.9313815832138062, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 68,  Mean reward: -6.654761904761905, Mean Entropy: 0.9025254249572754, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 69,  Mean reward: -5.7926829268292686, Mean Entropy: 0.8953070044517517, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 70,  Mean reward: -6.036585365853658, Mean Entropy: 0.9458035826683044, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 71,  Mean reward: -3.2209302325581395, Mean Entropy: 0.873651921749115, complete_episode_count: 43.0, Gather time: 0.53s, Train time: 1.36s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 72,  Mean reward: -1.2386363636363635, Mean Entropy: 0.931407630443573, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.31s
Iteration: 73,  Mean reward: -7.2875, Mean Entropy: 0.9314152598381042, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 74,  Mean reward: -2.607142857142857, Mean Entropy: 0.9530770778656006, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 75,  Mean reward: -4.85, Mean Entropy: 0.9530767798423767, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 76,  Mean reward: -1.6395348837209303, Mean Entropy: 0.9169756770133972, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 77,  Mean reward: -3.1951219512195124, Mean Entropy: 0.902535080909729, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.36s
Iteration: 78,  Mean reward: -6.146341463414634, Mean Entropy: 0.9169747829437256, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 79,  Mean reward: -2.8536585365853657, Mean Entropy: 1.0397183895111084, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 80,  Mean reward: -4.7625, Mean Entropy: 0.9675137400627136, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 81,  Mean reward: -5.293478260869565, Mean Entropy: 0.9313712120056152, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 82,  Mean reward: -2.3875, Mean Entropy: 0.974705696105957, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 83,  Mean reward: -7.060975609756097, Mean Entropy: 0.9746763110160828, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 84,  Mean reward: -5.244186046511628, Mean Entropy: 0.9674592018127441, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.51s
Iteration: 85,  Mean reward: -5.511627906976744, Mean Entropy: 0.9313774108886719, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 86,  Mean reward: 0.06818181818181818, Mean Entropy: 0.9314011335372925, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 87,  Mean reward: -5.611111111111111, Mean Entropy: 0.9386159777641296, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 88,  Mean reward: -4.315789473684211, Mean Entropy: 0.9241830706596375, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 89,  Mean reward: -4.038461538461538, Mean Entropy: 0.8953112363815308, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 90,  Mean reward: -3.3, Mean Entropy: 0.9097508192062378, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 91,  Mean reward: -4.625, Mean Entropy: 0.9963889122009277, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 92,  Mean reward: -6.1375, Mean Entropy: 0.8952755928039551, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 93,  Mean reward: -3.127906976744186, Mean Entropy: 0.9169195890426636, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 94,  Mean reward: -1.75, Mean Entropy: 0.9025297164916992, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 95,  Mean reward: -4.604651162790698, Mean Entropy: 0.8808722496032715, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 96,  Mean reward: -1.9880952380952381, Mean Entropy: 0.9819552302360535, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 97,  Mean reward: -4.5, Mean Entropy: 0.9241875410079956, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 98,  Mean reward: -3.6973684210526314, Mean Entropy: 0.8880805969238281, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 99,  Mean reward: -5.012195121951219, Mean Entropy: 0.9458121061325073, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 100,  Mean reward: -2.7195121951219514, Mean Entropy: 0.8447514772415161, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: -5.45945945945946, Mean Entropy: 0.9024821519851685, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 102,  Mean reward: -5.130952380952381, Mean Entropy: 0.9310316443443298, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 103,  Mean reward: -4.857142857142857, Mean Entropy: 0.9670310020446777, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 104,  Mean reward: -3.6707317073170733, Mean Entropy: 0.9241220355033875, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 105,  Mean reward: -1.8571428571428572, Mean Entropy: 0.931396484375, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 106,  Mean reward: -3.207317073170732, Mean Entropy: 0.8880834579467773, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 107,  Mean reward: -4.511111111111111, Mean Entropy: 0.8952851295471191, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 108,  Mean reward: -4.044444444444444, Mean Entropy: 0.9168753623962402, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 109,  Mean reward: -4.243589743589744, Mean Entropy: 0.9456321597099304, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 110,  Mean reward: -3.7777777777777777, Mean Entropy: 0.952937126159668, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 111,  Mean reward: -5.309523809523809, Mean Entropy: 0.9312676191329956, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 112,  Mean reward: -5.119047619047619, Mean Entropy: 0.9454607367515564, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 113,  Mean reward: -4.170454545454546, Mean Entropy: 0.9527686834335327, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 114,  Mean reward: -6.313953488372093, Mean Entropy: 0.9670825004577637, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 115,  Mean reward: -4.128205128205129, Mean Entropy: 0.9384123086929321, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 116,  Mean reward: -6.0, Mean Entropy: 0.9455596804618835, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 117,  Mean reward: -3.448717948717949, Mean Entropy: 0.9673330187797546, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 118,  Mean reward: -2.6666666666666665, Mean Entropy: 0.9746756553649902, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 119,  Mean reward: -4.5, Mean Entropy: 0.9025022387504578, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 120,  Mean reward: -5.225, Mean Entropy: 0.9890878200531006, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 121,  Mean reward: -6.5625, Mean Entropy: 0.9814083576202393, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 122,  Mean reward: -3.558139534883721, Mean Entropy: 0.909324586391449, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 123,  Mean reward: -5.865853658536586, Mean Entropy: 0.9308492541313171, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 124,  Mean reward: -6.038461538461538, Mean Entropy: 0.9159278869628906, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 125,  Mean reward: -4.073170731707317, Mean Entropy: 0.9087783694267273, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 126,  Mean reward: -5.295454545454546, Mean Entropy: 0.9374221563339233, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 127,  Mean reward: -3.0952380952380953, Mean Entropy: 0.9739036560058594, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 128,  Mean reward: -4.058139534883721, Mean Entropy: 0.9309535026550293, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 129,  Mean reward: -1.975609756097561, Mean Entropy: 0.9239760637283325, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 130,  Mean reward: -3.7065217391304346, Mean Entropy: 0.9891014099121094, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 131,  Mean reward: -3.380952380952381, Mean Entropy: 0.8807156085968018, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 132,  Mean reward: -3.4239130434782608, Mean Entropy: 0.9887509346008301, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 133,  Mean reward: -6.714285714285714, Mean Entropy: 0.9453483819961548, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 134,  Mean reward: -5.255813953488372, Mean Entropy: 0.9294079542160034, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 135,  Mean reward: -2.566666666666667, Mean Entropy: 0.9447099566459656, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 136,  Mean reward: -5.5777777777777775, Mean Entropy: 0.9731671214103699, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 137,  Mean reward: -4.046511627906977, Mean Entropy: 0.9300180673599243, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 138,  Mean reward: -2.9166666666666665, Mean Entropy: 0.9092261791229248, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 139,  Mean reward: -6.636363636363637, Mean Entropy: 0.9379932880401611, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 140,  Mean reward: -3.616279069767442, Mean Entropy: 0.9378892779350281, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 141,  Mean reward: -6.8125, Mean Entropy: 0.8869964480400085, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 142,  Mean reward: -3.090909090909091, Mean Entropy: 0.9735835790634155, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 143,  Mean reward: -5.154761904761905, Mean Entropy: 0.9232494235038757, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 144,  Mean reward: -4.695121951219512, Mean Entropy: 0.8648583889007568, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 145,  Mean reward: -4.348837209302325, Mean Entropy: 0.9069763422012329, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 146,  Mean reward: -3.6951219512195124, Mean Entropy: 0.9284666776657104, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 147,  Mean reward: -5.146341463414634, Mean Entropy: 0.9192137122154236, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 148,  Mean reward: -2.9878048780487805, Mean Entropy: 0.9199139475822449, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 149,  Mean reward: -1.930232558139535, Mean Entropy: 0.9447470903396606, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 150,  Mean reward: -4.575, Mean Entropy: 0.9153350591659546, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 151,  Mean reward: -3.526315789473684, Mean Entropy: 0.9495296478271484, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 152,  Mean reward: -5.395348837209302, Mean Entropy: 0.9622027277946472, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 153,  Mean reward: -3.011627906976744, Mean Entropy: 0.8918671607971191, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 154,  Mean reward: -3.880434782608696, Mean Entropy: 0.9824559092521667, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 155,  Mean reward: -4.329545454545454, Mean Entropy: 0.9191491007804871, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 156,  Mean reward: -3.7375, Mean Entropy: 0.8894557356834412, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 157,  Mean reward: -3.7555555555555555, Mean Entropy: 1.0218658447265625, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 158,  Mean reward: -4.351063829787234, Mean Entropy: 0.9235648512840271, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 159,  Mean reward: -3.8536585365853657, Mean Entropy: 0.9162837862968445, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 160,  Mean reward: -3.0319148936170213, Mean Entropy: 0.9003369212150574, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 161,  Mean reward: -4.093023255813954, Mean Entropy: 0.8664278388023376, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 162,  Mean reward: -3.4285714285714284, Mean Entropy: 0.8484654426574707, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 163,  Mean reward: -1.8297872340425532, Mean Entropy: 0.893543004989624, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 164,  Mean reward: -0.4574468085106383, Mean Entropy: 0.9471856951713562, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 165,  Mean reward: -2.2666666666666666, Mean Entropy: 0.9627773761749268, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 166,  Mean reward: -3.9148936170212765, Mean Entropy: 0.8848859667778015, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 167,  Mean reward: -4.35, Mean Entropy: 0.8881165981292725, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 168,  Mean reward: -4.836538461538462, Mean Entropy: 0.8990097045898438, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 169,  Mean reward: -4.288888888888889, Mean Entropy: 0.9106903076171875, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 170,  Mean reward: -2.933333333333333, Mean Entropy: 0.8506412506103516, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 171,  Mean reward: -5.285714285714286, Mean Entropy: 0.9021410942077637, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 172,  Mean reward: -3.802325581395349, Mean Entropy: 0.8972786664962769, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 173,  Mean reward: -3.0208333333333335, Mean Entropy: 0.8325421810150146, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 174,  Mean reward: -0.7555555555555555, Mean Entropy: 0.9542940855026245, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 175,  Mean reward: -3.869047619047619, Mean Entropy: 0.9552197456359863, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 176,  Mean reward: -5.322916666666667, Mean Entropy: 0.8846791982650757, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 177,  Mean reward: -5.25, Mean Entropy: 0.8515610694885254, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 178,  Mean reward: -3.03125, Mean Entropy: 0.8275630474090576, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 179,  Mean reward: -4.8173076923076925, Mean Entropy: 0.8132191896438599, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 180,  Mean reward: -4.553191489361702, Mean Entropy: 0.7604999542236328, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 181,  Mean reward: -3.0098039215686274, Mean Entropy: 0.7731708288192749, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 182,  Mean reward: -2.7115384615384617, Mean Entropy: 0.8059119582176208, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 183,  Mean reward: -2.6481481481481484, Mean Entropy: 0.7898246645927429, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 184,  Mean reward: -5.019607843137255, Mean Entropy: 0.7227453589439392, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 185,  Mean reward: -4.288461538461538, Mean Entropy: 0.7456166744232178, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 186,  Mean reward: -2.7758620689655173, Mean Entropy: 0.7756757140159607, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.34s
Iteration: 187,  Mean reward: -6.175925925925926, Mean Entropy: 0.7385331392288208, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 188,  Mean reward: -0.8823529411764706, Mean Entropy: 0.8080097436904907, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 189,  Mean reward: -5.427083333333333, Mean Entropy: 0.7605547308921814, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 190,  Mean reward: -2.9716981132075473, Mean Entropy: 0.7094956636428833, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 191,  Mean reward: -4.078431372549019, Mean Entropy: 0.7418153285980225, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 192,  Mean reward: -5.409090909090909, Mean Entropy: 0.7425619959831238, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 193,  Mean reward: -3.2641509433962264, Mean Entropy: 0.8636380434036255, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 194,  Mean reward: -4.086956521739131, Mean Entropy: 0.8730534911155701, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 195,  Mean reward: -3.8529411764705883, Mean Entropy: 0.9288429021835327, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 196,  Mean reward: -3.4347826086956523, Mean Entropy: 0.8869236707687378, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 197,  Mean reward: -5.151162790697675, Mean Entropy: 0.8939094543457031, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 198,  Mean reward: -3.989130434782609, Mean Entropy: 0.8429235816001892, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 199,  Mean reward: -2.326530612244898, Mean Entropy: 0.799229085445404, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 200,  Mean reward: -3.4716981132075473, Mean Entropy: 0.7732647657394409, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.35s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -2.8076923076923075, Mean Entropy: 0.7070061564445496, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.34s
Iteration: 202,  Mean reward: -2.9537037037037037, Mean Entropy: 0.6752413511276245, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 203,  Mean reward: -2.966666666666667, Mean Entropy: 0.5966951251029968, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.34s
Iteration: 204,  Mean reward: -4.698412698412699, Mean Entropy: 0.6145795583724976, complete_episode_count: 63.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 205,  Mean reward: -4.568965517241379, Mean Entropy: 0.5174379348754883, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.34s
Iteration: 206,  Mean reward: -5.771428571428571, Mean Entropy: 0.47062429785728455, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 207,  Mean reward: -3.984848484848485, Mean Entropy: 0.42043745517730713, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 208,  Mean reward: -2.1911764705882355, Mean Entropy: 0.32718902826309204, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 209,  Mean reward: -11.678082191780822, Mean Entropy: 0.45228201150894165, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 210,  Mean reward: -1.5, Mean Entropy: 0.4564227759838104, complete_episode_count: 54.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 211,  Mean reward: -10.605633802816902, Mean Entropy: 0.3211510181427002, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 212,  Mean reward: -1.2466666666666666, Mean Entropy: 0.41621607542037964, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 213,  Mean reward: -10.094202898550725, Mean Entropy: 0.6085436344146729, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 214,  Mean reward: -5.040983606557377, Mean Entropy: 0.4743671119213104, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 215,  Mean reward: -12.3515625, Mean Entropy: 0.5779688358306885, complete_episode_count: 64.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 216,  Mean reward: -1.0526315789473684, Mean Entropy: 0.6372201442718506, complete_episode_count: 57.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 217,  Mean reward: -5.09433962264151, Mean Entropy: 0.6942986249923706, complete_episode_count: 53.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 218,  Mean reward: -3.8421052631578947, Mean Entropy: 0.5688741207122803, complete_episode_count: 57.0, Gather time: 0.54s, Train time: 1.32s
Iteration: 219,  Mean reward: -10.83673469387755, Mean Entropy: 0.842100977897644, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 220,  Mean reward: -1.3076923076923077, Mean Entropy: 0.9300680160522461, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 221,  Mean reward: -2.1808510638297873, Mean Entropy: 0.978728711605072, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 222,  Mean reward: -4.326086956521739, Mean Entropy: 0.9141340255737305, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 223,  Mean reward: -3.107142857142857, Mean Entropy: 0.9077954292297363, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 224,  Mean reward: -1.7045454545454546, Mean Entropy: 0.9248055219650269, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 225,  Mean reward: -1.125, Mean Entropy: 0.9549508094787598, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 226,  Mean reward: -4.03921568627451, Mean Entropy: 0.7632214426994324, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 227,  Mean reward: -2.2777777777777777, Mean Entropy: 0.8672088980674744, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 228,  Mean reward: -0.20192307692307693, Mean Entropy: 0.6296610832214355, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 229,  Mean reward: -4.733870967741935, Mean Entropy: 0.3343258798122406, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 230,  Mean reward: -2.0347222222222223, Mean Entropy: 0.2940759062767029, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 231,  Mean reward: -4.054054054054054, Mean Entropy: 0.28319287300109863, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.84s
Iteration: 232,  Mean reward: -2.722972972972973, Mean Entropy: 0.3233211040496826, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 233,  Mean reward: -3.0616438356164384, Mean Entropy: 0.3714839816093445, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 234,  Mean reward: -5.014084507042254, Mean Entropy: 0.3660517632961273, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 235,  Mean reward: -4.578125, Mean Entropy: 0.4071936011314392, complete_episode_count: 64.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 236,  Mean reward: -4.190476190476191, Mean Entropy: 0.378376841545105, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 237,  Mean reward: -3.096774193548387, Mean Entropy: 0.434678316116333, complete_episode_count: 62.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 238,  Mean reward: -3.935483870967742, Mean Entropy: 0.4188253581523895, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 239,  Mean reward: -5.283333333333333, Mean Entropy: 0.4655059576034546, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 240,  Mean reward: -3.8645833333333335, Mean Entropy: 0.5233765840530396, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 241,  Mean reward: -5.544444444444444, Mean Entropy: 0.39853042364120483, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 242,  Mean reward: -0.22321428571428573, Mean Entropy: 0.4615156650543213, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 243,  Mean reward: -2.689655172413793, Mean Entropy: 0.5020720958709717, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 244,  Mean reward: -3.7589285714285716, Mean Entropy: 0.4386686682701111, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 245,  Mean reward: -3.4324324324324325, Mean Entropy: 0.2240171879529953, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 246,  Mean reward: -0.046511627906976744, Mean Entropy: 0.26462405920028687, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 247,  Mean reward: -0.0975609756097561, Mean Entropy: 0.5536229014396667, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.31s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 248,  Mean reward: 0.8181818181818182, Mean Entropy: 0.5045611262321472, complete_episode_count: 44.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 249,  Mean reward: -4.243589743589744, Mean Entropy: 0.43675488233566284, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 250,  Mean reward: -3.9125, Mean Entropy: 0.2551846504211426, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 251,  Mean reward: -0.4594594594594595, Mean Entropy: 0.30985409021377563, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 252,  Mean reward: -0.0875, Mean Entropy: 0.21437931060791016, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 253,  Mean reward: 2.5217391304347827, Mean Entropy: 0.3127196729183197, complete_episode_count: 46.0, Gather time: 0.54s, Train time: 1.32s
Iteration: 254,  Mean reward: -3.3289473684210527, Mean Entropy: 0.24746400117874146, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 255,  Mean reward: -3.6184210526315788, Mean Entropy: 0.2105998694896698, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 256,  Mean reward: -1.527027027027027, Mean Entropy: 0.16934852302074432, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 257,  Mean reward: 0.5813953488372093, Mean Entropy: 0.13323622941970825, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 258,  Mean reward: -0.32894736842105265, Mean Entropy: 0.15173639357089996, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 259,  Mean reward: -0.6578947368421053, Mean Entropy: 0.18155595660209656, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 260,  Mean reward: -0.975609756097561, Mean Entropy: 0.11461728066205978, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 261,  Mean reward: 1.391304347826087, Mean Entropy: 0.20601886510849, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 262,  Mean reward: -0.8, Mean Entropy: 0.06670927256345749, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 263,  Mean reward: -2.8066666666666666, Mean Entropy: 0.18278050422668457, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 264,  Mean reward: -2.335714285714286, Mean Entropy: 0.42649245262145996, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 265,  Mean reward: -1.0677966101694916, Mean Entropy: 0.37358301877975464, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 266,  Mean reward: -2.8225806451612905, Mean Entropy: 0.3876553475856781, complete_episode_count: 62.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 267,  Mean reward: -2.8846153846153846, Mean Entropy: 0.39092549681663513, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 268,  Mean reward: -1.5769230769230769, Mean Entropy: 0.3613201081752777, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 269,  Mean reward: -2.8137254901960786, Mean Entropy: 0.2670252323150635, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 270,  Mean reward: -1.303921568627451, Mean Entropy: 0.3114880919456482, complete_episode_count: 51.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 271,  Mean reward: -2.575, Mean Entropy: 0.07269450277090073, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 272,  Mean reward: 1.1904761904761905, Mean Entropy: 0.14316913485527039, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.33s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 273,  Mean reward: 4.180851063829787, Mean Entropy: 0.28218597173690796, complete_episode_count: 47.0, Gather time: 0.54s, Train time: 1.33s
Iteration: 274,  Mean reward: -3.2045454545454546, Mean Entropy: 0.2516215443611145, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 275,  Mean reward: -2.1578947368421053, Mean Entropy: 0.21171650290489197, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 276,  Mean reward: -1.4555555555555555, Mean Entropy: 0.2691844701766968, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 277,  Mean reward: 0.0875, Mean Entropy: 0.11549051105976105, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 278,  Mean reward: 0.03488372093023256, Mean Entropy: 0.1347160041332245, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 279,  Mean reward: -1.6875, Mean Entropy: 0.09558002650737762, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 280,  Mean reward: 0.7111111111111111, Mean Entropy: 0.04757525771856308, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 281,  Mean reward: 2.2448979591836733, Mean Entropy: 0.021622922271490097, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 282,  Mean reward: -0.9285714285714286, Mean Entropy: 0.016324765980243683, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 283,  Mean reward: 2.7, Mean Entropy: 0.016649279743433, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 284,  Mean reward: 0.7888888888888889, Mean Entropy: 0.013816509395837784, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 285,  Mean reward: 0.4222222222222222, Mean Entropy: 0.016192346811294556, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 286,  Mean reward: -0.5238095238095238, Mean Entropy: 0.04497009515762329, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 287,  Mean reward: 1.8229166666666667, Mean Entropy: 0.018946554511785507, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 288,  Mean reward: 2.1382978723404253, Mean Entropy: 0.06995224952697754, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 289,  Mean reward: -1.6125, Mean Entropy: 0.044409044086933136, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 290,  Mean reward: 1.4148936170212767, Mean Entropy: 0.07169046252965927, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 291,  Mean reward: 0.3977272727272727, Mean Entropy: 0.0935259759426117, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 292,  Mean reward: 0.9404761904761905, Mean Entropy: 0.09329430758953094, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 293,  Mean reward: 2.82, Mean Entropy: 0.07732535153627396, complete_episode_count: 50.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 294,  Mean reward: 0.17045454545454544, Mean Entropy: 0.0946057066321373, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 295,  Mean reward: -0.6190476190476191, Mean Entropy: 0.07300262153148651, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 296,  Mean reward: 1.173913043478261, Mean Entropy: 0.08177432417869568, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 297,  Mean reward: 0.2441860465116279, Mean Entropy: 0.07411882281303406, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 298,  Mean reward: 0.6590909090909091, Mean Entropy: 0.07616671174764633, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 299,  Mean reward: 0.011627906976744186, Mean Entropy: 0.06525970995426178, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 300,  Mean reward: 0.686046511627907, Mean Entropy: 0.08213378489017487, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.30s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 1.0340909090909092, Mean Entropy: 0.0810021460056305, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 302,  Mean reward: 0.19767441860465115, Mean Entropy: 0.03246656805276871, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 303,  Mean reward: 1.3804347826086956, Mean Entropy: 0.08904321491718292, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 304,  Mean reward: 0.047619047619047616, Mean Entropy: 0.04207422584295273, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 305,  Mean reward: 1.5, Mean Entropy: 0.059996556490659714, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 306,  Mean reward: 1.7708333333333333, Mean Entropy: 0.06242350488901138, complete_episode_count: 48.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 307,  Mean reward: 3.688679245283019, Mean Entropy: 0.05293014645576477, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 308,  Mean reward: 0.7111111111111111, Mean Entropy: 0.06581251323223114, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 309,  Mean reward: -0.14285714285714285, Mean Entropy: 0.04716913402080536, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 310,  Mean reward: 0.8444444444444444, Mean Entropy: 0.03605441749095917, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 311,  Mean reward: 0.19767441860465115, Mean Entropy: 0.014085663482546806, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 312,  Mean reward: 0.3977272727272727, Mean Entropy: 0.008739719167351723, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 313,  Mean reward: 1.0340909090909092, Mean Entropy: 0.014507761225104332, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 314,  Mean reward: 1.3804347826086956, Mean Entropy: 0.00727813970297575, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 315,  Mean reward: 0.75, Mean Entropy: 0.004234822932630777, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 316,  Mean reward: -1.6666666666666667, Mean Entropy: 0.04599916189908981, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 317,  Mean reward: 2.6979166666666665, Mean Entropy: 0.015785086899995804, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 318,  Mean reward: 1.6931818181818181, Mean Entropy: 0.04316640645265579, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 319,  Mean reward: 1.4270833333333333, Mean Entropy: 0.048113878816366196, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 320,  Mean reward: -0.5769230769230769, Mean Entropy: 0.07453121244907379, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 321,  Mean reward: 1.0333333333333334, Mean Entropy: 0.032310038805007935, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 322,  Mean reward: 1.3369565217391304, Mean Entropy: 0.0064214617013931274, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 323,  Mean reward: -1.6666666666666667, Mean Entropy: 0.008034473285079002, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 324,  Mean reward: 1.5333333333333334, Mean Entropy: 0.01092016976326704, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 325,  Mean reward: 1.2, Mean Entropy: 0.006445883773267269, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 326,  Mean reward: 1.8777777777777778, Mean Entropy: 0.017554590478539467, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 327,  Mean reward: -1.8076923076923077, Mean Entropy: 0.006421003025025129, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 328,  Mean reward: 0.05952380952380952, Mean Entropy: 0.005077672190964222, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 329,  Mean reward: 0.75, Mean Entropy: 0.003980431705713272, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 330,  Mean reward: 2.1382978723404253, Mean Entropy: 0.0049380469135940075, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 331,  Mean reward: 0.75, Mean Entropy: 0.0038960943929851055, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 332,  Mean reward: -0.2857142857142857, Mean Entropy: 0.006475493311882019, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 333,  Mean reward: 1.3804347826086956, Mean Entropy: 0.04562721773982048, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 334,  Mean reward: 2.0729166666666665, Mean Entropy: 0.07204755395650864, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 335,  Mean reward: 2.020408163265306, Mean Entropy: 0.017538337036967278, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 336,  Mean reward: 2.78, Mean Entropy: 0.003912593238055706, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 337,  Mean reward: 0.9111111111111111, Mean Entropy: 0.004616668447852135, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 338,  Mean reward: 3.07, Mean Entropy: 0.003865330247208476, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 339,  Mean reward: 0.9111111111111111, Mean Entropy: 0.0033163405023515224, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 340,  Mean reward: 2.75, Mean Entropy: 0.005384236574172974, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 341,  Mean reward: 1.5555555555555556, Mean Entropy: 0.005596074275672436, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 342,  Mean reward: 1.065217391304348, Mean Entropy: 0.009846723638474941, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 343,  Mean reward: 1.2333333333333334, Mean Entropy: 0.01113101840019226, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 344,  Mean reward: 3.2596153846153846, Mean Entropy: 0.028558142483234406, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 345,  Mean reward: 1.141304347826087, Mean Entropy: 0.00943182222545147, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 346,  Mean reward: 0.9111111111111111, Mean Entropy: 0.005568503402173519, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 347,  Mean reward: 0.75, Mean Entropy: 0.007457355968654156, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 348,  Mean reward: 0.42045454545454547, Mean Entropy: 0.005923121236264706, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 349,  Mean reward: 2.377551020408163, Mean Entropy: 0.0038109279703348875, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 350,  Mean reward: 1.9583333333333333, Mean Entropy: 0.005045431666076183, complete_episode_count: 48.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 351,  Mean reward: 1.065217391304348, Mean Entropy: 0.002663152990862727, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 352,  Mean reward: -0.09302325581395349, Mean Entropy: 0.0030525687616318464, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 353,  Mean reward: 2.5625, Mean Entropy: 0.0030381574761122465, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 354,  Mean reward: -0.09302325581395349, Mean Entropy: 0.0027921162545681, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 355,  Mean reward: 1.5555555555555556, Mean Entropy: 0.0027676362078636885, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 356,  Mean reward: 1.3804347826086956, Mean Entropy: 0.0026190709322690964, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 357,  Mean reward: 0.75, Mean Entropy: 0.002404013415798545, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 358,  Mean reward: 1.5212765957446808, Mean Entropy: 0.002024074550718069, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 359,  Mean reward: 2.0816326530612246, Mean Entropy: 0.0021485285833477974, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 360,  Mean reward: 1.2333333333333334, Mean Entropy: 0.0020630049984902143, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 361,  Mean reward: 0.05952380952380952, Mean Entropy: 0.002760607283562422, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 362,  Mean reward: -0.9230769230769231, Mean Entropy: 0.0025665152352303267, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 363,  Mean reward: 0.9111111111111111, Mean Entropy: 0.004216399975121021, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 364,  Mean reward: -0.8414634146341463, Mean Entropy: 0.01463269628584385, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 365,  Mean reward: 1.3369565217391304, Mean Entropy: 0.007270711939781904, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 366,  Mean reward: 0.75, Mean Entropy: 0.006968242581933737, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 367,  Mean reward: 0.03571428571428571, Mean Entropy: 0.003749638795852661, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 368,  Mean reward: 0.9111111111111111, Mean Entropy: 0.0046222154051065445, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 369,  Mean reward: 0.5813953488372093, Mean Entropy: 0.0028218762017786503, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 370,  Mean reward: 2.78, Mean Entropy: 0.00285762595012784, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 371,  Mean reward: 1.2333333333333334, Mean Entropy: 0.003661474911496043, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 372,  Mean reward: 0.5813953488372093, Mean Entropy: 0.0031314650550484657, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 373,  Mean reward: 1.5212765957446808, Mean Entropy: 0.0022672114428132772, complete_episode_count: 47.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 374,  Mean reward: 0.5813953488372093, Mean Entropy: 0.003995984327048063, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 375,  Mean reward: 1.3804347826086956, Mean Entropy: 0.003948490601032972, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 376,  Mean reward: 1.3804347826086956, Mean Entropy: 0.0036319780629128218, complete_episode_count: 46.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 377,  Mean reward: 0.7272727272727273, Mean Entropy: 0.004841268993914127, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 378,  Mean reward: 1.5212765957446808, Mean Entropy: 0.0032345778308808804, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 379,  Mean reward: -0.7, Mean Entropy: 0.002586598973721266, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 380,  Mean reward: 2.2604166666666665, Mean Entropy: 0.0034137885086238384, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 381,  Mean reward: -1.0625, Mean Entropy: 0.0030247922986745834, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 382,  Mean reward: 0.5813953488372093, Mean Entropy: 0.002784108277410269, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 383,  Mean reward: 2.673469387755102, Mean Entropy: 0.009846944361925125, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 384,  Mean reward: 2.5980392156862746, Mean Entropy: 0.01395045779645443, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 385,  Mean reward: 1.8297872340425532, Mean Entropy: 0.011065452359616756, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 386,  Mean reward: 0.21951219512195122, Mean Entropy: 0.00939885899424553, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 387,  Mean reward: 0.4666666666666667, Mean Entropy: 0.005000157281756401, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 388,  Mean reward: 2.377551020408163, Mean Entropy: 0.0035731352400034666, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 389,  Mean reward: 2.357142857142857, Mean Entropy: 0.0027212509885430336, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 390,  Mean reward: 1.3804347826086956, Mean Entropy: 0.002332406584173441, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 391,  Mean reward: 1.8297872340425532, Mean Entropy: 0.0023195233661681414, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 392,  Mean reward: -1.294871794871795, Mean Entropy: 0.0025692663621157408, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 393,  Mean reward: 0.42045454545454547, Mean Entropy: 0.002056688768789172, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 394,  Mean reward: 1.2333333333333334, Mean Entropy: 0.0029416074976325035, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 395,  Mean reward: 1.6956521739130435, Mean Entropy: 0.0027103344909846783, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 396,  Mean reward: 0.5813953488372093, Mean Entropy: 0.0031385491602122784, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 397,  Mean reward: 2.980769230769231, Mean Entropy: 0.002895978046581149, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 398,  Mean reward: 1.8297872340425532, Mean Entropy: 0.004457609262317419, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 399,  Mean reward: 1.6354166666666667, Mean Entropy: 0.004524512682110071, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 400,  Mean reward: 3.07, Mean Entropy: 0.0035359677858650684, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 1.065217391304348, Mean Entropy: 0.0027608133386820555, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 402,  Mean reward: 0.2441860465116279, Mean Entropy: 0.0027646683156490326, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 403,  Mean reward: 0.05952380952380952, Mean Entropy: 0.0027374080382287502, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 404,  Mean reward: 1.2333333333333334, Mean Entropy: 0.0024874801747500896, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 405,  Mean reward: 0.42045454545454547, Mean Entropy: 0.002479719463735819, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 406,  Mean reward: 1.6956521739130435, Mean Entropy: 0.002103924285620451, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 407,  Mean reward: 0.75, Mean Entropy: 0.002272969577461481, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 408,  Mean reward: 1.8297872340425532, Mean Entropy: 0.002480167429894209, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 409,  Mean reward: 1.2333333333333334, Mean Entropy: 0.003918908536434174, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 410,  Mean reward: 3.7818181818181817, Mean Entropy: 0.006591105833649635, complete_episode_count: 55.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 411,  Mean reward: 0.22093023255813954, Mean Entropy: 0.0024881218560039997, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 412,  Mean reward: 3.5384615384615383, Mean Entropy: 0.0029624546878039837, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 413,  Mean reward: 0.2441860465116279, Mean Entropy: 0.0024348830338567495, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 414,  Mean reward: 1.5212765957446808, Mean Entropy: 0.00287386286072433, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 415,  Mean reward: 0.05952380952380952, Mean Entropy: 0.0025060265325009823, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 416,  Mean reward: -0.7, Mean Entropy: 0.002221673959866166, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 417,  Mean reward: 0.2441860465116279, Mean Entropy: 0.0020746993832290173, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 418,  Mean reward: 0.2441860465116279, Mean Entropy: 0.0025052789133042097, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 419,  Mean reward: 1.8297872340425532, Mean Entropy: 0.002168312668800354, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 420,  Mean reward: -0.30952380952380953, Mean Entropy: 0.002759666880592704, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 421,  Mean reward: 1.6956521739130435, Mean Entropy: 0.0018393787322565913, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 422,  Mean reward: 1.3804347826086956, Mean Entropy: 0.0018851886270567775, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 423,  Mean reward: 1.0795454545454546, Mean Entropy: 0.002183647593483329, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 424,  Mean reward: 1.358695652173913, Mean Entropy: 0.004327522125095129, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 425,  Mean reward: 0.9186046511627907, Mean Entropy: 0.0028298187535256147, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 426,  Mean reward: 0.5888888888888889, Mean Entropy: 0.0021609629038721323, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 427,  Mean reward: 1.6956521739130435, Mean Entropy: 0.0027018869295716286, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 428,  Mean reward: -0.13414634146341464, Mean Entropy: 0.0019696089439094067, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 429,  Mean reward: 1.6956521739130435, Mean Entropy: 0.0016298142727464437, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 430,  Mean reward: 0.9111111111111111, Mean Entropy: 0.0021397829987108707, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 431,  Mean reward: 2.38, Mean Entropy: 0.0008853365434333682, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 432,  Mean reward: 0.40476190476190477, Mean Entropy: 0.0012079640291631222, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 433,  Mean reward: 0.42045454545454547, Mean Entropy: 0.0014272688422352076, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 434,  Mean reward: 1.5212765957446808, Mean Entropy: 0.001459043938666582, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 435,  Mean reward: 0.5813953488372093, Mean Entropy: 0.0012264529941603541, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 436,  Mean reward: 1.3804347826086956, Mean Entropy: 0.001529190456494689, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 437,  Mean reward: 2.0816326530612246, Mean Entropy: 0.0014986790483817458, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 438,  Mean reward: -0.09302325581395349, Mean Entropy: 0.0015208557015284896, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 439,  Mean reward: 2.673469387755102, Mean Entropy: 0.0016478103352710605, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 440,  Mean reward: -1.0625, Mean Entropy: 0.0015792552148923278, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 441,  Mean reward: 0.9111111111111111, Mean Entropy: 0.0013903835788369179, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 442,  Mean reward: -0.9230769230769231, Mean Entropy: 0.0014825479593127966, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 443,  Mean reward: 2.5625, Mean Entropy: 0.0015749635640531778, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 444,  Mean reward: 0.2441860465116279, Mean Entropy: 0.0019737943075597286, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 445,  Mean reward: 2.0816326530612246, Mean Entropy: 0.0017229713266715407, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 446,  Mean reward: 2.1382978723404253, Mean Entropy: 0.0023876409977674484, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 447,  Mean reward: 1.2333333333333334, Mean Entropy: 0.007611460983753204, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 448,  Mean reward: 1.8297872340425532, Mean Entropy: 0.004811817314475775, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 449,  Mean reward: 0.07777777777777778, Mean Entropy: 0.003958042711019516, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 450,  Mean reward: -0.6511627906976745, Mean Entropy: 0.022732427343726158, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 451,  Mean reward: -0.2857142857142857, Mean Entropy: 0.04236254096031189, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 452,  Mean reward: 1.8297872340425532, Mean Entropy: 0.13304485380649567, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 453,  Mean reward: 5.992307692307692, Mean Entropy: 0.1825612485408783, complete_episode_count: 65.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 454,  Mean reward: -5.34, Mean Entropy: 0.5259615182876587, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 455,  Mean reward: -5.011904761904762, Mean Entropy: 0.7733845114707947, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 456,  Mean reward: -6.276595744680851, Mean Entropy: 0.5889495611190796, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 457,  Mean reward: -6.5394736842105265, Mean Entropy: 0.564138650894165, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 458,  Mean reward: -8.846153846153847, Mean Entropy: 0.5336140394210815, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 459,  Mean reward: -8.805555555555555, Mean Entropy: 0.46979495882987976, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 460,  Mean reward: -7.176470588235294, Mean Entropy: 0.3082280158996582, complete_episode_count: 34.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 461,  Mean reward: -7.014705882352941, Mean Entropy: 0.5420324802398682, complete_episode_count: 34.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 462,  Mean reward: -5.0285714285714285, Mean Entropy: 0.707960844039917, complete_episode_count: 35.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 463,  Mean reward: -7.038461538461538, Mean Entropy: 0.837695300579071, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 464,  Mean reward: -1.430232558139535, Mean Entropy: 0.8178873062133789, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 465,  Mean reward: -6.439024390243903, Mean Entropy: 0.8469330072402954, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 466,  Mean reward: -6.585365853658536, Mean Entropy: 0.8547091484069824, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 467,  Mean reward: -1.9125, Mean Entropy: 0.8653530478477478, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 468,  Mean reward: -5.72093023255814, Mean Entropy: 0.7868146896362305, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 469,  Mean reward: -3.2125, Mean Entropy: 0.8672568798065186, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 470,  Mean reward: -4.662162162162162, Mean Entropy: 0.7653061747550964, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 471,  Mean reward: -2.3684210526315788, Mean Entropy: 0.86159348487854, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 472,  Mean reward: -4.914634146341464, Mean Entropy: 0.837016761302948, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 473,  Mean reward: -4.631578947368421, Mean Entropy: 0.8829023838043213, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 474,  Mean reward: -7.138888888888889, Mean Entropy: 0.7573337554931641, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 475,  Mean reward: -1.5512820512820513, Mean Entropy: 0.6896936297416687, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 476,  Mean reward: -1.2560975609756098, Mean Entropy: 0.6690295934677124, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 477,  Mean reward: -5.583333333333333, Mean Entropy: 0.8363908529281616, complete_episode_count: 36.0, Gather time: 0.55s, Train time: 1.36s
Iteration: 478,  Mean reward: -4.670731707317073, Mean Entropy: 0.821067750453949, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 479,  Mean reward: -3.0833333333333335, Mean Entropy: 0.7823814153671265, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 480,  Mean reward: -2.3854166666666665, Mean Entropy: 0.8184447288513184, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 481,  Mean reward: -6.2368421052631575, Mean Entropy: 0.6935767531394958, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 482,  Mean reward: -2.0121951219512195, Mean Entropy: 0.7739695310592651, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 483,  Mean reward: 0.38372093023255816, Mean Entropy: 0.8594846725463867, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 484,  Mean reward: -6.184210526315789, Mean Entropy: 0.7879070043563843, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 485,  Mean reward: 1.2613636363636365, Mean Entropy: 0.8497097492218018, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 486,  Mean reward: -3.3205128205128207, Mean Entropy: 0.8590196371078491, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 487,  Mean reward: -3.6052631578947367, Mean Entropy: 0.7778502702713013, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 488,  Mean reward: 2.8068181818181817, Mean Entropy: 0.8435325026512146, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 489,  Mean reward: -3.8125, Mean Entropy: 0.7155303359031677, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.49s
Iteration: 490,  Mean reward: 4.48936170212766, Mean Entropy: 0.8055939674377441, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 491,  Mean reward: -2.5652173913043477, Mean Entropy: 0.8282836675643921, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 492,  Mean reward: -6.078947368421052, Mean Entropy: 0.8754293322563171, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 493,  Mean reward: -7.769230769230769, Mean Entropy: 0.8720293045043945, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 494,  Mean reward: -2.4146341463414633, Mean Entropy: 0.8769869804382324, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 495,  Mean reward: -4.725, Mean Entropy: 0.8149227499961853, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 496,  Mean reward: -0.8333333333333334, Mean Entropy: 0.8615498542785645, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 497,  Mean reward: -3.9342105263157894, Mean Entropy: 0.7497476935386658, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 498,  Mean reward: 0.5609756097560976, Mean Entropy: 0.8141863346099854, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 499,  Mean reward: -0.35555555555555557, Mean Entropy: 0.8731765747070312, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 500,  Mean reward: -5.630952380952381, Mean Entropy: 0.7239967584609985, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: -0.574468085106383, Mean Entropy: 0.6937381625175476, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 502,  Mean reward: 1.6724137931034482, Mean Entropy: 0.7193316221237183, complete_episode_count: 58.0, Gather time: 0.53s, Train time: 1.35s
Iteration: 503,  Mean reward: -1.8977272727272727, Mean Entropy: 0.6651903986930847, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 504,  Mean reward: -2.0638297872340425, Mean Entropy: 0.7176303267478943, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 505,  Mean reward: -0.5, Mean Entropy: 0.6412066221237183, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 506,  Mean reward: -0.7596153846153846, Mean Entropy: 0.6337791681289673, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 507,  Mean reward: 0.9134615384615384, Mean Entropy: 0.6810250282287598, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 508,  Mean reward: 1.576086956521739, Mean Entropy: 0.6846524477005005, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 509,  Mean reward: 1.037037037037037, Mean Entropy: 0.6866337656974792, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 510,  Mean reward: -1.1981132075471699, Mean Entropy: 0.8589240312576294, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 511,  Mean reward: -3.0444444444444443, Mean Entropy: 0.9848178625106812, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 512,  Mean reward: -3.5555555555555554, Mean Entropy: 0.8854365348815918, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 513,  Mean reward: -7.1375, Mean Entropy: 0.8981794118881226, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 514,  Mean reward: -7.9743589743589745, Mean Entropy: 0.8569015860557556, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 515,  Mean reward: -8.232558139534884, Mean Entropy: 0.8886567950248718, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 516,  Mean reward: -5.9625, Mean Entropy: 0.8520600199699402, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 517,  Mean reward: -8.268292682926829, Mean Entropy: 0.8764504194259644, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 518,  Mean reward: -6.987179487179487, Mean Entropy: 0.8997070789337158, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 519,  Mean reward: -4.8625, Mean Entropy: 0.9048730731010437, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 520,  Mean reward: -6.695121951219512, Mean Entropy: 0.896517276763916, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 521,  Mean reward: -7.7073170731707314, Mean Entropy: 0.8912980556488037, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 522,  Mean reward: -8.878378378378379, Mean Entropy: 0.8728373050689697, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 523,  Mean reward: -8.71951219512195, Mean Entropy: 0.8974941968917847, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 524,  Mean reward: -6.421052631578948, Mean Entropy: 0.8541481494903564, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 525,  Mean reward: -7.406976744186046, Mean Entropy: 0.8803087472915649, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 526,  Mean reward: -6.948717948717949, Mean Entropy: 0.8873972296714783, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.48s
Iteration: 527,  Mean reward: -7.277777777777778, Mean Entropy: 0.8415343761444092, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 528,  Mean reward: -7.605263157894737, Mean Entropy: 0.8685381412506104, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 529,  Mean reward: -6.111111111111111, Mean Entropy: 0.848078191280365, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 530,  Mean reward: -8.275, Mean Entropy: 0.8766433596611023, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 531,  Mean reward: -6.324324324324325, Mean Entropy: 0.899377703666687, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 532,  Mean reward: -7.04054054054054, Mean Entropy: 0.8570261001586914, complete_episode_count: 37.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 533,  Mean reward: -5.524390243902439, Mean Entropy: 0.8581506609916687, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 534,  Mean reward: -5.575, Mean Entropy: 0.7949547171592712, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 535,  Mean reward: -8.52127659574468, Mean Entropy: 0.8090333342552185, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 536,  Mean reward: -7.8478260869565215, Mean Entropy: 0.8731998205184937, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 537,  Mean reward: -9.333333333333334, Mean Entropy: 0.7600041627883911, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 538,  Mean reward: -8.947916666666666, Mean Entropy: 0.8769810199737549, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 539,  Mean reward: -5.54, Mean Entropy: 0.9395675659179688, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 540,  Mean reward: -6.387755102040816, Mean Entropy: 0.9458844065666199, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 541,  Mean reward: -3.852272727272727, Mean Entropy: 0.9273895025253296, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 542,  Mean reward: -6.5, Mean Entropy: 0.8829063773155212, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 543,  Mean reward: -4.447368421052632, Mean Entropy: 0.9568952322006226, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 544,  Mean reward: -4.488095238095238, Mean Entropy: 0.9509134292602539, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 545,  Mean reward: -3.8875, Mean Entropy: 0.9340640306472778, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.28s
Iteration: 546,  Mean reward: -6.597560975609756, Mean Entropy: 0.9411307573318481, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 547,  Mean reward: -3.340909090909091, Mean Entropy: 0.9557945728302002, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.30s
Iteration: 548,  Mean reward: -2.5652173913043477, Mean Entropy: 0.965022087097168, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 549,  Mean reward: -5.625, Mean Entropy: 0.8846620321273804, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 550,  Mean reward: -3.0121951219512195, Mean Entropy: 0.9312168955802917, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 551,  Mean reward: -6.910256410256411, Mean Entropy: 0.8735111951828003, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 552,  Mean reward: -3.35, Mean Entropy: 0.9095110893249512, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 553,  Mean reward: -4.125, Mean Entropy: 0.901790976524353, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 554,  Mean reward: -3.8255813953488373, Mean Entropy: 0.981765866279602, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 555,  Mean reward: -4.024390243902439, Mean Entropy: 0.9077562093734741, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 556,  Mean reward: -7.428571428571429, Mean Entropy: 0.9384002089500427, complete_episode_count: 35.0, Gather time: 0.50s, Train time: 1.33s
Iteration: 557,  Mean reward: -6.1875, Mean Entropy: 0.9746227264404297, complete_episode_count: 40.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 558,  Mean reward: -3.902439024390244, Mean Entropy: 0.9457820653915405, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 559,  Mean reward: -2.488372093023256, Mean Entropy: 0.8952538967132568, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 560,  Mean reward: -3.522727272727273, Mean Entropy: 0.9385411739349365, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 561,  Mean reward: -4.217948717948718, Mean Entropy: 0.8879764080047607, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.37s
Iteration: 562,  Mean reward: -4.825, Mean Entropy: 0.9674334526062012, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.39s
Iteration: 563,  Mean reward: -4.17948717948718, Mean Entropy: 0.9083845019340515, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.54s
Iteration: 564,  Mean reward: -5.280487804878049, Mean Entropy: 0.9381799697875977, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.38s
Iteration: 565,  Mean reward: -6.0125, Mean Entropy: 0.9238387942314148, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 566,  Mean reward: -3.261904761904762, Mean Entropy: 0.9601260423660278, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 567,  Mean reward: -2.840909090909091, Mean Entropy: 0.9148222208023071, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 568,  Mean reward: -4.090909090909091, Mean Entropy: 0.9604184031486511, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 569,  Mean reward: -2.7738095238095237, Mean Entropy: 0.8302435874938965, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 570,  Mean reward: -0.23255813953488372, Mean Entropy: 0.8943003416061401, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 571,  Mean reward: -4.773809523809524, Mean Entropy: 0.799818754196167, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 572,  Mean reward: -2.32, Mean Entropy: 0.7661890983581543, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 573,  Mean reward: -2.8173076923076925, Mean Entropy: 0.8433167338371277, complete_episode_count: 52.0, Gather time: 0.53s, Train time: 1.34s
Iteration: 574,  Mean reward: -6.697916666666667, Mean Entropy: 0.8569628596305847, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 575,  Mean reward: -2.3666666666666667, Mean Entropy: 0.8275989294052124, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 576,  Mean reward: -4.333333333333333, Mean Entropy: 0.8936281204223633, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 577,  Mean reward: -5.45, Mean Entropy: 0.9296411275863647, complete_episode_count: 50.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 578,  Mean reward: -5.209302325581396, Mean Entropy: 0.9385084509849548, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 579,  Mean reward: -7.25, Mean Entropy: 0.8951095342636108, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 580,  Mean reward: -5.22093023255814, Mean Entropy: 0.9169324636459351, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 581,  Mean reward: -4.440476190476191, Mean Entropy: 0.9457205533981323, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 582,  Mean reward: -3.8488372093023258, Mean Entropy: 0.8854567408561707, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 583,  Mean reward: -5.413043478260869, Mean Entropy: 0.9188777804374695, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 584,  Mean reward: -0.5714285714285714, Mean Entropy: 0.9177756309509277, complete_episode_count: 49.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 585,  Mean reward: -2.852272727272727, Mean Entropy: 0.9071261882781982, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 586,  Mean reward: -5.548780487804878, Mean Entropy: 0.8159762620925903, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 587,  Mean reward: -5.127906976744186, Mean Entropy: 0.9665292501449585, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 588,  Mean reward: -5.872093023255814, Mean Entropy: 0.7673494219779968, complete_episode_count: 43.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 589,  Mean reward: -5.5, Mean Entropy: 0.7369271516799927, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 590,  Mean reward: -7.7407407407407405, Mean Entropy: 0.8389638662338257, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 591,  Mean reward: -6.0978260869565215, Mean Entropy: 0.9209402799606323, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 592,  Mean reward: -4.204545454545454, Mean Entropy: 0.9456315040588379, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 593,  Mean reward: -2.933333333333333, Mean Entropy: 0.8849878311157227, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 594,  Mean reward: -3.7282608695652173, Mean Entropy: 0.8146018385887146, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 595,  Mean reward: -0.20588235294117646, Mean Entropy: 0.8582863807678223, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 596,  Mean reward: -2.6666666666666665, Mean Entropy: 0.8829731941223145, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 597,  Mean reward: -3.604651162790698, Mean Entropy: 0.9180070757865906, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 598,  Mean reward: -2.1222222222222222, Mean Entropy: 0.8647620677947998, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 599,  Mean reward: -0.6739130434782609, Mean Entropy: 0.8978762626647949, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.51s
Iteration: 600,  Mean reward: 0.3431372549019608, Mean Entropy: 0.9165031313896179, complete_episode_count: 51.0, Gather time: 0.52s, Train time: 1.34s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: -1.4111111111111112, Mean Entropy: 0.9624478220939636, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 602,  Mean reward: -0.4431818181818182, Mean Entropy: 0.9218538999557495, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 603,  Mean reward: -5.785714285714286, Mean Entropy: 0.8656694889068604, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 604,  Mean reward: -5.420454545454546, Mean Entropy: 0.7972145080566406, complete_episode_count: 44.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 605,  Mean reward: -5.1415094339622645, Mean Entropy: 0.93278968334198, complete_episode_count: 53.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 606,  Mean reward: -2.4302325581395348, Mean Entropy: 0.7971088290214539, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 607,  Mean reward: 1.7717391304347827, Mean Entropy: 0.85869300365448, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.29s
Iteration: 608,  Mean reward: -1.1203703703703705, Mean Entropy: 0.6659203767776489, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 609,  Mean reward: 2.6666666666666665, Mean Entropy: 0.7935845851898193, complete_episode_count: 60.0, Gather time: 0.54s, Train time: 1.32s
Iteration: 610,  Mean reward: 0.6413043478260869, Mean Entropy: 0.8151750564575195, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 611,  Mean reward: -0.2608695652173913, Mean Entropy: 0.7959014773368835, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 612,  Mean reward: -0.7978723404255319, Mean Entropy: 0.7474937438964844, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 613,  Mean reward: 0.6458333333333334, Mean Entropy: 0.7933292388916016, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 614,  Mean reward: -2.9910714285714284, Mean Entropy: 0.8074692487716675, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 615,  Mean reward: -1.5803571428571428, Mean Entropy: 0.6803745627403259, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.31s
Iteration: 616,  Mean reward: 0.125, Mean Entropy: 0.607688307762146, complete_episode_count: 56.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 617,  Mean reward: 2.125, Mean Entropy: 0.5569032430648804, complete_episode_count: 60.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 618,  Mean reward: 4.920634920634921, Mean Entropy: 0.6742438673973083, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 619,  Mean reward: 0.3020833333333333, Mean Entropy: 0.5646507740020752, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 620,  Mean reward: 2.2142857142857144, Mean Entropy: 0.45028945803642273, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.32s
Iteration: 621,  Mean reward: 5.6525423728813555, Mean Entropy: 0.7954732775688171, complete_episode_count: 59.0, Gather time: 0.53s, Train time: 1.30s
Iteration: 622,  Mean reward: -2.0444444444444443, Mean Entropy: 0.7276642322540283, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 623,  Mean reward: 2.5104166666666665, Mean Entropy: 0.3869231343269348, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.34s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 624,  Mean reward: 6.3, Mean Entropy: 0.777428150177002, complete_episode_count: 55.0, Gather time: 0.54s, Train time: 1.34s
Iteration: 625,  Mean reward: -1.891304347826087, Mean Entropy: 0.943954348564148, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 626,  Mean reward: -4.031914893617022, Mean Entropy: 0.976689338684082, complete_episode_count: 47.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 627,  Mean reward: -5.9523809523809526, Mean Entropy: 0.9741015434265137, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 628,  Mean reward: -1.641025641025641, Mean Entropy: 0.9313618540763855, complete_episode_count: 39.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 629,  Mean reward: -5.670731707317073, Mean Entropy: 0.9386093616485596, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 630,  Mean reward: -4.309523809523809, Mean Entropy: 0.9458451271057129, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 631,  Mean reward: -5.487804878048781, Mean Entropy: 0.9314093589782715, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 632,  Mean reward: -4.321428571428571, Mean Entropy: 0.8880877494812012, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 633,  Mean reward: -7.351351351351352, Mean Entropy: 0.9386326670646667, complete_episode_count: 37.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 634,  Mean reward: -3.7954545454545454, Mean Entropy: 0.9458503723144531, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 635,  Mean reward: -3.5476190476190474, Mean Entropy: 0.9169387221336365, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 636,  Mean reward: -2.4285714285714284, Mean Entropy: 0.9815065264701843, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 637,  Mean reward: -4.219512195121951, Mean Entropy: 0.9159331321716309, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 638,  Mean reward: -6.788888888888889, Mean Entropy: 0.9601821899414062, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 639,  Mean reward: -3.1710526315789473, Mean Entropy: 0.9094745516777039, complete_episode_count: 38.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 640,  Mean reward: -4.313953488372093, Mean Entropy: 0.9164116382598877, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 641,  Mean reward: -4.232558139534884, Mean Entropy: 0.9330720901489258, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 642,  Mean reward: -4.682926829268292, Mean Entropy: 0.9368597865104675, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 643,  Mean reward: -6.926829268292683, Mean Entropy: 0.9313002824783325, complete_episode_count: 41.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 644,  Mean reward: -7.576086956521739, Mean Entropy: 0.8519763946533203, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 645,  Mean reward: -5.329545454545454, Mean Entropy: 0.9241641759872437, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.32s
Iteration: 646,  Mean reward: -6.083333333333333, Mean Entropy: 0.9521592855453491, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 647,  Mean reward: -4.7926829268292686, Mean Entropy: 0.9365667700767517, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 648,  Mean reward: -4.615384615384615, Mean Entropy: 0.9404162764549255, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 649,  Mean reward: -3.0, Mean Entropy: 0.9605611562728882, complete_episode_count: 38.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 650,  Mean reward: -1.3409090909090908, Mean Entropy: 0.8571598529815674, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 651,  Mean reward: -2.967391304347826, Mean Entropy: 0.9947697520256042, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 652,  Mean reward: -3.0729166666666665, Mean Entropy: 0.93113112449646, complete_episode_count: 48.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 653,  Mean reward: -4.340909090909091, Mean Entropy: 0.9742635488510132, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 654,  Mean reward: -4.108695652173913, Mean Entropy: 0.9815196394920349, complete_episode_count: 46.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 655,  Mean reward: -6.844444444444444, Mean Entropy: 0.9382177591323853, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 656,  Mean reward: -2.3333333333333335, Mean Entropy: 0.9393067359924316, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.35s
Iteration: 657,  Mean reward: -6.011363636363637, Mean Entropy: 1.0178241729736328, complete_episode_count: 44.0, Gather time: 0.52s, Train time: 1.36s
Iteration: 658,  Mean reward: -4.023255813953488, Mean Entropy: 0.9961074590682983, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 659,  Mean reward: -6.166666666666667, Mean Entropy: 0.9746042490005493, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 660,  Mean reward: -2.104651162790698, Mean Entropy: 0.9961904287338257, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 661,  Mean reward: -5.95, Mean Entropy: 0.9960411787033081, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.37s
Iteration: 662,  Mean reward: -4.525, Mean Entropy: 0.9456130862236023, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 663,  Mean reward: -4.4375, Mean Entropy: 0.9383901357650757, complete_episode_count: 40.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 664,  Mean reward: -2.5365853658536586, Mean Entropy: 0.937915027141571, complete_episode_count: 41.0, Gather time: 0.51s, Train time: 1.35s
Iteration: 665,  Mean reward: -5.688888888888889, Mean Entropy: 0.9544823169708252, complete_episode_count: 45.0, Gather time: 0.52s, Train time: 1.34s
Iteration: 666,  Mean reward: -5.819444444444445, Mean Entropy: 0.9217655062675476, complete_episode_count: 36.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 667,  Mean reward: -6.756410256410256, Mean Entropy: 0.8920199275016785, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.34s
Iteration: 668,  Mean reward: -5.628205128205129, Mean Entropy: 0.9215984344482422, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 669,  Mean reward: -4.337209302325581, Mean Entropy: 0.9068081378936768, complete_episode_count: 43.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 670,  Mean reward: -0.5333333333333333, Mean Entropy: 0.9295693635940552, complete_episode_count: 45.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 671,  Mean reward: -4.102564102564102, Mean Entropy: 0.9114062190055847, complete_episode_count: 39.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 672,  Mean reward: -2.9285714285714284, Mean Entropy: 0.9044567942619324, complete_episode_count: 42.0, Gather time: 0.52s, Train time: 1.50s
Iteration: 673,  Mean reward: -6.380952380952381, Mean Entropy: 0.9175190925598145, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 674,  Mean reward: -5.9523809523809526, Mean Entropy: 0.8882489204406738, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.32s
Iteration: 675,  Mean reward: -3.6547619047619047, Mean Entropy: 0.8885922431945801, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.33s
Iteration: 676,  Mean reward: -2.9523809523809526, Mean Entropy: 0.6794629096984863, complete_episode_count: 42.0, Gather time: 0.51s, Train time: 1.31s
Iteration: 677,  Mean reward: -2.6607142857142856, Mean Entropy: 0.7661713361740112, complete_episode_count: 56.0, Gather time: 0.53s, Train time: 1.31s
Iteration: 678,  Mean reward: -3.1538461538461537, Mean Entropy: 0.6398932933807373, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.33s
Iteration: 679,  Mean reward: -2.3461538461538463, Mean Entropy: 0.5819004774093628, complete_episode_count: 52.0, Gather time: 0.52s, Train time: 1.30s
Iteration: 680,  Mean reward: -2.1666666666666665, Mean Entropy: 0.43493038415908813, complete_episode_count: 54.0, Gather time: 0.52s, Train time: 1.31s
Iteration: 681,  Mean reward: -2.6475409836065573, Mean Entropy: 0.38951247930526733, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 682,  Mean reward: -0.6984126984126984, Mean Entropy: 0.2577877640724182, complete_episode_count: 63.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 683,  Mean reward: -2.776923076923077, Mean Entropy: 0.3060992360115051, complete_episode_count: 65.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 684,  Mean reward: -5.295081967213115, Mean Entropy: 0.1981193870306015, complete_episode_count: 61.0, Gather time: 0.53s, Train time: 1.33s
Iteration: 685,  Mean reward: -2.847826086956522, Mean Entropy: 0.15702275931835175, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 686,  Mean reward: -1.5285714285714285, Mean Entropy: 0.11649590730667114, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 687,  Mean reward: -0.8402777777777778, Mean Entropy: 0.14473946392536163, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 688,  Mean reward: -1.3958333333333333, Mean Entropy: 0.13624128699302673, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 689,  Mean reward: -2.9571428571428573, Mean Entropy: 0.15754882991313934, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 690,  Mean reward: -2.142857142857143, Mean Entropy: 0.14260827004909515, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 691,  Mean reward: -3.1159420289855073, Mean Entropy: 0.16371522843837738, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 692,  Mean reward: -2.407142857142857, Mean Entropy: 0.13221809267997742, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 693,  Mean reward: -2.013888888888889, Mean Entropy: 0.1411246955394745, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 694,  Mean reward: -3.0211267605633805, Mean Entropy: 0.14743752777576447, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 695,  Mean reward: -1.5285714285714285, Mean Entropy: 0.13990622758865356, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 696,  Mean reward: -0.4859154929577465, Mean Entropy: 0.15854772925376892, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 697,  Mean reward: -2.0347222222222223, Mean Entropy: 0.15759921073913574, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 698,  Mean reward: -0.9931506849315068, Mean Entropy: 0.13925701379776, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 699,  Mean reward: -3.0833333333333335, Mean Entropy: 0.15756987035274506, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 700,  Mean reward: -2.3835616438356166, Mean Entropy: 0.14917486906051636, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
rec seq len 2
actor lr 0.0005
Iteration: 701,  Mean reward: 0.6830985915492958, Mean Entropy: 0.09053140878677368, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 702,  Mean reward: -1.4583333333333333, Mean Entropy: 0.1644958257675171, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 703,  Mean reward: -1.2671232876712328, Mean Entropy: 0.15445968508720398, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 704,  Mean reward: -5.606060606060606, Mean Entropy: 0.21653270721435547, complete_episode_count: 66.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 705,  Mean reward: -2.4788732394366195, Mean Entropy: 0.18205592036247253, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 706,  Mean reward: -3.2816901408450705, Mean Entropy: 0.17379310727119446, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 707,  Mean reward: -2.0785714285714287, Mean Entropy: 0.15922290086746216, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 708,  Mean reward: -0.6041666666666666, Mean Entropy: 0.16135889291763306, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 709,  Mean reward: -1.852112676056338, Mean Entropy: 0.16202428936958313, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 710,  Mean reward: -0.3108108108108108, Mean Entropy: 0.16545739769935608, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 711,  Mean reward: -1.9565217391304348, Mean Entropy: 0.17745129764080048, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 712,  Mean reward: -1.8356164383561644, Mean Entropy: 0.1497631072998047, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 713,  Mean reward: -2.1095890410958904, Mean Entropy: 0.2011030912399292, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 714,  Mean reward: -2.73943661971831, Mean Entropy: 0.1920287311077118, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 715,  Mean reward: -2.289855072463768, Mean Entropy: 0.15893077850341797, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 716,  Mean reward: -1.9366197183098592, Mean Entropy: 0.1426527202129364, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 717,  Mean reward: -4.708955223880597, Mean Entropy: 0.20028966665267944, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 718,  Mean reward: -2.8043478260869565, Mean Entropy: 0.16693803668022156, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 719,  Mean reward: -1.6338028169014085, Mean Entropy: 0.15297049283981323, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 720,  Mean reward: -3.8661971830985915, Mean Entropy: 0.1850656121969223, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 721,  Mean reward: -1.6126760563380282, Mean Entropy: 0.17304497957229614, complete_episode_count: 71.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 722,  Mean reward: -2.3857142857142857, Mean Entropy: 0.1457420289516449, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 723,  Mean reward: -2.5, Mean Entropy: 0.191950261592865, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 724,  Mean reward: -1.1805555555555556, Mean Entropy: 0.12460990995168686, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 725,  Mean reward: -2.76056338028169, Mean Entropy: 0.168130025267601, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 726,  Mean reward: -1.3716216216216217, Mean Entropy: 0.10900187492370605, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 727,  Mean reward: -1.875, Mean Entropy: 0.11615066230297089, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 728,  Mean reward: -3.0135135135135136, Mean Entropy: 0.14352929592132568, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 729,  Mean reward: -3.563380281690141, Mean Entropy: 0.18948738276958466, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 730,  Mean reward: -2.3642857142857143, Mean Entropy: 0.17981189489364624, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 731,  Mean reward: -3.0211267605633805, Mean Entropy: 0.18233716487884521, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 732,  Mean reward: -2.342857142857143, Mean Entropy: 0.15006166696548462, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 733,  Mean reward: -2.3309859154929575, Mean Entropy: 0.20136892795562744, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 734,  Mean reward: -2.2916666666666665, Mean Entropy: 0.1811499297618866, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 735,  Mean reward: -2.657142857142857, Mean Entropy: 0.16910681128501892, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 736,  Mean reward: -1.75, Mean Entropy: 0.12303981184959412, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 737,  Mean reward: -1.72, Mean Entropy: 0.19786420464515686, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 738,  Mean reward: -2.6780821917808217, Mean Entropy: 0.16958433389663696, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 739,  Mean reward: -1.412162162162162, Mean Entropy: 0.10874112695455551, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 740,  Mean reward: -1.6883116883116882, Mean Entropy: 0.10535633563995361, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 741,  Mean reward: -2.727272727272727, Mean Entropy: 0.2205478847026825, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 742,  Mean reward: -1.3289473684210527, Mean Entropy: 0.11147342622280121, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 743,  Mean reward: -1.2266666666666666, Mean Entropy: 0.12274383008480072, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 744,  Mean reward: -3.0733333333333333, Mean Entropy: 0.1365426480770111, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 745,  Mean reward: -3.0422535211267605, Mean Entropy: 0.09873530268669128, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 746,  Mean reward: 0.26973684210526316, Mean Entropy: 0.005059364717453718, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 747,  Mean reward: -1.3670886075949367, Mean Entropy: 0.01083222683519125, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 748,  Mean reward: -0.25, Mean Entropy: 0.037067145109176636, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 749,  Mean reward: -1.8924050632911393, Mean Entropy: 0.11670907586812973, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 750,  Mean reward: -0.7191780821917808, Mean Entropy: 0.11413884162902832, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 751,  Mean reward: -1.0657894736842106, Mean Entropy: 0.11224690079689026, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 752,  Mean reward: -2.1824324324324325, Mean Entropy: 0.07910275459289551, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 753,  Mean reward: -1.3860759493670887, Mean Entropy: 0.028498876839876175, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 754,  Mean reward: -1.5256410256410255, Mean Entropy: 0.015225893817842007, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 755,  Mean reward: -2.3987341772151898, Mean Entropy: 0.04484401270747185, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.82s
Iteration: 756,  Mean reward: -2.1455696202531644, Mean Entropy: 0.040188468992710114, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 757,  Mean reward: -2.9240506329113924, Mean Entropy: 0.03244365006685257, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 758,  Mean reward: -1.25, Mean Entropy: 0.020602669566869736, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 759,  Mean reward: -3.25, Mean Entropy: 0.03964556008577347, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 760,  Mean reward: -1.8924050632911393, Mean Entropy: 0.05761684104800224, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 761,  Mean reward: -3.0641025641025643, Mean Entropy: 0.047303393483161926, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.64s
Iteration: 762,  Mean reward: -3.1582278481012658, Mean Entropy: 0.04858247935771942, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 763,  Mean reward: -2.551282051282051, Mean Entropy: 0.042836278676986694, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 764,  Mean reward: -1.8924050632911393, Mean Entropy: 0.036902204155921936, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 765,  Mean reward: -1.75, Mean Entropy: 0.02117866836488247, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 766,  Mean reward: -3.411392405063291, Mean Entropy: 0.05348585546016693, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 767,  Mean reward: -1.9285714285714286, Mean Entropy: 0.03633909672498703, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 768,  Mean reward: -0.879746835443038, Mean Entropy: 0.020037584006786346, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 769,  Mean reward: -1.25, Mean Entropy: 0.012684555724263191, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 770,  Mean reward: -1.0, Mean Entropy: 0.03319266065955162, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 771,  Mean reward: 0.2692307692307692, Mean Entropy: 0.014395512640476227, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 772,  Mean reward: -3.0, Mean Entropy: 0.03812523931264877, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 773,  Mean reward: -0.37341772151898733, Mean Entropy: 0.03707718104124069, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 774,  Mean reward: -0.7564102564102564, Mean Entropy: 0.032452575862407684, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 775,  Mean reward: -3.9177215189873418, Mean Entropy: 0.05036128684878349, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 776,  Mean reward: -1.7820512820512822, Mean Entropy: 0.04275748133659363, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 777,  Mean reward: -1.0320512820512822, Mean Entropy: 0.03373248130083084, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 778,  Mean reward: -0.75, Mean Entropy: 0.011805981397628784, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 779,  Mean reward: -2.9050632911392404, Mean Entropy: 0.06255820393562317, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 780,  Mean reward: -1.5256410256410255, Mean Entropy: 0.05054016411304474, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 781,  Mean reward: -0.12025316455696203, Mean Entropy: 0.028727203607559204, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 782,  Mean reward: -2.5705128205128207, Mean Entropy: 0.04430731385946274, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 783,  Mean reward: 0.13291139240506328, Mean Entropy: 0.01887030154466629, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 784,  Mean reward: -2.632911392405063, Mean Entropy: 0.06486471742391586, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 785,  Mean reward: -4.170886075949367, Mean Entropy: 0.1639527976512909, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 786,  Mean reward: -1.1418918918918919, Mean Entropy: 0.2128869742155075, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 787,  Mean reward: -0.6285714285714286, Mean Entropy: 0.19233715534210205, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 788,  Mean reward: -1.9577464788732395, Mean Entropy: 0.18325182795524597, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 789,  Mean reward: -4.387323943661972, Mean Entropy: 0.22549329698085785, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 790,  Mean reward: -4.236111111111111, Mean Entropy: 0.16282972693443298, complete_episode_count: 72.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 791,  Mean reward: -1.9866666666666666, Mean Entropy: 0.1893042027950287, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 792,  Mean reward: -0.98, Mean Entropy: 0.183672234416008, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 793,  Mean reward: -3.4927536231884058, Mean Entropy: 0.24641883373260498, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 794,  Mean reward: -1.8150684931506849, Mean Entropy: 0.17155930399894714, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 795,  Mean reward: -2.3642857142857143, Mean Entropy: 0.17655622959136963, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 796,  Mean reward: -1.7569444444444444, Mean Entropy: 0.11611582338809967, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 797,  Mean reward: -3.5337837837837838, Mean Entropy: 0.19451284408569336, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.84s
Iteration: 798,  Mean reward: -0.94, Mean Entropy: 0.15060192346572876, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 799,  Mean reward: -1.76, Mean Entropy: 0.15586209297180176, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 800,  Mean reward: -1.9324324324324325, Mean Entropy: 0.16471393406391144, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
rec seq len 2
actor lr 0.0005
Iteration: 801,  Mean reward: -1.4933333333333334, Mean Entropy: 0.13087645173072815, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 802,  Mean reward: -1.74, Mean Entropy: 0.12723702192306519, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 803,  Mean reward: -2.089041095890411, Mean Entropy: 0.16141575574874878, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 804,  Mean reward: -0.2905405405405405, Mean Entropy: 0.1076689213514328, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 805,  Mean reward: -1.3486842105263157, Mean Entropy: 0.14937421679496765, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 806,  Mean reward: -0.581081081081081, Mean Entropy: 0.08103969693183899, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 807,  Mean reward: -1.0320512820512822, Mean Entropy: 0.119105264544487, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 808,  Mean reward: -2.3133333333333335, Mean Entropy: 0.18150357902050018, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 809,  Mean reward: -2.869565217391304, Mean Entropy: 0.18177632987499237, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 810,  Mean reward: -2.4788732394366195, Mean Entropy: 0.12701080739498138, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 811,  Mean reward: -2.910958904109589, Mean Entropy: 0.14356482028961182, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 812,  Mean reward: -1.0657894736842106, Mean Entropy: 0.11565208435058594, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 813,  Mean reward: -2.7027027027027026, Mean Entropy: 0.12208700180053711, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 814,  Mean reward: -1.4533333333333334, Mean Entropy: 0.07643182575702667, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 815,  Mean reward: -1.0, Mean Entropy: 0.15514825284481049, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 816,  Mean reward: -2.76056338028169, Mean Entropy: 0.19307750463485718, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 817,  Mean reward: -1.582191780821918, Mean Entropy: 0.1572328805923462, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 818,  Mean reward: -2.5144927536231885, Mean Entropy: 0.17782513797283173, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 819,  Mean reward: -2.089041095890411, Mean Entropy: 0.13355372846126556, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 820,  Mean reward: -2.7432432432432434, Mean Entropy: 0.16478018462657928, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 821,  Mean reward: -1.1805555555555556, Mean Entropy: 0.08942794799804688, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 822,  Mean reward: -1.9113924050632911, Mean Entropy: 0.17059442400932312, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 823,  Mean reward: -4.340579710144928, Mean Entropy: 0.20121388137340546, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 824,  Mean reward: 0.22916666666666666, Mean Entropy: 0.12041470408439636, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 825,  Mean reward: -1.3092105263157894, Mean Entropy: 0.12809213995933533, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 826,  Mean reward: -3.125, Mean Entropy: 0.17307113111019135, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 827,  Mean reward: -1.5410958904109588, Mean Entropy: 0.13420972228050232, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 828,  Mean reward: -4.690140845070423, Mean Entropy: 0.18242482841014862, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 829,  Mean reward: -2.9315068493150687, Mean Entropy: 0.1692139357328415, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 830,  Mean reward: -3.184931506849315, Mean Entropy: 0.167905792593956, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 831,  Mean reward: -2.722972972972973, Mean Entropy: 0.11769407987594604, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 832,  Mean reward: -2.076923076923077, Mean Entropy: 0.187602698802948, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 833,  Mean reward: -1.5071428571428571, Mean Entropy: 0.15004214644432068, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 834,  Mean reward: -1.5616438356164384, Mean Entropy: 0.1364622414112091, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 835,  Mean reward: -2.026666666666667, Mean Entropy: 0.18746159970760345, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 836,  Mean reward: -1.6338028169014085, Mean Entropy: 0.11777134984731674, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 837,  Mean reward: -2.2467532467532467, Mean Entropy: 0.15382853150367737, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 838,  Mean reward: -1.7361111111111112, Mean Entropy: 0.12460337579250336, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 839,  Mean reward: -1.78, Mean Entropy: 0.11246468126773834, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.83s
Iteration: 840,  Mean reward: -3.0135135135135136, Mean Entropy: 0.17114143073558807, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 841,  Mean reward: -3.6805555555555554, Mean Entropy: 0.16456539928913116, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 842,  Mean reward: -1.368421052631579, Mean Entropy: 0.10989867150783539, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 843,  Mean reward: -0.2905405405405405, Mean Entropy: 0.12426811456680298, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 844,  Mean reward: -1.1688311688311688, Mean Entropy: 0.14328999817371368, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 845,  Mean reward: -0.625, Mean Entropy: 0.11316557973623276, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 846,  Mean reward: -1.2266666666666666, Mean Entropy: 0.11537041515111923, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 847,  Mean reward: -1.2466666666666666, Mean Entropy: 0.12973251938819885, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 848,  Mean reward: -0.581081081081081, Mean Entropy: 0.1075076013803482, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 849,  Mean reward: -3.0135135135135136, Mean Entropy: 0.20991063117980957, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 850,  Mean reward: -2.4927536231884058, Mean Entropy: 0.14037196338176727, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 851,  Mean reward: -2.962686567164179, Mean Entropy: 0.12526214122772217, complete_episode_count: 67.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 852,  Mean reward: -2.8466666666666667, Mean Entropy: 0.18071246147155762, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 853,  Mean reward: -2.5694444444444446, Mean Entropy: 0.17076687514781952, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 854,  Mean reward: -2.8055555555555554, Mean Entropy: 0.16138222813606262, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 855,  Mean reward: -1.8150684931506849, Mean Entropy: 0.10372613370418549, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 856,  Mean reward: -2.1095890410958904, Mean Entropy: 0.12669162452220917, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 857,  Mean reward: -2.7635135135135136, Mean Entropy: 0.13866083323955536, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 858,  Mean reward: -2.8266666666666667, Mean Entropy: 0.22207951545715332, complete_episode_count: 75.0, Gather time: 0.56s, Train time: 0.66s
Iteration: 859,  Mean reward: 0.7785714285714286, Mean Entropy: 0.11476203054189682, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 860,  Mean reward: -3.7171052631578947, Mean Entropy: 0.17449496686458588, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 861,  Mean reward: -3.323943661971831, Mean Entropy: 0.1693216860294342, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 862,  Mean reward: -3.7928571428571427, Mean Entropy: 0.15567395091056824, complete_episode_count: 70.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 863,  Mean reward: -2.463235294117647, Mean Entropy: 0.13041551411151886, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 864,  Mean reward: -3.458904109589041, Mean Entropy: 0.1744053065776825, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 865,  Mean reward: -3.0422535211267605, Mean Entropy: 0.1257075071334839, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 866,  Mean reward: -2.6164383561643834, Mean Entropy: 0.12436938285827637, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 867,  Mean reward: -3.8445945945945947, Mean Entropy: 0.16786113381385803, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 868,  Mean reward: -4.957142857142857, Mean Entropy: 0.20229828357696533, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 869,  Mean reward: -1.1805555555555556, Mean Entropy: 0.07890357077121735, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 870,  Mean reward: -1.4733333333333334, Mean Entropy: 0.12351306527853012, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 871,  Mean reward: -1.4675324675324675, Mean Entropy: 0.1545434296131134, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 872,  Mean reward: -1.8150684931506849, Mean Entropy: 0.1322087049484253, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 873,  Mean reward: -1.3486842105263157, Mean Entropy: 0.16546395421028137, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 874,  Mean reward: -3.125, Mean Entropy: 0.16094432771205902, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 875,  Mean reward: -1.7361111111111112, Mean Entropy: 0.12669599056243896, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 876,  Mean reward: -1.8150684931506849, Mean Entropy: 0.1212533563375473, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 877,  Mean reward: -4.147887323943662, Mean Entropy: 0.15089131891727448, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 878,  Mean reward: -1.0136986301369864, Mean Entropy: 0.11257036030292511, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 879,  Mean reward: -2.4527027027027026, Mean Entropy: 0.1611272692680359, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 880,  Mean reward: -1.0136986301369864, Mean Entropy: 0.10927164554595947, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 881,  Mean reward: -2.9932432432432434, Mean Entropy: 0.18444637954235077, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 882,  Mean reward: -1.8356164383561644, Mean Entropy: 0.1482909470796585, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 883,  Mean reward: -4.471830985915493, Mean Entropy: 0.1884458363056183, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 884,  Mean reward: -4.8161764705882355, Mean Entropy: 0.17981326580047607, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 885,  Mean reward: -1.934782608695652, Mean Entropy: 0.14800049364566803, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 886,  Mean reward: -0.1619718309859155, Mean Entropy: 0.08922432363033295, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 887,  Mean reward: -1.3289473684210527, Mean Entropy: 0.16805200278759003, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 888,  Mean reward: -1.9324324324324325, Mean Entropy: 0.16817288100719452, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 889,  Mean reward: -2.472972972972973, Mean Entropy: 0.15450090169906616, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 890,  Mean reward: -0.3108108108108108, Mean Entropy: 0.13477396965026855, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 891,  Mean reward: -3.6267605633802815, Mean Entropy: 0.17139403522014618, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 892,  Mean reward: -0.5608108108108109, Mean Entropy: 0.13599494099617004, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 893,  Mean reward: -2.5694444444444446, Mean Entropy: 0.16314184665679932, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 894,  Mean reward: -3.4794520547945207, Mean Entropy: 0.18501782417297363, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 895,  Mean reward: -1.4166666666666667, Mean Entropy: 0.1147579476237297, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 896,  Mean reward: -1.0810810810810811, Mean Entropy: 0.12355019897222519, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 897,  Mean reward: -3.8445945945945947, Mean Entropy: 0.20266768336296082, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 898,  Mean reward: -1.7569444444444444, Mean Entropy: 0.15250740945339203, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 899,  Mean reward: -2.952054794520548, Mean Entropy: 0.15214890241622925, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 900,  Mean reward: -0.8513513513513513, Mean Entropy: 0.12736836075782776, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
rec seq len 2
actor lr 0.0005
Iteration: 901,  Mean reward: -0.625, Mean Entropy: 0.11327557265758514, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 902,  Mean reward: -1.226027397260274, Mean Entropy: 0.1388414204120636, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 903,  Mean reward: -1.4933333333333334, Mean Entropy: 0.11294208467006683, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 904,  Mean reward: -0.25, Mean Entropy: 0.1539911925792694, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 905,  Mean reward: -1.5704225352112675, Mean Entropy: 0.12388791888952255, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 906,  Mean reward: -0.013157894736842105, Mean Entropy: 0.09889347851276398, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 907,  Mean reward: -2.1184210526315788, Mean Entropy: 0.1661996692419052, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 908,  Mean reward: -3.427536231884058, Mean Entropy: 0.15553158521652222, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 909,  Mean reward: -3.5285714285714285, Mean Entropy: 0.1540854424238205, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 910,  Mean reward: -2.1911764705882355, Mean Entropy: 0.15283411741256714, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 911,  Mean reward: -1.7361111111111112, Mean Entropy: 0.13777175545692444, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 912,  Mean reward: -2.826388888888889, Mean Entropy: 0.15932004153728485, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 913,  Mean reward: -4.227941176470588, Mean Entropy: 0.17386990785598755, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 914,  Mean reward: 0.09859154929577464, Mean Entropy: 0.13085737824440002, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 915,  Mean reward: -3.8142857142857145, Mean Entropy: 0.15245869755744934, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 916,  Mean reward: -3.449275362318841, Mean Entropy: 0.13079388439655304, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 917,  Mean reward: -3.6805555555555554, Mean Entropy: 0.14497938752174377, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 918,  Mean reward: -2.2681159420289854, Mean Entropy: 0.13082855939865112, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 919,  Mean reward: -1.5616438356164384, Mean Entropy: 0.12341665476560593, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 920,  Mean reward: -1.1597222222222223, Mean Entropy: 0.1377558410167694, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 921,  Mean reward: -1.8732394366197183, Mean Entropy: 0.16662639379501343, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 922,  Mean reward: -3.0955882352941178, Mean Entropy: 0.13070933520793915, complete_episode_count: 68.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 923,  Mean reward: -2.536231884057971, Mean Entropy: 0.1305980086326599, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 924,  Mean reward: -1.3768115942028984, Mean Entropy: 0.11594151705503464, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.83s
Iteration: 925,  Mean reward: -1.9366197183098592, Mean Entropy: 0.1301662027835846, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 926,  Mean reward: -2.363013698630137, Mean Entropy: 0.18072721362113953, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 927,  Mean reward: -3.8805970149253732, Mean Entropy: 0.11578880995512009, complete_episode_count: 67.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 928,  Mean reward: -2.246376811594203, Mean Entropy: 0.14452385902404785, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 929,  Mean reward: -3.782608695652174, Mean Entropy: 0.13532111048698425, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 930,  Mean reward: -3.8142857142857145, Mean Entropy: 0.13141125440597534, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 931,  Mean reward: -2.013888888888889, Mean Entropy: 0.06142718717455864, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 932,  Mean reward: -0.5, Mean Entropy: 0.03368170186877251, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 933,  Mean reward: -2.25, Mean Entropy: 0.1882103979587555, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 934,  Mean reward: -2.4788732394366195, Mean Entropy: 0.13697874546051025, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 935,  Mean reward: -1.9722222222222223, Mean Entropy: 0.14452067017555237, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 936,  Mean reward: -1.368421052631579, Mean Entropy: 0.16623756289482117, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 937,  Mean reward: -4.028985507246377, Mean Entropy: 0.1446622908115387, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 938,  Mean reward: -3.661764705882353, Mean Entropy: 0.15179714560508728, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 939,  Mean reward: -3.911764705882353, Mean Entropy: 0.15896402299404144, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 940,  Mean reward: -2.579710144927536, Mean Entropy: 0.1589597463607788, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 941,  Mean reward: -1.4583333333333333, Mean Entropy: 0.13003671169281006, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 942,  Mean reward: -0.18309859154929578, Mean Entropy: 0.1228242963552475, complete_episode_count: 71.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 943,  Mean reward: -4.25, Mean Entropy: 0.16622509062290192, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 944,  Mean reward: -3.887323943661972, Mean Entropy: 0.18043886125087738, complete_episode_count: 71.0, Gather time: 0.53s, Train time: 0.65s
Iteration: 945,  Mean reward: -2.9571428571428573, Mean Entropy: 0.12278161942958832, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 946,  Mean reward: -2.8680555555555554, Mean Entropy: 0.16560667753219604, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 947,  Mean reward: -0.581081081081081, Mean Entropy: 0.09852802753448486, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 948,  Mean reward: -2.472972972972973, Mean Entropy: 0.16942666471004486, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 949,  Mean reward: -1.6824324324324325, Mean Entropy: 0.14468544721603394, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 950,  Mean reward: -2.2708333333333335, Mean Entropy: 0.1304006427526474, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 951,  Mean reward: -1.855263157894737, Mean Entropy: 0.11986404657363892, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 952,  Mean reward: 0.0821917808219178, Mean Entropy: 0.10581745207309723, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 953,  Mean reward: -1.5333333333333334, Mean Entropy: 0.15112292766571045, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 954,  Mean reward: -4.006849315068493, Mean Entropy: 0.10385400056838989, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 955,  Mean reward: 1.0192307692307692, Mean Entropy: 0.09462862461805344, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 956,  Mean reward: -1.448051948051948, Mean Entropy: 0.11377456039190292, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 957,  Mean reward: -2.2933333333333334, Mean Entropy: 0.1197148784995079, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 958,  Mean reward: 0.06666666666666667, Mean Entropy: 0.08857958018779755, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 959,  Mean reward: -0.5192307692307693, Mean Entropy: 0.10607045888900757, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 960,  Mean reward: -1.2266666666666666, Mean Entropy: 0.11895491182804108, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 961,  Mean reward: -4.671428571428572, Mean Entropy: 0.12924277782440186, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 962,  Mean reward: -1.662162162162162, Mean Entropy: 0.08347373455762863, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 963,  Mean reward: -0.7828947368421053, Mean Entropy: 0.06307637691497803, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 964,  Mean reward: -2.2733333333333334, Mean Entropy: 0.10649071633815765, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 965,  Mean reward: -0.7133333333333334, Mean Entropy: 0.06466534733772278, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 966,  Mean reward: -2.207792207792208, Mean Entropy: 0.17289051413536072, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 967,  Mean reward: -1.3486842105263157, Mean Entropy: 0.12933552265167236, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.83s
Iteration: 968,  Mean reward: 0.3561643835616438, Mean Entropy: 0.12333725392818451, complete_episode_count: 73.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 969,  Mean reward: -3.423611111111111, Mean Entropy: 0.1417401283979416, complete_episode_count: 72.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 970,  Mean reward: -3.1159420289855073, Mean Entropy: 0.16863179206848145, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 971,  Mean reward: -2.914285714285714, Mean Entropy: 0.14342153072357178, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.66s
Iteration: 972,  Mean reward: -3.75, Mean Entropy: 0.1312248408794403, complete_episode_count: 70.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 973,  Mean reward: -0.6933333333333334, Mean Entropy: 0.11054299771785736, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 974,  Mean reward: -1.2266666666666666, Mean Entropy: 0.09291597455739975, complete_episode_count: 75.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 975,  Mean reward: -4.1, Mean Entropy: 0.09703487157821655, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 976,  Mean reward: -2.1184210526315788, Mean Entropy: 0.08848597854375839, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 977,  Mean reward: -0.7828947368421053, Mean Entropy: 0.04815692827105522, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 978,  Mean reward: -1.639240506329114, Mean Entropy: 0.05666986107826233, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 979,  Mean reward: -2.670886075949367, Mean Entropy: 0.12980005145072937, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 980,  Mean reward: -1.5256410256410255, Mean Entropy: 0.0909508764743805, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 981,  Mean reward: -2.2948717948717947, Mean Entropy: 0.10393211245536804, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 982,  Mean reward: -0.12987012987012986, Mean Entropy: 0.06700825691223145, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 983,  Mean reward: -2.551282051282051, Mean Entropy: 0.10028906911611557, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 984,  Mean reward: -1.8924050632911393, Mean Entropy: 0.1991400569677353, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 985,  Mean reward: -4.227941176470588, Mean Entropy: 0.11892583966255188, complete_episode_count: 68.0, Gather time: 0.53s, Train time: 0.67s
Iteration: 986,  Mean reward: -2.472972972972973, Mean Entropy: 0.15028439462184906, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 987,  Mean reward: -1.3918918918918919, Mean Entropy: 0.09554056823253632, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 988,  Mean reward: -2.972972972972973, Mean Entropy: 0.10455044358968735, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 989,  Mean reward: -1.6883116883116882, Mean Entropy: 0.0733632892370224, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 990,  Mean reward: -1.6883116883116882, Mean Entropy: 0.06585314869880676, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 991,  Mean reward: -3.2467532467532467, Mean Entropy: 0.05756671354174614, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 992,  Mean reward: 0.01282051282051282, Mean Entropy: 0.03846301883459091, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 993,  Mean reward: -2.207792207792208, Mean Entropy: 0.02430093288421631, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 994,  Mean reward: -1.25, Mean Entropy: 0.17480924725532532, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 995,  Mean reward: -2.0785714285714287, Mean Entropy: 0.1456652581691742, complete_episode_count: 70.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 996,  Mean reward: -1.710144927536232, Mean Entropy: 0.1385681927204132, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 997,  Mean reward: -4.007246376811594, Mean Entropy: 0.09835021197795868, complete_episode_count: 69.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 998,  Mean reward: -1.3289473684210527, Mean Entropy: 0.1281866878271103, complete_episode_count: 76.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 999,  Mean reward: -0.8896103896103896, Mean Entropy: 0.08167366683483124, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1000,  Mean reward: 1.6883116883116882, Mean Entropy: 0.06559492647647858, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
rec seq len 2
actor lr 0.0005
Iteration: 1001,  Mean reward: -0.6688311688311688, Mean Entropy: 0.043926697224378586, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1002,  Mean reward: -4.285714285714286, Mean Entropy: 0.06619461625814438, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1003,  Mean reward: -0.3924050632911392, Mean Entropy: 0.030258316546678543, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1004,  Mean reward: -0.7564102564102564, Mean Entropy: 0.027371905744075775, complete_episode_count: 78.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1005,  Mean reward: -0.75, Mean Entropy: 0.16139455139636993, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1006,  Mean reward: -2.246376811594203, Mean Entropy: 0.1386348158121109, complete_episode_count: 69.0, Gather time: 0.53s, Train time: 0.68s
Iteration: 1007,  Mean reward: -1.9527027027027026, Mean Entropy: 0.10376917570829391, complete_episode_count: 74.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1008,  Mean reward: -1.5205479452054795, Mean Entropy: 0.07909424602985382, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1009,  Mean reward: -0.6493506493506493, Mean Entropy: 0.09167402237653732, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.84s
Iteration: 1010,  Mean reward: -2.098684210526316, Mean Entropy: 0.1393374800682068, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1011,  Mean reward: -2.56, Mean Entropy: 0.08621466159820557, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1012,  Mean reward: -3.2662337662337664, Mean Entropy: 0.0982811450958252, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1013,  Mean reward: -2.56, Mean Entropy: 0.0338926687836647, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1014,  Mean reward: -1.5, Mean Entropy: 0.14056795835494995, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1015,  Mean reward: -2.5277777777777777, Mean Entropy: 0.09320555627346039, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1016,  Mean reward: -1.4675324675324675, Mean Entropy: 0.12048512697219849, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1017,  Mean reward: -3.736842105263158, Mean Entropy: 0.04000527039170265, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1018,  Mean reward: -2.207792207792208, Mean Entropy: 0.03960009291768074, complete_episode_count: 77.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1019,  Mean reward: -3.0, Mean Entropy: 0.1909114569425583, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 1020,  Mean reward: -2.5902777777777777, Mean Entropy: 0.11032812297344208, complete_episode_count: 72.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1021,  Mean reward: -3.1710526315789473, Mean Entropy: 0.06006639450788498, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1022,  Mean reward: -2.664473684210526, Mean Entropy: 0.05690976604819298, complete_episode_count: 76.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1023,  Mean reward: -2.2933333333333334, Mean Entropy: 0.02599191665649414, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1024,  Mean reward: -1.2692307692307692, Mean Entropy: 0.01460419874638319, complete_episode_count: 78.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1025,  Mean reward: -0.5, Mean Entropy: 0.013700686395168304, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.68s
Iteration: 1026,  Mean reward: -0.6265822784810127, Mean Entropy: 0.009436956606805325, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1027,  Mean reward: -2.0, Mean Entropy: 0.15763550996780396, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1028,  Mean reward: -1.2671232876712328, Mean Entropy: 0.07396911084651947, complete_episode_count: 73.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1029,  Mean reward: -3.32, Mean Entropy: 0.025997450575232506, complete_episode_count: 75.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1030,  Mean reward: -0.75, Mean Entropy: 0.10842926800251007, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1031,  Mean reward: 0.1891891891891892, Mean Entropy: 0.05348390340805054, complete_episode_count: 74.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1032,  Mean reward: -0.6493506493506493, Mean Entropy: 0.012400384061038494, complete_episode_count: 77.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1033,  Mean reward: -4.0, Mean Entropy: 0.026727601885795593, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1034,  Mean reward: -0.37341772151898733, Mean Entropy: 0.012174244970083237, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1035,  Mean reward: -2.3987341772151898, Mean Entropy: 0.008478937670588493, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1036,  Mean reward: -0.75, Mean Entropy: 0.008829602971673012, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1037,  Mean reward: 0.25, Mean Entropy: 0.015398161485791206, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1038,  Mean reward: -2.5, Mean Entropy: 0.008439073339104652, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1039,  Mean reward: -1.75, Mean Entropy: 0.008310300298035145, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1040,  Mean reward: -0.10126582278481013, Mean Entropy: 0.006548832170665264, complete_episode_count: 79.0, Gather time: 0.56s, Train time: 0.66s
Iteration: 1041,  Mean reward: -2.25, Mean Entropy: 0.007521528750658035, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1042,  Mean reward: -3.5, Mean Entropy: 0.010096745565533638, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1043,  Mean reward: -1.75, Mean Entropy: 0.01154760830104351, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1044,  Mean reward: -0.5, Mean Entropy: 0.012082974426448345, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1045,  Mean reward: -3.0, Mean Entropy: 0.01331899594515562, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1046,  Mean reward: -2.75, Mean Entropy: 0.011138061992824078, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1047,  Mean reward: -3.9177215189873418, Mean Entropy: 0.007097367197275162, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1048,  Mean reward: -1.0, Mean Entropy: 0.004825615789741278, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1049,  Mean reward: -2.0, Mean Entropy: 0.004925133194774389, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1050,  Mean reward: -3.0, Mean Entropy: 0.00633870717138052, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1051,  Mean reward: -1.75, Mean Entropy: 0.005186914466321468, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.83s
Iteration: 1052,  Mean reward: -3.0, Mean Entropy: 0.005088849924504757, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1053,  Mean reward: -1.0, Mean Entropy: 0.004861040040850639, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1054,  Mean reward: -2.25, Mean Entropy: 0.004907915368676186, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1055,  Mean reward: -2.25, Mean Entropy: 0.006259917747229338, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1056,  Mean reward: -2.5, Mean Entropy: 0.005978903733193874, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1057,  Mean reward: -1.25, Mean Entropy: 0.004751742351800203, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.65s
Iteration: 1058,  Mean reward: -1.5, Mean Entropy: 0.005699796136468649, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.65s
Iteration: 1059,  Mean reward: -3.75, Mean Entropy: 0.006250456906855106, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1060,  Mean reward: -2.25, Mean Entropy: 0.005199856590479612, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1061,  Mean reward: -3.5, Mean Entropy: 0.006541452370584011, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1062,  Mean reward: -2.0, Mean Entropy: 0.005946479272097349, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1063,  Mean reward: -0.75, Mean Entropy: 0.005467534996569157, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1064,  Mean reward: -2.5, Mean Entropy: 0.005993216298520565, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1065,  Mean reward: -1.5, Mean Entropy: 0.005158348474651575, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1066,  Mean reward: -3.0, Mean Entropy: 0.008841789327561855, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1067,  Mean reward: -2.25, Mean Entropy: 0.007121863774955273, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1068,  Mean reward: -2.5, Mean Entropy: 0.007080256938934326, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1069,  Mean reward: -2.651898734177215, Mean Entropy: 0.005934806074947119, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1070,  Mean reward: -0.879746835443038, Mean Entropy: 0.004287418909370899, complete_episode_count: 79.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1071,  Mean reward: -1.75, Mean Entropy: 0.004120406694710255, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1072,  Mean reward: -2.25, Mean Entropy: 0.004554384853690863, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.68s
Iteration: 1073,  Mean reward: -2.3987341772151898, Mean Entropy: 0.004110970534384251, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1074,  Mean reward: -1.5, Mean Entropy: 0.00254209921695292, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1075,  Mean reward: -1.5, Mean Entropy: 0.0027654613368213177, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1076,  Mean reward: -1.5, Mean Entropy: 0.002963461447507143, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1077,  Mean reward: -2.75, Mean Entropy: 0.003020090516656637, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1078,  Mean reward: -4.25, Mean Entropy: 0.003722554072737694, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1079,  Mean reward: -3.5, Mean Entropy: 0.0029770303517580032, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1080,  Mean reward: -2.5, Mean Entropy: 0.003111446276307106, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1081,  Mean reward: -3.25, Mean Entropy: 0.0029254755936563015, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1082,  Mean reward: -2.0, Mean Entropy: 0.002861976157873869, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1083,  Mean reward: -3.0, Mean Entropy: 0.003304659854620695, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.65s
Iteration: 1084,  Mean reward: -1.639240506329114, Mean Entropy: 0.00209828931838274, complete_episode_count: 79.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1085,  Mean reward: -1.5, Mean Entropy: 0.0028149927966296673, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1086,  Mean reward: -2.5, Mean Entropy: 0.002987203188240528, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1087,  Mean reward: -2.0, Mean Entropy: 0.002741830889135599, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1088,  Mean reward: -0.75, Mean Entropy: 0.0025130524300038815, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1089,  Mean reward: -2.0, Mean Entropy: 0.002621277002617717, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1090,  Mean reward: -2.75, Mean Entropy: 0.0029415511526167393, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1091,  Mean reward: -3.25, Mean Entropy: 0.0024901614524424076, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1092,  Mean reward: -3.5, Mean Entropy: 0.002448297804221511, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1093,  Mean reward: -2.5, Mean Entropy: 0.0023363162763416767, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.84s
Iteration: 1094,  Mean reward: -2.75, Mean Entropy: 0.003120867069810629, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1095,  Mean reward: -1.25, Mean Entropy: 0.002037663711234927, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1096,  Mean reward: -1.25, Mean Entropy: 0.0024921416770666838, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1097,  Mean reward: -2.25, Mean Entropy: 0.002801069989800453, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1098,  Mean reward: -3.75, Mean Entropy: 0.002734825015068054, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1099,  Mean reward: -2.5, Mean Entropy: 0.002345208777114749, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1100,  Mean reward: -2.5, Mean Entropy: 0.0026237010024487972, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
rec seq len 2
actor lr 0.0005
Iteration: 1101,  Mean reward: -2.25, Mean Entropy: 0.002451390726491809, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1102,  Mean reward: -3.0, Mean Entropy: 0.0027797080110758543, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1103,  Mean reward: -3.0, Mean Entropy: 0.0027879253029823303, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1104,  Mean reward: -1.5, Mean Entropy: 0.0025853565894067287, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1105,  Mean reward: -2.5, Mean Entropy: 0.002502975519746542, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1106,  Mean reward: -0.25, Mean Entropy: 0.002829731907695532, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.66s
Iteration: 1107,  Mean reward: -0.5, Mean Entropy: 0.0022185631096363068, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.67s
Iteration: 1108,  Mean reward: -1.0, Mean Entropy: 0.002306773094460368, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1109,  Mean reward: -2.5, Mean Entropy: 0.002500245114788413, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1110,  Mean reward: -0.25, Mean Entropy: 0.0020064893178641796, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1111,  Mean reward: -2.5, Mean Entropy: 0.0028066090308129787, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1112,  Mean reward: -2.5, Mean Entropy: 0.002888025250285864, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1113,  Mean reward: -2.5, Mean Entropy: 0.0026790047995746136, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1114,  Mean reward: -1.0, Mean Entropy: 0.002585317939519882, complete_episode_count: 80.0, Gather time: 0.55s, Train time: 0.65s
Iteration: 1115,  Mean reward: -2.5, Mean Entropy: 0.002751318272203207, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1116,  Mean reward: -1.75, Mean Entropy: 0.0021395590156316757, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1117,  Mean reward: -2.5, Mean Entropy: 0.003155754879117012, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1118,  Mean reward: -4.5, Mean Entropy: 0.003141659079119563, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1119,  Mean reward: -2.0, Mean Entropy: 0.0023867825511842966, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1120,  Mean reward: 0.75, Mean Entropy: 0.0018245450919494033, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1121,  Mean reward: -2.0, Mean Entropy: 0.0026419213972985744, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1122,  Mean reward: -4.0, Mean Entropy: 0.0027397151570767164, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.67s
Iteration: 1123,  Mean reward: -1.25, Mean Entropy: 0.0022906700614839792, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Iteration: 1124,  Mean reward: -1.75, Mean Entropy: 0.002110518980771303, complete_episode_count: 80.0, Gather time: 0.54s, Train time: 0.66s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.50it/s]100%|| 1/1 [00:00<00:00,  1.50it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 0.3 0.2 0.2 0.2 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.4 0.6 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 0.3 0.2 0.2 0.2 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 1/2, solve ratio: 0.500
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: -2.00 +/- 10.00
   Returns :[-12.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -2.00
Goal reached: 1 (50.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type EMB
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.44it/s]100%|| 1/1 [00:00<00:00,  1.44it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 1)
  s:             (3, 1) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  s:             (3, 2) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 2)
  Done after 5 steps, Captured: False Reward: -7.5


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 7)
  s:             (3, 7) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  s:             (3, 5) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  s:             (3, 5) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  s:             (3, 5) a 3 action_probs [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  Done after 5 steps, Captured: False Reward: -7.5


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 0/2, solve ratio: 0.000
Average episode length: 5.00 +/- 0.00
   Lengths :[5 5]
Average return: -7.50 +/- 0.00
   Returns :[-7.5 -7.5]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -7.50
Goal reached: 0 (0.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type EMB
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=False, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00,  1.64it/s]100%|| 1/1 [00:00<00:00,  1.64it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 2 action_probs [0.0 0.0 0.6 0.1 0.2 0.1 0.0 0.0] r -11.0 s_ (2, 2)
  Done after 2 steps, Captured: True Reward: -12.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 3 action_probs [0.0 0.0 0.0 0.9 0.1 0.0 0.0 0.0] r -1.5 s_ (3, 7)
  s:             (3, 7) a 3 action_probs [0.0 0.0 0.0 0.9 0.1 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  s:             (3, 5) a 3 action_probs [0.0 0.0 0.0 0.9 0.1 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  s:             (3, 5) a 3 action_probs [0.0 0.0 0.0 0.9 0.1 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  s:             (3, 5) a 3 action_probs [0.0 0.0 0.0 0.9 0.1 0.0 0.0 0.0] r -1.5 s_ (3, 5)
  Done after 5 steps, Captured: False Reward: -7.5


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 0.500
   Captures:[1 0]
Goal reached: 0/2, solve ratio: 0.000
Average episode length: 3.50 +/- 1.50
   Lengths :[2 5]
Average return: -9.75 +/- 2.25
   Returns :[-12.0 -7.5]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: -9.75
Goal reached: 0 (0.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type EMB
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_EMB_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: -6.416666666666667
  std over seeds: 3.2553374974374347
  per seed: [-2.000 -7.500 -9.750]

success_rate.......
  avg over seeds: 0.16666666666666666
  std over seeds: 0.23570226039551584
  per seed: [0.500 0.000 0.000]

