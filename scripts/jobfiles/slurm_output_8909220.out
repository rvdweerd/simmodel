batch_count: 1.6666666666666667
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
config args:
-----------------
train_on: MemTask-U1
batch_size: 48
obs_mask: freq
obs_rate: 0.2
emb_dim: 24
lstm_type: Dual
lstm_hdim: 24
lstm_layers: 1
emb_iterT: 5
nfm_func: NFM_ev_ec_t_dt_at_um_us
edge_blocking: True
solve_select: solvable
qnet: gat2
critic: v
train: True
eval: True
test: False
num_seeds: 3
seed0: 0
seedrange: range(0, 3)
demoruns: False
rootdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir: ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes: 8
max_edges: 14

hyperparameters:
-----------------
max_possible_nodes: 8
max_possible_edges: 14
emb_dim: 24
critic: v
node_dim: 7
lstm_on: True
hidden_size: 24
recurrent_layers: 1
batch_size: 48
min_reward: -1000000.0
discount: 0.99
gae_lambda: 0.95
ppo_clip: 0.2
ppo_epochs: 10
scale_reward: 1.0
max_grad_norm: 0.5
entropy_factor: 0.0
learning_rate: 0.0005
recurrent_seq_len: 2
parallel_rollouts: 4
rollout_steps: 40
patience: 500
trainable_std_dev: False
init_log_std_dev: 0.0
env_mask_velocity: False

train parameters:
-----------------
world_name: MemTask-U1
force_cpu_gather: True
gather_device: cpu
train_device: cuda
asynchronous_environment: False
invalid_tag_characters: re.compile('[^-/\\w\\.]')
save_metrics_tensorboard: True
save_parameters_tensorboard: False
checkpoint_frequency: 100
eval_deterministic: True

Model layout:
-----------------
MaskablePPOPolicy(
  (FE): FeatureExtractor(
    (gat): GATv2(7, 24, num_layers=5)
  )
  (PI): Actor(
    (theta5_pi): Linear(in_features=48, out_features=1, bias=True)
    (lstm): LSTM(24, 24)
    (theta6_pi): Linear(in_features=24, out_features=24, bias=True)
    (theta7_pi): Linear(in_features=24, out_features=24, bias=True)
  )
  (V): Critic(
    (lstm): LSTM(24, 24)
    (theta8_v): Linear(in_features=24, out_features=1, bias=True)
  )
)

Number of trainable parameters:
-----------------
Action Masked PPO Policy with A+C LSTMs and GATv2 feature extraction
------------------------------------------
FE.gat.convs.0.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.0.bias      [24]         requires_grad=True
FE.gat.convs.0.lin_l.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_l.bias [24]         requires_grad=True
FE.gat.convs.0.lin_r.weight [24, 7]      requires_grad=True
FE.gat.convs.0.lin_r.bias [24]         requires_grad=True
FE.gat.convs.1.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.1.bias      [24]         requires_grad=True
FE.gat.convs.1.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_l.bias [24]         requires_grad=True
FE.gat.convs.1.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.1.lin_r.bias [24]         requires_grad=True
FE.gat.convs.2.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.2.bias      [24]         requires_grad=True
FE.gat.convs.2.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_l.bias [24]         requires_grad=True
FE.gat.convs.2.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.2.lin_r.bias [24]         requires_grad=True
FE.gat.convs.3.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.3.bias      [24]         requires_grad=True
FE.gat.convs.3.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_l.bias [24]         requires_grad=True
FE.gat.convs.3.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.3.lin_r.bias [24]         requires_grad=True
FE.gat.convs.4.att       [1, 1, 24]   requires_grad=True
FE.gat.convs.4.bias      [24]         requires_grad=True
FE.gat.convs.4.lin_l.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_l.bias [24]         requires_grad=True
FE.gat.convs.4.lin_r.weight [24, 24]     requires_grad=True
FE.gat.convs.4.lin_r.bias [24]         requires_grad=True
PI.log_std_dev           [33]         requires_grad=False
PI.theta5_pi.weight      [1, 48]      requires_grad=True
PI.theta5_pi.bias        [1]          requires_grad=True
PI.lstm.weight_ih_l0     [96, 24]     requires_grad=True
PI.lstm.weight_hh_l0     [96, 24]     requires_grad=True
PI.lstm.bias_ih_l0       [96]         requires_grad=True
PI.lstm.bias_hh_l0       [96]         requires_grad=True
PI.theta6_pi.weight      [24, 24]     requires_grad=True
PI.theta6_pi.bias        [24]         requires_grad=True
PI.theta7_pi.weight      [24, 24]     requires_grad=True
PI.theta7_pi.bias        [24]         requires_grad=True
V.lstm.weight_ih_l0      [96, 24]     requires_grad=True
V.lstm.weight_hh_l0      [96, 24]     requires_grad=True
V.lstm.bias_ih_l0        [96]         requires_grad=True
V.lstm.bias_hh_l0        [96]         requires_grad=True
V.theta8_v.weight        [1, 24]      requires_grad=True
V.theta8_v.bias          [1]          requires_grad=True
Total number of trainable parameters: 16298
------------------------------------------
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -2.302325581395349, Mean Entropy: 0.9675179719924927, complete_episode_count: 43.0, Gather time: 5.76s, Train time: 3.51s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -5.136363636363637, Mean Entropy: 0.9747382402420044, complete_episode_count: 44.0, Gather time: 0.60s, Train time: 1.61s
Iteration: 2,  Mean reward: -3.0952380952380953, Mean Entropy: 0.9530773162841797, complete_episode_count: 42.0, Gather time: 0.60s, Train time: 1.55s
Iteration: 3,  Mean reward: -7.2625, Mean Entropy: 0.9819585680961609, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.54s
Iteration: 4,  Mean reward: -6.604651162790698, Mean Entropy: 0.9530773162841797, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 5,  Mean reward: -4.775, Mean Entropy: 0.9747382402420044, complete_episode_count: 40.0, Gather time: 0.57s, Train time: 1.70s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 6,  Mean reward: -0.5957446808510638, Mean Entropy: 0.8953150510787964, complete_episode_count: 47.0, Gather time: 0.60s, Train time: 1.54s
Iteration: 7,  Mean reward: -7.171052631578948, Mean Entropy: 0.9314162135124207, complete_episode_count: 38.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 8,  Mean reward: -6.404761904761905, Mean Entropy: 0.9458567500114441, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 9,  Mean reward: -4.7682926829268295, Mean Entropy: 0.9097546339035034, complete_episode_count: 41.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 10,  Mean reward: -1.2045454545454546, Mean Entropy: 0.9241952896118164, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.61s
Iteration: 11,  Mean reward: -6.409090909090909, Mean Entropy: 0.9169756174087524, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 12,  Mean reward: -1.7613636363636365, Mean Entropy: 0.9963985681533813, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 13,  Mean reward: -4.6976744186046515, Mean Entropy: 0.9963981509208679, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 14,  Mean reward: -2.7023809523809526, Mean Entropy: 0.9675168395042419, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 15,  Mean reward: -2.902439024390244, Mean Entropy: 0.9675154089927673, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 16,  Mean reward: -4.7976190476190474, Mean Entropy: 0.9314109086990356, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 17,  Mean reward: -4.878048780487805, Mean Entropy: 0.960284948348999, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 18,  Mean reward: -2.3068181818181817, Mean Entropy: 0.974716305732727, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.55s
Iteration: 19,  Mean reward: -3.75, Mean Entropy: 0.9097404479980469, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 20,  Mean reward: -4.4523809523809526, Mean Entropy: 0.9963399767875671, complete_episode_count: 42.0, Gather time: 0.60s, Train time: 1.60s
Iteration: 21,  Mean reward: -4.511904761904762, Mean Entropy: 0.9241148233413696, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.61s
Iteration: 22,  Mean reward: -2.5568181818181817, Mean Entropy: 1.0105608701705933, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.59s
Iteration: 23,  Mean reward: -5.5, Mean Entropy: 0.9308089017868042, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.79s
Iteration: 24,  Mean reward: -7.214285714285714, Mean Entropy: 0.9310152530670166, complete_episode_count: 42.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 25,  Mean reward: -4.939024390243903, Mean Entropy: 0.9655553102493286, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 26,  Mean reward: -4.209302325581396, Mean Entropy: 0.9071881175041199, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 27,  Mean reward: -4.666666666666667, Mean Entropy: 0.9733365774154663, complete_episode_count: 48.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 28,  Mean reward: -5.726415094339623, Mean Entropy: 0.9948004484176636, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 29,  Mean reward: -3.8181818181818183, Mean Entropy: 0.9882634878158569, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 30,  Mean reward: -4.298076923076923, Mean Entropy: 0.9394776225090027, complete_episode_count: 52.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 31,  Mean reward: -4.67, Mean Entropy: 0.9564062356948853, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 32,  Mean reward: -3.5, Mean Entropy: 0.9076846837997437, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 33,  Mean reward: -5.0, Mean Entropy: 0.7685432434082031, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.69s
Iteration: 34,  Mean reward: -1.9919354838709677, Mean Entropy: 0.8246415853500366, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 35,  Mean reward: -2.1271186440677967, Mean Entropy: 0.7115164995193481, complete_episode_count: 59.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 36,  Mean reward: -1.9615384615384615, Mean Entropy: 0.7437605261802673, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 37,  Mean reward: -2.871212121212121, Mean Entropy: 0.709459662437439, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 38,  Mean reward: -2.6666666666666665, Mean Entropy: 0.6268629431724548, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 39,  Mean reward: -0.6911764705882353, Mean Entropy: 0.8223028182983398, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 40,  Mean reward: -3.306451612903226, Mean Entropy: 0.7595227956771851, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 41,  Mean reward: -2.5785714285714287, Mean Entropy: 0.7152522802352905, complete_episode_count: 70.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 42,  Mean reward: -4.216666666666667, Mean Entropy: 0.6845940947532654, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 43,  Mean reward: 0.5384615384615384, Mean Entropy: 0.7499618530273438, complete_episode_count: 65.0, Gather time: 0.74s, Train time: 0.78s
Iteration: 44,  Mean reward: -2.08955223880597, Mean Entropy: 0.6426502466201782, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 45,  Mean reward: -2.8333333333333335, Mean Entropy: 0.6221811771392822, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 46,  Mean reward: -1.4057971014492754, Mean Entropy: 0.6216721534729004, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 47,  Mean reward: -3.36, Mean Entropy: 0.5821447372436523, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 48,  Mean reward: -0.7205882352941176, Mean Entropy: 0.6694858074188232, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 49,  Mean reward: -1.3958333333333333, Mean Entropy: 0.5961155891418457, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 50,  Mean reward: 1.5298507462686568, Mean Entropy: 0.537788987159729, complete_episode_count: 67.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 51,  Mean reward: 0.6690140845070423, Mean Entropy: 0.4938705563545227, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 52,  Mean reward: -0.04929577464788732, Mean Entropy: 0.47351786494255066, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 53,  Mean reward: 0.25, Mean Entropy: 0.533012866973877, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 54,  Mean reward: 2.4305555555555554, Mean Entropy: 0.4851522445678711, complete_episode_count: 72.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 55,  Mean reward: 1.0416666666666667, Mean Entropy: 0.4694109857082367, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 56,  Mean reward: 2.084507042253521, Mean Entropy: 0.5093421936035156, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 57,  Mean reward: 0.9420289855072463, Mean Entropy: 0.5650540590286255, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 58,  Mean reward: 2.955223880597015, Mean Entropy: 0.5325275659561157, complete_episode_count: 67.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 59,  Mean reward: 1.7463768115942029, Mean Entropy: 0.5613579750061035, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 60,  Mean reward: 1.6544117647058822, Mean Entropy: 0.5255113244056702, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 61,  Mean reward: 2.65625, Mean Entropy: 0.5891749858856201, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 1.00s
Iteration: 62,  Mean reward: 1.046875, Mean Entropy: 0.6478942632675171, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 63,  Mean reward: 1.540983606557377, Mean Entropy: 0.5700847506523132, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 1.54s
Iteration: 64,  Mean reward: 2.4130434782608696, Mean Entropy: 0.612345278263092, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 65,  Mean reward: 1.1825396825396826, Mean Entropy: 0.6244663596153259, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 66,  Mean reward: -0.29365079365079366, Mean Entropy: 0.5926535129547119, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 67,  Mean reward: 1.628787878787879, Mean Entropy: 0.5703406929969788, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 68,  Mean reward: 2.146153846153846, Mean Entropy: 0.5098875761032104, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 69,  Mean reward: 3.376923076923077, Mean Entropy: 0.40855321288108826, complete_episode_count: 65.0, Gather time: 0.63s, Train time: 0.82s
Iteration: 70,  Mean reward: 2.2928571428571427, Mean Entropy: 0.38974854350090027, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 71,  Mean reward: 2.0, Mean Entropy: 0.5731768012046814, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 72,  Mean reward: 0.3790322580645161, Mean Entropy: 0.6293514966964722, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 1.56s
Iteration: 73,  Mean reward: 2.2205882352941178, Mean Entropy: 0.48351120948791504, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 74,  Mean reward: 2.2426470588235294, Mean Entropy: 0.5564997792243958, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 75,  Mean reward: 1.2890625, Mean Entropy: 0.5562571287155151, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 76,  Mean reward: 4.143939393939394, Mean Entropy: 0.45871782302856445, complete_episode_count: 66.0, Gather time: 0.62s, Train time: 0.75s
Iteration: 77,  Mean reward: 1.9794520547945205, Mean Entropy: 0.5212846994400024, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 78,  Mean reward: 3.463235294117647, Mean Entropy: 0.4649699628353119, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 79,  Mean reward: 1.4044117647058822, Mean Entropy: 0.5274219512939453, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 80,  Mean reward: 0.27611940298507465, Mean Entropy: 0.5694559812545776, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 81,  Mean reward: 3.707692307692308, Mean Entropy: 0.6105844378471375, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 82,  Mean reward: 2.661290322580645, Mean Entropy: 0.532045304775238, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 83,  Mean reward: 2.971014492753623, Mean Entropy: 0.5500686168670654, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 84,  Mean reward: 1.5076923076923077, Mean Entropy: 0.5211730599403381, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 85,  Mean reward: 3.515151515151515, Mean Entropy: 0.49593186378479004, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 86,  Mean reward: 4.243243243243243, Mean Entropy: 0.4233698844909668, complete_episode_count: 74.0, Gather time: 0.62s, Train time: 0.75s
Iteration: 87,  Mean reward: 3.4527027027027026, Mean Entropy: 0.4067550599575043, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 88,  Mean reward: 3.472972972972973, Mean Entropy: 0.3981606662273407, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 89,  Mean reward: 3.3904109589041096, Mean Entropy: 0.3639141023159027, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 90,  Mean reward: 3.4675324675324677, Mean Entropy: 0.2988331615924835, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 91,  Mean reward: 4.7368421052631575, Mean Entropy: 0.28868091106414795, complete_episode_count: 76.0, Gather time: 0.64s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 92,  Mean reward: 5.358974358974359, Mean Entropy: 0.2715739607810974, complete_episode_count: 78.0, Gather time: 0.65s, Train time: 0.82s
Iteration: 93,  Mean reward: 4.966666666666667, Mean Entropy: 0.29363131523132324, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 94,  Mean reward: 5.473333333333334, Mean Entropy: 0.2656920552253723, complete_episode_count: 75.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 95,  Mean reward: 5.026315789473684, Mean Entropy: 0.24985456466674805, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 96,  Mean reward: 5.084415584415584, Mean Entropy: 0.2125738263130188, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 97,  Mean reward: 5.910256410256411, Mean Entropy: 0.1994587630033493, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 98,  Mean reward: 6.304054054054054, Mean Entropy: 0.21232931315898895, complete_episode_count: 74.0, Gather time: 0.62s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 99,  Mean reward: 7.25, Mean Entropy: 0.13061273097991943, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 100,  Mean reward: 7.474683544303797, Mean Entropy: 0.13676008582115173, complete_episode_count: 79.0, Gather time: 0.63s, Train time: 0.97s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 7.474683544303797, Mean Entropy: 0.17385880649089813, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 102,  Mean reward: 7.448717948717949, Mean Entropy: 0.09894940257072449, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 103,  Mean reward: 6.67948717948718, Mean Entropy: 0.068511001765728, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 104,  Mean reward: 7.75, Mean Entropy: 0.053941063582897186, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.81s
Iteration: 105,  Mean reward: 7.25, Mean Entropy: 0.04255010932683945, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 106,  Mean reward: 7.705128205128205, Mean Entropy: 0.12890943884849548, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 107,  Mean reward: 7.75, Mean Entropy: 0.10570021718740463, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 108,  Mean reward: 7.941558441558442, Mean Entropy: 0.05925862491130829, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 109,  Mean reward: 7.75, Mean Entropy: 0.11820467561483383, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 110,  Mean reward: 7.657894736842105, Mean Entropy: 0.11360593140125275, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 111,  Mean reward: 6.25, Mean Entropy: 0.0422557108104229, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 112,  Mean reward: 8.0, Mean Entropy: 0.030725467950105667, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 113,  Mean reward: 7.75, Mean Entropy: 0.023055914789438248, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 114,  Mean reward: 8.0, Mean Entropy: 0.019330304116010666, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 115,  Mean reward: 7.981012658227848, Mean Entropy: 0.021266063675284386, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 116,  Mean reward: 8.0, Mean Entropy: 0.031991779804229736, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 117,  Mean reward: 7.727848101265823, Mean Entropy: 0.025585226714611053, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 118,  Mean reward: 7.961538461538462, Mean Entropy: 0.02184969000518322, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 119,  Mean reward: 7.75, Mean Entropy: 0.018226278945803642, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 120,  Mean reward: 8.0, Mean Entropy: 0.01443114411085844, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 121,  Mean reward: 8.0, Mean Entropy: 0.012579098343849182, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 122,  Mean reward: 8.0, Mean Entropy: 0.012357231229543686, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 123,  Mean reward: 8.0, Mean Entropy: 0.011494363658130169, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 124,  Mean reward: 8.0, Mean Entropy: 0.010243913158774376, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 125,  Mean reward: 8.0, Mean Entropy: 0.01063881628215313, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 126,  Mean reward: 8.0, Mean Entropy: 0.010189925320446491, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 127,  Mean reward: 8.0, Mean Entropy: 0.009765435941517353, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 128,  Mean reward: 8.0, Mean Entropy: 0.008926755748689175, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 129,  Mean reward: 8.0, Mean Entropy: 0.009395076893270016, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 130,  Mean reward: 8.0, Mean Entropy: 0.00902293436229229, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 131,  Mean reward: 8.0, Mean Entropy: 0.008176283910870552, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 132,  Mean reward: 8.0, Mean Entropy: 0.008536032401025295, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 133,  Mean reward: 8.0, Mean Entropy: 0.00831296294927597, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 134,  Mean reward: 8.0, Mean Entropy: 0.008178578689694405, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 135,  Mean reward: 8.0, Mean Entropy: 0.007715105079114437, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 136,  Mean reward: 7.981012658227848, Mean Entropy: 0.00973972212523222, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.82s
Iteration: 137,  Mean reward: 8.0, Mean Entropy: 0.011278476566076279, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 138,  Mean reward: 8.0, Mean Entropy: 0.008767390623688698, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.12s
Iteration: 139,  Mean reward: 7.75, Mean Entropy: 0.22725477814674377, complete_episode_count: 80.0, Gather time: 0.69s, Train time: 0.78s
Iteration: 140,  Mean reward: 6.425373134328358, Mean Entropy: 0.16718046367168427, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 141,  Mean reward: 3.930379746835443, Mean Entropy: 0.0370354950428009, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 142,  Mean reward: 7.5, Mean Entropy: 0.09198781102895737, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 143,  Mean reward: 7.474683544303797, Mean Entropy: 0.012756414711475372, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 144,  Mean reward: 7.75, Mean Entropy: 0.012341714464128017, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 145,  Mean reward: 7.981012658227848, Mean Entropy: 0.024830777198076248, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 146,  Mean reward: 8.0, Mean Entropy: 0.5640627145767212, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.84s
Iteration: 147,  Mean reward: 0.55, Mean Entropy: 0.680294930934906, complete_episode_count: 60.0, Gather time: 0.60s, Train time: 1.58s
Iteration: 148,  Mean reward: -0.5373134328358209, Mean Entropy: 0.6637663841247559, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 149,  Mean reward: -1.0857142857142856, Mean Entropy: 0.5666642785072327, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 150,  Mean reward: 1.1666666666666667, Mean Entropy: 0.4212743043899536, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 151,  Mean reward: 2.26, Mean Entropy: 0.45064133405685425, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 152,  Mean reward: 2.6901408450704225, Mean Entropy: 0.3687470555305481, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 153,  Mean reward: 3.84, Mean Entropy: 0.33251190185546875, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 154,  Mean reward: 3.985294117647059, Mean Entropy: 0.4299851357936859, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 155,  Mean reward: 3.7083333333333335, Mean Entropy: 0.39884400367736816, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 156,  Mean reward: 2.4583333333333335, Mean Entropy: 0.4303264617919922, complete_episode_count: 72.0, Gather time: 0.62s, Train time: 0.74s
Iteration: 157,  Mean reward: 2.236111111111111, Mean Entropy: 0.37784910202026367, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 158,  Mean reward: 1.8958333333333333, Mean Entropy: 0.39069828391075134, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 159,  Mean reward: 2.2205882352941178, Mean Entropy: 0.3774986267089844, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 160,  Mean reward: 0.7638888888888888, Mean Entropy: 0.40752753615379333, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 161,  Mean reward: 1.5405405405405406, Mean Entropy: 0.36394643783569336, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 162,  Mean reward: 2.141891891891892, Mean Entropy: 0.32054540514945984, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 163,  Mean reward: 3.239130434782609, Mean Entropy: 0.355207622051239, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 164,  Mean reward: 2.992857142857143, Mean Entropy: 0.32598018646240234, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 165,  Mean reward: 3.493150684931507, Mean Entropy: 0.3189658224582672, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 166,  Mean reward: 3.5933333333333333, Mean Entropy: 0.2853301465511322, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 167,  Mean reward: 3.25, Mean Entropy: 0.27737581729888916, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 168,  Mean reward: 4.694444444444445, Mean Entropy: 0.29183220863342285, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 169,  Mean reward: 7.898648648648648, Mean Entropy: 0.07648590952157974, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 170,  Mean reward: 7.0, Mean Entropy: 0.020653847604990005, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 171,  Mean reward: 7.981012658227848, Mean Entropy: 0.03161470219492912, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 172,  Mean reward: 7.75, Mean Entropy: 0.01067414041608572, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 173,  Mean reward: 8.0, Mean Entropy: 0.005027410574257374, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 174,  Mean reward: 8.0, Mean Entropy: 0.004715644288808107, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 175,  Mean reward: 8.0, Mean Entropy: 0.004161243326961994, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 176,  Mean reward: 8.0, Mean Entropy: 0.002658053068444133, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 1.01s
Iteration: 177,  Mean reward: 8.0, Mean Entropy: 0.0028294739313423634, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 178,  Mean reward: 8.0, Mean Entropy: 0.0023101952392607927, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 179,  Mean reward: 8.0, Mean Entropy: 0.0021323205437511206, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 180,  Mean reward: 8.0, Mean Entropy: 0.002395437564700842, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 181,  Mean reward: 8.0, Mean Entropy: 0.0018951086094602942, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 182,  Mean reward: 8.0, Mean Entropy: 0.0019267778843641281, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 183,  Mean reward: 8.0, Mean Entropy: 0.001918717985972762, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 184,  Mean reward: 8.0, Mean Entropy: 0.001655462896451354, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 185,  Mean reward: 8.0, Mean Entropy: 0.0014317238237708807, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 186,  Mean reward: 8.0, Mean Entropy: 0.001519202021881938, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 187,  Mean reward: 8.0, Mean Entropy: 0.0015027178451418877, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 188,  Mean reward: 8.0, Mean Entropy: 0.0013266311725601554, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 189,  Mean reward: 8.0, Mean Entropy: 0.0013370038941502571, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 190,  Mean reward: 8.0, Mean Entropy: 0.001387048396281898, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 191,  Mean reward: 8.0, Mean Entropy: 0.0014317450113594532, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 192,  Mean reward: 8.0, Mean Entropy: 0.0018203777726739645, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 193,  Mean reward: 8.0, Mean Entropy: 0.0025086209643632174, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 194,  Mean reward: 8.0, Mean Entropy: 0.012399250641465187, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 195,  Mean reward: 7.75, Mean Entropy: 0.0014994058292359114, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 196,  Mean reward: 8.0, Mean Entropy: 0.4528864622116089, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 197,  Mean reward: 3.287878787878788, Mean Entropy: 0.6953414678573608, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 198,  Mean reward: 1.4014084507042253, Mean Entropy: 0.5457193851470947, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 199,  Mean reward: 2.3, Mean Entropy: 0.49623554944992065, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 200,  Mean reward: 4.113333333333333, Mean Entropy: 0.43845027685165405, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 1.7013888888888888, Mean Entropy: 0.5533134341239929, complete_episode_count: 72.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 202,  Mean reward: 3.4527027027027026, Mean Entropy: 0.36688828468322754, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.93s
Iteration: 203,  Mean reward: 3.3066666666666666, Mean Entropy: 0.4018971920013428, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 204,  Mean reward: 2.2837837837837838, Mean Entropy: 0.4666014611721039, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 205,  Mean reward: 4.273333333333333, Mean Entropy: 0.17858460545539856, complete_episode_count: 75.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 206,  Mean reward: 6.67948717948718, Mean Entropy: 0.02019783854484558, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 207,  Mean reward: 8.0, Mean Entropy: 0.007751786150038242, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 208,  Mean reward: 8.0, Mean Entropy: 0.020760904997587204, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 209,  Mean reward: 7.25, Mean Entropy: 0.016291821375489235, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.82s
Iteration: 210,  Mean reward: 8.0, Mean Entropy: 0.4154728055000305, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 211,  Mean reward: 1.5916666666666666, Mean Entropy: 0.6309486627578735, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 212,  Mean reward: 0.6194029850746269, Mean Entropy: 0.5820741057395935, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 213,  Mean reward: 3.3493150684931505, Mean Entropy: 0.34591144323349, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 214,  Mean reward: 4.673611111111111, Mean Entropy: 0.20128273963928223, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.95s
Iteration: 215,  Mean reward: 5.955696202531645, Mean Entropy: 0.07096342742443085, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 216,  Mean reward: 7.5, Mean Entropy: 0.008415058255195618, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.81s
Iteration: 217,  Mean reward: 8.0, Mean Entropy: 0.0010229945182800293, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 218,  Mean reward: 8.0, Mean Entropy: 0.001970029203221202, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 219,  Mean reward: 8.0, Mean Entropy: 0.0057809255085885525, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 220,  Mean reward: 7.75, Mean Entropy: 0.007134683430194855, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 221,  Mean reward: 8.0, Mean Entropy: 0.06476148962974548, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 222,  Mean reward: 6.67948717948718, Mean Entropy: 0.007953990250825882, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 223,  Mean reward: 8.0, Mean Entropy: 0.07036517560482025, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 224,  Mean reward: 6.75, Mean Entropy: 0.003054080531001091, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 225,  Mean reward: 8.0, Mean Entropy: 0.027097171172499657, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 226,  Mean reward: 7.75, Mean Entropy: 0.016607515513896942, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 227,  Mean reward: 7.5, Mean Entropy: 0.005757886916399002, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 228,  Mean reward: 8.0, Mean Entropy: 0.047875504940748215, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 229,  Mean reward: 8.0, Mean Entropy: 0.5246838331222534, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 230,  Mean reward: -1.2619047619047619, Mean Entropy: 0.7299053072929382, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 231,  Mean reward: -3.640625, Mean Entropy: 0.6848828792572021, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 232,  Mean reward: -0.6428571428571429, Mean Entropy: 0.6970049738883972, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 233,  Mean reward: 0.3, Mean Entropy: 0.603545069694519, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 234,  Mean reward: 2.3203125, Mean Entropy: 0.6016128063201904, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 235,  Mean reward: 0.2803030303030303, Mean Entropy: 0.5336353182792664, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 236,  Mean reward: 4.845070422535211, Mean Entropy: 0.23849737644195557, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 237,  Mean reward: 3.6025641025641026, Mean Entropy: 0.22518198192119598, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 238,  Mean reward: 6.208860759493671, Mean Entropy: 0.10064105689525604, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 239,  Mean reward: 8.0, Mean Entropy: 0.125139981508255, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 240,  Mean reward: 4.75, Mean Entropy: 0.18148159980773926, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 241,  Mean reward: 5.584415584415584, Mean Entropy: 0.1933038830757141, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 242,  Mean reward: 6.5, Mean Entropy: 0.08487975597381592, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 243,  Mean reward: 7.685897435897436, Mean Entropy: 0.08748582005500793, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 244,  Mean reward: 7.901315789473684, Mean Entropy: 0.023669391870498657, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 245,  Mean reward: 7.961538461538462, Mean Entropy: 0.1651589721441269, complete_episode_count: 78.0, Gather time: 0.62s, Train time: 0.78s
Iteration: 246,  Mean reward: 6.883116883116883, Mean Entropy: 0.025241045281291008, complete_episode_count: 77.0, Gather time: 0.69s, Train time: 0.82s
Iteration: 247,  Mean reward: 7.708860759493671, Mean Entropy: 0.0010159574449062347, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 248,  Mean reward: 8.0, Mean Entropy: 0.04893144592642784, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 249,  Mean reward: 8.0, Mean Entropy: 0.11123731732368469, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 250,  Mean reward: 6.185897435897436, Mean Entropy: 0.0009967437945306301, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 251,  Mean reward: 8.0, Mean Entropy: 0.004890909418463707, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 252,  Mean reward: 7.981012658227848, Mean Entropy: 0.0015751754399389029, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.92s
Iteration: 253,  Mean reward: 8.0, Mean Entropy: 0.004363652318716049, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.81s
Iteration: 254,  Mean reward: 8.0, Mean Entropy: 0.009277606382966042, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 255,  Mean reward: 8.0, Mean Entropy: 0.03573271632194519, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 256,  Mean reward: 7.981012658227848, Mean Entropy: 0.09253750741481781, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 257,  Mean reward: -3.409090909090909, Mean Entropy: 0.5483757853507996, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 258,  Mean reward: 3.3285714285714287, Mean Entropy: 0.29458779096603394, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 259,  Mean reward: 6.828947368421052, Mean Entropy: 0.122406005859375, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 260,  Mean reward: 7.5, Mean Entropy: 0.007341439835727215, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 261,  Mean reward: 8.0, Mean Entropy: 0.19516144692897797, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 262,  Mean reward: 4.326086956521739, Mean Entropy: 0.3394205570220947, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 263,  Mean reward: 5.324675324675325, Mean Entropy: 0.012162654660642147, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 264,  Mean reward: 8.0, Mean Entropy: 0.05009869486093521, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 265,  Mean reward: 8.0, Mean Entropy: 0.127708300948143, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 266,  Mean reward: 4.25, Mean Entropy: 0.004202382639050484, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 267,  Mean reward: 8.0, Mean Entropy: 0.002287858398631215, complete_episode_count: 80.0, Gather time: 0.65s, Train time: 0.76s
Iteration: 268,  Mean reward: 8.0, Mean Entropy: 0.008216220885515213, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 269,  Mean reward: 8.0, Mean Entropy: 0.4202858805656433, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 270,  Mean reward: 1.8189655172413792, Mean Entropy: 0.6640546321868896, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 271,  Mean reward: 3.064516129032258, Mean Entropy: 0.554965615272522, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 272,  Mean reward: 4.554054054054054, Mean Entropy: 0.3847366273403168, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 273,  Mean reward: 5.732394366197183, Mean Entropy: 0.3573767840862274, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 274,  Mean reward: 7.006756756756757, Mean Entropy: 0.16084685921669006, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.81s
Iteration: 275,  Mean reward: 7.04, Mean Entropy: 0.01674671098589897, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 276,  Mean reward: 7.9423076923076925, Mean Entropy: 0.0010727079352363944, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 277,  Mean reward: 8.0, Mean Entropy: 0.0026078736409544945, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 278,  Mean reward: 7.981012658227848, Mean Entropy: 0.0027172639966011047, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 279,  Mean reward: 8.0, Mean Entropy: 0.18419921398162842, complete_episode_count: 80.0, Gather time: 0.62s, Train time: 0.76s
Iteration: 280,  Mean reward: 6.5, Mean Entropy: 0.014232742600142956, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 281,  Mean reward: 8.0, Mean Entropy: 0.2457299679517746, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 282,  Mean reward: 5.52054794520548, Mean Entropy: 0.21040713787078857, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 283,  Mean reward: 7.655405405405405, Mean Entropy: 0.06450128555297852, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 284,  Mean reward: 7.75, Mean Entropy: 0.011381130665540695, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 285,  Mean reward: 8.0, Mean Entropy: 0.17444291710853577, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 286,  Mean reward: 4.217105263157895, Mean Entropy: 0.19379472732543945, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 287,  Mean reward: 4.943037974683544, Mean Entropy: 0.1352919042110443, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 288,  Mean reward: 7.5, Mean Entropy: 0.07357411086559296, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 289,  Mean reward: 7.474683544303797, Mean Entropy: 0.14231964945793152, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 290,  Mean reward: 7.9423076923076925, Mean Entropy: 0.14905191957950592, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 291,  Mean reward: 6.0, Mean Entropy: 0.09666943550109863, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 292,  Mean reward: 6.715189873417722, Mean Entropy: 0.02362976223230362, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 293,  Mean reward: 8.0, Mean Entropy: 0.17567133903503418, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 294,  Mean reward: 5.634615384615385, Mean Entropy: 0.18495160341262817, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 295,  Mean reward: 7.326666666666667, Mean Entropy: 0.08480663597583771, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 296,  Mean reward: 7.941558441558442, Mean Entropy: 0.005657976493239403, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 297,  Mean reward: 8.0, Mean Entropy: 0.42590898275375366, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 298,  Mean reward: 0.5526315789473685, Mean Entropy: 0.64397132396698, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 299,  Mean reward: 1.1825396825396826, Mean Entropy: 0.6299129724502563, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 300,  Mean reward: -3.6557377049180326, Mean Entropy: 0.7379450798034668, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 0.76s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: -4.088709677419355, Mean Entropy: 0.748853862285614, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 302,  Mean reward: 0.6307692307692307, Mean Entropy: 1.0436049699783325, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 303,  Mean reward: -1.73, Mean Entropy: 0.8456950187683105, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 304,  Mean reward: -4.694444444444445, Mean Entropy: 0.9494923949241638, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 305,  Mean reward: -3.0113636363636362, Mean Entropy: 0.910637378692627, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 306,  Mean reward: -3.339622641509434, Mean Entropy: 0.9555240869522095, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 307,  Mean reward: -0.9166666666666666, Mean Entropy: 0.8980234861373901, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 308,  Mean reward: -2.3596491228070176, Mean Entropy: 0.8612194061279297, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.48s
Iteration: 309,  Mean reward: -1.6929824561403508, Mean Entropy: 0.8139710426330566, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 310,  Mean reward: -0.868421052631579, Mean Entropy: 0.7502491474151611, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 311,  Mean reward: -1.6, Mean Entropy: 0.8061763644218445, complete_episode_count: 55.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 312,  Mean reward: 2.9649122807017543, Mean Entropy: 0.7223925590515137, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 313,  Mean reward: 1.6101694915254237, Mean Entropy: 0.48622581362724304, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 314,  Mean reward: 2.7733333333333334, Mean Entropy: 0.3641897439956665, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 315,  Mean reward: 0.5333333333333333, Mean Entropy: 0.5518810749053955, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 316,  Mean reward: -0.4696969696969697, Mean Entropy: 0.539615273475647, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 317,  Mean reward: 5.753521126760563, Mean Entropy: 0.500150203704834, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 318,  Mean reward: 5.229166666666667, Mean Entropy: 0.31558912992477417, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 319,  Mean reward: 5.8441558441558445, Mean Entropy: 0.3625389337539673, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 320,  Mean reward: 4.021428571428571, Mean Entropy: 0.07133209705352783, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 321,  Mean reward: -1.25, Mean Entropy: 0.5312022566795349, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 322,  Mean reward: -3.025, Mean Entropy: 0.7530779838562012, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 323,  Mean reward: -4.224137931034483, Mean Entropy: 0.7979444861412048, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.57s
Iteration: 324,  Mean reward: -1.5583333333333333, Mean Entropy: 0.7079594135284424, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 325,  Mean reward: -1.7076923076923076, Mean Entropy: 0.6855615377426147, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.92s
Iteration: 326,  Mean reward: -1.876923076923077, Mean Entropy: 0.708803117275238, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 327,  Mean reward: -4.258064516129032, Mean Entropy: 0.8122929334640503, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 328,  Mean reward: -3.261904761904762, Mean Entropy: 0.7326011657714844, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 329,  Mean reward: -3.871212121212121, Mean Entropy: 0.664071261882782, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 330,  Mean reward: -4.315384615384615, Mean Entropy: 0.6871992945671082, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 331,  Mean reward: -0.5606060606060606, Mean Entropy: 0.7103540301322937, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 332,  Mean reward: -2.134920634920635, Mean Entropy: 0.721960723400116, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 333,  Mean reward: -2.65625, Mean Entropy: 0.7221209406852722, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 334,  Mean reward: -4.4453125, Mean Entropy: 0.7450279593467712, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 335,  Mean reward: -3.6311475409836067, Mean Entropy: 0.7336190938949585, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 336,  Mean reward: -2.357142857142857, Mean Entropy: 0.7448534369468689, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 337,  Mean reward: -5.274647887323944, Mean Entropy: 0.6229833364486694, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 338,  Mean reward: -2.2681159420289854, Mean Entropy: 0.6506250500679016, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 339,  Mean reward: -2.976923076923077, Mean Entropy: 0.5622478723526001, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 340,  Mean reward: -2.361842105263158, Mean Entropy: 0.4803277850151062, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 341,  Mean reward: -1.2066666666666668, Mean Entropy: 0.4724998474121094, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 342,  Mean reward: -5.689873417721519, Mean Entropy: 0.5612286329269409, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 343,  Mean reward: -3.5555555555555554, Mean Entropy: 0.7555809020996094, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 344,  Mean reward: -2.1666666666666665, Mean Entropy: 0.7100545763969421, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 345,  Mean reward: -2.1048387096774195, Mean Entropy: 0.7330085635185242, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 346,  Mean reward: -0.19230769230769232, Mean Entropy: 0.6877044439315796, complete_episode_count: 65.0, Gather time: 0.75s, Train time: 0.77s
Iteration: 347,  Mean reward: -2.265625, Mean Entropy: 0.7341920733451843, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 348,  Mean reward: -4.275, Mean Entropy: 0.7464424967765808, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 349,  Mean reward: -4.0, Mean Entropy: 0.7225670218467712, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 350,  Mean reward: -2.703125, Mean Entropy: 0.721211850643158, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 351,  Mean reward: -2.921875, Mean Entropy: 0.675377368927002, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 352,  Mean reward: -2.7222222222222223, Mean Entropy: 0.7211048007011414, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 353,  Mean reward: -1.4296875, Mean Entropy: 0.7325815558433533, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 354,  Mean reward: -1.7222222222222223, Mean Entropy: 0.721333920955658, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.81s
Iteration: 355,  Mean reward: -1.1825396825396826, Mean Entropy: 0.7216514348983765, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 356,  Mean reward: -1.3046875, Mean Entropy: 0.699033260345459, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 357,  Mean reward: -3.088235294117647, Mean Entropy: 0.7102820873260498, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 358,  Mean reward: -1.9444444444444444, Mean Entropy: 0.7216863036155701, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 359,  Mean reward: -1.598360655737705, Mean Entropy: 0.7216004133224487, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 360,  Mean reward: -2.484375, Mean Entropy: 0.7101188898086548, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 361,  Mean reward: -3.757575757575758, Mean Entropy: 0.6757199764251709, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 362,  Mean reward: -3.338709677419355, Mean Entropy: 0.7439338564872742, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 363,  Mean reward: -1.6363636363636365, Mean Entropy: 0.6638244986534119, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 364,  Mean reward: -3.7419354838709675, Mean Entropy: 0.7551608085632324, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 365,  Mean reward: -3.8968253968253967, Mean Entropy: 0.7093881368637085, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.96s
Iteration: 366,  Mean reward: -3.6507936507936507, Mean Entropy: 0.6980915069580078, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.87s
Iteration: 367,  Mean reward: -2.7983870967741935, Mean Entropy: 0.7324624061584473, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 368,  Mean reward: -3.4285714285714284, Mean Entropy: 0.7095283269882202, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 369,  Mean reward: -3.953125, Mean Entropy: 0.755351185798645, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 370,  Mean reward: -1.3333333333333333, Mean Entropy: 0.7782194018363953, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 371,  Mean reward: -2.346774193548387, Mean Entropy: 0.7429441213607788, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 372,  Mean reward: -0.5, Mean Entropy: 0.45678719878196716, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 373,  Mean reward: -2.75, Mean Entropy: 0.5245170593261719, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 374,  Mean reward: -2.536231884057971, Mean Entropy: 0.6173926591873169, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 375,  Mean reward: -0.6714285714285714, Mean Entropy: 0.5269457101821899, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 376,  Mean reward: -1.78, Mean Entropy: 0.3577379584312439, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 377,  Mean reward: -1.25, Mean Entropy: 0.4600277841091156, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.83s
Iteration: 378,  Mean reward: -2.532051282051282, Mean Entropy: 0.5370938181877136, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 379,  Mean reward: -2.6194029850746268, Mean Entropy: 0.6238390803337097, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 380,  Mean reward: -3.6056338028169015, Mean Entropy: 0.6520347595214844, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 381,  Mean reward: -3.6597222222222223, Mean Entropy: 0.5859606266021729, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 382,  Mean reward: -2.463768115942029, Mean Entropy: 0.6801693439483643, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 383,  Mean reward: -1.2205882352941178, Mean Entropy: 0.590630054473877, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 384,  Mean reward: -1.412162162162162, Mean Entropy: 0.3678940236568451, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 385,  Mean reward: -2.25, Mean Entropy: 0.32885581254959106, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 386,  Mean reward: -1.75, Mean Entropy: 0.3502364158630371, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 387,  Mean reward: -1.8924050632911393, Mean Entropy: 0.3521270751953125, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 388,  Mean reward: -3.25, Mean Entropy: 0.3421027660369873, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 389,  Mean reward: -2.75, Mean Entropy: 0.33214348554611206, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 390,  Mean reward: -1.25, Mean Entropy: 0.4043130576610565, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 391,  Mean reward: -0.6688311688311688, Mean Entropy: 0.37803587317466736, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 392,  Mean reward: -2.25, Mean Entropy: 0.5225896835327148, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 393,  Mean reward: -1.792857142857143, Mean Entropy: 0.629156231880188, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 394,  Mean reward: 0.20945945945945946, Mean Entropy: 0.4654012620449066, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 395,  Mean reward: -0.12025316455696203, Mean Entropy: 0.5038780570030212, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 396,  Mean reward: -1.5723684210526316, Mean Entropy: 0.44695794582366943, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 397,  Mean reward: -1.8924050632911393, Mean Entropy: 0.445717990398407, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 398,  Mean reward: -2.727272727272727, Mean Entropy: 0.5643370151519775, complete_episode_count: 77.0, Gather time: 0.62s, Train time: 0.76s
Iteration: 399,  Mean reward: -3.6769230769230767, Mean Entropy: 0.6919949054718018, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 400,  Mean reward: -2.6538461538461537, Mean Entropy: 0.7318296432495117, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.73s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: -4.204918032786885, Mean Entropy: 0.8143255710601807, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 402,  Mean reward: -2.153225806451613, Mean Entropy: 0.725084662437439, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 403,  Mean reward: -1.5606060606060606, Mean Entropy: 0.6778217554092407, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 404,  Mean reward: -3.4846153846153847, Mean Entropy: 0.708651065826416, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 405,  Mean reward: -2.441666666666667, Mean Entropy: 0.7409467101097107, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 406,  Mean reward: -5.598484848484849, Mean Entropy: 0.7044321894645691, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 407,  Mean reward: -2.142857142857143, Mean Entropy: 0.7580363154411316, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 408,  Mean reward: -3.6746031746031744, Mean Entropy: 0.7212445139884949, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 409,  Mean reward: -3.8688524590163933, Mean Entropy: 0.7665531039237976, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 410,  Mean reward: -0.44696969696969696, Mean Entropy: 0.6866297721862793, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 411,  Mean reward: -1.0, Mean Entropy: 0.7304109334945679, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 412,  Mean reward: -1.4296875, Mean Entropy: 0.6738568544387817, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 413,  Mean reward: 1.2611940298507462, Mean Entropy: 0.46150100231170654, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 414,  Mean reward: 3.0538461538461537, Mean Entropy: 0.4862043261528015, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 415,  Mean reward: 2.4714285714285715, Mean Entropy: 0.5195476412773132, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 416,  Mean reward: 2.753623188405797, Mean Entropy: 0.4904453754425049, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 417,  Mean reward: 4.3933333333333335, Mean Entropy: 0.18788431584835052, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 418,  Mean reward: 3.5, Mean Entropy: 0.18535786867141724, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 419,  Mean reward: 5.891025641025641, Mean Entropy: 0.16251985728740692, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 420,  Mean reward: 7.474683544303797, Mean Entropy: 0.18042632937431335, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 421,  Mean reward: 7.402597402597403, Mean Entropy: 0.1695796251296997, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 422,  Mean reward: 7.618421052631579, Mean Entropy: 0.1503087282180786, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 423,  Mean reward: 7.981012658227848, Mean Entropy: 0.2571834921836853, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 424,  Mean reward: 6.966216216216216, Mean Entropy: 0.2647334635257721, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 425,  Mean reward: 6.854166666666667, Mean Entropy: 0.36785265803337097, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 426,  Mean reward: 6.253846153846154, Mean Entropy: 0.4195573329925537, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 427,  Mean reward: 6.949275362318841, Mean Entropy: 0.3819034993648529, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 428,  Mean reward: 5.96875, Mean Entropy: 0.3873112201690674, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 429,  Mean reward: 6.440298507462686, Mean Entropy: 0.339519739151001, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 430,  Mean reward: 6.276923076923077, Mean Entropy: 0.3686663508415222, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 431,  Mean reward: 6.319672131147541, Mean Entropy: 0.4125101566314697, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 432,  Mean reward: 6.595238095238095, Mean Entropy: 0.3177810609340668, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 433,  Mean reward: 7.053846153846154, Mean Entropy: 0.2679674029350281, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 434,  Mean reward: 6.811594202898551, Mean Entropy: 0.3009090721607208, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 435,  Mean reward: 6.758064516129032, Mean Entropy: 0.34394773840904236, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 436,  Mean reward: 7.0, Mean Entropy: 0.39767253398895264, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 437,  Mean reward: 6.884615384615385, Mean Entropy: 0.4175759255886078, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 438,  Mean reward: 5.841269841269841, Mean Entropy: 0.41209346055984497, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 439,  Mean reward: 7.242424242424242, Mean Entropy: 0.35388457775115967, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 440,  Mean reward: 7.409090909090909, Mean Entropy: 0.2996063232421875, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 441,  Mean reward: 6.484848484848484, Mean Entropy: 0.36709147691726685, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 442,  Mean reward: 6.185483870967742, Mean Entropy: 0.4208357036113739, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 443,  Mean reward: 7.264705882352941, Mean Entropy: 0.32438337802886963, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 444,  Mean reward: 6.634920634920635, Mean Entropy: 0.34914106130599976, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.92s
Iteration: 445,  Mean reward: 7.037313432835821, Mean Entropy: 0.4239077568054199, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 446,  Mean reward: 7.635714285714286, Mean Entropy: 0.28927621245384216, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 447,  Mean reward: 6.887096774193548, Mean Entropy: 0.3691592812538147, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 448,  Mean reward: 7.123076923076923, Mean Entropy: 0.36798804998397827, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 449,  Mean reward: 6.477941176470588, Mean Entropy: 0.307303786277771, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 450,  Mean reward: 6.130769230769231, Mean Entropy: 0.3746277689933777, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 451,  Mean reward: 7.464285714285714, Mean Entropy: 0.3061003088951111, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 452,  Mean reward: 7.25, Mean Entropy: 0.2461595982313156, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 453,  Mean reward: 6.463235294117647, Mean Entropy: 0.3142925500869751, complete_episode_count: 68.0, Gather time: 0.75s, Train time: 0.77s
Iteration: 454,  Mean reward: 7.203125, Mean Entropy: 0.36411261558532715, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 455,  Mean reward: 6.731343283582089, Mean Entropy: 0.32366296648979187, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 456,  Mean reward: 7.291666666666667, Mean Entropy: 0.35155054926872253, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 457,  Mean reward: 5.063492063492063, Mean Entropy: 0.4444957673549652, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 458,  Mean reward: 7.1923076923076925, Mean Entropy: 0.3307082951068878, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 459,  Mean reward: 7.076923076923077, Mean Entropy: 0.3535749316215515, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 460,  Mean reward: 6.7, Mean Entropy: 0.4116687774658203, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 461,  Mean reward: 7.113636363636363, Mean Entropy: 0.2902326285839081, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 462,  Mean reward: 7.2835820895522385, Mean Entropy: 0.28579282760620117, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 463,  Mean reward: 7.1, Mean Entropy: 0.320672869682312, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 464,  Mean reward: 6.204545454545454, Mean Entropy: 0.2889882028102875, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 465,  Mean reward: 7.4, Mean Entropy: 0.29719918966293335, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 466,  Mean reward: 7.161538461538462, Mean Entropy: 0.24490754306316376, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 467,  Mean reward: 6.992647058823529, Mean Entropy: 0.29807889461517334, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 468,  Mean reward: 7.022388059701493, Mean Entropy: 0.29534441232681274, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 469,  Mean reward: 5.358333333333333, Mean Entropy: 0.36620569229125977, complete_episode_count: 60.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 470,  Mean reward: 7.6571428571428575, Mean Entropy: 0.2526167631149292, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 471,  Mean reward: 6.9921875, Mean Entropy: 0.3318684697151184, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 472,  Mean reward: 7.3671875, Mean Entropy: 0.36596640944480896, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 473,  Mean reward: 7.2727272727272725, Mean Entropy: 0.4191243350505829, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 474,  Mean reward: 6.1, Mean Entropy: 0.4058254659175873, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 475,  Mean reward: 6.677966101694915, Mean Entropy: 0.4425429105758667, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 476,  Mean reward: 6.461538461538462, Mean Entropy: 0.35385164618492126, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 477,  Mean reward: 6.714285714285714, Mean Entropy: 0.4124935567378998, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 478,  Mean reward: 7.521739130434782, Mean Entropy: 0.3309977948665619, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 479,  Mean reward: 7.0, Mean Entropy: 0.30152449011802673, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 480,  Mean reward: 6.573770491803279, Mean Entropy: 0.3850257396697998, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 481,  Mean reward: 7.3161764705882355, Mean Entropy: 0.44628581404685974, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 482,  Mean reward: 5.659090909090909, Mean Entropy: 0.38330671191215515, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 483,  Mean reward: 7.430555555555555, Mean Entropy: 0.2880512475967407, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 484,  Mean reward: 6.084615384615384, Mean Entropy: 0.5658403635025024, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.93s
Iteration: 485,  Mean reward: 5.388059701492537, Mean Entropy: 0.29587146639823914, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 486,  Mean reward: 6.555555555555555, Mean Entropy: 0.33065444231033325, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 487,  Mean reward: 6.838461538461538, Mean Entropy: 0.3785324990749359, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 488,  Mean reward: 6.7265625, Mean Entropy: 0.3765558898448944, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 489,  Mean reward: 6.880597014925373, Mean Entropy: 0.31507885456085205, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 490,  Mean reward: 6.420634920634921, Mean Entropy: 0.46201619505882263, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 491,  Mean reward: 5.061538461538461, Mean Entropy: 0.3993402421474457, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 492,  Mean reward: 7.0661764705882355, Mean Entropy: 0.308769166469574, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 493,  Mean reward: 7.360294117647059, Mean Entropy: 0.3235001564025879, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 494,  Mean reward: 4.849206349206349, Mean Entropy: 0.4170137345790863, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 495,  Mean reward: 7.426470588235294, Mean Entropy: 0.24652759730815887, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 496,  Mean reward: 6.17741935483871, Mean Entropy: 0.3200830817222595, complete_episode_count: 62.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 497,  Mean reward: 7.007462686567164, Mean Entropy: 0.29790550470352173, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 498,  Mean reward: 7.015151515151516, Mean Entropy: 0.2746918797492981, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 499,  Mean reward: 6.246153846153846, Mean Entropy: 0.38297176361083984, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 500,  Mean reward: 7.2, Mean Entropy: 0.2999456524848938, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 6.548387096774194, Mean Entropy: 0.3652079105377197, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 502,  Mean reward: 6.809523809523809, Mean Entropy: 0.38416776061058044, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 503,  Mean reward: 7.007692307692308, Mean Entropy: 0.3301481008529663, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 504,  Mean reward: 7.409090909090909, Mean Entropy: 0.2997860312461853, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 505,  Mean reward: 5.898305084745763, Mean Entropy: 0.3820146918296814, complete_episode_count: 59.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 506,  Mean reward: 7.074626865671642, Mean Entropy: 0.34869417548179626, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 507,  Mean reward: 7.076923076923077, Mean Entropy: 0.2952161431312561, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 508,  Mean reward: 6.9453125, Mean Entropy: 0.36597177386283875, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 509,  Mean reward: 6.4921875, Mean Entropy: 0.516883373260498, complete_episode_count: 64.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 510,  Mean reward: 6.111111111111111, Mean Entropy: 0.4937770366668701, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 511,  Mean reward: 5.475, Mean Entropy: 0.912319540977478, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.56s
Iteration: 512,  Mean reward: -2.47, Mean Entropy: 1.078155517578125, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 513,  Mean reward: -5.317073170731708, Mean Entropy: 0.9241824150085449, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 514,  Mean reward: -3.9404761904761907, Mean Entropy: 0.960290789604187, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.58s
Iteration: 515,  Mean reward: -3.658536585365854, Mean Entropy: 0.8953127861022949, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.56s
Iteration: 516,  Mean reward: -3.292682926829268, Mean Entropy: 0.9314039945602417, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 517,  Mean reward: -3.0609756097560976, Mean Entropy: 0.9891566038131714, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 518,  Mean reward: -4.128205128205129, Mean Entropy: 0.902496337890625, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 519,  Mean reward: -4.488095238095238, Mean Entropy: 0.9386072158813477, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 520,  Mean reward: -5.621951219512195, Mean Entropy: 0.9675135612487793, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.47s
Iteration: 521,  Mean reward: -5.948717948717949, Mean Entropy: 0.9386336207389832, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 522,  Mean reward: -2.857142857142857, Mean Entropy: 0.9096765518188477, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.72s
Iteration: 523,  Mean reward: -4.67948717948718, Mean Entropy: 0.9168795347213745, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 524,  Mean reward: -5.804878048780488, Mean Entropy: 0.9385029077529907, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 525,  Mean reward: -4.2368421052631575, Mean Entropy: 0.8951517939567566, complete_episode_count: 38.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 526,  Mean reward: -3.951219512195122, Mean Entropy: 0.9023213386535645, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 527,  Mean reward: -5.392857142857143, Mean Entropy: 0.9816438555717468, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 528,  Mean reward: -3.7, Mean Entropy: 0.9598247408866882, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 529,  Mean reward: -3.188888888888889, Mean Entropy: 0.981552243232727, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 530,  Mean reward: -3.5813953488372094, Mean Entropy: 1.0179007053375244, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 531,  Mean reward: -5.012195121951219, Mean Entropy: 0.9891626834869385, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 532,  Mean reward: -4.85, Mean Entropy: 0.9517544507980347, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 533,  Mean reward: -4.892857142857143, Mean Entropy: 0.9058961868286133, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 534,  Mean reward: -3.5875, Mean Entropy: 0.8592783808708191, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 535,  Mean reward: -3.2804878048780486, Mean Entropy: 0.9093931913375854, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 536,  Mean reward: -3.3846153846153846, Mean Entropy: 0.7982949018478394, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 537,  Mean reward: 3.8085106382978724, Mean Entropy: 0.7439128160476685, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 538,  Mean reward: 3.7, Mean Entropy: 0.6891548037528992, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 539,  Mean reward: 3.9166666666666665, Mean Entropy: 0.6529415845870972, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.44s
Iteration: 540,  Mean reward: 4.975, Mean Entropy: 0.575842559337616, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.52s
Iteration: 541,  Mean reward: 4.396825396825397, Mean Entropy: 0.5563805103302002, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 542,  Mean reward: 4.875, Mean Entropy: 0.5191918611526489, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 543,  Mean reward: 5.333333333333333, Mean Entropy: 0.6481422781944275, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 544,  Mean reward: 5.666666666666667, Mean Entropy: 0.5509788990020752, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 545,  Mean reward: 6.0661764705882355, Mean Entropy: 0.39023077487945557, complete_episode_count: 68.0, Gather time: 0.75s, Train time: 0.77s
Iteration: 546,  Mean reward: 4.090909090909091, Mean Entropy: 0.3934718072414398, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 547,  Mean reward: 7.359154929577465, Mean Entropy: 0.3207474946975708, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 548,  Mean reward: 7.3125, Mean Entropy: 0.2842121720314026, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 549,  Mean reward: 7.6066666666666665, Mean Entropy: 0.21559831500053406, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 550,  Mean reward: 7.84, Mean Entropy: 0.214277982711792, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 551,  Mean reward: 7.902597402597403, Mean Entropy: 0.11908008903265, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 552,  Mean reward: 7.981012658227848, Mean Entropy: 0.12219232320785522, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 553,  Mean reward: 7.6381578947368425, Mean Entropy: 0.09750869125127792, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 554,  Mean reward: 7.705128205128205, Mean Entropy: 0.07897095382213593, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 555,  Mean reward: 7.961538461538462, Mean Entropy: 0.03957096487283707, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 556,  Mean reward: 7.981012658227848, Mean Entropy: 0.025079837068915367, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 557,  Mean reward: 7.981012658227848, Mean Entropy: 0.04828156530857086, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.0239965058863163, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.93s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 0.01476141344755888, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 560,  Mean reward: 7.75, Mean Entropy: 0.04599139839410782, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 561,  Mean reward: 7.5, Mean Entropy: 0.0757034569978714, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 562,  Mean reward: 7.402597402597403, Mean Entropy: 0.19388386607170105, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 563,  Mean reward: 7.164383561643835, Mean Entropy: 0.2907634675502777, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 564,  Mean reward: 5.757142857142857, Mean Entropy: 0.18641361594200134, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 565,  Mean reward: 7.5675675675675675, Mean Entropy: 0.06466864049434662, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 566,  Mean reward: 7.961538461538462, Mean Entropy: 0.057802777737379074, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 567,  Mean reward: 7.981012658227848, Mean Entropy: 0.12481123208999634, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 568,  Mean reward: 7.881578947368421, Mean Entropy: 0.08012741059064865, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 569,  Mean reward: 7.981012658227848, Mean Entropy: 0.3362559676170349, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 570,  Mean reward: -3.2662337662337664, Mean Entropy: 0.1936107575893402, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 571,  Mean reward: 1.1455696202531647, Mean Entropy: 0.30230021476745605, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 572,  Mean reward: 1.3223684210526316, Mean Entropy: 0.3789216876029968, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 573,  Mean reward: 5.726666666666667, Mean Entropy: 0.3067791163921356, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 574,  Mean reward: 1.7465753424657535, Mean Entropy: 0.5140030384063721, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 575,  Mean reward: -1.6736111111111112, Mean Entropy: 0.5243800282478333, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 576,  Mean reward: 2.9675324675324677, Mean Entropy: 0.18037621676921844, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 577,  Mean reward: 7.236486486486487, Mean Entropy: 0.3848254382610321, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 578,  Mean reward: 1.3805970149253732, Mean Entropy: 0.4039795398712158, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 579,  Mean reward: 5.824675324675325, Mean Entropy: 0.16038335859775543, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 580,  Mean reward: 7.922077922077922, Mean Entropy: 0.06429654359817505, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 581,  Mean reward: 7.981012658227848, Mean Entropy: 0.037999123334884644, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 582,  Mean reward: 7.981012658227848, Mean Entropy: 0.053983625024557114, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.73s
Iteration: 583,  Mean reward: 7.173076923076923, Mean Entropy: 0.15277911722660065, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 584,  Mean reward: 7.5, Mean Entropy: 0.119107186794281, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 585,  Mean reward: 7.9423076923076925, Mean Entropy: 0.034480080008506775, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 586,  Mean reward: 7.9423076923076925, Mean Entropy: 0.013203807175159454, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 587,  Mean reward: 7.961538461538462, Mean Entropy: 0.01219453290104866, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 588,  Mean reward: 8.0, Mean Entropy: 0.6522324085235596, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 589,  Mean reward: -3.6818181818181817, Mean Entropy: 0.7057594656944275, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 590,  Mean reward: -1.0076923076923077, Mean Entropy: 0.7649469375610352, complete_episode_count: 65.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 591,  Mean reward: -2.507936507936508, Mean Entropy: 1.0685455799102783, complete_episode_count: 63.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 592,  Mean reward: -3.94, Mean Entropy: 1.0159046649932861, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.57s
Iteration: 593,  Mean reward: -3.9651162790697674, Mean Entropy: 0.8857969045639038, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 594,  Mean reward: 0.3645833333333333, Mean Entropy: 0.7544487714767456, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 595,  Mean reward: 2.5267857142857144, Mean Entropy: 0.70842045545578, complete_episode_count: 56.0, Gather time: 0.58s, Train time: 1.69s
Iteration: 596,  Mean reward: 3.3666666666666667, Mean Entropy: 0.4894087314605713, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 597,  Mean reward: 6.753623188405797, Mean Entropy: 0.26597416400909424, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 598,  Mean reward: 7.359154929577465, Mean Entropy: 0.15910908579826355, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 599,  Mean reward: 7.881578947368421, Mean Entropy: 0.23117247223854065, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 600,  Mean reward: 6.895833333333333, Mean Entropy: 0.22439377009868622, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 6.695945945945946, Mean Entropy: 0.15690064430236816, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 602,  Mean reward: 7.685897435897436, Mean Entropy: 0.04651692137122154, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 603,  Mean reward: 7.901315789473684, Mean Entropy: 0.035406745970249176, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 604,  Mean reward: 7.708860759493671, Mean Entropy: 0.04907584935426712, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 605,  Mean reward: 7.727848101265823, Mean Entropy: 0.017131362110376358, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.05191347002983093, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 607,  Mean reward: 7.961538461538462, Mean Entropy: 0.021125836297869682, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 608,  Mean reward: 8.0, Mean Entropy: 0.039604805409908295, complete_episode_count: 80.0, Gather time: 0.65s, Train time: 0.79s
Iteration: 609,  Mean reward: 7.981012658227848, Mean Entropy: 0.02741118334233761, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 610,  Mean reward: 7.75, Mean Entropy: 0.04537179321050644, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 611,  Mean reward: 7.962025316455696, Mean Entropy: 0.022943805903196335, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 612,  Mean reward: 7.75, Mean Entropy: 0.022262703627347946, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -4.688888888888889, Mean Entropy: 0.9241962432861328, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.49s
rec seq len 2
actor lr 0.0005
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 1,  Mean reward: -4.65, Mean Entropy: 0.9458569288253784, complete_episode_count: 40.0, Gather time: 0.58s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -3.953488372093023, Mean Entropy: 0.9097553491592407, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 3,  Mean reward: -7.5227272727272725, Mean Entropy: 0.9458566904067993, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 4,  Mean reward: -4.902439024390244, Mean Entropy: 0.9097557067871094, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 5,  Mean reward: -5.568181818181818, Mean Entropy: 0.9386353492736816, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 6,  Mean reward: -5.4375, Mean Entropy: 0.9314162731170654, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 7,  Mean reward: -4.666666666666667, Mean Entropy: 0.938633143901825, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 8,  Mean reward: -6.5, Mean Entropy: 0.9747132658958435, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 9,  Mean reward: -4.548780487804878, Mean Entropy: 0.953021228313446, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.53s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 10,  Mean reward: -2.3152173913043477, Mean Entropy: 0.9890777468681335, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 11,  Mean reward: -2.965909090909091, Mean Entropy: 0.902389645576477, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 12,  Mean reward: -3.0365853658536586, Mean Entropy: 0.90940260887146, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 13,  Mean reward: -5.151162790697675, Mean Entropy: 0.9165242910385132, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.56s
Iteration: 14,  Mean reward: -5.056818181818182, Mean Entropy: 0.9308730363845825, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 15,  Mean reward: -4.728260869565218, Mean Entropy: 0.959703803062439, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 16,  Mean reward: -3.7857142857142856, Mean Entropy: 0.9784552454948425, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 17,  Mean reward: -2.033333333333333, Mean Entropy: 0.9716591835021973, complete_episode_count: 45.0, Gather time: 0.58s, Train time: 1.70s
Iteration: 18,  Mean reward: -3.7, Mean Entropy: 0.9686676859855652, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 19,  Mean reward: -3.659090909090909, Mean Entropy: 0.9692355990409851, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 20,  Mean reward: -6.2682926829268295, Mean Entropy: 0.9294483065605164, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 21,  Mean reward: -3.5208333333333335, Mean Entropy: 0.8612076044082642, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 22,  Mean reward: -1.434782608695652, Mean Entropy: 0.9009830355644226, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 23,  Mean reward: -4.488636363636363, Mean Entropy: 0.9125567674636841, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 24,  Mean reward: -2.426829268292683, Mean Entropy: 0.8789889216423035, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 25,  Mean reward: -4.566666666666666, Mean Entropy: 0.925137460231781, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 26,  Mean reward: -2.8068181818181817, Mean Entropy: 0.894438624382019, complete_episode_count: 44.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 27,  Mean reward: -3.183673469387755, Mean Entropy: 0.8193590044975281, complete_episode_count: 49.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 28,  Mean reward: -3.6666666666666665, Mean Entropy: 0.8408564329147339, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 29,  Mean reward: -2.6176470588235294, Mean Entropy: 0.729418158531189, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 30,  Mean reward: -3.574074074074074, Mean Entropy: 0.8641254305839539, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 31,  Mean reward: 0.42592592592592593, Mean Entropy: 0.7681773900985718, complete_episode_count: 54.0, Gather time: 0.59s, Train time: 1.51s
Iteration: 32,  Mean reward: -1.12, Mean Entropy: 0.7627142667770386, complete_episode_count: 50.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 33,  Mean reward: -2.625, Mean Entropy: 0.8301579356193542, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 34,  Mean reward: 0.9150943396226415, Mean Entropy: 0.83042973279953, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.57s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 35,  Mean reward: 1.47, Mean Entropy: 0.7088190317153931, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 36,  Mean reward: 2.1530612244897958, Mean Entropy: 0.6612610220909119, complete_episode_count: 49.0, Gather time: 0.59s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 37,  Mean reward: 4.339622641509434, Mean Entropy: 0.6447103023529053, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 38,  Mean reward: 3.7636363636363637, Mean Entropy: 0.6494588255882263, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 39,  Mean reward: 4.916666666666667, Mean Entropy: 0.6520662903785706, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 40,  Mean reward: 3.236842105263158, Mean Entropy: 0.6788220405578613, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 41,  Mean reward: 3.3425925925925926, Mean Entropy: 0.5984932780265808, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 42,  Mean reward: 6.8, Mean Entropy: 0.507609486579895, complete_episode_count: 65.0, Gather time: 0.60s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 43,  Mean reward: 6.896825396825397, Mean Entropy: 0.41913992166519165, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 44,  Mean reward: 6.483333333333333, Mean Entropy: 0.389974445104599, complete_episode_count: 60.0, Gather time: 0.58s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 45,  Mean reward: 6.944444444444445, Mean Entropy: 0.3180909752845764, complete_episode_count: 63.0, Gather time: 0.60s, Train time: 0.73s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 46,  Mean reward: 7.409090909090909, Mean Entropy: 0.37340259552001953, complete_episode_count: 66.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 47,  Mean reward: 7.286764705882353, Mean Entropy: 0.3578159213066101, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 48,  Mean reward: 7.164179104477612, Mean Entropy: 0.3410812318325043, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.71s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 49,  Mean reward: 7.454545454545454, Mean Entropy: 0.31902197003364563, complete_episode_count: 66.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 50,  Mean reward: 7.152777777777778, Mean Entropy: 0.3109423518180847, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.91s
Iteration: 51,  Mean reward: 6.865671641791045, Mean Entropy: 0.29999005794525146, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 52,  Mean reward: 7.608695652173913, Mean Entropy: 0.25348037481307983, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 53,  Mean reward: 7.27536231884058, Mean Entropy: 0.3350266218185425, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 54,  Mean reward: 7.458904109589041, Mean Entropy: 0.23523300886154175, complete_episode_count: 73.0, Gather time: 0.61s, Train time: 0.77s
Iteration: 55,  Mean reward: 7.140845070422535, Mean Entropy: 0.2486613690853119, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 56,  Mean reward: 6.916666666666667, Mean Entropy: 0.2345377653837204, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 57,  Mean reward: 7.797297297297297, Mean Entropy: 0.2866743803024292, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 58,  Mean reward: 6.910958904109589, Mean Entropy: 0.27829355001449585, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 59,  Mean reward: 6.635714285714286, Mean Entropy: 0.21389852464199066, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 60,  Mean reward: 7.8175675675675675, Mean Entropy: 0.1460440456867218, complete_episode_count: 74.0, Gather time: 0.61s, Train time: 0.80s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 61,  Mean reward: 7.8618421052631575, Mean Entropy: 0.12012302875518799, complete_episode_count: 76.0, Gather time: 0.61s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 62,  Mean reward: 7.922077922077922, Mean Entropy: 0.08732552826404572, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 63,  Mean reward: 7.923076923076923, Mean Entropy: 0.054474011063575745, complete_episode_count: 78.0, Gather time: 0.96s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 64,  Mean reward: 7.941558441558442, Mean Entropy: 0.06756442785263062, complete_episode_count: 77.0, Gather time: 0.61s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 65,  Mean reward: 7.980769230769231, Mean Entropy: 0.04554787278175354, complete_episode_count: 78.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 66,  Mean reward: 7.75, Mean Entropy: 0.1426316797733307, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 67,  Mean reward: 7.148648648648648, Mean Entropy: 0.16744253039360046, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 68,  Mean reward: 7.901315789473684, Mean Entropy: 0.03713463991880417, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 69,  Mean reward: 8.0, Mean Entropy: 0.029371287673711777, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 70,  Mean reward: 7.961538461538462, Mean Entropy: 0.040531136095523834, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 71,  Mean reward: 8.0, Mean Entropy: 0.06310287117958069, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 72,  Mean reward: 7.961538461538462, Mean Entropy: 0.04913751408457756, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 73,  Mean reward: 7.981012658227848, Mean Entropy: 0.03247128054499626, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 74,  Mean reward: 7.981012658227848, Mean Entropy: 0.05002579092979431, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 75,  Mean reward: 8.0, Mean Entropy: 0.026118431240320206, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 76,  Mean reward: -2.0, Mean Entropy: 0.03362324461340904, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 77,  Mean reward: -3.411392405063291, Mean Entropy: 0.03612887114286423, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 78,  Mean reward: -1.0, Mean Entropy: 0.03715255483984947, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 79,  Mean reward: -1.948051948051948, Mean Entropy: 0.02456553280353546, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 80,  Mean reward: -1.0, Mean Entropy: 0.028582222759723663, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 81,  Mean reward: -2.0, Mean Entropy: 0.10952087491750717, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 82,  Mean reward: -1.6883116883116882, Mean Entropy: 0.33006778359413147, complete_episode_count: 77.0, Gather time: 0.63s, Train time: 0.77s
Iteration: 83,  Mean reward: 1.3987341772151898, Mean Entropy: 0.1958971917629242, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 84,  Mean reward: 7.142857142857143, Mean Entropy: 0.054028552025556564, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 85,  Mean reward: 8.0, Mean Entropy: 0.04916022717952728, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 86,  Mean reward: 8.0, Mean Entropy: 0.03863288834691048, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 87,  Mean reward: 7.75, Mean Entropy: 0.04347584769129753, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 88,  Mean reward: 8.0, Mean Entropy: 0.06679429113864899, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 89,  Mean reward: 7.9423076923076925, Mean Entropy: 0.060537658631801605, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 90,  Mean reward: 8.0, Mean Entropy: 0.5107095837593079, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 91,  Mean reward: 0.22972972972972974, Mean Entropy: 0.1935916244983673, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 92,  Mean reward: 5.802631578947368, Mean Entropy: 0.14908427000045776, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 93,  Mean reward: 7.961538461538462, Mean Entropy: 0.07349465787410736, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 94,  Mean reward: 8.0, Mean Entropy: 0.038838040083646774, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 95,  Mean reward: 8.0, Mean Entropy: 0.7932958602905273, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 96,  Mean reward: -2.4274193548387095, Mean Entropy: 0.34936994314193726, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 97,  Mean reward: 7.961538461538462, Mean Entropy: 0.43143877387046814, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 98,  Mean reward: -1.028169014084507, Mean Entropy: 0.22092662751674652, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 99,  Mean reward: 8.0, Mean Entropy: 0.03166131302714348, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 100,  Mean reward: 8.0, Mean Entropy: 0.6405478715896606, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 2.507142857142857, Mean Entropy: 0.22858643531799316, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 102,  Mean reward: 7.5, Mean Entropy: 0.12576943635940552, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 103,  Mean reward: 7.42948717948718, Mean Entropy: 0.08335685729980469, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 104,  Mean reward: 7.922077922077922, Mean Entropy: 0.22574695944786072, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 105,  Mean reward: 5.094594594594595, Mean Entropy: 0.3569519519805908, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 106,  Mean reward: 5.706666666666667, Mean Entropy: 0.0343523845076561, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 107,  Mean reward: 8.0, Mean Entropy: 0.31657081842422485, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 108,  Mean reward: 4.395833333333333, Mean Entropy: 0.6928496360778809, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 109,  Mean reward: 3.1384615384615384, Mean Entropy: 0.45336803793907166, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 110,  Mean reward: 6.863636363636363, Mean Entropy: 0.08355643600225449, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 111,  Mean reward: 7.981012658227848, Mean Entropy: 0.21843120455741882, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 112,  Mean reward: 4.480263157894737, Mean Entropy: 0.09168128669261932, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 113,  Mean reward: 7.922077922077922, Mean Entropy: 0.07993795722723007, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 114,  Mean reward: 7.961538461538462, Mean Entropy: 0.028665728867053986, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 115,  Mean reward: 8.0, Mean Entropy: 0.6103459596633911, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 116,  Mean reward: 2.5303030303030303, Mean Entropy: 0.4014619290828705, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 117,  Mean reward: 3.987012987012987, Mean Entropy: 0.04776381701231003, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 118,  Mean reward: 8.0, Mean Entropy: 0.03188149631023407, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 119,  Mean reward: 8.0, Mean Entropy: 0.02192002162337303, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 120,  Mean reward: 8.0, Mean Entropy: 0.4908845126628876, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 121,  Mean reward: 3.246376811594203, Mean Entropy: 0.6324387788772583, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 122,  Mean reward: 3.652173913043478, Mean Entropy: 0.4895756244659424, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 123,  Mean reward: 5.197183098591549, Mean Entropy: 0.2865371108055115, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 124,  Mean reward: 5.993333333333333, Mean Entropy: 0.06903388351202011, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 125,  Mean reward: 7.9423076923076925, Mean Entropy: 0.036095742136240005, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 126,  Mean reward: 7.981012658227848, Mean Entropy: 0.16479068994522095, complete_episode_count: 79.0, Gather time: 0.62s, Train time: 0.80s
Iteration: 127,  Mean reward: 4.689873417721519, Mean Entropy: 0.1398025006055832, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 128,  Mean reward: 6.935897435897436, Mean Entropy: 0.014254230074584484, complete_episode_count: 78.0, Gather time: 0.78s, Train time: 0.78s
Iteration: 129,  Mean reward: 8.0, Mean Entropy: 0.027610789984464645, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.76s
Iteration: 130,  Mean reward: 8.0, Mean Entropy: 0.015027686953544617, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 131,  Mean reward: 8.0, Mean Entropy: 0.010440042242407799, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 132,  Mean reward: 8.0, Mean Entropy: 0.020709343254566193, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 133,  Mean reward: 7.981012658227848, Mean Entropy: 0.008469536900520325, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 134,  Mean reward: 8.0, Mean Entropy: 0.007334679365158081, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 135,  Mean reward: 7.981012658227848, Mean Entropy: 0.018622344359755516, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 136,  Mean reward: 7.981012658227848, Mean Entropy: 0.011029484681785107, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 137,  Mean reward: 8.0, Mean Entropy: 0.04806346818804741, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 138,  Mean reward: 7.708860759493671, Mean Entropy: 0.017685849219560623, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 139,  Mean reward: 8.0, Mean Entropy: 0.01793571189045906, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 140,  Mean reward: 8.0, Mean Entropy: 0.09123969078063965, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.78s
Iteration: 141,  Mean reward: 2.25, Mean Entropy: 0.37065911293029785, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 142,  Mean reward: 1.8333333333333333, Mean Entropy: 0.49793124198913574, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 143,  Mean reward: 3.7739726027397262, Mean Entropy: 0.2758539617061615, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 144,  Mean reward: 6.342105263157895, Mean Entropy: 0.12046496570110321, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 145,  Mean reward: 7.727848101265823, Mean Entropy: 0.04815475642681122, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 146,  Mean reward: 8.0, Mean Entropy: 0.010228198021650314, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 147,  Mean reward: 8.0, Mean Entropy: 0.1755775511264801, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 148,  Mean reward: 5.706666666666667, Mean Entropy: 0.058862101286649704, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 149,  Mean reward: 8.0, Mean Entropy: 0.02448951080441475, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 150,  Mean reward: 8.0, Mean Entropy: 0.17686805129051208, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 151,  Mean reward: 5.084415584415584, Mean Entropy: 0.37449750304222107, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 152,  Mean reward: 6.126760563380282, Mean Entropy: 0.312836229801178, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 153,  Mean reward: 6.0394736842105265, Mean Entropy: 0.0633757933974266, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 154,  Mean reward: 8.0, Mean Entropy: 0.4511650800704956, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 155,  Mean reward: 0.42063492063492064, Mean Entropy: 0.7927154302597046, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 156,  Mean reward: -3.09375, Mean Entropy: 0.7974646091461182, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 157,  Mean reward: -0.35344827586206895, Mean Entropy: 0.7378933429718018, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 158,  Mean reward: 1.7954545454545454, Mean Entropy: 0.7199481725692749, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 159,  Mean reward: 1.9357142857142857, Mean Entropy: 0.8513533473014832, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 160,  Mean reward: -1.3055555555555556, Mean Entropy: 0.9791185855865479, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 161,  Mean reward: -4.5, Mean Entropy: 0.8952271342277527, complete_episode_count: 39.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 162,  Mean reward: -3.802325581395349, Mean Entropy: 0.945826530456543, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 163,  Mean reward: -5.157894736842105, Mean Entropy: 0.9241933822631836, complete_episode_count: 38.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 164,  Mean reward: -6.976190476190476, Mean Entropy: 0.9314095973968506, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.70s
Iteration: 165,  Mean reward: -3.5238095238095237, Mean Entropy: 0.8880901336669922, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 166,  Mean reward: -4.512195121951219, Mean Entropy: 0.9747270345687866, complete_episode_count: 41.0, Gather time: 0.59s, Train time: 1.54s
Iteration: 167,  Mean reward: -4.8076923076923075, Mean Entropy: 0.9314032793045044, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 168,  Mean reward: -7.511904761904762, Mean Entropy: 0.9458536505699158, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 169,  Mean reward: -7.225, Mean Entropy: 0.9458550810813904, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 170,  Mean reward: -7.608108108108108, Mean Entropy: 0.9458553194999695, complete_episode_count: 37.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 171,  Mean reward: -4.430232558139535, Mean Entropy: 0.9458309412002563, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 172,  Mean reward: -3.263157894736842, Mean Entropy: 0.9528381824493408, complete_episode_count: 38.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 173,  Mean reward: -4.512820512820513, Mean Entropy: 0.9311851263046265, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 174,  Mean reward: -3.5444444444444443, Mean Entropy: 0.9526316523551941, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 175,  Mean reward: -4.658536585365853, Mean Entropy: 0.960141658782959, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 176,  Mean reward: -4.463414634146342, Mean Entropy: 0.9747085571289062, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 177,  Mean reward: -4.4875, Mean Entropy: 0.8591008186340332, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 178,  Mean reward: -3.8974358974358974, Mean Entropy: 0.9313890337944031, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 179,  Mean reward: -5.714285714285714, Mean Entropy: 0.974432110786438, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 180,  Mean reward: -5.871794871794871, Mean Entropy: 0.9958836436271667, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 181,  Mean reward: -4.5, Mean Entropy: 1.0177720785140991, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 182,  Mean reward: -3.761904761904762, Mean Entropy: 1.0107700824737549, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 183,  Mean reward: -6.011363636363637, Mean Entropy: 0.9595796465873718, complete_episode_count: 44.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 184,  Mean reward: -5.022222222222222, Mean Entropy: 0.9522907137870789, complete_episode_count: 45.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 185,  Mean reward: -7.104651162790698, Mean Entropy: 0.8298847675323486, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 186,  Mean reward: -3.926829268292683, Mean Entropy: 0.9657899141311646, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 187,  Mean reward: -4.290697674418604, Mean Entropy: 0.9598631262779236, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 188,  Mean reward: -6.3625, Mean Entropy: 0.9499702453613281, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 189,  Mean reward: -4.337209302325581, Mean Entropy: 0.8924757838249207, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 190,  Mean reward: -4.329545454545454, Mean Entropy: 0.9245103597640991, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 191,  Mean reward: -7.0, Mean Entropy: 0.8773414492607117, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 192,  Mean reward: -2.1808510638297873, Mean Entropy: 0.9067026972770691, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 193,  Mean reward: -2.619565217391304, Mean Entropy: 0.8406292796134949, complete_episode_count: 46.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 194,  Mean reward: -6.1395348837209305, Mean Entropy: 0.8878622055053711, complete_episode_count: 43.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 195,  Mean reward: -5.27906976744186, Mean Entropy: 0.848326563835144, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 196,  Mean reward: -1.3863636363636365, Mean Entropy: 0.8915714025497437, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 197,  Mean reward: -1.4787234042553192, Mean Entropy: 0.954729437828064, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 198,  Mean reward: -3.2717391304347827, Mean Entropy: 0.9516984820365906, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.69s
Iteration: 199,  Mean reward: -5.4186046511627906, Mean Entropy: 0.8174961805343628, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 200,  Mean reward: -4.73404255319149, Mean Entropy: 0.8562060594558716, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.52s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: -5.315217391304348, Mean Entropy: 0.7564563751220703, complete_episode_count: 46.0, Gather time: 0.57s, Train time: 1.48s
Iteration: 202,  Mean reward: -4.846938775510204, Mean Entropy: 0.7732219099998474, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.56s
Iteration: 203,  Mean reward: -3.701923076923077, Mean Entropy: 0.7310504913330078, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 204,  Mean reward: -4.25, Mean Entropy: 0.745631217956543, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 205,  Mean reward: -3.9489795918367347, Mean Entropy: 0.8250142931938171, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 206,  Mean reward: -0.8269230769230769, Mean Entropy: 0.8821743726730347, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 207,  Mean reward: -2.630434782608696, Mean Entropy: 0.8857426047325134, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 208,  Mean reward: -3.15625, Mean Entropy: 0.9387803077697754, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 209,  Mean reward: -5.8625, Mean Entropy: 0.8776904940605164, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 210,  Mean reward: -3.197674418604651, Mean Entropy: 0.835681676864624, complete_episode_count: 43.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 211,  Mean reward: -3.5096153846153846, Mean Entropy: 0.7411598563194275, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 212,  Mean reward: -2.0754716981132075, Mean Entropy: 0.7715764045715332, complete_episode_count: 53.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 213,  Mean reward: -1.9607843137254901, Mean Entropy: 0.6952512264251709, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 214,  Mean reward: -5.26530612244898, Mean Entropy: 0.6131124496459961, complete_episode_count: 49.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 215,  Mean reward: -2.212962962962963, Mean Entropy: 0.7170284986495972, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 216,  Mean reward: -2.5104166666666665, Mean Entropy: 0.7254759073257446, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 217,  Mean reward: -1.8181818181818181, Mean Entropy: 0.8832630515098572, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 218,  Mean reward: -4.77906976744186, Mean Entropy: 0.5916936993598938, complete_episode_count: 43.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 219,  Mean reward: -3.1578947368421053, Mean Entropy: 0.574047863483429, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 220,  Mean reward: -4.833333333333333, Mean Entropy: 0.5960770845413208, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 221,  Mean reward: -3.3846153846153846, Mean Entropy: 0.5770954489707947, complete_episode_count: 52.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 222,  Mean reward: -5.1875, Mean Entropy: 0.5398819446563721, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.54s
Iteration: 223,  Mean reward: -5.8125, Mean Entropy: 0.5826783776283264, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 224,  Mean reward: -2.016949152542373, Mean Entropy: 0.7459416389465332, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 225,  Mean reward: -5.811320754716981, Mean Entropy: 0.8017222285270691, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 226,  Mean reward: -5.130434782608695, Mean Entropy: 0.7236047983169556, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 227,  Mean reward: -1.3818181818181818, Mean Entropy: 0.7903923392295837, complete_episode_count: 55.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 228,  Mean reward: -1.2456140350877194, Mean Entropy: 0.7280575037002563, complete_episode_count: 57.0, Gather time: 0.57s, Train time: 1.57s
Iteration: 229,  Mean reward: -2.440677966101695, Mean Entropy: 0.7035357356071472, complete_episode_count: 59.0, Gather time: 0.58s, Train time: 1.71s
Iteration: 230,  Mean reward: -1.1796875, Mean Entropy: 0.8264278173446655, complete_episode_count: 64.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 231,  Mean reward: -3.3796296296296298, Mean Entropy: 0.7199163436889648, complete_episode_count: 54.0, Gather time: 0.58s, Train time: 1.54s
Iteration: 232,  Mean reward: -2.6792452830188678, Mean Entropy: 0.8166495561599731, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 233,  Mean reward: -5.87037037037037, Mean Entropy: 0.5735318660736084, complete_episode_count: 54.0, Gather time: 0.57s, Train time: 1.53s
Iteration: 234,  Mean reward: 0.09154929577464789, Mean Entropy: 0.5207632184028625, complete_episode_count: 71.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 235,  Mean reward: -1.6126760563380282, Mean Entropy: 0.4571625888347626, complete_episode_count: 71.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 236,  Mean reward: 0.9066666666666666, Mean Entropy: 0.2055414915084839, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 237,  Mean reward: 5.993333333333333, Mean Entropy: 0.14449253678321838, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 238,  Mean reward: 3.2222222222222223, Mean Entropy: 0.47288811206817627, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 239,  Mean reward: -1.5803571428571428, Mean Entropy: 0.5074869990348816, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 240,  Mean reward: -4.907692307692308, Mean Entropy: 0.4417147636413574, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 241,  Mean reward: -3.1587301587301586, Mean Entropy: 0.3560747504234314, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 242,  Mean reward: -2.664285714285714, Mean Entropy: 0.4882359504699707, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 243,  Mean reward: -1.5076923076923077, Mean Entropy: 0.5635025501251221, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 244,  Mean reward: -0.7946428571428571, Mean Entropy: 0.5413440465927124, complete_episode_count: 56.0, Gather time: 0.58s, Train time: 1.50s
Iteration: 245,  Mean reward: -0.2076923076923077, Mean Entropy: 0.5740808844566345, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 246,  Mean reward: 3.75, Mean Entropy: 0.48316606879234314, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 247,  Mean reward: 4.880281690140845, Mean Entropy: 0.2955048084259033, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 248,  Mean reward: -2.657142857142857, Mean Entropy: 0.36108893156051636, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 249,  Mean reward: -0.1917808219178082, Mean Entropy: 0.29760581254959106, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 250,  Mean reward: -3.2837837837837838, Mean Entropy: 0.33816275000572205, complete_episode_count: 74.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 251,  Mean reward: -3.5142857142857142, Mean Entropy: 0.5198289155960083, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 252,  Mean reward: 2.1363636363636362, Mean Entropy: 0.38945087790489197, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 253,  Mean reward: 3.536231884057971, Mean Entropy: 0.3965536952018738, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 254,  Mean reward: 4.051470588235294, Mean Entropy: 0.37943196296691895, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 255,  Mean reward: 4.098484848484849, Mean Entropy: 0.3227348327636719, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 256,  Mean reward: 4.889705882352941, Mean Entropy: 0.29489296674728394, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 257,  Mean reward: 5.4338235294117645, Mean Entropy: 0.31137579679489136, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 258,  Mean reward: 5.35, Mean Entropy: 0.26695650815963745, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 259,  Mean reward: 7.729166666666667, Mean Entropy: 0.1657571643590927, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 260,  Mean reward: 7.458904109589041, Mean Entropy: 0.15975186228752136, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 261,  Mean reward: 7.729166666666667, Mean Entropy: 0.18209800124168396, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 262,  Mean reward: 7.580882352941177, Mean Entropy: 0.17736098170280457, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 263,  Mean reward: 7.725352112676056, Mean Entropy: 0.23147469758987427, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 264,  Mean reward: 7.064285714285714, Mean Entropy: 0.22407189011573792, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 265,  Mean reward: 4.3283582089552235, Mean Entropy: 0.3692358732223511, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 266,  Mean reward: 6.45, Mean Entropy: 0.1668987274169922, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 267,  Mean reward: 7.708333333333333, Mean Entropy: 0.17288784682750702, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 268,  Mean reward: 2.573529411764706, Mean Entropy: 0.2999435365200043, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 269,  Mean reward: 1.9140625, Mean Entropy: 0.19533556699752808, complete_episode_count: 64.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 270,  Mean reward: 3.8582089552238807, Mean Entropy: 0.23948848247528076, complete_episode_count: 67.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 271,  Mean reward: 7.485074626865671, Mean Entropy: 0.2638072669506073, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 272,  Mean reward: 7.547297297297297, Mean Entropy: 0.20164060592651367, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 273,  Mean reward: 7.451388888888889, Mean Entropy: 0.18532046675682068, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 274,  Mean reward: 7.6571428571428575, Mean Entropy: 0.1276334971189499, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 275,  Mean reward: 7.635714285714286, Mean Entropy: 0.3877254128456116, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 276,  Mean reward: 2.9318181818181817, Mean Entropy: 0.5858261585235596, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 277,  Mean reward: -0.1206896551724138, Mean Entropy: 0.6391363143920898, complete_episode_count: 58.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 278,  Mean reward: -0.5873015873015873, Mean Entropy: 0.3734142780303955, complete_episode_count: 63.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 279,  Mean reward: -1.3114754098360655, Mean Entropy: 0.34116309881210327, complete_episode_count: 61.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 280,  Mean reward: 3.8333333333333335, Mean Entropy: 0.2898338735103607, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 281,  Mean reward: 7.614285714285714, Mean Entropy: 0.21589331328868866, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 282,  Mean reward: 7.565217391304348, Mean Entropy: 0.1611933559179306, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 283,  Mean reward: 7.007246376811594, Mean Entropy: 0.21431003510951996, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 284,  Mean reward: 7.392857142857143, Mean Entropy: 0.19014200568199158, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 285,  Mean reward: 7.608695652173913, Mean Entropy: 0.14943504333496094, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 286,  Mean reward: 7.5588235294117645, Mean Entropy: 0.1664147973060608, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 287,  Mean reward: 7.773972602739726, Mean Entropy: 0.27192771434783936, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 288,  Mean reward: 2.0403225806451615, Mean Entropy: 0.5415881872177124, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 1.51s
Iteration: 289,  Mean reward: 1.2537313432835822, Mean Entropy: 0.30726510286331177, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 290,  Mean reward: 0.3384615384615385, Mean Entropy: 0.15140078961849213, complete_episode_count: 65.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 291,  Mean reward: 7.661971830985915, Mean Entropy: 0.16644181311130524, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 292,  Mean reward: 7.635714285714286, Mean Entropy: 0.15961652994155884, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 293,  Mean reward: 7.307142857142857, Mean Entropy: 0.15605610609054565, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.82s
Iteration: 294,  Mean reward: 7.683098591549296, Mean Entropy: 0.14339342713356018, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 295,  Mean reward: 4.395833333333333, Mean Entropy: 0.22185549139976501, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 296,  Mean reward: 5.9, Mean Entropy: 0.13049298524856567, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 297,  Mean reward: 7.529850746268656, Mean Entropy: 0.15463142096996307, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 298,  Mean reward: 7.661971830985915, Mean Entropy: 0.15546296536922455, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 299,  Mean reward: 7.75, Mean Entropy: 0.15492115914821625, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 300,  Mean reward: 7.756756756756757, Mean Entropy: 0.1240563616156578, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.77s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 7.704225352112676, Mean Entropy: 0.30566510558128357, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 302,  Mean reward: -1.5725806451612903, Mean Entropy: 0.6404244899749756, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 303,  Mean reward: -1.076271186440678, Mean Entropy: 0.5407072305679321, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.46s
Iteration: 304,  Mean reward: 0.20689655172413793, Mean Entropy: 0.34288114309310913, complete_episode_count: 58.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 305,  Mean reward: 2.9420289855072466, Mean Entropy: 0.219000905752182, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 306,  Mean reward: 1.7681159420289856, Mean Entropy: 0.1523578017950058, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.94s
Iteration: 307,  Mean reward: 1.3602941176470589, Mean Entropy: 0.25934702157974243, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 308,  Mean reward: 0.6417910447761194, Mean Entropy: 0.43914228677749634, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 309,  Mean reward: 4.456521739130435, Mean Entropy: 0.3591585159301758, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 310,  Mean reward: 5.971830985915493, Mean Entropy: 0.22659818828105927, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 311,  Mean reward: 6.963235294117647, Mean Entropy: 0.1725359857082367, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 312,  Mean reward: 7.608695652173913, Mean Entropy: 0.17532716691493988, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 313,  Mean reward: 7.683098591549296, Mean Entropy: 0.12648692727088928, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 314,  Mean reward: 7.608695652173913, Mean Entropy: 0.11986379325389862, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 315,  Mean reward: 7.704225352112676, Mean Entropy: 0.13805758953094482, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 316,  Mean reward: 7.580882352941177, Mean Entropy: 0.15312953293323517, complete_episode_count: 68.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 317,  Mean reward: 7.5928571428571425, Mean Entropy: 0.16273561120033264, complete_episode_count: 70.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 318,  Mean reward: 7.507462686567164, Mean Entropy: 0.14038708806037903, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 319,  Mean reward: 7.608695652173913, Mean Entropy: 0.11768297851085663, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 320,  Mean reward: 7.640845070422535, Mean Entropy: 0.13745325803756714, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 321,  Mean reward: 7.683098591549296, Mean Entropy: 0.10798775404691696, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 322,  Mean reward: 0.9044117647058824, Mean Entropy: 0.40753501653671265, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 323,  Mean reward: 1.9924242424242424, Mean Entropy: 0.22376355528831482, complete_episode_count: 66.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 324,  Mean reward: 7.683098591549296, Mean Entropy: 0.10145212709903717, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 325,  Mean reward: 7.630434782608695, Mean Entropy: 0.2271035760641098, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 326,  Mean reward: 1.9402985074626866, Mean Entropy: 0.39495885372161865, complete_episode_count: 67.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 327,  Mean reward: 2.985074626865672, Mean Entropy: 0.18746136128902435, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 328,  Mean reward: 7.286764705882353, Mean Entropy: 0.1522015929222107, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 329,  Mean reward: 7.098591549295775, Mean Entropy: 0.1326727718114853, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 330,  Mean reward: 7.708333333333333, Mean Entropy: 0.18726882338523865, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 331,  Mean reward: 0.855072463768116, Mean Entropy: 0.16941499710083008, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 332,  Mean reward: 1.0441176470588236, Mean Entropy: 0.24141070246696472, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 333,  Mean reward: 7.586956521739131, Mean Entropy: 0.14903073012828827, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 334,  Mean reward: 7.773972602739726, Mean Entropy: 0.1531178057193756, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 335,  Mean reward: 7.565217391304348, Mean Entropy: 0.14211276173591614, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 336,  Mean reward: 6.610294117647059, Mean Entropy: 0.15392330288887024, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 337,  Mean reward: 7.565217391304348, Mean Entropy: 0.10941275954246521, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 338,  Mean reward: 7.704225352112676, Mean Entropy: 0.14556075632572174, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 339,  Mean reward: 7.635714285714286, Mean Entropy: 0.11712130159139633, complete_episode_count: 70.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 340,  Mean reward: 7.586956521739131, Mean Entropy: 0.15443043410778046, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 341,  Mean reward: 7.6875, Mean Entropy: 0.1534576117992401, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 342,  Mean reward: 7.661971830985915, Mean Entropy: 0.17632339894771576, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 343,  Mean reward: 7.7534246575342465, Mean Entropy: 0.15302734076976776, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 344,  Mean reward: 7.704225352112676, Mean Entropy: 0.25044047832489014, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 345,  Mean reward: 7.297101449275362, Mean Entropy: 0.1342272162437439, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 346,  Mean reward: 7.6875, Mean Entropy: 0.2131977379322052, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.97s
Iteration: 347,  Mean reward: 7.359154929577465, Mean Entropy: 0.1220635399222374, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 348,  Mean reward: 7.630434782608695, Mean Entropy: 0.13159549236297607, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 349,  Mean reward: 7.608695652173913, Mean Entropy: 0.14723840355873108, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 350,  Mean reward: 7.454545454545454, Mean Entropy: 0.1823510378599167, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 351,  Mean reward: 7.6875, Mean Entropy: 0.1502346396446228, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 352,  Mean reward: 7.586956521739131, Mean Entropy: 0.14595866203308105, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 353,  Mean reward: 7.635714285714286, Mean Entropy: 0.13846957683563232, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 354,  Mean reward: 7.614285714285714, Mean Entropy: 0.15404319763183594, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 355,  Mean reward: 7.543478260869565, Mean Entropy: 0.12147846072912216, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 356,  Mean reward: 7.725352112676056, Mean Entropy: 0.14660786092281342, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 357,  Mean reward: 7.586956521739131, Mean Entropy: 0.16925612092018127, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 358,  Mean reward: 7.431818181818182, Mean Entropy: 0.16028450429439545, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 359,  Mean reward: 7.640845070422535, Mean Entropy: 0.14866751432418823, complete_episode_count: 71.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 360,  Mean reward: 7.5928571428571425, Mean Entropy: 0.163192018866539, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 361,  Mean reward: 7.514705882352941, Mean Entropy: 0.15205177664756775, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 362,  Mean reward: 7.586956521739131, Mean Entropy: 0.14614877104759216, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 363,  Mean reward: 7.704225352112676, Mean Entropy: 0.14280393719673157, complete_episode_count: 71.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 364,  Mean reward: 7.797297297297297, Mean Entropy: 0.1315908282995224, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 365,  Mean reward: 7.514705882352941, Mean Entropy: 0.14159798622131348, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 366,  Mean reward: 7.602941176470588, Mean Entropy: 0.1446232795715332, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 367,  Mean reward: 7.5928571428571425, Mean Entropy: 0.16784334182739258, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 368,  Mean reward: 7.586956521739131, Mean Entropy: 0.12260725349187851, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 369,  Mean reward: 7.297101449275362, Mean Entropy: 0.12660536170005798, complete_episode_count: 69.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 370,  Mean reward: 7.536764705882353, Mean Entropy: 0.14250531792640686, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 371,  Mean reward: 7.708333333333333, Mean Entropy: 0.11262809485197067, complete_episode_count: 72.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 372,  Mean reward: 7.86, Mean Entropy: 0.0801531970500946, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 373,  Mean reward: 7.623376623376624, Mean Entropy: 0.007227152585983276, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 374,  Mean reward: 8.0, Mean Entropy: 0.025086022913455963, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 375,  Mean reward: 7.25, Mean Entropy: 0.001938476925715804, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.79s
Iteration: 376,  Mean reward: 8.0, Mean Entropy: 0.013352090492844582, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.80s
Iteration: 377,  Mean reward: 7.75, Mean Entropy: 0.003490281291306019, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 378,  Mean reward: 8.0, Mean Entropy: 0.0005138273118063807, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 379,  Mean reward: 8.0, Mean Entropy: 0.0006411981303244829, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.80s
Iteration: 380,  Mean reward: 8.0, Mean Entropy: 0.0028268126770853996, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 381,  Mean reward: 8.0, Mean Entropy: 0.017519615590572357, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 382,  Mean reward: 7.75, Mean Entropy: 0.008026177063584328, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 383,  Mean reward: 8.0, Mean Entropy: 0.14869822561740875, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 384,  Mean reward: 6.462025316455696, Mean Entropy: 0.10111000388860703, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 385,  Mean reward: 6.935897435897436, Mean Entropy: 0.015177195891737938, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 386,  Mean reward: 8.0, Mean Entropy: 0.0074907392263412476, complete_episode_count: 80.0, Gather time: 0.76s, Train time: 0.78s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 0.0296127051115036, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 0.4296252727508545, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 389,  Mean reward: 3.2027027027027026, Mean Entropy: 0.2934558689594269, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 390,  Mean reward: 6.383116883116883, Mean Entropy: 0.0611531063914299, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 0.09061788767576218, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 392,  Mean reward: 6.935897435897436, Mean Entropy: 0.016903720796108246, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 0.0008166597108356655, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 7.06870632711798e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 395,  Mean reward: 8.0, Mean Entropy: 5.986628093523905e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 396,  Mean reward: 8.0, Mean Entropy: 3.6308043490862474e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 4.3782813008874655e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 4.8971087380778044e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 4.25573016400449e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 3.968813325627707e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 8.0, Mean Entropy: 4.414461363921873e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 402,  Mean reward: 8.0, Mean Entropy: 4.2795825720531866e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 403,  Mean reward: 8.0, Mean Entropy: 5.79479310545139e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 404,  Mean reward: 8.0, Mean Entropy: 6.185879465192556e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 405,  Mean reward: 8.0, Mean Entropy: 8.329882984980941e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 0.00012965315545443445, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 407,  Mean reward: 8.0, Mean Entropy: 0.00024303111422341317, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 0.0011238257866352797, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 0.1243181824684143, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 410,  Mean reward: 6.059210526315789, Mean Entropy: 0.4757528305053711, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.79s
Iteration: 411,  Mean reward: 4.007246376811594, Mean Entropy: 0.4196072816848755, complete_episode_count: 69.0, Gather time: 0.60s, Train time: 0.82s
Iteration: 412,  Mean reward: 4.927536231884058, Mean Entropy: 0.3174992501735687, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 413,  Mean reward: 6.445945945945946, Mean Entropy: 0.10131068527698517, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 414,  Mean reward: 7.448717948717949, Mean Entropy: 0.04682523012161255, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 415,  Mean reward: 7.727848101265823, Mean Entropy: 0.002300779800862074, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 416,  Mean reward: 8.0, Mean Entropy: 0.017387453466653824, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 417,  Mean reward: 7.961538461538462, Mean Entropy: 0.001958663808181882, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 418,  Mean reward: 8.0, Mean Entropy: 0.0029988265596330166, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 419,  Mean reward: 8.0, Mean Entropy: 0.004981501959264278, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 0.029259931296110153, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 421,  Mean reward: 7.5, Mean Entropy: 0.005283647682517767, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.0013898536562919617, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 423,  Mean reward: 8.0, Mean Entropy: 0.001986036542803049, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.95s
Iteration: 424,  Mean reward: 8.0, Mean Entropy: 0.0036179362796247005, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 0.005054935347288847, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 0.020753255113959312, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.83s
Iteration: 427,  Mean reward: 7.75, Mean Entropy: 0.007970355451107025, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 428,  Mean reward: 8.0, Mean Entropy: 0.02905813418328762, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 429,  Mean reward: 7.5, Mean Entropy: 0.017448604106903076, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 430,  Mean reward: 7.75, Mean Entropy: 0.0022266528103500605, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 0.10144307464361191, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 432,  Mean reward: 7.86, Mean Entropy: 0.028126422315835953, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 433,  Mean reward: 8.0, Mean Entropy: 0.025029536336660385, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 434,  Mean reward: 8.0, Mean Entropy: 0.11024996638298035, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 435,  Mean reward: 7.474683544303797, Mean Entropy: 0.15243202447891235, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 436,  Mean reward: 7.1923076923076925, Mean Entropy: 0.1151496097445488, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 437,  Mean reward: 7.962025316455696, Mean Entropy: 0.06679810583591461, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 438,  Mean reward: 7.902597402597403, Mean Entropy: 0.028327925130724907, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 0.08963444828987122, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 440,  Mean reward: 7.1923076923076925, Mean Entropy: 0.09502957761287689, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 441,  Mean reward: 6.5, Mean Entropy: 0.01479911245405674, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 0.039417050778865814, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 443,  Mean reward: 7.25, Mean Entropy: 0.011355086229741573, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 444,  Mean reward: 8.0, Mean Entropy: 0.011392354965209961, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 445,  Mean reward: 8.0, Mean Entropy: 0.07618657499551773, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 446,  Mean reward: 6.5, Mean Entropy: 0.07867967337369919, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 447,  Mean reward: 7.25, Mean Entropy: 0.014629870653152466, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 448,  Mean reward: 8.0, Mean Entropy: 0.008036080747842789, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 449,  Mean reward: 8.0, Mean Entropy: 0.0035085929557681084, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 450,  Mean reward: 8.0, Mean Entropy: 0.0020625186152756214, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 451,  Mean reward: 8.0, Mean Entropy: 0.004727210849523544, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 452,  Mean reward: 8.0, Mean Entropy: 0.02132205292582512, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 453,  Mean reward: 8.0, Mean Entropy: 0.07053681463003159, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 454,  Mean reward: 6.5, Mean Entropy: 0.03996516764163971, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 455,  Mean reward: 7.5, Mean Entropy: 0.006464102771133184, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 456,  Mean reward: 8.0, Mean Entropy: 0.002246411517262459, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 457,  Mean reward: 8.0, Mean Entropy: 0.0015405867015942931, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 458,  Mean reward: 8.0, Mean Entropy: 0.002224371302872896, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 459,  Mean reward: 8.0, Mean Entropy: 0.15777802467346191, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 460,  Mean reward: 7.614285714285714, Mean Entropy: 0.13121500611305237, complete_episode_count: 70.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 461,  Mean reward: 7.901315789473684, Mean Entropy: 0.007231724914163351, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 462,  Mean reward: 8.0, Mean Entropy: 0.0024255949538201094, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 463,  Mean reward: 8.0, Mean Entropy: 0.0034033479169011116, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 464,  Mean reward: 7.75, Mean Entropy: 0.02095760405063629, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 465,  Mean reward: 7.962025316455696, Mean Entropy: 0.007775416597723961, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 466,  Mean reward: 8.0, Mean Entropy: 0.006092920899391174, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 467,  Mean reward: 8.0, Mean Entropy: 0.012932589277625084, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 468,  Mean reward: 7.981012658227848, Mean Entropy: 0.0028774775564670563, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 469,  Mean reward: 8.0, Mean Entropy: 0.0016464653890579939, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 470,  Mean reward: 8.0, Mean Entropy: 0.002203742740675807, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 471,  Mean reward: 8.0, Mean Entropy: 0.0033330796286463737, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 472,  Mean reward: 7.981012658227848, Mean Entropy: 0.0012115671997889876, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 473,  Mean reward: 8.0, Mean Entropy: 0.0009301042882725596, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 474,  Mean reward: 8.0, Mean Entropy: 0.0015219891211017966, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 475,  Mean reward: 8.0, Mean Entropy: 0.0020700213499367237, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 476,  Mean reward: 8.0, Mean Entropy: 0.003061921801418066, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 477,  Mean reward: 8.0, Mean Entropy: 0.006776377558708191, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 478,  Mean reward: 8.0, Mean Entropy: 0.024857981130480766, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 479,  Mean reward: 7.25, Mean Entropy: 0.007328844629228115, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 480,  Mean reward: 8.0, Mean Entropy: 0.005009962245821953, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 481,  Mean reward: 8.0, Mean Entropy: 0.008646978065371513, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 482,  Mean reward: 7.75, Mean Entropy: 0.0030184010975062847, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 483,  Mean reward: 8.0, Mean Entropy: 0.002338837832212448, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 484,  Mean reward: 8.0, Mean Entropy: 0.0025450082030147314, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 485,  Mean reward: 8.0, Mean Entropy: 0.00224312674254179, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 486,  Mean reward: 8.0, Mean Entropy: 0.0022571254521608353, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 487,  Mean reward: 8.0, Mean Entropy: 0.001737498096190393, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 488,  Mean reward: 8.0, Mean Entropy: 0.0016059314366430044, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 489,  Mean reward: 8.0, Mean Entropy: 0.001021329895593226, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 490,  Mean reward: 8.0, Mean Entropy: 0.0012391051277518272, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 491,  Mean reward: 8.0, Mean Entropy: 0.0009623865480534732, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 492,  Mean reward: 8.0, Mean Entropy: 0.0009703485993668437, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 493,  Mean reward: 8.0, Mean Entropy: 0.0015287294518202543, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 494,  Mean reward: 8.0, Mean Entropy: 0.007725318428128958, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 495,  Mean reward: 8.0, Mean Entropy: 0.0021405040752142668, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 496,  Mean reward: 7.981012658227848, Mean Entropy: 0.0014685054775327444, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 497,  Mean reward: 8.0, Mean Entropy: 0.001752957934513688, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 498,  Mean reward: 8.0, Mean Entropy: 0.0018479421269148588, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 499,  Mean reward: 8.0, Mean Entropy: 0.00205793883651495, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 500,  Mean reward: 8.0, Mean Entropy: 0.0021152347326278687, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 8.0, Mean Entropy: 0.0020936126820743084, complete_episode_count: 80.0, Gather time: 0.63s, Train time: 0.77s
Iteration: 502,  Mean reward: 8.0, Mean Entropy: 0.0024074141401797533, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 503,  Mean reward: 8.0, Mean Entropy: 0.0028445040807127953, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 504,  Mean reward: 8.0, Mean Entropy: 0.004350751638412476, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 505,  Mean reward: 8.0, Mean Entropy: 0.00878775492310524, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 506,  Mean reward: 8.0, Mean Entropy: 0.018316704779863358, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 507,  Mean reward: 7.25, Mean Entropy: 0.005819213576614857, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 508,  Mean reward: 8.0, Mean Entropy: 0.002430541208013892, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 509,  Mean reward: 8.0, Mean Entropy: 0.0024434335064142942, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 510,  Mean reward: 8.0, Mean Entropy: 0.0036224396899342537, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 511,  Mean reward: 8.0, Mean Entropy: 0.0022978568449616432, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 512,  Mean reward: 8.0, Mean Entropy: 0.0018222881481051445, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 513,  Mean reward: 8.0, Mean Entropy: 0.0024737457279115915, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 514,  Mean reward: 8.0, Mean Entropy: 0.001451273332349956, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 515,  Mean reward: 8.0, Mean Entropy: 0.001439828658476472, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 516,  Mean reward: 8.0, Mean Entropy: 0.0027077775448560715, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 517,  Mean reward: 8.0, Mean Entropy: 0.0030134585686028004, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 518,  Mean reward: 8.0, Mean Entropy: 0.0011330051347613335, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 519,  Mean reward: 8.0, Mean Entropy: 0.0012393590295687318, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 520,  Mean reward: 8.0, Mean Entropy: 0.0011640437878668308, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 521,  Mean reward: 8.0, Mean Entropy: 0.001582944649271667, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 522,  Mean reward: 8.0, Mean Entropy: 0.0016358805587515235, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.82s
Iteration: 523,  Mean reward: 8.0, Mean Entropy: 0.0012249178253114223, complete_episode_count: 80.0, Gather time: 0.74s, Train time: 0.79s
Iteration: 524,  Mean reward: 8.0, Mean Entropy: 0.0016258114483207464, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 525,  Mean reward: 8.0, Mean Entropy: 0.00334936729632318, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 526,  Mean reward: 8.0, Mean Entropy: 0.0020270268432796, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 527,  Mean reward: 8.0, Mean Entropy: 0.0015677337069064379, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 528,  Mean reward: 8.0, Mean Entropy: 0.001284708734601736, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 529,  Mean reward: 8.0, Mean Entropy: 0.0013607507571578026, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 530,  Mean reward: 8.0, Mean Entropy: 0.0013356551062315702, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 531,  Mean reward: 8.0, Mean Entropy: 0.0017052296316251159, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 532,  Mean reward: 8.0, Mean Entropy: 0.0016369232907891273, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 533,  Mean reward: 8.0, Mean Entropy: 0.0018103427719324827, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 534,  Mean reward: 8.0, Mean Entropy: 0.00165105692576617, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 535,  Mean reward: 8.0, Mean Entropy: 0.002651254180818796, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 536,  Mean reward: 8.0, Mean Entropy: 0.002539796754717827, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 537,  Mean reward: 8.0, Mean Entropy: 0.002479519695043564, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.94s
Iteration: 538,  Mean reward: 8.0, Mean Entropy: 0.0025596769992262125, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 539,  Mean reward: 8.0, Mean Entropy: 0.002366598229855299, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 540,  Mean reward: 8.0, Mean Entropy: 0.002292775548994541, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 541,  Mean reward: 8.0, Mean Entropy: 0.0023585688322782516, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 542,  Mean reward: 8.0, Mean Entropy: 0.005393678322434425, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 543,  Mean reward: 8.0, Mean Entropy: 0.005685669835656881, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 544,  Mean reward: 8.0, Mean Entropy: 0.0026785675436258316, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 545,  Mean reward: 8.0, Mean Entropy: 0.002113920636475086, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 546,  Mean reward: 8.0, Mean Entropy: 0.002228989265859127, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 547,  Mean reward: 8.0, Mean Entropy: 0.001947882934473455, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 548,  Mean reward: 8.0, Mean Entropy: 0.0013889460824429989, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 549,  Mean reward: 8.0, Mean Entropy: 0.001079984474927187, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 550,  Mean reward: 8.0, Mean Entropy: 0.001396417384967208, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 551,  Mean reward: 8.0, Mean Entropy: 0.001428239862434566, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 552,  Mean reward: 8.0, Mean Entropy: 0.000984740792773664, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 553,  Mean reward: 8.0, Mean Entropy: 0.0008839878137223423, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 554,  Mean reward: 8.0, Mean Entropy: 0.0009083309560082853, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 555,  Mean reward: 8.0, Mean Entropy: 0.0010372400283813477, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 556,  Mean reward: 8.0, Mean Entropy: 0.0008620434673503041, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 557,  Mean reward: 8.0, Mean Entropy: 0.0008545581949874759, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.0007963086245581508, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 0.0008101607672870159, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 560,  Mean reward: 8.0, Mean Entropy: 0.0006592649733647704, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 561,  Mean reward: 8.0, Mean Entropy: 0.0005986534524708986, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 562,  Mean reward: 8.0, Mean Entropy: 0.0005942147690802813, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 563,  Mean reward: 8.0, Mean Entropy: 0.0007717594271525741, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 564,  Mean reward: 8.0, Mean Entropy: 0.0005172683740966022, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 565,  Mean reward: 8.0, Mean Entropy: 0.0006407744367606938, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 566,  Mean reward: 8.0, Mean Entropy: 0.0005993517115712166, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 567,  Mean reward: 8.0, Mean Entropy: 0.0005437210202217102, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 568,  Mean reward: 8.0, Mean Entropy: 0.0004901948850601912, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 569,  Mean reward: 8.0, Mean Entropy: 0.00048409978626295924, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
Iteration: 0,  Mean reward: -5.7875, Mean Entropy: 0.9025353193283081, complete_episode_count: 40.0, Gather time: 0.56s, Train time: 1.52s
rec seq len 2
actor lr 0.0005
Iteration: 1,  Mean reward: -6.107142857142857, Mean Entropy: 0.9386361837387085, complete_episode_count: 42.0, Gather time: 0.57s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 2,  Mean reward: -3.141304347826087, Mean Entropy: 0.9314103126525879, complete_episode_count: 46.0, Gather time: 0.58s, Train time: 1.55s
Iteration: 3,  Mean reward: -3.7111111111111112, Mean Entropy: 0.8808689117431641, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 4,  Mean reward: -4.280487804878049, Mean Entropy: 0.960289716720581, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.67s
Iteration: 5,  Mean reward: -4.595238095238095, Mean Entropy: 0.9530668258666992, complete_episode_count: 42.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 6,  Mean reward: -5.825581395348837, Mean Entropy: 0.9241663813591003, complete_episode_count: 43.0, Gather time: 0.54s, Train time: 1.52s
Iteration: 7,  Mean reward: -4.423913043478261, Mean Entropy: 0.8808002471923828, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 8,  Mean reward: -6.602564102564102, Mean Entropy: 0.9891351461410522, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 9,  Mean reward: -4.023255813953488, Mean Entropy: 0.9241313338279724, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 10,  Mean reward: -4.595238095238095, Mean Entropy: 0.9456137418746948, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 11,  Mean reward: -3.5625, Mean Entropy: 0.9023417234420776, complete_episode_count: 40.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 12,  Mean reward: -6.2682926829268295, Mean Entropy: 1.025040864944458, complete_episode_count: 41.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 13,  Mean reward: -4.6891891891891895, Mean Entropy: 0.988480269908905, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.45s
Iteration: 14,  Mean reward: -6.476190476190476, Mean Entropy: 0.8949103355407715, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 15,  Mean reward: -4.226190476190476, Mean Entropy: 0.9166571497917175, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 16,  Mean reward: -5.674418604651163, Mean Entropy: 0.8802539706230164, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 17,  Mean reward: -3.590909090909091, Mean Entropy: 0.9375056624412537, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 18,  Mean reward: -5.193181818181818, Mean Entropy: 0.9969179630279541, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 19,  Mean reward: -5.125, Mean Entropy: 0.9541850090026855, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 20,  Mean reward: -2.182926829268293, Mean Entropy: 0.9374658465385437, complete_episode_count: 41.0, Gather time: 0.57s, Train time: 1.50s
Iteration: 21,  Mean reward: -4.7926829268292686, Mean Entropy: 0.9021196365356445, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 22,  Mean reward: -5.410256410256411, Mean Entropy: 0.9096918106079102, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 23,  Mean reward: -6.337837837837838, Mean Entropy: 0.9385678172111511, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 24,  Mean reward: -2.5833333333333335, Mean Entropy: 0.9600259065628052, complete_episode_count: 42.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 25,  Mean reward: -1.9864864864864864, Mean Entropy: 0.8729370832443237, complete_episode_count: 37.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 26,  Mean reward: -5.451219512195122, Mean Entropy: 0.8925401568412781, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.60s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 27,  Mean reward: -1.7023809523809523, Mean Entropy: 0.9401795864105225, complete_episode_count: 42.0, Gather time: 0.62s, Train time: 1.50s
Iteration: 28,  Mean reward: -3.4146341463414633, Mean Entropy: 0.943452000617981, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 29,  Mean reward: -4.6477272727272725, Mean Entropy: 0.9579910039901733, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 30,  Mean reward: -5.144444444444445, Mean Entropy: 0.893932044506073, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 31,  Mean reward: -3.7325581395348837, Mean Entropy: 0.9313712120056152, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 32,  Mean reward: -3.966666666666667, Mean Entropy: 0.8751182556152344, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 33,  Mean reward: -1.6, Mean Entropy: 0.8966724872589111, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.52s
Iteration: 34,  Mean reward: -2.148936170212766, Mean Entropy: 0.8588391542434692, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 35,  Mean reward: -1.7653061224489797, Mean Entropy: 0.8498474955558777, complete_episode_count: 49.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 36,  Mean reward: -2.7444444444444445, Mean Entropy: 0.8228170275688171, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 37,  Mean reward: 0.5851063829787234, Mean Entropy: 0.8574409484863281, complete_episode_count: 47.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 38,  Mean reward: -1.755813953488372, Mean Entropy: 0.8644067645072937, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.66s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 39,  Mean reward: 1.2346938775510203, Mean Entropy: 0.8253486156463623, complete_episode_count: 49.0, Gather time: 0.59s, Train time: 1.46s
Iteration: 40,  Mean reward: -5.524390243902439, Mean Entropy: 0.9307646751403809, complete_episode_count: 41.0, Gather time: 0.54s, Train time: 1.48s
Iteration: 41,  Mean reward: -4.177777777777778, Mean Entropy: 0.922897219657898, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 42,  Mean reward: -3.951219512195122, Mean Entropy: 0.9782425761222839, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 43,  Mean reward: -2.0113636363636362, Mean Entropy: 0.9053985476493835, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 44,  Mean reward: -1.069767441860465, Mean Entropy: 0.8473910093307495, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 45,  Mean reward: -1.5319148936170213, Mean Entropy: 0.8582110404968262, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 46,  Mean reward: -0.125, Mean Entropy: 0.8718308210372925, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 47,  Mean reward: -4.628205128205129, Mean Entropy: 0.8935467600822449, complete_episode_count: 39.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 48,  Mean reward: -4.7631578947368425, Mean Entropy: 0.9596971273422241, complete_episode_count: 38.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 49,  Mean reward: -6.769230769230769, Mean Entropy: 0.8880715370178223, complete_episode_count: 39.0, Gather time: 0.54s, Train time: 1.49s
Iteration: 50,  Mean reward: -4.346153846153846, Mean Entropy: 0.9097504615783691, complete_episode_count: 39.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 51,  Mean reward: -4.4324324324324325, Mean Entropy: 0.9241418242454529, complete_episode_count: 37.0, Gather time: 0.54s, Train time: 1.50s
Iteration: 52,  Mean reward: -3.9047619047619047, Mean Entropy: 0.8735579252243042, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 53,  Mean reward: -4.75, Mean Entropy: 0.8879752159118652, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 54,  Mean reward: -4.817073170731708, Mean Entropy: 0.9023324847221375, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 55,  Mean reward: -4.573170731707317, Mean Entropy: 0.8651816844940186, complete_episode_count: 41.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 56,  Mean reward: -2.9767441860465116, Mean Entropy: 0.9136428833007812, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 57,  Mean reward: -3.7045454545454546, Mean Entropy: 0.997307300567627, complete_episode_count: 44.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 58,  Mean reward: -1.1818181818181819, Mean Entropy: 0.9215126037597656, complete_episode_count: 44.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 59,  Mean reward: -3.8043478260869565, Mean Entropy: 0.8848837614059448, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.54s
Iteration: 60,  Mean reward: 0.6195652173913043, Mean Entropy: 0.8130673170089722, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 61,  Mean reward: -0.6979166666666666, Mean Entropy: 0.8060662150382996, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 62,  Mean reward: -0.7978723404255319, Mean Entropy: 0.8785907626152039, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 63,  Mean reward: -4.333333333333333, Mean Entropy: 0.8557472229003906, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 64,  Mean reward: -2.7738095238095237, Mean Entropy: 0.9484970569610596, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.47s
Iteration: 65,  Mean reward: 0.32978723404255317, Mean Entropy: 0.9398145079612732, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 66,  Mean reward: -3.425531914893617, Mean Entropy: 0.9723376035690308, complete_episode_count: 47.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 67,  Mean reward: -2.3214285714285716, Mean Entropy: 0.8738951683044434, complete_episode_count: 42.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 68,  Mean reward: -0.2553191489361702, Mean Entropy: 0.9191884994506836, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 69,  Mean reward: -0.5098039215686274, Mean Entropy: 0.8273452520370483, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.54s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 70,  Mean reward: 2.0576923076923075, Mean Entropy: 0.6897763013839722, complete_episode_count: 52.0, Gather time: 0.58s, Train time: 1.46s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 71,  Mean reward: 3.0526315789473686, Mean Entropy: 0.5274307727813721, complete_episode_count: 57.0, Gather time: 0.60s, Train time: 1.68s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 72,  Mean reward: 3.789473684210526, Mean Entropy: 0.5941230058670044, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 73,  Mean reward: 4.60377358490566, Mean Entropy: 0.7141953706741333, complete_episode_count: 53.0, Gather time: 0.59s, Train time: 1.48s
Iteration: 74,  Mean reward: 1.6826923076923077, Mean Entropy: 0.6683270335197449, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 75,  Mean reward: 1.8703703703703705, Mean Entropy: 0.7072689533233643, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 76,  Mean reward: 3.6481481481481484, Mean Entropy: 0.6451252698898315, complete_episode_count: 54.0, Gather time: 0.55s, Train time: 1.48s
Iteration: 77,  Mean reward: 2.0918367346938775, Mean Entropy: 0.670975387096405, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 78,  Mean reward: 4.651785714285714, Mean Entropy: 0.6253403425216675, complete_episode_count: 56.0, Gather time: 0.59s, Train time: 1.50s
Iteration: 79,  Mean reward: 1.3272727272727274, Mean Entropy: 0.5764619708061218, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 80,  Mean reward: 4.027777777777778, Mean Entropy: 0.5878832340240479, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 81,  Mean reward: 4.603448275862069, Mean Entropy: 0.6790685653686523, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.45s
Iteration: 82,  Mean reward: 4.186274509803922, Mean Entropy: 0.6193081140518188, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.46s
Iteration: 83,  Mean reward: 3.358490566037736, Mean Entropy: 0.6077635884284973, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.47s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 84,  Mean reward: 5.25, Mean Entropy: 0.6851100921630859, complete_episode_count: 58.0, Gather time: 0.58s, Train time: 1.49s
Iteration: 85,  Mean reward: 3.4035087719298245, Mean Entropy: 0.61226886510849, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 86,  Mean reward: 3.4479166666666665, Mean Entropy: 0.729820191860199, complete_episode_count: 48.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 87,  Mean reward: 1.5, Mean Entropy: 0.7270780801773071, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 88,  Mean reward: 2.823529411764706, Mean Entropy: 0.7112735509872437, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 89,  Mean reward: 2.4903846153846154, Mean Entropy: 0.5815714597702026, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 90,  Mean reward: 3.61, Mean Entropy: 0.8424010276794434, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 91,  Mean reward: 1.36, Mean Entropy: 0.7414271235466003, complete_episode_count: 50.0, Gather time: 0.58s, Train time: 1.47s
Iteration: 92,  Mean reward: 2.7547169811320753, Mean Entropy: 0.6247960925102234, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 93,  Mean reward: 3.07, Mean Entropy: 0.6249210238456726, complete_episode_count: 50.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 94,  Mean reward: 4.0964912280701755, Mean Entropy: 0.5477858781814575, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 95,  Mean reward: 4.157407407407407, Mean Entropy: 0.6090956926345825, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.53s
Iteration: 96,  Mean reward: 3.6636363636363636, Mean Entropy: 0.6143091917037964, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.55s
Iteration: 97,  Mean reward: 4.833333333333333, Mean Entropy: 0.7149671912193298, complete_episode_count: 51.0, Gather time: 0.57s, Train time: 1.51s
Iteration: 98,  Mean reward: 4.0, Mean Entropy: 0.6327404379844666, complete_episode_count: 55.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 99,  Mean reward: 4.33, Mean Entropy: 0.6909767985343933, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 100,  Mean reward: 4.556603773584905, Mean Entropy: 0.5959593057632446, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.50s
rec seq len 2
actor lr 0.0005
Iteration: 101,  Mean reward: 1.6666666666666667, Mean Entropy: 0.6504995822906494, complete_episode_count: 57.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 102,  Mean reward: 3.0408163265306123, Mean Entropy: 0.6239736080169678, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.70s
Iteration: 103,  Mean reward: 4.820754716981132, Mean Entropy: 0.6301119923591614, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 104,  Mean reward: 3.297872340425532, Mean Entropy: 0.6781468391418457, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.52s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 105,  Mean reward: 5.640350877192983, Mean Entropy: 0.6157240867614746, complete_episode_count: 57.0, Gather time: 0.58s, Train time: 1.53s
Iteration: 106,  Mean reward: 2.033333333333333, Mean Entropy: 0.5067901015281677, complete_episode_count: 60.0, Gather time: 0.56s, Train time: 1.52s
Iteration: 107,  Mean reward: 1.2410714285714286, Mean Entropy: 0.5565211772918701, complete_episode_count: 56.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 108,  Mean reward: 4.336538461538462, Mean Entropy: 0.6702281832695007, complete_episode_count: 52.0, Gather time: 0.55s, Train time: 1.51s
Iteration: 109,  Mean reward: 2.490740740740741, Mean Entropy: 0.6997551918029785, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.46s
Iteration: 110,  Mean reward: -1.1226415094339623, Mean Entropy: 0.839492917060852, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.48s
Iteration: 111,  Mean reward: 1.6470588235294117, Mean Entropy: 0.8833909034729004, complete_episode_count: 51.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 112,  Mean reward: -5.311111111111111, Mean Entropy: 0.8989911079406738, complete_episode_count: 45.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 113,  Mean reward: -6.104651162790698, Mean Entropy: 0.6146844625473022, complete_episode_count: 43.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 114,  Mean reward: 2.574468085106383, Mean Entropy: 0.7053477168083191, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 115,  Mean reward: 0.7021276595744681, Mean Entropy: 0.7469058036804199, complete_episode_count: 47.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 116,  Mean reward: -0.011111111111111112, Mean Entropy: 0.7201402187347412, complete_episode_count: 45.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 117,  Mean reward: 4.923913043478261, Mean Entropy: 0.7175073623657227, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 118,  Mean reward: 1.0434782608695652, Mean Entropy: 0.7396568059921265, complete_episode_count: 46.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 119,  Mean reward: 0.75, Mean Entropy: 0.6819235682487488, complete_episode_count: 46.0, Gather time: 0.55s, Train time: 1.52s
Iteration: 120,  Mean reward: 2.683673469387755, Mean Entropy: 0.7062815427780151, complete_episode_count: 49.0, Gather time: 0.55s, Train time: 1.53s
Iteration: 121,  Mean reward: 4.10377358490566, Mean Entropy: 0.5542299151420593, complete_episode_count: 53.0, Gather time: 0.56s, Train time: 1.49s
Iteration: 122,  Mean reward: 3.5283018867924527, Mean Entropy: 0.6055505275726318, complete_episode_count: 53.0, Gather time: 0.57s, Train time: 1.52s
Iteration: 123,  Mean reward: 3.96875, Mean Entropy: 0.6837818026542664, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.54s
Iteration: 124,  Mean reward: 4.788461538461538, Mean Entropy: 0.6083608865737915, complete_episode_count: 52.0, Gather time: 0.56s, Train time: 1.51s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 125,  Mean reward: 6.107843137254902, Mean Entropy: 0.6492220163345337, complete_episode_count: 51.0, Gather time: 0.58s, Train time: 1.48s
Iteration: 126,  Mean reward: 5.913793103448276, Mean Entropy: 0.505606472492218, complete_episode_count: 58.0, Gather time: 0.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 127,  Mean reward: 6.139344262295082, Mean Entropy: 0.45515260100364685, complete_episode_count: 61.0, Gather time: 0.59s, Train time: 1.47s
Iteration: 128,  Mean reward: 6.116071428571429, Mean Entropy: 0.5157514214515686, complete_episode_count: 56.0, Gather time: 0.57s, Train time: 1.50s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 129,  Mean reward: 6.532258064516129, Mean Entropy: 0.3043820858001709, complete_episode_count: 62.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 130,  Mean reward: 6.354838709677419, Mean Entropy: 0.40643101930618286, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.74s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 131,  Mean reward: 6.666666666666667, Mean Entropy: 0.4408368468284607, complete_episode_count: 60.0, Gather time: 0.59s, Train time: 1.49s
Iteration: 132,  Mean reward: 5.942622950819672, Mean Entropy: 0.29128193855285645, complete_episode_count: 61.0, Gather time: 0.57s, Train time: 1.48s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 133,  Mean reward: 6.754237288135593, Mean Entropy: 0.3642158508300781, complete_episode_count: 59.0, Gather time: 1.56s, Train time: 1.49s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 134,  Mean reward: 7.145161290322581, Mean Entropy: 0.3135264813899994, complete_episode_count: 62.0, Gather time: 0.58s, Train time: 0.94s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 135,  Mean reward: 7.470588235294118, Mean Entropy: 0.25996527075767517, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 136,  Mean reward: 7.346153846153846, Mean Entropy: 0.2142437994480133, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.72s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 137,  Mean reward: 7.7534246575342465, Mean Entropy: 0.21473436057567596, complete_episode_count: 73.0, Gather time: 0.60s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 138,  Mean reward: 7.756944444444445, Mean Entropy: 0.1955835074186325, complete_episode_count: 72.0, Gather time: 0.60s, Train time: 0.78s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 139,  Mean reward: 7.8618421052631575, Mean Entropy: 0.14957034587860107, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 140,  Mean reward: 7.787671232876712, Mean Entropy: 0.16196313500404358, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 141,  Mean reward: 7.693333333333333, Mean Entropy: 0.09248535335063934, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.81s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 142,  Mean reward: 7.881578947368421, Mean Entropy: 0.13115425407886505, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 143,  Mean reward: 7.962025316455696, Mean Entropy: 0.08621078729629517, complete_episode_count: 79.0, Gather time: 0.61s, Train time: 0.78s
Iteration: 144,  Mean reward: 7.88, Mean Entropy: 0.08671017736196518, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 145,  Mean reward: 7.961538461538462, Mean Entropy: 0.0638871043920517, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 146,  Mean reward: 7.955128205128205, Mean Entropy: 0.0978960320353508, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 147,  Mean reward: 7.86, Mean Entropy: 0.04702363163232803, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.76s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 148,  Mean reward: 7.981012658227848, Mean Entropy: 0.05613843724131584, complete_episode_count: 79.0, Gather time: 0.60s, Train time: 0.71s
Iteration: 149,  Mean reward: 7.961538461538462, Mean Entropy: 0.041833311319351196, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 150,  Mean reward: 7.961538461538462, Mean Entropy: 0.028449539095163345, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.77s
NEW BEST MEAN REWARD----------------------<<<<<<<<<<< 
rec seq len 2
actor lr 0.0005
#################### SAVE CHECKPOINT #######################
Iteration: 151,  Mean reward: 8.0, Mean Entropy: 0.027714941650629044, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 152,  Mean reward: 8.0, Mean Entropy: 0.05182233080267906, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 153,  Mean reward: 7.962025316455696, Mean Entropy: 0.04370377957820892, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 154,  Mean reward: 8.0, Mean Entropy: 0.17975366115570068, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 155,  Mean reward: 7.533783783783784, Mean Entropy: 0.05160859227180481, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 156,  Mean reward: 7.980769230769231, Mean Entropy: 0.037734292447566986, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 157,  Mean reward: 8.0, Mean Entropy: 0.12711438536643982, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 158,  Mean reward: 7.207692307692308, Mean Entropy: 0.09448304027318954, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 159,  Mean reward: 7.923076923076923, Mean Entropy: 0.09650688618421555, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 160,  Mean reward: 7.961538461538462, Mean Entropy: 0.06375429034233093, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 161,  Mean reward: 7.9423076923076925, Mean Entropy: 0.07971187680959702, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 162,  Mean reward: 8.0, Mean Entropy: 0.19754156470298767, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 163,  Mean reward: 7.431506849315069, Mean Entropy: 0.17445093393325806, complete_episode_count: 73.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 164,  Mean reward: 7.79054054054054, Mean Entropy: 0.08436349034309387, complete_episode_count: 74.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 165,  Mean reward: 7.82, Mean Entropy: 0.11501263827085495, complete_episode_count: 75.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 166,  Mean reward: 7.901315789473684, Mean Entropy: 0.1280575692653656, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 167,  Mean reward: 7.84, Mean Entropy: 0.08010640740394592, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 168,  Mean reward: 7.935064935064935, Mean Entropy: 0.1413625329732895, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 169,  Mean reward: 7.228571428571429, Mean Entropy: 0.08698618412017822, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 170,  Mean reward: 7.915584415584416, Mean Entropy: 0.02424476109445095, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 171,  Mean reward: 7.981012658227848, Mean Entropy: 0.010790718719363213, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 172,  Mean reward: 8.0, Mean Entropy: 0.013920260593295097, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.90s
Iteration: 173,  Mean reward: 8.0, Mean Entropy: 0.008948281407356262, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 174,  Mean reward: 8.0, Mean Entropy: 0.01353270560503006, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 175,  Mean reward: 8.0, Mean Entropy: 0.014870706014335155, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.70s
Iteration: 176,  Mean reward: 7.961538461538462, Mean Entropy: 0.04155104607343674, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 177,  Mean reward: 7.974683544303797, Mean Entropy: 0.018099863082170486, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 178,  Mean reward: 8.0, Mean Entropy: 0.050469111651182175, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 179,  Mean reward: 7.941558441558442, Mean Entropy: 0.04099854454398155, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 180,  Mean reward: 7.981012658227848, Mean Entropy: 0.21396899223327637, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 181,  Mean reward: 5.92741935483871, Mean Entropy: 0.278236985206604, complete_episode_count: 62.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 182,  Mean reward: 7.902597402597403, Mean Entropy: 0.07045041769742966, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 183,  Mean reward: 7.9423076923076925, Mean Entropy: 0.048466622829437256, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 184,  Mean reward: 7.955128205128205, Mean Entropy: 0.017281539738178253, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 185,  Mean reward: 8.0, Mean Entropy: 0.008284933865070343, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 186,  Mean reward: 8.0, Mean Entropy: 0.011137227527797222, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 187,  Mean reward: 7.981012658227848, Mean Entropy: 0.012889310717582703, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 188,  Mean reward: 8.0, Mean Entropy: 0.007784147746860981, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 189,  Mean reward: 7.82051282051282, Mean Entropy: 0.015385381877422333, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 190,  Mean reward: 8.0, Mean Entropy: 0.011605635285377502, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.72s
Iteration: 191,  Mean reward: 8.0, Mean Entropy: 0.011677511036396027, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 192,  Mean reward: 8.0, Mean Entropy: 0.015732109546661377, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 193,  Mean reward: 8.0, Mean Entropy: 0.02310296706855297, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 194,  Mean reward: 7.981012658227848, Mean Entropy: 0.029983391985297203, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 195,  Mean reward: 8.0, Mean Entropy: 0.052924782037734985, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 196,  Mean reward: 7.922077922077922, Mean Entropy: 0.06557607650756836, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 197,  Mean reward: 7.981012658227848, Mean Entropy: 0.018429651856422424, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 198,  Mean reward: 8.0, Mean Entropy: 0.021378949284553528, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 199,  Mean reward: 7.981012658227848, Mean Entropy: 0.02153429388999939, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 200,  Mean reward: 8.0, Mean Entropy: 0.016828808933496475, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 201,  Mean reward: 8.0, Mean Entropy: 0.03126884624361992, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 202,  Mean reward: 8.0, Mean Entropy: 0.08379596471786499, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 203,  Mean reward: 7.881578947368421, Mean Entropy: 0.055160194635391235, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 204,  Mean reward: 7.981012658227848, Mean Entropy: 0.026844045147299767, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 205,  Mean reward: 8.0, Mean Entropy: 0.054799191653728485, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 206,  Mean reward: 7.961538461538462, Mean Entropy: 0.04997146129608154, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 207,  Mean reward: 7.981012658227848, Mean Entropy: 0.027413666248321533, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 208,  Mean reward: 7.981012658227848, Mean Entropy: 0.012630043551325798, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 209,  Mean reward: 8.0, Mean Entropy: 0.009421157650649548, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 210,  Mean reward: 8.0, Mean Entropy: 0.009725244715809822, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.93s
Iteration: 211,  Mean reward: 8.0, Mean Entropy: 0.011195551604032516, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 212,  Mean reward: 8.0, Mean Entropy: 0.013034715317189693, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 213,  Mean reward: 7.981012658227848, Mean Entropy: 0.008546194061636925, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 214,  Mean reward: 8.0, Mean Entropy: 0.007451359182596207, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 215,  Mean reward: 8.0, Mean Entropy: 0.009510263800621033, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 216,  Mean reward: 8.0, Mean Entropy: 0.010060916654765606, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 217,  Mean reward: 7.981012658227848, Mean Entropy: 0.008805686607956886, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 218,  Mean reward: 8.0, Mean Entropy: 0.011800534091889858, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 219,  Mean reward: 8.0, Mean Entropy: 0.018388448283076286, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 220,  Mean reward: 8.0, Mean Entropy: 0.024794256314635277, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 221,  Mean reward: 8.0, Mean Entropy: 0.045953959226608276, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 222,  Mean reward: 7.961538461538462, Mean Entropy: 0.03880711644887924, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 223,  Mean reward: 8.0, Mean Entropy: 0.15305712819099426, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 224,  Mean reward: 7.057971014492754, Mean Entropy: 0.1155160665512085, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 225,  Mean reward: 8.0, Mean Entropy: 0.05574673414230347, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 226,  Mean reward: 7.961538461538462, Mean Entropy: 0.05426657944917679, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 227,  Mean reward: 8.0, Mean Entropy: 0.060203246772289276, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 228,  Mean reward: 7.961538461538462, Mean Entropy: 0.06615282595157623, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 229,  Mean reward: 7.922077922077922, Mean Entropy: 0.06752632558345795, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 230,  Mean reward: 8.0, Mean Entropy: 0.05746164172887802, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 231,  Mean reward: 7.52027027027027, Mean Entropy: 0.11346016824245453, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 232,  Mean reward: 8.0, Mean Entropy: 0.062212005257606506, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 233,  Mean reward: 7.941558441558442, Mean Entropy: 0.01995348557829857, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 234,  Mean reward: 7.981012658227848, Mean Entropy: 0.006767679005861282, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 235,  Mean reward: 8.0, Mean Entropy: 0.0029585021547973156, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 236,  Mean reward: 8.0, Mean Entropy: 0.0019269322510808706, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 237,  Mean reward: 8.0, Mean Entropy: 0.0031227674335241318, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 238,  Mean reward: 8.0, Mean Entropy: 0.003961528651416302, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 239,  Mean reward: 8.0, Mean Entropy: 0.005133218131959438, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 240,  Mean reward: 8.0, Mean Entropy: 0.010328738018870354, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 241,  Mean reward: 7.981012658227848, Mean Entropy: 0.007685612887144089, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 242,  Mean reward: 8.0, Mean Entropy: 0.004765243735164404, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 243,  Mean reward: 8.0, Mean Entropy: 0.005514179822057486, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 244,  Mean reward: 7.974683544303797, Mean Entropy: 0.04005054011940956, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.71s
Iteration: 245,  Mean reward: 7.961538461538462, Mean Entropy: 0.020456209778785706, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 246,  Mean reward: 8.0, Mean Entropy: 0.021722665056586266, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 247,  Mean reward: 8.0, Mean Entropy: 0.04128023236989975, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 248,  Mean reward: 7.961538461538462, Mean Entropy: 0.0355052724480629, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.92s
Iteration: 249,  Mean reward: 8.0, Mean Entropy: 0.21467304229736328, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 250,  Mean reward: 1.7796610169491525, Mean Entropy: 0.23599573969841003, complete_episode_count: 59.0, Gather time: 0.57s, Train time: 1.49s
Iteration: 251,  Mean reward: 7.623376623376624, Mean Entropy: 0.061606697738170624, complete_episode_count: 77.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 252,  Mean reward: 8.0, Mean Entropy: 0.11729496717453003, complete_episode_count: 80.0, Gather time: 0.61s, Train time: 0.75s
Iteration: 253,  Mean reward: 7.602941176470588, Mean Entropy: 0.10804003477096558, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 254,  Mean reward: 7.981012658227848, Mean Entropy: 0.008937227539718151, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 255,  Mean reward: 8.0, Mean Entropy: 0.012227259576320648, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 256,  Mean reward: 8.0, Mean Entropy: 0.023149430751800537, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 257,  Mean reward: 8.0, Mean Entropy: 0.11677992343902588, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 258,  Mean reward: 7.486486486486487, Mean Entropy: 0.12400001287460327, complete_episode_count: 74.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 259,  Mean reward: 7.9423076923076925, Mean Entropy: 0.034070681780576706, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 260,  Mean reward: 8.0, Mean Entropy: 0.04487457126379013, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 261,  Mean reward: 7.7368421052631575, Mean Entropy: 0.04487600177526474, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 262,  Mean reward: 8.0, Mean Entropy: 0.013312743976712227, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 263,  Mean reward: 7.981012658227848, Mean Entropy: 0.01100139319896698, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 264,  Mean reward: 7.981012658227848, Mean Entropy: 0.0037576481699943542, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 265,  Mean reward: 8.0, Mean Entropy: 0.002739356830716133, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 266,  Mean reward: 8.0, Mean Entropy: 0.002480358351022005, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 267,  Mean reward: 8.0, Mean Entropy: 0.0019892945419996977, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 268,  Mean reward: 8.0, Mean Entropy: 0.002558546606451273, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 269,  Mean reward: 8.0, Mean Entropy: 0.003256383817642927, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 270,  Mean reward: 8.0, Mean Entropy: 0.0034940734039992094, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 271,  Mean reward: 8.0, Mean Entropy: 0.00339932506904006, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 272,  Mean reward: 8.0, Mean Entropy: 0.008334603160619736, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 273,  Mean reward: 8.0, Mean Entropy: 0.05885925143957138, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 274,  Mean reward: 7.961538461538462, Mean Entropy: 0.07100405544042587, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 275,  Mean reward: 7.981012658227848, Mean Entropy: 0.156919464468956, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 276,  Mean reward: 6.302816901408451, Mean Entropy: 0.03477579355239868, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 277,  Mean reward: 8.0, Mean Entropy: 0.02717963419854641, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 278,  Mean reward: 7.974683544303797, Mean Entropy: 0.045875608921051025, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 279,  Mean reward: 7.961538461538462, Mean Entropy: 0.04240185022354126, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 280,  Mean reward: 7.962025316455696, Mean Entropy: 0.02971050702035427, complete_episode_count: 79.0, Gather time: 0.57s, Train time: 0.86s
Iteration: 281,  Mean reward: 7.981012658227848, Mean Entropy: 0.013407565653324127, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 282,  Mean reward: 7.981012658227848, Mean Entropy: 0.01026986539363861, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 283,  Mean reward: 8.0, Mean Entropy: 0.011697359383106232, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 284,  Mean reward: 8.0, Mean Entropy: 0.035846345126628876, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 285,  Mean reward: 7.941558441558442, Mean Entropy: 0.04827791452407837, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 286,  Mean reward: 8.0, Mean Entropy: 0.019768040627241135, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.91s
Iteration: 287,  Mean reward: 8.0, Mean Entropy: 0.20388492941856384, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 288,  Mean reward: 7.457142857142857, Mean Entropy: 0.16701161861419678, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 289,  Mean reward: 7.86, Mean Entropy: 0.029118238016963005, complete_episode_count: 75.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 290,  Mean reward: 8.0, Mean Entropy: 0.12369909137487411, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 291,  Mean reward: 7.881578947368421, Mean Entropy: 0.08937245607376099, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 292,  Mean reward: 7.9423076923076925, Mean Entropy: 0.0968291163444519, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 293,  Mean reward: 7.605263157894737, Mean Entropy: 0.06441396474838257, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 294,  Mean reward: 7.961538461538462, Mean Entropy: 0.028559153899550438, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 295,  Mean reward: 8.0, Mean Entropy: 0.03502694517374039, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 296,  Mean reward: 7.981012658227848, Mean Entropy: 0.032242268323898315, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 297,  Mean reward: 7.961538461538462, Mean Entropy: 0.013483159244060516, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 298,  Mean reward: 8.0, Mean Entropy: 0.009515400975942612, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 299,  Mean reward: 8.0, Mean Entropy: 0.010907444171607494, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 300,  Mean reward: 7.981012658227848, Mean Entropy: 0.004168995656073093, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 301,  Mean reward: 7.981012658227848, Mean Entropy: 0.0014922420959919691, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 302,  Mean reward: 8.0, Mean Entropy: 0.0011015980271622539, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 303,  Mean reward: 8.0, Mean Entropy: 0.0009144414216279984, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 304,  Mean reward: 8.0, Mean Entropy: 0.0008733766153454781, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 305,  Mean reward: 8.0, Mean Entropy: 0.0007592263864353299, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 306,  Mean reward: 8.0, Mean Entropy: 0.0009402378345839679, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 307,  Mean reward: 8.0, Mean Entropy: 0.0009655570611357689, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 308,  Mean reward: 8.0, Mean Entropy: 0.0009013063972815871, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 309,  Mean reward: 8.0, Mean Entropy: 0.0009314889903180301, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 310,  Mean reward: 8.0, Mean Entropy: 0.0009455840918235481, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 311,  Mean reward: 8.0, Mean Entropy: 0.0012196180177852511, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 312,  Mean reward: 8.0, Mean Entropy: 0.001373711391352117, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 313,  Mean reward: 8.0, Mean Entropy: 0.002507858444005251, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 314,  Mean reward: 8.0, Mean Entropy: 0.00933723896741867, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 315,  Mean reward: 8.0, Mean Entropy: 0.2405831664800644, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 316,  Mean reward: 7.0546875, Mean Entropy: 0.004350667353719473, complete_episode_count: 64.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 317,  Mean reward: 7.981012658227848, Mean Entropy: 0.00536485156044364, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 318,  Mean reward: 7.981012658227848, Mean Entropy: 0.002910679206252098, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 319,  Mean reward: 8.0, Mean Entropy: 0.0016322077717632055, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 320,  Mean reward: 8.0, Mean Entropy: 0.0010223810095340014, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 321,  Mean reward: 8.0, Mean Entropy: 0.0010288041085004807, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 322,  Mean reward: 8.0, Mean Entropy: 0.0007200014661066234, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 323,  Mean reward: 8.0, Mean Entropy: 0.0008580292342230678, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 324,  Mean reward: 8.0, Mean Entropy: 0.0008326463284902275, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 325,  Mean reward: 8.0, Mean Entropy: 0.0007157537620514631, complete_episode_count: 80.0, Gather time: 0.65s, Train time: 0.74s
Iteration: 326,  Mean reward: 8.0, Mean Entropy: 0.0009165660012513399, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 327,  Mean reward: 8.0, Mean Entropy: 0.0009497327264398336, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 328,  Mean reward: 8.0, Mean Entropy: 0.0009233138407580554, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 329,  Mean reward: 8.0, Mean Entropy: 0.0023004505783319473, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 330,  Mean reward: 8.0, Mean Entropy: 0.005512806586921215, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 331,  Mean reward: 8.0, Mean Entropy: 0.1821889877319336, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 332,  Mean reward: 6.983606557377049, Mean Entropy: 0.12485842406749725, complete_episode_count: 61.0, Gather time: 0.56s, Train time: 1.51s
Iteration: 333,  Mean reward: 7.75, Mean Entropy: 0.05247887223958969, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 334,  Mean reward: 8.0, Mean Entropy: 0.21828600764274597, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 335,  Mean reward: 6.5396825396825395, Mean Entropy: 0.20269887149333954, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 336,  Mean reward: 7.881578947368421, Mean Entropy: 0.04034678637981415, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 337,  Mean reward: 8.0, Mean Entropy: 0.007489349227398634, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 338,  Mean reward: 8.0, Mean Entropy: 0.13892363011837006, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.82s
Iteration: 339,  Mean reward: 7.328571428571428, Mean Entropy: 0.14352652430534363, complete_episode_count: 70.0, Gather time: 0.64s, Train time: 0.80s
Iteration: 340,  Mean reward: 7.84, Mean Entropy: 0.10999307036399841, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 341,  Mean reward: 7.597402597402597, Mean Entropy: 0.06352923065423965, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 342,  Mean reward: 7.928571428571429, Mean Entropy: 0.01726289466023445, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 343,  Mean reward: 8.0, Mean Entropy: 0.178223118185997, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 344,  Mean reward: 7.837837837837838, Mean Entropy: 0.12542402744293213, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 345,  Mean reward: 7.729166666666667, Mean Entropy: 0.2379562109708786, complete_episode_count: 72.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 346,  Mean reward: 7.119047619047619, Mean Entropy: 0.04225839301943779, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.86s
Iteration: 347,  Mean reward: 7.961538461538462, Mean Entropy: 0.05474633723497391, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 348,  Mean reward: 7.902597402597403, Mean Entropy: 0.014401555061340332, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 349,  Mean reward: 8.0, Mean Entropy: 0.06251221895217896, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.71s
Iteration: 350,  Mean reward: 7.923076923076923, Mean Entropy: 0.03171452134847641, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 351,  Mean reward: 7.980769230769231, Mean Entropy: 0.020908113569021225, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 352,  Mean reward: 7.981012658227848, Mean Entropy: 0.028105448931455612, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 353,  Mean reward: 7.981012658227848, Mean Entropy: 0.0014343520160764456, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 354,  Mean reward: 8.0, Mean Entropy: 0.0007287319749593735, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 355,  Mean reward: 8.0, Mean Entropy: 0.0006145339575596154, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 356,  Mean reward: 8.0, Mean Entropy: 0.0007636998780071735, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 357,  Mean reward: 8.0, Mean Entropy: 0.0007966157281771302, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 358,  Mean reward: 8.0, Mean Entropy: 0.0007268100744113326, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 359,  Mean reward: 8.0, Mean Entropy: 0.0007986437412910163, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 360,  Mean reward: 8.0, Mean Entropy: 0.001338971545919776, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 361,  Mean reward: 8.0, Mean Entropy: 0.001468744594603777, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 362,  Mean reward: 8.0, Mean Entropy: 0.001795171294361353, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 363,  Mean reward: 8.0, Mean Entropy: 0.0027542412281036377, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 364,  Mean reward: 8.0, Mean Entropy: 0.011160781607031822, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 365,  Mean reward: 8.0, Mean Entropy: 0.08977216482162476, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 366,  Mean reward: 7.366197183098592, Mean Entropy: 0.11285382509231567, complete_episode_count: 71.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 367,  Mean reward: 7.901315789473684, Mean Entropy: 0.025036951526999474, complete_episode_count: 76.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 368,  Mean reward: 7.981012658227848, Mean Entropy: 0.007233345415443182, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 369,  Mean reward: 8.0, Mean Entropy: 0.1975838840007782, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 370,  Mean reward: 7.309523809523809, Mean Entropy: 0.1225014179944992, complete_episode_count: 63.0, Gather time: 0.56s, Train time: 0.75s
Iteration: 371,  Mean reward: 7.902597402597403, Mean Entropy: 0.1502058058977127, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 372,  Mean reward: 7.858108108108108, Mean Entropy: 0.07749557495117188, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 373,  Mean reward: 7.961538461538462, Mean Entropy: 0.07521810382604599, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 374,  Mean reward: 7.955128205128205, Mean Entropy: 0.05627208203077316, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 375,  Mean reward: 7.941558441558442, Mean Entropy: 0.03592311590909958, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 376,  Mean reward: 7.981012658227848, Mean Entropy: 0.0232959296554327, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 377,  Mean reward: 8.0, Mean Entropy: 0.12509912252426147, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 378,  Mean reward: 7.618421052631579, Mean Entropy: 0.09809938073158264, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 379,  Mean reward: 7.883116883116883, Mean Entropy: 0.05312373489141464, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 380,  Mean reward: 7.981012658227848, Mean Entropy: 0.040731824934482574, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 381,  Mean reward: 7.961538461538462, Mean Entropy: 0.06212658807635307, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 382,  Mean reward: 7.9423076923076925, Mean Entropy: 0.07553137838840485, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 383,  Mean reward: 7.961538461538462, Mean Entropy: 0.06601957231760025, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 384,  Mean reward: 7.730263157894737, Mean Entropy: 0.02756347879767418, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 385,  Mean reward: 8.0, Mean Entropy: 0.011649839580059052, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 386,  Mean reward: 7.981012658227848, Mean Entropy: 0.0010167800355702639, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 387,  Mean reward: 8.0, Mean Entropy: 0.0005714419530704618, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 388,  Mean reward: 8.0, Mean Entropy: 0.0004858328029513359, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 389,  Mean reward: 8.0, Mean Entropy: 0.0005695290747098625, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 390,  Mean reward: 8.0, Mean Entropy: 0.0007906283717602491, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 391,  Mean reward: 8.0, Mean Entropy: 0.0013679687399417162, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 392,  Mean reward: 8.0, Mean Entropy: 0.002263444708660245, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 393,  Mean reward: 8.0, Mean Entropy: 0.011728249490261078, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 394,  Mean reward: 8.0, Mean Entropy: 0.07963308691978455, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.80s
Iteration: 395,  Mean reward: 7.902597402597403, Mean Entropy: 0.053284063935279846, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 396,  Mean reward: 7.941558441558442, Mean Entropy: 0.006375109311193228, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 397,  Mean reward: 8.0, Mean Entropy: 0.0011714339489117265, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 398,  Mean reward: 8.0, Mean Entropy: 0.0022247284650802612, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 399,  Mean reward: 8.0, Mean Entropy: 0.018016453832387924, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 400,  Mean reward: 8.0, Mean Entropy: 0.07708920538425446, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.93s
rec seq len 2
actor lr 0.0005
Iteration: 401,  Mean reward: 7.902597402597403, Mean Entropy: 0.05626719444990158, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 402,  Mean reward: 7.981012658227848, Mean Entropy: 0.06639081239700317, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 403,  Mean reward: 7.981012658227848, Mean Entropy: 0.03377857431769371, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 404,  Mean reward: 7.981012658227848, Mean Entropy: 0.04073014855384827, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 405,  Mean reward: 7.961538461538462, Mean Entropy: 0.012002321891486645, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 406,  Mean reward: 8.0, Mean Entropy: 0.023924848064780235, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 407,  Mean reward: 7.961538461538462, Mean Entropy: 0.005261266138404608, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 408,  Mean reward: 8.0, Mean Entropy: 0.0019307562615722418, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 409,  Mean reward: 8.0, Mean Entropy: 0.005803605075925589, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 410,  Mean reward: 8.0, Mean Entropy: 0.014963465742766857, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 411,  Mean reward: 7.981012658227848, Mean Entropy: 0.01286675687879324, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 412,  Mean reward: 8.0, Mean Entropy: 0.02318969927728176, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 413,  Mean reward: 8.0, Mean Entropy: 0.04121846333146095, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 414,  Mean reward: 8.0, Mean Entropy: 0.031580328941345215, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 415,  Mean reward: 8.0, Mean Entropy: 0.03349093720316887, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 416,  Mean reward: 7.981012658227848, Mean Entropy: 0.023960355669260025, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 417,  Mean reward: 8.0, Mean Entropy: 0.033069826662540436, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 418,  Mean reward: 7.981012658227848, Mean Entropy: 0.023510819301009178, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 419,  Mean reward: 7.962025316455696, Mean Entropy: 0.014699186198413372, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 420,  Mean reward: 8.0, Mean Entropy: 0.01793232001364231, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 421,  Mean reward: 8.0, Mean Entropy: 0.02705179713666439, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 422,  Mean reward: 8.0, Mean Entropy: 0.0921013355255127, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 423,  Mean reward: 7.573333333333333, Mean Entropy: 0.10377514362335205, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 424,  Mean reward: 7.921052631578948, Mean Entropy: 0.02119717374444008, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 425,  Mean reward: 8.0, Mean Entropy: 0.022341288626194, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 426,  Mean reward: 8.0, Mean Entropy: 0.07232724130153656, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 427,  Mean reward: 7.922077922077922, Mean Entropy: 0.060214266180992126, complete_episode_count: 77.0, Gather time: 0.60s, Train time: 0.77s
Iteration: 428,  Mean reward: 7.941558441558442, Mean Entropy: 0.04179573059082031, complete_episode_count: 77.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 429,  Mean reward: 7.961538461538462, Mean Entropy: 0.015734529122710228, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 430,  Mean reward: 7.980769230769231, Mean Entropy: 0.0013176355278119445, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 431,  Mean reward: 8.0, Mean Entropy: 4.0474769775755703e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 432,  Mean reward: 8.0, Mean Entropy: 5.1551258366089314e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 433,  Mean reward: 8.0, Mean Entropy: 8.400129445362836e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 434,  Mean reward: 8.0, Mean Entropy: 7.922663644421846e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 435,  Mean reward: 8.0, Mean Entropy: 6.050846423022449e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 436,  Mean reward: 8.0, Mean Entropy: 6.155909795779735e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 437,  Mean reward: 8.0, Mean Entropy: 5.459978638100438e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 438,  Mean reward: 8.0, Mean Entropy: 5.7626333727966994e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 439,  Mean reward: 8.0, Mean Entropy: 5.5788634199416265e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 440,  Mean reward: 8.0, Mean Entropy: 6.0610655054915696e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 441,  Mean reward: 8.0, Mean Entropy: 4.921148865832947e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 442,  Mean reward: 8.0, Mean Entropy: 3.6369310691952705e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 443,  Mean reward: 8.0, Mean Entropy: 4.815119245904498e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 444,  Mean reward: 8.0, Mean Entropy: 4.130677552893758e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 445,  Mean reward: 8.0, Mean Entropy: 2.983187005156651e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 446,  Mean reward: 8.0, Mean Entropy: 3.854636452160776e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 447,  Mean reward: 8.0, Mean Entropy: 3.009658757946454e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 448,  Mean reward: 8.0, Mean Entropy: 3.23244821629487e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 449,  Mean reward: 8.0, Mean Entropy: 3.123685019090772e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 450,  Mean reward: 8.0, Mean Entropy: 3.758823368116282e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 451,  Mean reward: 8.0, Mean Entropy: 3.7045996577944607e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 452,  Mean reward: 8.0, Mean Entropy: 3.802352512138896e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 453,  Mean reward: 8.0, Mean Entropy: 3.2422656659036875e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 454,  Mean reward: 8.0, Mean Entropy: 3.055266279261559e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 455,  Mean reward: 8.0, Mean Entropy: 3.507463407004252e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 456,  Mean reward: 8.0, Mean Entropy: 2.8527760150609538e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 457,  Mean reward: 8.0, Mean Entropy: 4.104050458408892e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 458,  Mean reward: 8.0, Mean Entropy: 3.7482088373508304e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 459,  Mean reward: 8.0, Mean Entropy: 3.9557031414005905e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 460,  Mean reward: 8.0, Mean Entropy: 3.657939305412583e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.70s
Iteration: 461,  Mean reward: 8.0, Mean Entropy: 3.0038207114557736e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 462,  Mean reward: 8.0, Mean Entropy: 4.013365105492994e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 463,  Mean reward: 8.0, Mean Entropy: 4.3108913814648986e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 464,  Mean reward: 8.0, Mean Entropy: 3.989031392848119e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 465,  Mean reward: 8.0, Mean Entropy: 4.34017856605351e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 466,  Mean reward: 8.0, Mean Entropy: 4.611790063790977e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 467,  Mean reward: 8.0, Mean Entropy: 4.327455098973587e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 468,  Mean reward: 8.0, Mean Entropy: 3.538024611771107e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 469,  Mean reward: 8.0, Mean Entropy: 4.897657345281914e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 470,  Mean reward: 8.0, Mean Entropy: 6.24012463958934e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 471,  Mean reward: 8.0, Mean Entropy: 6.232651503523812e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 472,  Mean reward: 8.0, Mean Entropy: 6.491367821581662e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 473,  Mean reward: 8.0, Mean Entropy: 6.167704123072326e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 474,  Mean reward: 8.0, Mean Entropy: 6.451595982071012e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 475,  Mean reward: 8.0, Mean Entropy: 0.00010551762534305453, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 476,  Mean reward: 8.0, Mean Entropy: 0.000121135642984882, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.91s
Iteration: 477,  Mean reward: 8.0, Mean Entropy: 0.00012666350812651217, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 478,  Mean reward: 8.0, Mean Entropy: 0.00014494868810288608, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 479,  Mean reward: 8.0, Mean Entropy: 0.00016870074614416808, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 480,  Mean reward: 8.0, Mean Entropy: 0.0002026623988058418, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 481,  Mean reward: 8.0, Mean Entropy: 0.0002670046524144709, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 482,  Mean reward: 8.0, Mean Entropy: 0.0004192765336483717, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 483,  Mean reward: 8.0, Mean Entropy: 0.0007423992501571774, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 484,  Mean reward: 8.0, Mean Entropy: 0.001630963757634163, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 485,  Mean reward: 8.0, Mean Entropy: 0.0038763955235481262, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 486,  Mean reward: 8.0, Mean Entropy: 0.015189595520496368, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 487,  Mean reward: 8.0, Mean Entropy: 0.04433499276638031, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 488,  Mean reward: 7.980769230769231, Mean Entropy: 0.018490448594093323, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 489,  Mean reward: 8.0, Mean Entropy: 0.00011117913527414203, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 490,  Mean reward: 8.0, Mean Entropy: 0.00015243643429130316, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 491,  Mean reward: 8.0, Mean Entropy: 0.00024286039115395397, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 492,  Mean reward: 8.0, Mean Entropy: 0.0003044071781914681, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 493,  Mean reward: 8.0, Mean Entropy: 0.0002196751011069864, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.77s
Iteration: 494,  Mean reward: 8.0, Mean Entropy: 0.00023542786948382854, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 495,  Mean reward: 8.0, Mean Entropy: 0.0001325943594565615, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 496,  Mean reward: 8.0, Mean Entropy: 0.00014485439169220626, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 497,  Mean reward: 8.0, Mean Entropy: 0.00016924572992138565, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.77s
Iteration: 498,  Mean reward: 8.0, Mean Entropy: 0.0001454589219065383, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 499,  Mean reward: 8.0, Mean Entropy: 9.392783977091312e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 500,  Mean reward: 8.0, Mean Entropy: 9.728343866299838e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
rec seq len 2
actor lr 0.0005
Iteration: 501,  Mean reward: 8.0, Mean Entropy: 8.29245982458815e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 502,  Mean reward: 8.0, Mean Entropy: 8.037588850129396e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 503,  Mean reward: 8.0, Mean Entropy: 8.11441132100299e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 504,  Mean reward: 8.0, Mean Entropy: 7.048244879115373e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 505,  Mean reward: 8.0, Mean Entropy: 7.982902752701193e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 506,  Mean reward: 8.0, Mean Entropy: 6.294589547906071e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 507,  Mean reward: 8.0, Mean Entropy: 6.008569471305236e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 508,  Mean reward: 8.0, Mean Entropy: 4.792767867911607e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 509,  Mean reward: 8.0, Mean Entropy: 5.270111432764679e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 510,  Mean reward: 8.0, Mean Entropy: 4.4038108171662316e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 511,  Mean reward: 8.0, Mean Entropy: 5.2862065786030143e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 512,  Mean reward: 8.0, Mean Entropy: 4.529363286565058e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 513,  Mean reward: 8.0, Mean Entropy: 4.171087493887171e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 514,  Mean reward: 8.0, Mean Entropy: 4.441308192326687e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 515,  Mean reward: 8.0, Mean Entropy: 3.977609958383255e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 516,  Mean reward: 8.0, Mean Entropy: 4.7575256758136675e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 517,  Mean reward: 8.0, Mean Entropy: 3.1064715585671365e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 518,  Mean reward: 8.0, Mean Entropy: 4.2438143282197416e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 519,  Mean reward: 8.0, Mean Entropy: 4.345586057752371e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 520,  Mean reward: 8.0, Mean Entropy: 3.627654223237187e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 521,  Mean reward: 8.0, Mean Entropy: 3.42680505127646e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 522,  Mean reward: 8.0, Mean Entropy: 3.998800821136683e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 523,  Mean reward: 8.0, Mean Entropy: 3.995070801465772e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 524,  Mean reward: 8.0, Mean Entropy: 4.4509637518785894e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 525,  Mean reward: 8.0, Mean Entropy: 3.1668048904975876e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 526,  Mean reward: 8.0, Mean Entropy: 4.1121220419881865e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 527,  Mean reward: 8.0, Mean Entropy: 3.6595949495676905e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 528,  Mean reward: 8.0, Mean Entropy: 4.466708924155682e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 529,  Mean reward: 8.0, Mean Entropy: 4.8066558520076796e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.71s
Iteration: 530,  Mean reward: 8.0, Mean Entropy: 3.5230252251494676e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 531,  Mean reward: 8.0, Mean Entropy: 4.0392707887804136e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 532,  Mean reward: 8.0, Mean Entropy: 4.082082159584388e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 533,  Mean reward: 8.0, Mean Entropy: 4.2997708078473806e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 534,  Mean reward: 8.0, Mean Entropy: 4.531761078396812e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 535,  Mean reward: 8.0, Mean Entropy: 3.79136145056691e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 536,  Mean reward: 8.0, Mean Entropy: 4.574909689836204e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.85s
Iteration: 537,  Mean reward: 8.0, Mean Entropy: 4.3105326767545193e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 538,  Mean reward: 8.0, Mean Entropy: 3.291579560027458e-05, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 539,  Mean reward: 8.0, Mean Entropy: 5.519636761164293e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 540,  Mean reward: 8.0, Mean Entropy: 4.690560672315769e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 541,  Mean reward: 8.0, Mean Entropy: 5.296451490721665e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 542,  Mean reward: 8.0, Mean Entropy: 4.851002449868247e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 543,  Mean reward: 8.0, Mean Entropy: 5.4894404456717893e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.71s
Iteration: 544,  Mean reward: 8.0, Mean Entropy: 5.9639307437464595e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 545,  Mean reward: 8.0, Mean Entropy: 5.809164576930925e-05, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.74s
Iteration: 546,  Mean reward: 8.0, Mean Entropy: 7.1747723268345e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 547,  Mean reward: 8.0, Mean Entropy: 5.434850390884094e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 548,  Mean reward: 8.0, Mean Entropy: 6.422594742616639e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 549,  Mean reward: 8.0, Mean Entropy: 8.627748320577666e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 550,  Mean reward: 8.0, Mean Entropy: 5.478132516145706e-05, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.81s
Iteration: 551,  Mean reward: 8.0, Mean Entropy: 0.00012037437409162521, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 552,  Mean reward: 8.0, Mean Entropy: 0.00011467721196822822, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.90s
Iteration: 553,  Mean reward: 8.0, Mean Entropy: 9.498011786490679e-05, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 554,  Mean reward: 8.0, Mean Entropy: 0.00013532328011933714, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 555,  Mean reward: 8.0, Mean Entropy: 0.00020631166989915073, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 556,  Mean reward: 8.0, Mean Entropy: 0.0003160018823109567, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 557,  Mean reward: 8.0, Mean Entropy: 0.0004362639447208494, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 558,  Mean reward: 8.0, Mean Entropy: 0.0007086399709805846, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 559,  Mean reward: 8.0, Mean Entropy: 0.002858672058209777, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 560,  Mean reward: 8.0, Mean Entropy: 0.013618726283311844, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 561,  Mean reward: 8.0, Mean Entropy: 0.028787806630134583, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 562,  Mean reward: 8.0, Mean Entropy: 0.131511852145195, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 563,  Mean reward: 7.5, Mean Entropy: 0.04145308956503868, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 564,  Mean reward: 8.0, Mean Entropy: 0.20002858340740204, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 565,  Mean reward: 3.63, Mean Entropy: 0.18500392138957977, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 566,  Mean reward: 7.974683544303797, Mean Entropy: 0.12720414996147156, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 567,  Mean reward: 2.52, Mean Entropy: 0.35952550172805786, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.49s
Iteration: 568,  Mean reward: 7.128787878787879, Mean Entropy: 0.26575323939323425, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.76s
Iteration: 569,  Mean reward: 7.2835820895522385, Mean Entropy: 0.32936975359916687, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.72s
Iteration: 570,  Mean reward: 7.384057971014493, Mean Entropy: 0.3063342571258545, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 571,  Mean reward: 7.7534246575342465, Mean Entropy: 0.11816229671239853, complete_episode_count: 73.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 572,  Mean reward: 7.981012658227848, Mean Entropy: 0.3063206672668457, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.70s
Iteration: 573,  Mean reward: 1.5277777777777777, Mean Entropy: 0.5661253333091736, complete_episode_count: 54.0, Gather time: 0.56s, Train time: 1.47s
Iteration: 574,  Mean reward: 4.950980392156863, Mean Entropy: 0.2568776309490204, complete_episode_count: 51.0, Gather time: 0.55s, Train time: 1.50s
Iteration: 575,  Mean reward: 7.981012658227848, Mean Entropy: 0.0016757349949330091, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 576,  Mean reward: 8.0, Mean Entropy: 0.1670684516429901, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 577,  Mean reward: 7.253623188405797, Mean Entropy: 0.11274544894695282, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 578,  Mean reward: 7.961538461538462, Mean Entropy: 0.009721679612994194, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.79s
Iteration: 579,  Mean reward: 8.0, Mean Entropy: 0.0076017617247998714, complete_episode_count: 80.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 580,  Mean reward: 8.0, Mean Entropy: 0.17454206943511963, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 581,  Mean reward: 1.8645833333333333, Mean Entropy: 0.2920585572719574, complete_episode_count: 48.0, Gather time: 0.56s, Train time: 1.50s
Iteration: 582,  Mean reward: 8.0, Mean Entropy: 0.22701847553253174, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.78s
Iteration: 583,  Mean reward: 6.653846153846154, Mean Entropy: 0.33738943934440613, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.80s
Iteration: 584,  Mean reward: 7.121212121212121, Mean Entropy: 0.34216660261154175, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 585,  Mean reward: 7.253623188405797, Mean Entropy: 0.29696130752563477, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 586,  Mean reward: 7.454545454545454, Mean Entropy: 0.3038446307182312, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.79s
Iteration: 587,  Mean reward: 7.507462686567164, Mean Entropy: 0.31731414794921875, complete_episode_count: 67.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 588,  Mean reward: 7.375, Mean Entropy: 0.09248002618551254, complete_episode_count: 68.0, Gather time: 0.60s, Train time: 0.75s
Iteration: 589,  Mean reward: 7.948717948717949, Mean Entropy: 0.32523050904273987, complete_episode_count: 78.0, Gather time: 0.69s, Train time: 0.76s
Iteration: 590,  Mean reward: 7.261904761904762, Mean Entropy: 0.1270638108253479, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.92s
Iteration: 591,  Mean reward: 7.9423076923076925, Mean Entropy: 0.017909135669469833, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 592,  Mean reward: 8.0, Mean Entropy: 0.20244906842708588, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 593,  Mean reward: 7.174242424242424, Mean Entropy: 0.35855358839035034, complete_episode_count: 66.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 594,  Mean reward: 7.635714285714286, Mean Entropy: 0.276029109954834, complete_episode_count: 70.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 595,  Mean reward: 7.608695652173913, Mean Entropy: 0.2660540044307709, complete_episode_count: 69.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 596,  Mean reward: 6.977611940298507, Mean Entropy: 0.28021055459976196, complete_episode_count: 67.0, Gather time: 0.57s, Train time: 0.73s
Iteration: 597,  Mean reward: 6.76984126984127, Mean Entropy: 0.35288071632385254, complete_episode_count: 63.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 598,  Mean reward: 7.0, Mean Entropy: 0.3580182194709778, complete_episode_count: 65.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 599,  Mean reward: 6.901515151515151, Mean Entropy: 0.03542499244213104, complete_episode_count: 66.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 600,  Mean reward: 8.0, Mean Entropy: 0.0021241323556751013, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.79s
rec seq len 2
actor lr 0.0005
Iteration: 601,  Mean reward: 8.0, Mean Entropy: 0.001830176799558103, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 602,  Mean reward: 8.0, Mean Entropy: 0.001808444387279451, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 603,  Mean reward: 8.0, Mean Entropy: 0.0016981740482151508, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.78s
Iteration: 604,  Mean reward: 8.0, Mean Entropy: 0.0015875024255365133, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 605,  Mean reward: 8.0, Mean Entropy: 0.001930272439494729, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 606,  Mean reward: 8.0, Mean Entropy: 0.002795268315821886, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 607,  Mean reward: 8.0, Mean Entropy: 0.007104109972715378, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 608,  Mean reward: 8.0, Mean Entropy: 0.04074397683143616, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 609,  Mean reward: 7.9423076923076925, Mean Entropy: 0.055112116038799286, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 610,  Mean reward: 7.961538461538462, Mean Entropy: 0.025983860716223717, complete_episode_count: 78.0, Gather time: 0.59s, Train time: 0.75s
Iteration: 611,  Mean reward: 8.0, Mean Entropy: 0.15332743525505066, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 612,  Mean reward: 7.382352941176471, Mean Entropy: 0.14188802242279053, complete_episode_count: 68.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 613,  Mean reward: 7.961538461538462, Mean Entropy: 0.06080257147550583, complete_episode_count: 78.0, Gather time: 0.60s, Train time: 0.76s
Iteration: 614,  Mean reward: 7.961538461538462, Mean Entropy: 0.06470207124948502, complete_episode_count: 78.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 615,  Mean reward: 7.88, Mean Entropy: 0.026436593383550644, complete_episode_count: 75.0, Gather time: 0.58s, Train time: 0.78s
Iteration: 616,  Mean reward: 8.0, Mean Entropy: 0.04401835426688194, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.81s
Iteration: 617,  Mean reward: 7.922077922077922, Mean Entropy: 0.009367583319544792, complete_episode_count: 77.0, Gather time: 0.58s, Train time: 0.79s
Iteration: 618,  Mean reward: 8.0, Mean Entropy: 0.002659653080627322, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 619,  Mean reward: 8.0, Mean Entropy: 0.0044255247339606285, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 620,  Mean reward: 8.0, Mean Entropy: 0.006871383637189865, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 621,  Mean reward: 8.0, Mean Entropy: 0.014294130727648735, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 622,  Mean reward: 7.981012658227848, Mean Entropy: 0.009220937266945839, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 623,  Mean reward: 8.0, Mean Entropy: 0.007301810663193464, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.73s
Iteration: 624,  Mean reward: 8.0, Mean Entropy: 0.01442091166973114, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 625,  Mean reward: 8.0, Mean Entropy: 0.10994386672973633, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 626,  Mean reward: 7.881578947368421, Mean Entropy: 0.04064670205116272, complete_episode_count: 76.0, Gather time: 0.57s, Train time: 0.74s
Iteration: 627,  Mean reward: 8.0, Mean Entropy: 0.12189090251922607, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.76s
Iteration: 628,  Mean reward: 7.901315789473684, Mean Entropy: 0.15168777108192444, complete_episode_count: 76.0, Gather time: 0.58s, Train time: 0.94s
Iteration: 629,  Mean reward: 7.608695652173913, Mean Entropy: 0.12675438821315765, complete_episode_count: 69.0, Gather time: 0.57s, Train time: 0.78s
Iteration: 630,  Mean reward: 7.382352941176471, Mean Entropy: 0.09262527525424957, complete_episode_count: 68.0, Gather time: 0.57s, Train time: 0.75s
Iteration: 631,  Mean reward: 8.0, Mean Entropy: 0.013496873900294304, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 632,  Mean reward: 7.981012658227848, Mean Entropy: 0.0037981695495545864, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Iteration: 633,  Mean reward: 8.0, Mean Entropy: 0.0035798929166048765, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 634,  Mean reward: 8.0, Mean Entropy: 0.0048074969090521336, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 635,  Mean reward: 8.0, Mean Entropy: 0.009056756272912025, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 636,  Mean reward: 7.981012658227848, Mean Entropy: 0.019984522834420204, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.74s
Iteration: 637,  Mean reward: 8.0, Mean Entropy: 0.14158277213573456, complete_episode_count: 80.0, Gather time: 0.60s, Train time: 0.73s
Iteration: 638,  Mean reward: 2.67, Mean Entropy: 0.21258829534053802, complete_episode_count: 50.0, Gather time: 0.55s, Train time: 1.55s
Iteration: 639,  Mean reward: 8.0, Mean Entropy: 0.019686125218868256, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.80s
Iteration: 640,  Mean reward: 7.962025316455696, Mean Entropy: 0.004933408927172422, complete_episode_count: 79.0, Gather time: 0.59s, Train time: 0.76s
Iteration: 641,  Mean reward: 8.0, Mean Entropy: 0.003688347525894642, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 642,  Mean reward: 8.0, Mean Entropy: 0.004556916654109955, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 643,  Mean reward: 8.0, Mean Entropy: 0.011294934898614883, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 644,  Mean reward: 8.0, Mean Entropy: 0.05878753587603569, complete_episode_count: 80.0, Gather time: 0.59s, Train time: 0.73s
Iteration: 645,  Mean reward: 7.901315789473684, Mean Entropy: 0.06222304329276085, complete_episode_count: 76.0, Gather time: 0.59s, Train time: 0.72s
Iteration: 646,  Mean reward: 7.981012658227848, Mean Entropy: 0.04245966672897339, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.72s
Iteration: 647,  Mean reward: 7.981012658227848, Mean Entropy: 0.024334009736776352, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 648,  Mean reward: 8.0, Mean Entropy: 0.07928640395402908, complete_episode_count: 80.0, Gather time: 0.58s, Train time: 0.74s
Iteration: 649,  Mean reward: 7.8175675675675675, Mean Entropy: 0.039480436593294144, complete_episode_count: 74.0, Gather time: 0.58s, Train time: 0.75s
Iteration: 650,  Mean reward: 7.981012658227848, Mean Entropy: 0.02732725813984871, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.77s
Iteration: 651,  Mean reward: 7.981012658227848, Mean Entropy: 0.047717608511447906, complete_episode_count: 79.0, Gather time: 0.58s, Train time: 0.81s
Policy has not yielded higher reward for 500 iterations...  Stopping now.
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
No database found.
Wrapping the env with a customized observation definition for GNN integration: flattened nfm-W-reachable_nodes-N-E
Wrapping the env with an action wrapper to redefine action inputs as node labels
env func executed...
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.44it/s]100%|██████████| 1/1 [00:00<00:00,  1.44it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.80it/s]100%|██████████| 1/1 [00:00<00:00,  1.80it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
HyperParameters(max_possible_nodes=8, max_possible_edges=14, emb_dim=24, critic='v', node_dim=7, lstm_on=True, hidden_size=24, recurrent_layers=1, batch_size=48, min_reward=-1000000.0, discount=0.99, gae_lambda=0.95, ppo_clip=0.2, ppo_epochs=10, scale_reward=1.0, max_grad_norm=0.5, entropy_factor=0.0, learning_rate=0.0005, recurrent_seq_len=2, parallel_rollouts=4, rollout_steps=40, patience=500, trainable_std_dev=False, init_log_std_dev=0.0, env_mask_velocity=False)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.80it/s]100%|██████████| 1/1 [00:00<00:00,  1.80it/s]

-------------------------------------------------------------------------------------------------------

Run 1: dataset entry 0, Initial state (3, 0), Deterministic policy: True
  s:             (3, 0) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 1)
  s:             (4, 1) a 5 action_probs [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0] r 9.0 s_ (5, 2)
  Done after 2 steps, Captured: False Reward: 8.0


Run 2: dataset entry 1, Initial state (3, 6), Deterministic policy: True
  s:             (3, 6) a 4 action_probs [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0] r -1.0 s_ (4, 7)
  s:             (4, 7) a 2 action_probs [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (2, 5)
  Done after 2 steps, Captured: False Reward: 8.0


Aggregated test results.
  > Environment : MemGraph_N=6_U=1_T=5_Ndir=F
  > Policy      : LSTM_GNN_PPO_Policy, Deterministic: True
Test set size: 2
Observed escape ratio: 1.000
   Captures:[0 0]
Goal reached: 2/2, solve ratio: 1.000
Average episode length: 2.00 +/- 0.00
   Lengths :[2 2]
Average return: 8.00 +/- 0.00
   Returns :[8.0 8.0]

Escape ratio at data generation: last 0.000, avg at generation 0.000, avg sampled 0.000
-------------------------------------------------------------------------------------------------------
Total unique graphs evaluated: -1
Total instances evaluated: 2 Avg reward: 8.00
Goal reached: 2 (100.0%)
---------------------------------------
train_on MemTask-U1
batch_size 48
obs_mask freq
obs_rate 0.2
emb_dim 24
lstm_type Dual
lstm_hdim 24
lstm_layers 1
emb_iterT 5
nfm_func NFM_ev_ec_t_dt_at_um_us
edge_blocking True
solve_select solvable
qnet gat2
critic v
train True
eval True
test False
num_seeds 3
seed0 0
seedrange range(0, 3)
demoruns False
rootdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1
logdir ./results/results_Phase3/ppo/MemTask-U1/gat2-v/emb24_itT5/lstm_Dual_24_1/NFM_ev_ec_t_dt_at_um_us/omask_freq0.2/bsize48
max_nodes 8
max_edges 14
Results over seeds for evaluation on trainset

num_graphs.........
  avg over seeds: -1.0
  std over seeds: 0.0
  per seed: [-1 -1 -1]

num_graph_instances
  avg over seeds: 2.0
  std over seeds: 0.0
  per seed: [2 2 2]

avg_return.........
  avg over seeds: 8.0
  std over seeds: 0.0
  per seed: [8.000 8.000 8.000]

success_rate.......
  avg over seeds: 1.0
  std over seeds: 0.0
  per seed: [1.000 1.000 1.000]

