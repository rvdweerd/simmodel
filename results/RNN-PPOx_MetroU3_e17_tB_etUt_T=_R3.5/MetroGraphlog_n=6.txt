
Run 1: dataset entry 0, Initial state (17, 14, 20, 25)
  s:   (17, 14, 20, 25) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (13, 7, 19, 24)
  s:    (13, 7, 19, 24) a 1 action_probs [0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (12, 6, 18, 23)
  s:    (12, 6, 18, 23) a 1 action_probs [0.0 1.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (11, 6, 18, 23)
  s:    (11, 6, 18, 23) a 1 action_probs [0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (10, 6, 18, 23)
  Done after 4 steps, Captured: False Reward: 6.0

Run 2: dataset entry 1000, Initial state (17, 20, 23, 23)
  s:   (17, 20, 23, 23) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (13, 19, 22, 22)
  s:   (13, 19, 22, 22) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (6, 8, 17, 22)
  s:     (6, 8, 17, 22) a 1 action_probs [0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (3, 7, 13, 22)
  Done after 3 steps, Captured: False Reward: 7.0

Run 3: dataset entry 2000, Initial state (17, 23, 23, 25)
  s:   (17, 23, 23, 25) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (13, 19, 22, 22)
  s:   (13, 19, 22, 22) a 0 action_probs [0.9 0.1 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (6, 8, 17, 22)
  s:     (6, 8, 17, 22) a 1 action_probs [0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (3, 7, 13, 22)
  Done after 3 steps, Captured: False Reward: 7.0

Run 4: dataset entry 3000, Initial state (17, 1, 6, 29)
  s:     (17, 1, 6, 29) a 0 action_probs [0.7 0.0 0.0 0.3 0.0 0.0 0.0] r -11.0 s_ (13, 5, 13, 28)
  Done after 1 steps, Captured: True Reward: -11.0

Run 5: dataset entry 4000, Initial state (17, 10, 19, 22)
  s:   (17, 10, 19, 22) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (13, 8, 11, 22)
  s:    (13, 8, 11, 22) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (6, 7, 12, 22)
  s:     (6, 7, 12, 22) a 1 action_probs [0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (3, 7, 12, 22)
  Done after 3 steps, Captured: False Reward: 7.0

Run 6: dataset entry 5000, Initial state (17, 4, 9, 18)
  s:     (17, 4, 9, 18) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (13, 8, 22, 32)
  s:    (13, 8, 22, 32) a 0 action_probs [1.0 0.0 0.0 0.0 0.0 0.0 0.0] r -1.0 s_ (6, 7, 7, 22)
  s:      (6, 7, 7, 22) a 1 action_probs [0.0 1.0 0.0 0.0 0.0 0.0 0.0] r 9.0 s_ (3, 6, 7, 22)
  Done after 3 steps, Captured: False Reward: 7.0

Aggregated test results:
  > Environment : MetroGraph_N=33_U=3_T=7_Ndir=F
  > Policy      : EpsGreedy, LSTM_PPO2
Test set size: 6 Observed escape ratio: 0.833, Average episode length: 2.83, Average return: 3.83
Escape ratio at data generation: last 0.240, avg at generation 0.243, avg sampled 0.252
Returns:[6.0, 7.0, 7.0, -11.0, 7.0, 7.0]
